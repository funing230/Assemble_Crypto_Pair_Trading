[1/200] Training loss: 0.08938121
[2/200] Training loss: 0.06544483
[3/200] Training loss: 0.06759191
[4/200] Training loss: 0.06503169
[5/200] Training loss: 0.06470994
[6/200] Training loss: 0.06690153
[7/200] Training loss: 0.06599988
[8/200] Training loss: 0.06508679
[9/200] Training loss: 0.06506504
[10/200] Training loss: 0.06615406
[50/200] Training loss: 0.06174110
[100/200] Training loss: 0.05746603
[150/200] Training loss: 0.05919360
[200/200] Training loss: 0.05695625
[1/200] Training loss: 0.08938121
[2/200] Training loss: 0.06544483
[3/200] Training loss: 0.06759191
[4/200] Training loss: 0.06503169
[5/200] Training loss: 0.06470994
[6/200] Training loss: 0.06690153
[7/200] Training loss: 0.06599988
[8/200] Training loss: 0.06508679
[9/200] Training loss: 0.06506504
[10/200] Training loss: 0.06615406
[50/200] Training loss: 0.06174110
[100/200] Training loss: 0.05746603
[150/200] Training loss: 0.05919360
[200/200] Training loss: 0.05695625
---batch_size---: 4 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 16 ---dropout---: 0.5207805082202462 ---num_layers---: 2
---loss---: L1Loss ---opt---: Rprop ---learning_rate---: 0.01836109604171406
----FITNESS-----------RMSE---- 34410.44678582363 ----------
[1/50] Training loss: 0.11681673
[2/50] Training loss: 0.00570203
[3/50] Training loss: 0.00415510
[4/50] Training loss: 0.00401648
[5/50] Training loss: 0.00342045
[6/50] Training loss: 0.00295104
[7/50] Training loss: 0.00246617
[8/50] Training loss: 0.00192334
[9/50] Training loss: 0.00132941
[10/50] Training loss: 0.00107742
[50/50] Training loss: 0.00035396
---batch_size---: 32 ---n_steps---: 2 ---network_epoch---: 50
---hidden_dim---: 48 ---dropout---: 0.5282970263056657 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adam ---learning_rate---: 0.006318730785749581
----FITNESS-----------RMSE---- 12955.749302915676 ----------
[1/200] Training loss: 0.11216379
[2/200] Training loss: 0.05817427
[3/200] Training loss: 0.04613206
[4/200] Training loss: 0.03260385
[5/200] Training loss: 0.02729558
[6/200] Training loss: 0.03226634
[7/200] Training loss: 0.02870104
[8/200] Training loss: 0.02545325
[9/200] Training loss: 0.02669973
[10/200] Training loss: 0.02615667
[50/200] Training loss: 0.01768204
[100/200] Training loss: 0.01960602
[150/200] Training loss: 0.01404594
[200/200] Training loss: 0.01683844
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.14553611508969566 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adam ---learning_rate---: 0.015912798713994737
----FITNESS-----------RMSE---- 83452.8772421898 ----------
[1/50] Training loss: 0.02007873
[2/50] Training loss: 0.00507118
[3/50] Training loss: 0.00510707
[4/50] Training loss: 0.00536528
[5/50] Training loss: 0.00521638
[6/50] Training loss: 0.00545258
[7/50] Training loss: 0.00491718
[8/50] Training loss: 0.00517010
[9/50] Training loss: 0.00507933
[10/50] Training loss: 0.00516211
[50/50] Training loss: 0.00495284
---batch_size---: 4 ---n_steps---: 1 ---network_epoch---: 50
---hidden_dim---: 16 ---dropout---: 0.2811459871200119 ---num_layers---: 2
---loss---: MSELoss ---opt---: Rprop ---learning_rate---: 0.013764422318448436
----FITNESS-----------RMSE---- 29805.43842992416 ----------
[1/50] Training loss: 0.07630797
[2/50] Training loss: 0.00583761
[3/50] Training loss: 0.00578111
[4/50] Training loss: 0.00574257
[5/50] Training loss: 0.00571690
[6/50] Training loss: 0.00567065
[7/50] Training loss: 0.00563028
[8/50] Training loss: 0.00555709
[9/50] Training loss: 0.00553105
[10/50] Training loss: 0.00542892
[50/50] Training loss: 0.00200489
---batch_size---: 8 ---n_steps---: 8 ---network_epoch---: 50
---hidden_dim---: 48 ---dropout---: 0.16194475143634365 ---num_layers---: 2
---loss---: SmoothL1Loss ---opt---: SGD ---learning_rate---: 0.023478851877472322
----FITNESS-----------RMSE---- 30642.306962759838 ----------
[1/100] Training loss: 0.06487110
[2/100] Training loss: 0.00452288
[3/100] Training loss: 0.00456186
[4/100] Training loss: 0.00442811
[5/100] Training loss: 0.00460370
[6/100] Training loss: 0.00454825
[7/100] Training loss: 0.00450268
[8/100] Training loss: 0.00452961
[9/100] Training loss: 0.00457210
[10/100] Training loss: 0.00447861
[50/100] Training loss: 0.00438000
[100/100] Training loss: 0.00418004
---batch_size---: 4 ---n_steps---: 2 ---network_epoch---: 100
---hidden_dim---: 16 ---dropout---: 0.1521854505758396 ---num_layers---: 2
---loss---: MSELoss ---opt---: Rprop ---learning_rate---: 0.011345846474114088
----FITNESS-----------RMSE---- 31338.164847355052 ----------
[1/200] Training loss: 0.05221222
[2/200] Training loss: 0.04436923
[3/200] Training loss: 0.04416254
[4/200] Training loss: 0.04396021
[5/200] Training loss: 0.04381847
[6/200] Training loss: 0.04370061
[7/200] Training loss: 0.04358544
[8/200] Training loss: 0.04350722
[9/200] Training loss: 0.04340094
[10/200] Training loss: 0.04330387
[50/200] Training loss: 0.04024839
[100/200] Training loss: 0.03637274
[150/200] Training loss: 0.03241649
[200/200] Training loss: 0.02847032
---batch_size---: 2 ---n_steps---: 8 ---network_epoch---: 200
---hidden_dim---: 32 ---dropout---: 0.7041723981129155 ---num_layers---: 1
---loss---: L1Loss ---opt---: Rprop ---learning_rate---: 0.022141207873188632
----FITNESS-----------RMSE---- 27418.944691581404 ----------
[1/100] Training loss: 0.10967726
[2/100] Training loss: 0.01276929
[3/100] Training loss: 0.01205325
[4/100] Training loss: 0.01240714
[5/100] Training loss: 0.01178280
[6/100] Training loss: 0.01236033
[7/100] Training loss: 0.01250341
[8/100] Training loss: 0.01140836
[9/100] Training loss: 0.01142265
[10/100] Training loss: 0.01109463
[50/100] Training loss: 0.00794733
[100/100] Training loss: 0.00650037
---batch_size---: 4 ---n_steps---: 8 ---network_epoch---: 100
---hidden_dim---: 16 ---dropout---: 0.6325495340318282 ---num_layers---: 2
---loss---: MSELoss ---opt---: Rprop ---learning_rate---: 0.01948916666930118
----FITNESS-----------RMSE---- 31143.024901251967 ----------
[1/100] Training loss: 0.03848060
[2/100] Training loss: 0.00774972
[3/100] Training loss: 0.00707784
[4/100] Training loss: 0.00611644
[5/100] Training loss: 0.00573378
[6/100] Training loss: 0.00524848
[7/100] Training loss: 0.00472116
[8/100] Training loss: 0.00471044
[9/100] Training loss: 0.00459270
[10/100] Training loss: 0.00447885
[1/200] Training loss: 0.08938121
[2/200] Training loss: 0.06544483
[3/200] Training loss: 0.06759191
[4/200] Training loss: 0.06503169
[5/200] Training loss: 0.06470994
[6/200] Training loss: 0.06690153
[7/200] Training loss: 0.06599988
[8/200] Training loss: 0.06508679
[9/200] Training loss: 0.06506504
[10/200] Training loss: 0.06615406
[50/200] Training loss: 0.06174110
[100/200] Training loss: 0.05746603
[150/200] Training loss: 0.05919360
[200/200] Training loss: 0.05695625
---batch_size---: 4 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 16 ---dropout---: 0.5207805082202462 ---num_layers---: 2
---loss---: L1Loss ---opt---: Rprop ---learning_rate---: 0.01836109604171406
----FITNESS-----------RMSE---- 34410.44678582363 ----------
[1/50] Training loss: 0.11681673
[2/50] Training loss: 0.00570203
[3/50] Training loss: 0.00415510
[4/50] Training loss: 0.00401648
[5/50] Training loss: 0.00342045
[6/50] Training loss: 0.00295104
[7/50] Training loss: 0.00246617
[8/50] Training loss: 0.00192334
[9/50] Training loss: 0.00132941
[10/50] Training loss: 0.00107742
[50/50] Training loss: 0.00035396
---batch_size---: 32 ---n_steps---: 2 ---network_epoch---: 50
---hidden_dim---: 48 ---dropout---: 0.5282970263056657 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adam ---learning_rate---: 0.006318730785749581
----FITNESS-----------RMSE---- 12955.749302915676 ----------
[1/200] Training loss: 0.11216379
[2/200] Training loss: 0.05817427
[3/200] Training loss: 0.04613206
[4/200] Training loss: 0.03260385
[5/200] Training loss: 0.02729558
[6/200] Training loss: 0.03226634
[7/200] Training loss: 0.02870104
[8/200] Training loss: 0.02545325
[9/200] Training loss: 0.02669973
[10/200] Training loss: 0.02615667
[50/200] Training loss: 0.01768204
[100/200] Training loss: 0.01960602
[150/200] Training loss: 0.01404594
[200/200] Training loss: 0.01683844
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.14553611508969566 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adam ---learning_rate---: 0.015912798713994737
----FITNESS-----------RMSE---- 83452.8772421898 ----------
[1/50] Training loss: 0.02007873
[2/50] Training loss: 0.00507118
[3/50] Training loss: 0.00510707
[4/50] Training loss: 0.00536528
[5/50] Training loss: 0.00521638
[6/50] Training loss: 0.00545258
[7/50] Training loss: 0.00491718
[8/50] Training loss: 0.00517010
[9/50] Training loss: 0.00507933
[10/50] Training loss: 0.00516211
[50/50] Training loss: 0.00495284
---batch_size---: 4 ---n_steps---: 1 ---network_epoch---: 50
---hidden_dim---: 16 ---dropout---: 0.2811459871200119 ---num_layers---: 2
---loss---: MSELoss ---opt---: Rprop ---learning_rate---: 0.013764422318448436
----FITNESS-----------RMSE---- 29805.43842992416 ----------
[1/50] Training loss: 0.07630797
[2/50] Training loss: 0.00583761
[3/50] Training loss: 0.00578111
[4/50] Training loss: 0.00574257
[5/50] Training loss: 0.00571690
[6/50] Training loss: 0.00567065
[7/50] Training loss: 0.00563028
[8/50] Training loss: 0.00555709
[9/50] Training loss: 0.00553105
[10/50] Training loss: 0.00542892
[50/50] Training loss: 0.00200489
---batch_size---: 8 ---n_steps---: 8 ---network_epoch---: 50
---hidden_dim---: 48 ---dropout---: 0.16194475143634365 ---num_layers---: 2
---loss---: SmoothL1Loss ---opt---: SGD ---learning_rate---: 0.023478851877472322
----FITNESS-----------RMSE---- 30642.306962759838 ----------
[1/100] Training loss: 0.06487110
[2/100] Training loss: 0.00452288
[3/100] Training loss: 0.00456186
[4/100] Training loss: 0.00442811
[5/100] Training loss: 0.00460370
[6/100] Training loss: 0.00454825
[7/100] Training loss: 0.00450268
[8/100] Training loss: 0.00452961
[9/100] Training loss: 0.00457210
[10/100] Training loss: 0.00447861
[50/100] Training loss: 0.00438000
[100/100] Training loss: 0.00418004
---batch_size---: 4 ---n_steps---: 2 ---network_epoch---: 100
---hidden_dim---: 16 ---dropout---: 0.1521854505758396 ---num_layers---: 2
---loss---: MSELoss ---opt---: Rprop ---learning_rate---: 0.011345846474114088
----FITNESS-----------RMSE---- 31338.164847355052 ----------
[1/200] Training loss: 0.05221222
[2/200] Training loss: 0.04436923
[3/200] Training loss: 0.04416254
[4/200] Training loss: 0.04396021
[5/200] Training loss: 0.04381847
[6/200] Training loss: 0.04370061
[7/200] Training loss: 0.04358544
[8/200] Training loss: 0.04350722
[9/200] Training loss: 0.04340094
[10/200] Training loss: 0.04330387
[50/200] Training loss: 0.04024839
[100/200] Training loss: 0.03637274
[150/200] Training loss: 0.03241649
[200/200] Training loss: 0.02847032
---batch_size---: 2 ---n_steps---: 8 ---network_epoch---: 200
---hidden_dim---: 32 ---dropout---: 0.7041723981129155 ---num_layers---: 1
---loss---: L1Loss ---opt---: Rprop ---learning_rate---: 0.022141207873188632
----FITNESS-----------RMSE---- 27418.944691581404 ----------
[1/100] Training loss: 0.10967726
[2/100] Training loss: 0.01276929
[3/100] Training loss: 0.01205325
[4/100] Training loss: 0.01240714
[5/100] Training loss: 0.01178280
[6/100] Training loss: 0.01236033
[7/100] Training loss: 0.01250341
[8/100] Training loss: 0.01140836
[9/100] Training loss: 0.01142265
[10/100] Training loss: 0.01109463
[50/100] Training loss: 0.00794733
[100/100] Training loss: 0.00650037
---batch_size---: 4 ---n_steps---: 8 ---network_epoch---: 100
---hidden_dim---: 16 ---dropout---: 0.6325495340318282 ---num_layers---: 2
---loss---: MSELoss ---opt---: Rprop ---learning_rate---: 0.01948916666930118
----FITNESS-----------RMSE---- 31143.024901251967 ----------
[1/100] Training loss: 0.03848060
[2/100] Training loss: 0.00774972
[3/100] Training loss: 0.00707784
[4/100] Training loss: 0.00611644
[5/100] Training loss: 0.00573378
[6/100] Training loss: 0.00524848
[7/100] Training loss: 0.00472116
[8/100] Training loss: 0.00471044
[9/100] Training loss: 0.00459270
[10/100] Training loss: 0.00447885
[50/100] Training loss: 0.00118371
[100/100] Training loss: 0.00073291
---batch_size---: 8 ---n_steps---: 1 ---network_epoch---: 100
---hidden_dim---: 32 ---dropout---: 0.7352965317482651 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.004128851382805829
----FITNESS-----------RMSE---- 18361.09974919803 ----------
[1/50] Training loss: 0.00806720
[2/50] Training loss: 0.00210245
[3/50] Training loss: 0.00116149
[4/50] Training loss: 0.00127177
[5/50] Training loss: 0.00188461
[6/50] Training loss: 0.00119544
[7/50] Training loss: 0.00106793
[8/50] Training loss: 0.00077571
[9/50] Training loss: 0.00080201
[10/50] Training loss: 0.00111964
[50/50] Training loss: 0.00044575
---batch_size---: 4 ---n_steps---: 8 ---network_epoch---: 50
---hidden_dim---: 64 ---dropout---: 0.6625704538293802 ---num_layers---: 2
---loss---: SmoothL1Loss ---opt---: Adam ---learning_rate---: 0.005675417330366128
----FITNESS-----------RMSE---- 27185.174783326296 ----------
[1/200] Training loss: 0.11950867
[2/200] Training loss: 0.01198818
[3/200] Training loss: 0.00705001
[4/200] Training loss: 0.00461205
[5/200] Training loss: 0.00442051
[6/200] Training loss: 0.00410614
[7/200] Training loss: 0.00380690
[8/200] Training loss: 0.00373389
[9/200] Training loss: 0.00303282
[10/200] Training loss: 0.00241535
[50/200] Training loss: 0.00046183
[100/200] Training loss: 0.00051056
[150/200] Training loss: 0.00021512
[200/200] Training loss: 0.00027671
---batch_size---: 32 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.1048664913718335 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adam ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 11507.381283332885 ----------
[1/100] Training loss: 0.03044868
[2/100] Training loss: 0.00594765
[3/100] Training loss: 0.00581330
[4/100] Training loss: 0.00597016
[5/100] Training loss: 0.00590503
[6/100] Training loss: 0.00600122
[7/100] Training loss: 0.00586470
[8/100] Training loss: 0.00588304
[9/100] Training loss: 0.00579271
[10/100] Training loss: 0.00567834
[50/100] Training loss: 0.00245556
[100/100] Training loss: 0.00168456
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 32 ---dropout---: 0.7737131064594779 ---num_layers---: 2
---loss---: SmoothL1Loss ---opt---: SGD ---learning_rate---: 0.0103728850286019
----FITNESS-----------RMSE---- 29721.97032499696 ----------
[1/50] Training loss: 0.62812279
[2/50] Training loss: 0.30618678
[3/50] Training loss: 0.17233255
[4/50] Training loss: 0.10106852
[5/50] Training loss: 0.08194287
[6/50] Training loss: 0.07128000
[7/50] Training loss: 0.06719840
[8/50] Training loss: 0.06248422
[9/50] Training loss: 0.05896276
[10/50] Training loss: 0.05804184
[50/50] Training loss: 0.03217845
---batch_size---: 4 ---n_steps---: 1 ---network_epoch---: 50
---hidden_dim---: 80 ---dropout---: 0.20142641046385618 ---num_layers---: 1
---loss---: L1Loss ---opt---: RMSprop ---learning_rate---: 0.01557769167363698
----FITNESS-----------RMSE---- 165799.9210615011 ----------
[1/200] Training loss: 0.06295885
[2/200] Training loss: 0.00420793
[3/200] Training loss: 0.00279876
[4/200] Training loss: 0.00172025
[5/200] Training loss: 0.00122206
[6/200] Training loss: 0.00133441
[7/200] Training loss: 0.00103887
[8/200] Training loss: 0.00104997
[9/200] Training loss: 0.00124350
[10/200] Training loss: 0.00166754
[50/200] Training loss: 0.00046546
[100/200] Training loss: 0.00036264
[150/200] Training loss: 0.00025005
[200/200] Training loss: 0.00034030
---batch_size---: 16 ---n_steps---: 2 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.16320283903808583 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adam ---learning_rate---: 0.022118274109743926
----FITNESS-----------RMSE---- 28516.091737824103 ----------
[1/150] Training loss: 0.07902679
[2/150] Training loss: 0.05885314
[3/150] Training loss: 0.05659540
[4/150] Training loss: 0.05595669
[5/150] Training loss: 0.05509028
[6/150] Training loss: 0.05410609
[7/150] Training loss: 0.05346189
[8/150] Training loss: 0.05292936
[9/150] Training loss: 0.05233849
[10/150] Training loss: 0.05178041
[50/150] Training loss: 0.04208560
[100/150] Training loss: 0.03526099
[150/150] Training loss: 0.02859176
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 150
---hidden_dim---: 64 ---dropout---: 0.5836564166717262 ---num_layers---: 1
---loss---: L1Loss ---opt---: Rprop ---learning_rate---: 0.0014810470389078284
----FITNESS-----------RMSE---- 30124.107024109446 ----------
[1/50] Training loss: 0.02762897
[2/50] Training loss: 0.01178439
[3/50] Training loss: 0.01172216
[4/50] Training loss: 0.01152892
[5/50] Training loss: 0.01150451
[6/50] Training loss: 0.01150076
[7/50] Training loss: 0.01140166
[8/50] Training loss: 0.01139130
[9/50] Training loss: 0.01127964
[10/50] Training loss: 0.01103308
[50/50] Training loss: 0.00357739
---batch_size---: 4 ---n_steps---: 2 ---network_epoch---: 50
---hidden_dim---: 80 ---dropout---: 0.47075540484920453 ---num_layers---: 2
---loss---: MSELoss ---opt---: SGD ---learning_rate---: 0.026442841248048448
----FITNESS-----------RMSE---- 28720.700827103785 ----------
[1/150] Training loss: 0.04118437
[2/150] Training loss: 0.00497771
[3/150] Training loss: 0.00329162
[4/150] Training loss: 0.00278734
[5/150] Training loss: 0.00266328
[6/150] Training loss: 0.00243731
[7/150] Training loss: 0.00231512
[8/150] Training loss: 0.00214103
[9/150] Training loss: 0.00199929
[10/150] Training loss: 0.00187831
[50/150] Training loss: 0.00033042
[100/150] Training loss: 0.00025700
[150/150] Training loss: 0.00018113
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 150
---hidden_dim---: 80 ---dropout---: 0.5494221523080015 ---num_layers---: 2
---loss---: SmoothL1Loss ---opt---: Adamax ---learning_rate---: 0.011127077663767165
----FITNESS-----------RMSE---- 16448.11867661466 ----------
[1/50] Training loss: 0.18763341
[2/50] Training loss: 0.06229086
[3/50] Training loss: 0.05868369
[4/50] Training loss: 0.05262058
[5/50] Training loss: 0.05194931
[6/50] Training loss: 0.04929439
[7/50] Training loss: 0.04702032
[8/50] Training loss: 0.04243393
[9/50] Training loss: 0.03986433
[10/50] Training loss: 0.04136491
[50/50] Training loss: 0.02209951
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 50
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 21060.078632331835 ----------
[1/150] Training loss: 2.01275339
[2/150] Training loss: 0.28383684
[3/150] Training loss: 0.19939736
[4/150] Training loss: 0.18975983
[5/150] Training loss: 0.13665245
[6/150] Training loss: 0.14283774
[7/150] Training loss: 0.20948759
[8/150] Training loss: 0.14145095
[9/150] Training loss: 0.06471268
[10/150] Training loss: 0.10835552
[50/150] Training loss: 0.06057116
[100/150] Training loss: 0.06946479
[150/150] Training loss: 0.08207048
---batch_size---: 4 ---n_steps---: 4 ---network_epoch---: 150
---hidden_dim---: 16 ---dropout---: 0.2856416573772078 ---num_layers---: 1
---loss---: MSELoss ---opt---: RMSprop ---learning_rate---: 0.02562747890433116
----FITNESS-----------RMSE---- 42215.62630116957 ----------
[1/150] Training loss: 0.12053918
[2/150] Training loss: 0.00498580
[3/150] Training loss: 0.00401982
[4/150] Training loss: 0.00288239
[5/150] Training loss: 0.00248881
[6/150] Training loss: 0.00222160
[7/150] Training loss: 0.00213140
[8/150] Training loss: 0.00211462
[9/150] Training loss: 0.00217422
[10/150] Training loss: 0.00224168
[50/150] Training loss: 0.00182053
[100/150] Training loss: 0.00177042
[150/150] Training loss: 0.00162311
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 150
---hidden_dim---: 16 ---dropout---: 0.29654065370859906 ---num_layers---: 2
---loss---: MSELoss ---opt---: Rprop ---learning_rate---: 0.015576475700050569
----FITNESS-----------RMSE---- 18566.973689861254 ----------
[1/200] Training loss: 0.21417210
[2/200] Training loss: 0.09514775
[3/200] Training loss: 0.04297543
[4/200] Training loss: 0.02108839
[5/200] Training loss: 0.01199812
[6/200] Training loss: 0.00819189
[7/200] Training loss: 0.00678625
[8/200] Training loss: 0.00630189
[9/200] Training loss: 0.00605151
[10/200] Training loss: 0.00588267
[50/200] Training loss: 0.00584156
[100/200] Training loss: 0.00600881
[150/200] Training loss: 0.00586624
[200/200] Training loss: 0.00578974
---batch_size---: 32 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.7745339839624771 ---num_layers---: 2
---loss---: SmoothL1Loss ---opt---: SGD ---learning_rate---: 0.011734602307407888
----FITNESS-----------RMSE---- 39419.71892340177 ----------
[1/50] Training loss: 0.00983214
[2/50] Training loss: 0.00217660
[3/50] Training loss: 0.00177648
[4/50] Training loss: 0.00117811
[5/50] Training loss: 0.00112140
[6/50] Training loss: 0.00093991
[7/50] Training loss: 0.00076861
[8/50] Training loss: 0.00091864
[9/50] Training loss: 0.00066623
[10/50] Training loss: 0.00063646
[50/50] Training loss: 0.00033613
---batch_size---: 8 ---n_steps---: 8 ---network_epoch---: 50
---hidden_dim---: 64 ---dropout---: 0.6625704538293802 ---num_layers---: 2
---loss---: SmoothL1Loss ---opt---: Adam ---learning_rate---: 0.005675417330366128
----FITNESS-----------RMSE---- 25588.7450258898 ----------
[1/50] Training loss: 0.03009727
[2/50] Training loss: 0.00607051
[3/50] Training loss: 0.00603476
[4/50] Training loss: 0.00606487
[5/50] Training loss: 0.00603651
[6/50] Training loss: 0.00598606
[7/50] Training loss: 0.00594806
[8/50] Training loss: 0.00586498
[9/50] Training loss: 0.00577442
[10/50] Training loss: 0.00572388
[50/50] Training loss: 0.00195797
---batch_size---: 4 ---n_steps---: 8 ---network_epoch---: 50
---hidden_dim---: 48 ---dropout---: 0.16194475143634365 ---num_layers---: 2
---loss---: SmoothL1Loss ---opt---: SGD ---learning_rate---: 0.023478851877472322
----FITNESS-----------RMSE---- 29716.52308060282 ----------
[1/50] Training loss: 0.01185102
[2/50] Training loss: 0.00230100
[3/50] Training loss: 0.00120065
[4/50] Training loss: 0.00136412
[5/50] Training loss: 0.00100624
[6/50] Training loss: 0.00132554
[7/50] Training loss: 0.00106565
[8/50] Training loss: 0.00091349
[9/50] Training loss: 0.00094646
[10/50] Training loss: 0.00107729
[50/50] Training loss: 0.00094730
---batch_size---: 4 ---n_steps---: 8 ---network_epoch---: 50
---hidden_dim---: 64 ---dropout---: 0.6625704538293802 ---num_layers---: 2
---loss---: SmoothL1Loss ---opt---: Adam ---learning_rate---: 0.0103728850286019
----FITNESS-----------RMSE---- 29440.7260780029 ----------
[1/100] Training loss: 0.05268470
[2/100] Training loss: 0.00618699
[3/100] Training loss: 0.00596617
[4/100] Training loss: 0.00605641
[5/100] Training loss: 0.00602739
[6/100] Training loss: 0.00598951
[7/100] Training loss: 0.00587765
[8/100] Training loss: 0.00604468
[9/100] Training loss: 0.00603261
[10/100] Training loss: 0.00595941
[50/100] Training loss: 0.00588854
[100/100] Training loss: 0.00491866
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 32 ---dropout---: 0.7737131064594779 ---num_layers---: 2
---loss---: SmoothL1Loss ---opt---: SGD ---learning_rate---: 0.005675417330366128
----FITNESS-----------RMSE---- 38397.19823112098 ----------
[1/200] Training loss: 0.02788390
[2/200] Training loss: 0.00797801
[3/200] Training loss: 0.00581401
[4/200] Training loss: 0.00261364
[5/200] Training loss: 0.00908103
[6/200] Training loss: 0.00939301
[7/200] Training loss: 0.00748449
[8/200] Training loss: 0.00776760
[9/200] Training loss: 0.00755402
[10/200] Training loss: 0.00619062
[50/200] Training loss: 0.00869595
[100/200] Training loss: 0.00837040
[150/200] Training loss: 0.00576495
[200/200] Training loss: 0.00730376
---batch_size---: 2 ---n_steps---: 2 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.7041723981129155 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adam ---learning_rate---: 0.022141207873188632
----FITNESS-----------RMSE---- 36597.30962789478 ----------
[1/200] Training loss: 0.18481652
[2/200] Training loss: 0.03555658
[3/200] Training loss: 0.03173720
[4/200] Training loss: 0.03128952
[5/200] Training loss: 0.03132613
[6/200] Training loss: 0.03097913
[7/200] Training loss: 0.03062544
[8/200] Training loss: 0.03039659
[9/200] Training loss: 0.03017836
[10/200] Training loss: 0.03000651
[50/200] Training loss: 0.02707298
[100/200] Training loss: 0.02605024
[150/200] Training loss: 0.02532394
[200/200] Training loss: 0.02424743
---batch_size---: 16 ---n_steps---: 8 ---network_epoch---: 200
---hidden_dim---: 32 ---dropout---: 0.16320283903808583 ---num_layers---: 1
---loss---: L1Loss ---opt---: Rprop ---learning_rate---: 0.022118274109743926
----FITNESS-----------RMSE---- 22159.96389888756 ----------
[1/50] Training loss: 0.16114171
[2/50] Training loss: 0.06545399
[3/50] Training loss: 0.05309670
[4/50] Training loss: 0.04772352
[5/50] Training loss: 0.04179818
[6/50] Training loss: 0.03917466
[7/50] Training loss: 0.03710166
[8/50] Training loss: 0.04033291
[9/50] Training loss: 0.03552840
[10/50] Training loss: 0.02804912
[50/50] Training loss: 0.01674892
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 50
---hidden_dim---: 80 ---dropout---: 0.1048664913718335 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adam ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 8209.431649024187 ----------
[1/200] Training loss: 0.18216910
[2/200] Training loss: 0.00955714
[3/200] Training loss: 0.00562478
[4/200] Training loss: 0.00469197
[5/200] Training loss: 0.00519070
[6/200] Training loss: 0.00407442
[7/200] Training loss: 0.00414622
[8/200] Training loss: 0.00354387
[9/200] Training loss: 0.00344239
[10/200] Training loss: 0.00382982
[50/200] Training loss: 0.00088339
[100/200] Training loss: 0.00065738
[150/200] Training loss: 0.00069150
[200/200] Training loss: 0.00040966
---batch_size---: 32 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13531.459640408348 ----------
[1/50] Training loss: 0.12973562
[2/50] Training loss: 0.01332556
[3/50] Training loss: 0.01354043
[4/50] Training loss: 0.01204312
[5/50] Training loss: 0.01193111
[6/50] Training loss: 0.01192578
[7/50] Training loss: 0.01175208
[8/50] Training loss: 0.01175982
[9/50] Training loss: 0.01180262
[10/50] Training loss: 0.01192437
[50/50] Training loss: 0.00996750
---batch_size---: 32 ---n_steps---: 8 ---network_epoch---: 50
---hidden_dim---: 80 ---dropout---: 0.16194475143634365 ---num_layers---: 2
---loss---: MSELoss ---opt---: SGD ---learning_rate---: 0.023478851877472322
----FITNESS-----------RMSE---- 38710.04665458309 ----------
[1/200] Training loss: 0.02138754
[2/200] Training loss: 0.00261143
[3/200] Training loss: 0.00210214
[4/200] Training loss: 0.00153600
[5/200] Training loss: 0.00108549
[6/200] Training loss: 0.00076873
[7/200] Training loss: 0.00065396
[8/200] Training loss: 0.00062785
[9/200] Training loss: 0.00056031
[10/200] Training loss: 0.00048231
[50/200] Training loss: 0.00022943
[100/200] Training loss: 0.00012455
[150/200] Training loss: 0.00011005
[200/200] Training loss: 0.00011127
---batch_size---: 8 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.1048664913718335 ---num_layers---: 2
---loss---: SmoothL1Loss ---opt---: Adam ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 26132.727067797576 ----------
[1/150] Training loss: 0.23542488
[2/150] Training loss: 0.00606851
[3/150] Training loss: 0.00604145
[4/150] Training loss: 0.00544070
[5/150] Training loss: 0.00476801
[6/150] Training loss: 0.00445331
[7/150] Training loss: 0.00398502
[8/150] Training loss: 0.00328813
[9/150] Training loss: 0.00327776
[10/150] Training loss: 0.00295583
[50/150] Training loss: 0.00209140
[100/150] Training loss: 0.00198730
[150/150] Training loss: 0.00193163
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 150
---hidden_dim---: 16 ---dropout---: 0.29654065370859906 ---num_layers---: 2
---loss---: MSELoss ---opt---: Rprop ---learning_rate---: 0.004128851382805829
----FITNESS-----------RMSE---- 28255.80860637331 ----------
[1/100] Training loss: 0.03225097
[2/100] Training loss: 0.00556339
[3/100] Training loss: 0.00511090
[4/100] Training loss: 0.00439362
[5/100] Training loss: 0.00412711
[6/100] Training loss: 0.00377415
[7/100] Training loss: 0.00325596
[8/100] Training loss: 0.00282191
[9/100] Training loss: 0.00235419
[10/100] Training loss: 0.00215287
[50/100] Training loss: 0.00063024
[100/100] Training loss: 0.00034898
---batch_size---: 8 ---n_steps---: 1 ---network_epoch---: 100
---hidden_dim---: 32 ---dropout---: 0.7352965317482651 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.015576475700050569
----FITNESS-----------RMSE---- 18209.777593369996 ----------
[1/200] Training loss: 0.03516534
[2/200] Training loss: 0.00441814
[3/200] Training loss: 0.00274742
[4/200] Training loss: 0.00186516
[5/200] Training loss: 0.00151357
[6/200] Training loss: 0.00168070
[7/200] Training loss: 0.00116221
[8/200] Training loss: 0.00139393
[9/200] Training loss: 0.00151978
[10/200] Training loss: 0.00114723
[50/200] Training loss: 0.00033005
[100/200] Training loss: 0.00044060
[150/200] Training loss: 0.00024922
[200/200] Training loss: 0.00023670
---batch_size---: 16 ---n_steps---: 2 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.16320283903808583 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adam ---learning_rate---: 0.011734602307407888
----FITNESS-----------RMSE---- 17830.978885075267 ----------
[1/200] Training loss: 0.19521124
[2/200] Training loss: 0.04236444
[3/200] Training loss: 0.01213551
[4/200] Training loss: 0.00692998
[5/200] Training loss: 0.00587895
[6/200] Training loss: 0.00571633
[7/200] Training loss: 0.00571052
[8/200] Training loss: 0.00576163
[9/200] Training loss: 0.00573539
[10/200] Training loss: 0.00599589
[50/200] Training loss: 0.00574452
[100/200] Training loss: 0.00585762
[150/200] Training loss: 0.00573424
[200/200] Training loss: 0.00556480
---batch_size---: 32 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.7745339839624771 ---num_layers---: 2
---loss---: SmoothL1Loss ---opt---: SGD ---learning_rate---: 0.022118274109743926
----FITNESS-----------RMSE---- 39243.21699351367 ----------
[1/50] Training loss: 0.51260117
[2/50] Training loss: 0.05523298
[3/50] Training loss: 0.05420309
[4/50] Training loss: 0.05463366
[5/50] Training loss: 0.05437802
[6/50] Training loss: 0.05347794
[7/50] Training loss: 0.05108621
[8/50] Training loss: 0.04762019
[9/50] Training loss: 0.04741671
[10/50] Training loss: 0.04368178
[50/50] Training loss: 0.03871542
---batch_size---: 32 ---n_steps---: 1 ---network_epoch---: 50
---hidden_dim---: 48 ---dropout---: 0.29654065370859906 ---num_layers---: 2
---loss---: L1Loss ---opt---: Rprop ---learning_rate---: 0.015576475700050569
----FITNESS-----------RMSE---- 35022.11781146309 ----------
[1/150] Training loss: 0.09523369
[2/150] Training loss: 0.00477408
[3/150] Training loss: 0.00476154
[4/150] Training loss: 0.00402172
[5/150] Training loss: 0.00401008
[6/150] Training loss: 0.00346306
[7/150] Training loss: 0.00327002
[8/150] Training loss: 0.00292739
[9/150] Training loss: 0.00263489
[10/150] Training loss: 0.00221080
[50/150] Training loss: 0.00062467
[100/150] Training loss: 0.00051971
[150/150] Training loss: 0.00042835
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 150
---hidden_dim---: 16 ---dropout---: 0.5563728816518204 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 22586.46745287983 ----------
[1/50] Training loss: 0.16522934
[2/50] Training loss: 0.01290681
[3/50] Training loss: 0.01153891
[4/50] Training loss: 0.01189282
[5/50] Training loss: 0.01143625
[6/50] Training loss: 0.01177548
[7/50] Training loss: 0.01162063
[8/50] Training loss: 0.01159583
[9/50] Training loss: 0.01175829
[10/50] Training loss: 0.01155664
[50/50] Training loss: 0.01148170
---batch_size---: 32 ---n_steps---: 2 ---network_epoch---: 50
---hidden_dim---: 80 ---dropout---: 0.5494221523080015 ---num_layers---: 2
---loss---: MSELoss ---opt---: SGD ---learning_rate---: 0.026442841248048448
----FITNESS-----------RMSE---- 39430.973409237566 ----------
[1/150] Training loss: 0.01072938
[2/150] Training loss: 0.00254376
[3/150] Training loss: 0.00200812
[4/150] Training loss: 0.00169294
[5/150] Training loss: 0.00109203
[6/150] Training loss: 0.00083539
[7/150] Training loss: 0.00084182
[8/150] Training loss: 0.00074644
[9/150] Training loss: 0.00080429
[10/150] Training loss: 0.00071907
[50/150] Training loss: 0.00028017
[100/150] Training loss: 0.00016367
[150/150] Training loss: 0.00011824
---batch_size---: 4 ---n_steps---: 4 ---network_epoch---: 150
---hidden_dim---: 80 ---dropout---: 0.47075540484920453 ---num_layers---: 2
---loss---: SmoothL1Loss ---opt---: Adamax ---learning_rate---: 0.011127077663767165
----FITNESS-----------RMSE---- 31295.235164478316 ----------
[1/100] Training loss: 0.04809418
[2/100] Training loss: 0.01130982
[3/100] Training loss: 0.01115403
[4/100] Training loss: 0.01096488
[5/100] Training loss: 0.01073701
[6/100] Training loss: 0.01056201
[7/100] Training loss: 0.00989892
[8/100] Training loss: 0.00942821
[9/100] Training loss: 0.00893678
[10/100] Training loss: 0.00820839
[50/100] Training loss: 0.00326952
[100/100] Training loss: 0.00148961
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 32 ---dropout---: 0.29654065370859906 ---num_layers---: 2
---loss---: MSELoss ---opt---: SGD ---learning_rate---: 0.0103728850286019
----FITNESS-----------RMSE---- 23182.731504289997 ----------
[1/150] Training loss: 0.11643009
[2/150] Training loss: 0.00351296
[3/150] Training loss: 0.00293395
[4/150] Training loss: 0.00263940
[5/150] Training loss: 0.00252764
[6/150] Training loss: 0.00200567
[7/150] Training loss: 0.00197953
[8/150] Training loss: 0.00194756
[9/150] Training loss: 0.00205369
[10/150] Training loss: 0.00207820
[50/150] Training loss: 0.00177097
[100/150] Training loss: 0.00154862
[150/150] Training loss: 0.00158477
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 150
---hidden_dim---: 16 ---dropout---: 0.7737131064594779 ---num_layers---: 2
---loss---: SmoothL1Loss ---opt---: Rprop ---learning_rate---: 0.015576475700050569
----FITNESS-----------RMSE---- 33978.59844078328 ----------
[1/150] Training loss: 0.14130310
[2/150] Training loss: 0.06207359
[3/150] Training loss: 0.05516215
[4/150] Training loss: 0.05409752
[5/150] Training loss: 0.05362678
[6/150] Training loss: 0.04846144
[7/150] Training loss: 0.04619002
[8/150] Training loss: 0.03885433
[9/150] Training loss: 0.03511717
[10/150] Training loss: 0.03742001
[50/150] Training loss: 0.02164556
[100/150] Training loss: 0.01760334
[150/150] Training loss: 0.01135099
---batch_size---: 32 ---n_steps---: 1 ---network_epoch---: 150
---hidden_dim---: 80 ---dropout---: 0.5494221523080015 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adam ---learning_rate---: 0.011127077663767165
----FITNESS-----------RMSE---- 27317.640893752156 ----------
[1/50] Training loss: 0.03274029
[2/50] Training loss: 0.00254734
[3/50] Training loss: 0.00201433
[4/50] Training loss: 0.00168669
[5/50] Training loss: 0.00137440
[6/50] Training loss: 0.00108670
[7/50] Training loss: 0.00081896
[8/50] Training loss: 0.00066236
[9/50] Training loss: 0.00063267
[10/50] Training loss: 0.00050251
[50/50] Training loss: 0.00020116
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 50
---hidden_dim---: 80 ---dropout---: 0.1048664913718335 ---num_layers---: 2
---loss---: SmoothL1Loss ---opt---: Adamax ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 13934.66641150767 ----------
[1/100] Training loss: 0.10497435
[2/100] Training loss: 0.05183053
[3/100] Training loss: 0.05663977
[4/100] Training loss: 0.04855095
[5/100] Training loss: 0.04749767
[6/100] Training loss: 0.04732931
[7/100] Training loss: 0.03662822
[8/100] Training loss: 0.03470791
[9/100] Training loss: 0.03761202
[10/100] Training loss: 0.02811638
[50/100] Training loss: 0.01805531
[100/100] Training loss: 0.01331141
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.1048664913718335 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adam ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 7505.591515663506 ----------
[1/50] Training loss: 0.04161766
[2/50] Training loss: 0.00628250
[3/50] Training loss: 0.00548966
[4/50] Training loss: 0.00508314
[5/50] Training loss: 0.00455503
[6/50] Training loss: 0.00426094
[7/50] Training loss: 0.00393255
[8/50] Training loss: 0.00382633
[9/50] Training loss: 0.00355907
[10/50] Training loss: 0.00314360
[50/50] Training loss: 0.00069921
---batch_size---: 8 ---n_steps---: 1 ---network_epoch---: 50
---hidden_dim---: 32 ---dropout---: 0.7352965317482651 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.015576475700050569
----FITNESS-----------RMSE---- 8726.31193574926 ----------
[1/200] Training loss: 0.13327623
[2/200] Training loss: 0.00572631
[3/200] Training loss: 0.00473294
[4/200] Training loss: 0.00458969
[5/200] Training loss: 0.00389583
[6/200] Training loss: 0.00384646
[7/200] Training loss: 0.00372619
[8/200] Training loss: 0.00323125
[9/200] Training loss: 0.00273844
[10/200] Training loss: 0.00238487
[50/200] Training loss: 0.00076343
[100/200] Training loss: 0.00055073
[150/200] Training loss: 0.00039065
[200/200] Training loss: 0.00035357
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15965.855066359583 ----------
[1/200] Training loss: 0.17621835
[2/200] Training loss: 0.01010550
[3/200] Training loss: 0.00497839
[4/200] Training loss: 0.00478038
[5/200] Training loss: 0.00409800
[6/200] Training loss: 0.00313645
[7/200] Training loss: 0.00236587
[8/200] Training loss: 0.00168014
[9/200] Training loss: 0.00122833
[10/200] Training loss: 0.00106782
[50/200] Training loss: 0.00037824
[100/200] Training loss: 0.00027064
[150/200] Training loss: 0.00026957
[200/200] Training loss: 0.00020015
---batch_size---: 32 ---n_steps---: 2 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.16320283903808583 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adam ---learning_rate---: 0.011734602307407888
----FITNESS-----------RMSE---- 21778.747071399677 ----------
[1/50] Training loss: 0.00483802
[2/50] Training loss: 0.00162446
[3/50] Training loss: 0.00110817
[4/50] Training loss: 0.00090681
[5/50] Training loss: 0.00070706
[6/50] Training loss: 0.00079566
[7/50] Training loss: 0.00069570
[8/50] Training loss: 0.00091715
[9/50] Training loss: 0.00058695
[10/50] Training loss: 0.00061901
[50/50] Training loss: 0.00095479
---batch_size---: 4 ---n_steps---: 8 ---network_epoch---: 50
---hidden_dim---: 48 ---dropout---: 0.16194475143634365 ---num_layers---: 2
---loss---: SmoothL1Loss ---opt---: Adam ---learning_rate---: 0.023478851877472322
----FITNESS-----------RMSE---- 29063.622348220808 ----------
[1/50] Training loss: 0.54172761
[2/50] Training loss: 0.24633675
[3/50] Training loss: 0.08605001
[4/50] Training loss: 0.08053713
[5/50] Training loss: 0.08012268
[6/50] Training loss: 0.07990022
[7/50] Training loss: 0.08037945
[8/50] Training loss: 0.08035038
[9/50] Training loss: 0.08014307
[10/50] Training loss: 0.08027327
[50/50] Training loss: 0.07943441
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 50
---hidden_dim---: 80 ---dropout---: 0.1048664913718335 ---num_layers---: 2
---loss---: L1Loss ---opt---: SGD ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 39479.69280528915 ----------
[1/200] Training loss: 0.14862855
[2/200] Training loss: 0.00569550
[3/200] Training loss: 0.00516853
[4/200] Training loss: 0.00470279
[5/200] Training loss: 0.00456859
[6/200] Training loss: 0.00414079
[7/200] Training loss: 0.00416060
[8/200] Training loss: 0.00392580
[9/200] Training loss: 0.00350016
[10/200] Training loss: 0.00370893
[50/200] Training loss: 0.00100417
[100/200] Training loss: 0.00070414
[150/200] Training loss: 0.00049737
[200/200] Training loss: 0.00043044
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 16 ---dropout---: 0.29654065370859906 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14489.552650099313 ----------
[1/150] Training loss: 0.60107679
[2/150] Training loss: 0.00591917
[3/150] Training loss: 0.00529397
[4/150] Training loss: 0.00486997
[5/150] Training loss: 0.00422114
[6/150] Training loss: 0.00358983
[7/150] Training loss: 0.00330776
[8/150] Training loss: 0.00320843
[9/150] Training loss: 0.00283671
[10/150] Training loss: 0.00279171
[50/150] Training loss: 0.00202806
[100/150] Training loss: 0.00206285
[150/150] Training loss: 0.00191540
---batch_size---: 32 ---n_steps---: 1 ---network_epoch---: 150
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 2
---loss---: MSELoss ---opt---: Rprop ---learning_rate---: 0.015576475700050569
----FITNESS-----------RMSE---- 23484.377104790325 ----------
[1/50] Training loss: 0.10094387
[2/50] Training loss: 0.06427894
[3/50] Training loss: 0.06377409
[4/50] Training loss: 0.06460973
[5/50] Training loss: 0.06183730
[6/50] Training loss: 0.06219339
[7/50] Training loss: 0.05998264
[8/50] Training loss: 0.06672582
[9/50] Training loss: 0.06516519
[10/50] Training loss: 0.06431553
[50/50] Training loss: 0.05629141
---batch_size---: 4 ---n_steps---: 8 ---network_epoch---: 50
---hidden_dim---: 32 ---dropout---: 0.16320283903808583 ---num_layers---: 2
---loss---: L1Loss ---opt---: Rprop ---learning_rate---: 0.022118274109743926
----FITNESS-----------RMSE---- 32813.60303288866 ----------
[1/200] Training loss: 0.02414877
[2/200] Training loss: 0.00187672
[3/200] Training loss: 0.00137985
[4/200] Training loss: 0.00088775
[5/200] Training loss: 0.00064120
[6/200] Training loss: 0.00048811
[7/200] Training loss: 0.00044350
[8/200] Training loss: 0.00038850
[9/200] Training loss: 0.00038254
[10/200] Training loss: 0.00041090
[50/200] Training loss: 0.00021637
[100/200] Training loss: 0.00013594
[150/200] Training loss: 0.00009678
[200/200] Training loss: 0.00005344
---batch_size---: 16 ---n_steps---: 8 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adam ---learning_rate---: 0.005675417330366128
----FITNESS-----------RMSE---- 15605.01714193227 ----------
[1/50] Training loss: 0.18381104
[2/50] Training loss: 0.05681562
[3/50] Training loss: 0.04602715
[4/50] Training loss: 0.03454559
[5/50] Training loss: 0.03719495
[6/50] Training loss: 0.04022958
[7/50] Training loss: 0.03258283
[8/50] Training loss: 0.03634783
[9/50] Training loss: 0.02827363
[10/50] Training loss: 0.03395166
[50/50] Training loss: 0.01962310
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 50
---hidden_dim---: 16 ---dropout---: 0.1048664913718335 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adam ---learning_rate---: 0.015576475700050569
----FITNESS-----------RMSE---- 27709.915625999296 ----------
[1/150] Training loss: 0.10594636
[2/150] Training loss: 0.00571501
[3/150] Training loss: 0.00493317
[4/150] Training loss: 0.00520437
[5/150] Training loss: 0.00474003
[6/150] Training loss: 0.00513368
[7/150] Training loss: 0.00457543
[8/150] Training loss: 0.00475735
[9/150] Training loss: 0.00460160
[10/150] Training loss: 0.00438590
[50/150] Training loss: 0.00327066
[100/150] Training loss: 0.00322613
[150/150] Training loss: 0.00311813
---batch_size---: 32 ---n_steps---: 1 ---network_epoch---: 150
---hidden_dim---: 80 ---dropout---: 0.29654065370859906 ---num_layers---: 2
---loss---: MSELoss ---opt---: Rprop ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 24288.088273884383 ----------
[1/200] Training loss: 0.06057977
[2/200] Training loss: 0.00481318
[3/200] Training loss: 0.00424373
[4/200] Training loss: 0.00370647
[5/200] Training loss: 0.00295331
[6/200] Training loss: 0.00314281
[7/200] Training loss: 0.00239478
[8/200] Training loss: 0.00202814
[9/200] Training loss: 0.00171889
[10/200] Training loss: 0.00160408
[50/200] Training loss: 0.00070693
[100/200] Training loss: 0.00035682
[150/200] Training loss: 0.00026810
[200/200] Training loss: 0.00023397
---batch_size---: 8 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 32 ---dropout---: 0.5563728816518204 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17334.745686049162 ----------
[1/100] Training loss: 0.06548742
[2/100] Training loss: 0.00662632
[3/100] Training loss: 0.00567163
[4/100] Training loss: 0.00556285
[5/100] Training loss: 0.00528395
[6/100] Training loss: 0.00512186
[7/100] Training loss: 0.00453166
[8/100] Training loss: 0.00455953
[9/100] Training loss: 0.00418259
[10/100] Training loss: 0.00420395
[50/100] Training loss: 0.00079761
[100/100] Training loss: 0.00051629
---batch_size---: 32 ---n_steps---: 1 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.7352965317482651 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.015576475700050569
----FITNESS-----------RMSE---- 21362.857486768946 ----------
[1/200] Training loss: 0.02137444
[2/200] Training loss: 0.00233694
[3/200] Training loss: 0.00136902
[4/200] Training loss: 0.00080705
[5/200] Training loss: 0.00084694
[6/200] Training loss: 0.00074614
[7/200] Training loss: 0.00077200
[8/200] Training loss: 0.00069951
[9/200] Training loss: 0.00053309
[10/200] Training loss: 0.00060665
[50/200] Training loss: 0.00023654
[100/200] Training loss: 0.00018350
[150/200] Training loss: 0.00020956
[200/200] Training loss: 0.00012608
---batch_size---: 8 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 16 ---dropout---: 0.1048664913718335 ---num_layers---: 2
---loss---: SmoothL1Loss ---opt---: Adam ---learning_rate---: 0.015576475700050569
----FITNESS-----------RMSE---- 28631.13605849408 ----------
[1/150] Training loss: 0.21135677
[2/150] Training loss: 0.00517358
[3/150] Training loss: 0.00504767
[4/150] Training loss: 0.00422194
[5/150] Training loss: 0.00389209
[6/150] Training loss: 0.00378842
[7/150] Training loss: 0.00377402
[8/150] Training loss: 0.00369815
[9/150] Training loss: 0.00398694
[10/150] Training loss: 0.00384565
[50/150] Training loss: 0.00304547
[100/150] Training loss: 0.00248268
[150/150] Training loss: 0.00256782
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 150
---hidden_dim---: 48 ---dropout---: 0.29654065370859906 ---num_layers---: 2
---loss---: MSELoss ---opt---: Rprop ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 26664.50044534868 ----------
[1/200] Training loss: 0.03739863
[2/200] Training loss: 0.00381329
[3/200] Training loss: 0.00212988
[4/200] Training loss: 0.00211472
[5/200] Training loss: 0.00147766
[6/200] Training loss: 0.00168155
[7/200] Training loss: 0.00156260
[8/200] Training loss: 0.00117385
[9/200] Training loss: 0.00111607
[10/200] Training loss: 0.00102195
[50/200] Training loss: 0.00032232
[100/200] Training loss: 0.00029060
[150/200] Training loss: 0.00036525
[200/200] Training loss: 0.00020854
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.1048664913718335 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adam ---learning_rate---: 0.022118274109743926
----FITNESS-----------RMSE---- 32709.13462627833 ----------
[1/50] Training loss: 0.14733584
[2/50] Training loss: 0.05080176
[3/50] Training loss: 0.04466652
[4/50] Training loss: 0.04271193
[5/50] Training loss: 0.04076742
[6/50] Training loss: 0.03113267
[7/50] Training loss: 0.03112829
[8/50] Training loss: 0.03155428
[9/50] Training loss: 0.03115615
[10/50] Training loss: 0.02979277
[50/50] Training loss: 0.01671452
---batch_size---: 16 ---n_steps---: 2 ---network_epoch---: 50
---hidden_dim---: 64 ---dropout---: 0.16320283903808583 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adam ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 13566.325958047742 ----------
[1/200] Training loss: 0.01911584
[2/200] Training loss: 0.00180159
[3/200] Training loss: 0.00118841
[4/200] Training loss: 0.00060240
[5/200] Training loss: 0.00046943
[6/200] Training loss: 0.00046893
[7/200] Training loss: 0.00040277
[8/200] Training loss: 0.00037578
[9/200] Training loss: 0.00040130
[10/200] Training loss: 0.00041299
[50/200] Training loss: 0.00024633
[100/200] Training loss: 0.00019413
[150/200] Training loss: 0.00014938
[200/200] Training loss: 0.00006492
---batch_size---: 16 ---n_steps---: 8 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adam ---learning_rate---: 0.005675417330366128
----FITNESS-----------RMSE---- 28957.93141783439 ----------
[1/150] Training loss: 0.17499190
[2/150] Training loss: 0.00529509
[3/150] Training loss: 0.00412127
[4/150] Training loss: 0.00348730
[5/150] Training loss: 0.00324423
[6/150] Training loss: 0.00314762
[7/150] Training loss: 0.00324725
[8/150] Training loss: 0.00309657
[9/150] Training loss: 0.00300815
[10/150] Training loss: 0.00298489
[50/150] Training loss: 0.00228611
[100/150] Training loss: 0.00200823
[150/150] Training loss: 0.00187417
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 150
---hidden_dim---: 48 ---dropout---: 0.29654065370859906 ---num_layers---: 2
---loss---: MSELoss ---opt---: Rprop ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 23437.905708488546 ----------
[1/200] Training loss: 0.04119407
[2/200] Training loss: 0.00472360
[3/200] Training loss: 0.00366250
[4/200] Training loss: 0.00255735
[5/200] Training loss: 0.00216770
[6/200] Training loss: 0.00160540
[7/200] Training loss: 0.00118089
[8/200] Training loss: 0.00165219
[9/200] Training loss: 0.00134288
[10/200] Training loss: 0.00135057
[50/200] Training loss: 0.00052565
[100/200] Training loss: 0.00032498
[150/200] Training loss: 0.00029701
[200/200] Training loss: 0.00023910
---batch_size---: 8 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.5563728816518204 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adam ---learning_rate---: 0.005675417330366128
----FITNESS-----------RMSE---- 28661.41880647223 ----------
[1/200] Training loss: 0.02049162
[2/200] Training loss: 0.00238802
[3/200] Training loss: 0.00208935
[4/200] Training loss: 0.00198252
[5/200] Training loss: 0.00171631
[6/200] Training loss: 0.00149702
[7/200] Training loss: 0.00136522
[8/200] Training loss: 0.00115297
[9/200] Training loss: 0.00111432
[10/200] Training loss: 0.00097107
[50/200] Training loss: 0.00031231
[100/200] Training loss: 0.00021896
[150/200] Training loss: 0.00016197
[200/200] Training loss: 0.00014681
---batch_size---: 16 ---n_steps---: 8 ---network_epoch---: 200
---hidden_dim---: 32 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16152.018325893516 ----------
[1/200] Training loss: 0.05102186
[2/200] Training loss: 0.01181295
[3/200] Training loss: 0.01172803
[4/200] Training loss: 0.01172617
[5/200] Training loss: 0.01164666
[6/200] Training loss: 0.01171480
[7/200] Training loss: 0.01171003
[8/200] Training loss: 0.01172680
[9/200] Training loss: 0.01168795
[10/200] Training loss: 0.01166146
[50/200] Training loss: 0.00989512
[100/200] Training loss: 0.00432880
[150/200] Training loss: 0.00266856
[200/200] Training loss: 0.00126707
---batch_size---: 2 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 2
---loss---: MSELoss ---opt---: SGD ---learning_rate---: 0.0103728850286019
----FITNESS-----------RMSE---- 20049.985536154385 ----------
[1/100] Training loss: 0.18055493
[2/100] Training loss: 0.06312095
[3/100] Training loss: 0.05694878
[4/100] Training loss: 0.05358657
[5/100] Training loss: 0.05282273
[6/100] Training loss: 0.04923782
[7/100] Training loss: 0.04859519
[8/100] Training loss: 0.04515024
[9/100] Training loss: 0.04446521
[10/100] Training loss: 0.04186436
[50/100] Training loss: 0.02100963
[100/100] Training loss: 0.01856298
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 32 ---dropout---: 0.29654065370859906 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 20200.152870708676 ----------
[1/200] Training loss: 0.04992260
[2/200] Training loss: 0.00446388
[3/200] Training loss: 0.00263824
[4/200] Training loss: 0.00182973
[5/200] Training loss: 0.00107211
[6/200] Training loss: 0.00105915
[7/200] Training loss: 0.00114046
[8/200] Training loss: 0.00086538
[9/200] Training loss: 0.00098167
[10/200] Training loss: 0.00109977
[50/200] Training loss: 0.00056887
[100/200] Training loss: 0.00042361
[150/200] Training loss: 0.00027675
[200/200] Training loss: 0.00020288
---batch_size---: 8 ---n_steps---: 2 ---network_epoch---: 200
---hidden_dim---: 32 ---dropout---: 0.16320283903808583 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adam ---learning_rate---: 0.011734602307407888
----FITNESS-----------RMSE---- 27308.400465790743 ----------
[1/200] Training loss: 0.25327918
[2/200] Training loss: 0.01144399
[3/200] Training loss: 0.00693872
[4/200] Training loss: 0.00553920
[5/200] Training loss: 0.00494894
[6/200] Training loss: 0.00412843
[7/200] Training loss: 0.00424863
[8/200] Training loss: 0.00396849
[9/200] Training loss: 0.00373464
[10/200] Training loss: 0.00375110
[50/200] Training loss: 0.00093334
[100/200] Training loss: 0.00051533
[150/200] Training loss: 0.00038127
[200/200] Training loss: 0.00033916
---batch_size---: 32 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16422.037875976293 ----------
[1/100] Training loss: 0.11537865
[2/100] Training loss: 0.08184720
[3/100] Training loss: 0.08108270
[4/100] Training loss: 0.08058264
[5/100] Training loss: 0.07958155
[6/100] Training loss: 0.07667657
[7/100] Training loss: 0.07160429
[8/100] Training loss: 0.06168534
[9/100] Training loss: 0.05341931
[10/100] Training loss: 0.05044818
[50/100] Training loss: 0.02419063
[100/100] Training loss: 0.01693115
---batch_size---: 2 ---n_steps---: 1 ---network_epoch---: 100
---hidden_dim---: 32 ---dropout---: 0.1048664913718335 ---num_layers---: 2
---loss---: L1Loss ---opt---: SGD ---learning_rate---: 0.0103728850286019
----FITNESS-----------RMSE---- 11625.837432202465 ----------
[1/100] Training loss: 0.03668380
[2/100] Training loss: 0.00421345
[3/100] Training loss: 0.00229236
[4/100] Training loss: 0.00168765
[5/100] Training loss: 0.00196000
[6/100] Training loss: 0.00111137
[7/100] Training loss: 0.00098588
[8/100] Training loss: 0.00148509
[9/100] Training loss: 0.00105287
[10/100] Training loss: 0.00126784
[50/100] Training loss: 0.00047121
[100/100] Training loss: 0.00045283
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.29654065370859906 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adam ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 10314.975133271044 ----------
[1/100] Training loss: 0.12437978
[2/100] Training loss: 0.05623470
[3/100] Training loss: 0.05377890
[4/100] Training loss: 0.04816819
[5/100] Training loss: 0.03898688
[6/100] Training loss: 0.03898659
[7/100] Training loss: 0.03217431
[8/100] Training loss: 0.03525822
[9/100] Training loss: 0.03639300
[10/100] Training loss: 0.03394485
[50/100] Training loss: 0.01501121
[100/100] Training loss: 0.01581342
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.1048664913718335 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adam ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 7929.796214279406 ----------
[1/50] Training loss: 0.17749458
[2/50] Training loss: 0.05397201
[3/50] Training loss: 0.05330263
[4/50] Training loss: 0.04571848
[5/50] Training loss: 0.03897395
[6/50] Training loss: 0.03718203
[7/50] Training loss: 0.03627509
[8/50] Training loss: 0.03029754
[9/50] Training loss: 0.02616723
[10/50] Training loss: 0.02861023
[50/50] Training loss: 0.01405322
---batch_size---: 16 ---n_steps---: 2 ---network_epoch---: 50
---hidden_dim---: 64 ---dropout---: 0.16320283903808583 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adam ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 7247.53613305929 ----------
[1/50] Training loss: 0.17786888
[2/50] Training loss: 0.05641042
[3/50] Training loss: 0.05465146
[4/50] Training loss: 0.05721480
[5/50] Training loss: 0.04828308
[6/50] Training loss: 0.04641565
[7/50] Training loss: 0.04379084
[8/50] Training loss: 0.04316497
[9/50] Training loss: 0.03914136
[10/50] Training loss: 0.03345904
[50/50] Training loss: 0.01793351
---batch_size---: 16 ---n_steps---: 2 ---network_epoch---: 50
---hidden_dim---: 64 ---dropout---: 0.16320283903808583 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adamax ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 24764.70262288647 ----------
[1/50] Training loss: 0.01858363
[2/50] Training loss: 0.00226865
[3/50] Training loss: 0.00176144
[4/50] Training loss: 0.00112157
[5/50] Training loss: 0.00103829
[6/50] Training loss: 0.00052799
[7/50] Training loss: 0.00065165
[8/50] Training loss: 0.00048449
[9/50] Training loss: 0.00047399
[10/50] Training loss: 0.00061351
[50/50] Training loss: 0.00023642
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 50
---hidden_dim---: 80 ---dropout---: 0.1048664913718335 ---num_layers---: 2
---loss---: SmoothL1Loss ---opt---: Adam ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 32675.013573065276 ----------
[1/100] Training loss: 0.10745284
[2/100] Training loss: 0.06893966
[3/100] Training loss: 0.07262395
[4/100] Training loss: 0.07041065
[5/100] Training loss: 0.07059471
[6/100] Training loss: 0.07192838
[7/100] Training loss: 0.07053214
[8/100] Training loss: 0.06958289
[9/100] Training loss: 0.06917373
[10/100] Training loss: 0.07177359
[50/100] Training loss: 0.06472859
[100/100] Training loss: 0.06523559
---batch_size---: 8 ---n_steps---: 1 ---network_epoch---: 100
---hidden_dim---: 32 ---dropout---: 0.7352965317482651 ---num_layers---: 2
---loss---: L1Loss ---opt---: Rprop ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 34145.28020092967 ----------
[1/50] Training loss: 0.23461274
[2/50] Training loss: 0.00643871
[3/50] Training loss: 0.00442372
[4/50] Training loss: 0.00274099
[5/50] Training loss: 0.00167024
[6/50] Training loss: 0.00157298
[7/50] Training loss: 0.00126696
[8/50] Training loss: 0.00134239
[9/50] Training loss: 0.00124276
[10/50] Training loss: 0.00088171
[50/50] Training loss: 0.00037489
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 50
---hidden_dim---: 80 ---dropout---: 0.1048664913718335 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adam ---learning_rate---: 0.015576475700050569
----FITNESS-----------RMSE---- 24832.826017189425 ----------
[1/50] Training loss: 0.07729715
[2/50] Training loss: 0.00493580
[3/50] Training loss: 0.00424614
[4/50] Training loss: 0.00404115
[5/50] Training loss: 0.00361824
[6/50] Training loss: 0.00329045
[7/50] Training loss: 0.00312688
[8/50] Training loss: 0.00259501
[9/50] Training loss: 0.00224998
[10/50] Training loss: 0.00188601
[50/50] Training loss: 0.00047308
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 50
---hidden_dim---: 16 ---dropout---: 0.1048664913718335 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adamax ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13494.392316810712 ----------
[1/200] Training loss: 0.04540673
[2/200] Training loss: 0.00326984
[3/200] Training loss: 0.00271911
[4/200] Training loss: 0.00247158
[5/200] Training loss: 0.00245880
[6/200] Training loss: 0.00225724
[7/200] Training loss: 0.00227897
[8/200] Training loss: 0.00225324
[9/200] Training loss: 0.00203578
[10/200] Training loss: 0.00201354
[50/200] Training loss: 0.00067515
[100/200] Training loss: 0.00041886
[150/200] Training loss: 0.00026767
[200/200] Training loss: 0.00025290
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.29654065370859906 ---num_layers---: 2
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 19599.471013269722 ----------
[1/100] Training loss: 0.04203158
[2/100] Training loss: 0.01194753
[3/100] Training loss: 0.01195750
[4/100] Training loss: 0.01191762
[5/100] Training loss: 0.01190442
[6/100] Training loss: 0.01190253
[7/100] Training loss: 0.01191728
[8/100] Training loss: 0.01187266
[9/100] Training loss: 0.01192456
[10/100] Training loss: 0.01182999
[50/100] Training loss: 0.00896719
[100/100] Training loss: 0.00246442
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.29654065370859906 ---num_layers---: 2
---loss---: MSELoss ---opt---: SGD ---learning_rate---: 0.0103728850286019
----FITNESS-----------RMSE---- 27535.68274076385 ----------
[1/100] Training loss: 0.14419841
[2/100] Training loss: 0.00803543
[3/100] Training loss: 0.00625642
[4/100] Training loss: 0.00575631
[5/100] Training loss: 0.00573050
[6/100] Training loss: 0.00535794
[7/100] Training loss: 0.00496384
[8/100] Training loss: 0.00529385
[9/100] Training loss: 0.00466168
[10/100] Training loss: 0.00477193
[50/100] Training loss: 0.00121634
[100/100] Training loss: 0.00079409
---batch_size---: 32 ---n_steps---: 1 ---network_epoch---: 100
---hidden_dim---: 32 ---dropout---: 0.7352965317482651 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.015576475700050569
----FITNESS-----------RMSE---- 14906.975548380027 ----------
[1/100] Training loss: 0.07718775
[2/100] Training loss: 0.00455541
[3/100] Training loss: 0.00378994
[4/100] Training loss: 0.00301342
[5/100] Training loss: 0.00256950
[6/100] Training loss: 0.00206166
[7/100] Training loss: 0.00179446
[8/100] Training loss: 0.00156726
[9/100] Training loss: 0.00148537
[10/100] Training loss: 0.00128976
[50/100] Training loss: 0.00067531
[100/100] Training loss: 0.00044105
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11051.079585271296 ----------
[1/200] Training loss: 0.03257450
[2/200] Training loss: 0.00537616
[3/200] Training loss: 0.00427439
[4/200] Training loss: 0.00407978
[5/200] Training loss: 0.00355508
[6/200] Training loss: 0.00229330
[7/200] Training loss: 0.00152967
[8/200] Training loss: 0.00146748
[9/200] Training loss: 0.00221556
[10/200] Training loss: 0.00152671
[50/200] Training loss: 0.00036142
[100/200] Training loss: 0.00033030
[150/200] Training loss: 0.00031548
[200/200] Training loss: 0.00024860
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.29654065370859906 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adam ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 28378.399672990723 ----------
[1/50] Training loss: 0.20120542
[2/50] Training loss: 0.05893503
[3/50] Training loss: 0.04507521
[4/50] Training loss: 0.03923543
[5/50] Training loss: 0.03750098
[6/50] Training loss: 0.03086036
[7/50] Training loss: 0.03089744
[8/50] Training loss: 0.03059833
[9/50] Training loss: 0.03732219
[10/50] Training loss: 0.03247811
[50/50] Training loss: 0.01957955
---batch_size---: 16 ---n_steps---: 2 ---network_epoch---: 50
---hidden_dim---: 16 ---dropout---: 0.1048664913718335 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adam ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 29573.320949802037 ----------
[1/50] Training loss: 0.05586896
[2/50] Training loss: 0.00620129
[3/50] Training loss: 0.00517482
[4/50] Training loss: 0.00475744
[5/50] Training loss: 0.00456004
[6/50] Training loss: 0.00429984
[7/50] Training loss: 0.00401213
[8/50] Training loss: 0.00393388
[9/50] Training loss: 0.00367962
[10/50] Training loss: 0.00386432
[50/50] Training loss: 0.00075725
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 50
---hidden_dim---: 64 ---dropout---: 0.16320283903808583 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 7668.600133009935 ----------
[1/200] Training loss: 0.16455581
[2/200] Training loss: 0.07234836
[3/200] Training loss: 0.06271804
[4/200] Training loss: 0.06181171
[5/200] Training loss: 0.05763192
[6/200] Training loss: 0.05760096
[7/200] Training loss: 0.05706173
[8/200] Training loss: 0.05676088
[9/200] Training loss: 0.05572406
[10/200] Training loss: 0.05497209
[50/200] Training loss: 0.03083519
[100/200] Training loss: 0.02177466
[150/200] Training loss: 0.01927434
[200/200] Training loss: 0.01688269
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 19104.330817906186 ----------
[1/100] Training loss: 0.36250460
[2/100] Training loss: 0.01896954
[3/100] Training loss: 0.01271276
[4/100] Training loss: 0.01246167
[5/100] Training loss: 0.01321269
[6/100] Training loss: 0.01417272
[7/100] Training loss: 0.01330899
[8/100] Training loss: 0.01298073
[9/100] Training loss: 0.01151551
[10/100] Training loss: 0.01236665
[50/100] Training loss: 0.00141589
[100/100] Training loss: 0.00055144
---batch_size---: 32 ---n_steps---: 1 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.1048664913718335 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adam ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 26318.403599002733 ----------
[1/100] Training loss: 0.02216157
[2/100] Training loss: 0.00255276
[3/100] Training loss: 0.00209758
[4/100] Training loss: 0.00188066
[5/100] Training loss: 0.00165228
[6/100] Training loss: 0.00157953
[7/100] Training loss: 0.00148326
[8/100] Training loss: 0.00129096
[9/100] Training loss: 0.00104028
[10/100] Training loss: 0.00116417
[50/100] Training loss: 0.00035573
[100/100] Training loss: 0.00024007
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 32 ---dropout---: 0.29654065370859906 ---num_layers---: 2
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10199.022698278497 ----------
[1/200] Training loss: 0.15767401
[2/200] Training loss: 0.05615575
[3/200] Training loss: 0.05181265
[4/200] Training loss: 0.04677098
[5/200] Training loss: 0.04451275
[6/200] Training loss: 0.04233800
[7/200] Training loss: 0.03863292
[8/200] Training loss: 0.03479567
[9/200] Training loss: 0.03542345
[10/200] Training loss: 0.03372354
[50/200] Training loss: 0.01744314
[100/200] Training loss: 0.01517890
[150/200] Training loss: 0.01422106
[200/200] Training loss: 0.01346173
---batch_size---: 16 ---n_steps---: 8 ---network_epoch---: 200
---hidden_dim---: 32 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10699.349512937692 ----------
[1/100] Training loss: 0.04802876
[2/100] Training loss: 0.00392867
[3/100] Training loss: 0.00364057
[4/100] Training loss: 0.00289212
[5/100] Training loss: 0.00204919
[6/100] Training loss: 0.00146547
[7/100] Training loss: 0.00110954
[8/100] Training loss: 0.00087316
[9/100] Training loss: 0.00076146
[10/100] Training loss: 0.00087318
[50/100] Training loss: 0.00061204
[100/100] Training loss: 0.00031258
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 32 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adam ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 20471.72098285828 ----------
[1/200] Training loss: 0.02721617
[2/200] Training loss: 0.00241609
[3/200] Training loss: 0.00193028
[4/200] Training loss: 0.00176256
[5/200] Training loss: 0.00151011
[6/200] Training loss: 0.00131727
[7/200] Training loss: 0.00115443
[8/200] Training loss: 0.00092612
[9/200] Training loss: 0.00084462
[10/200] Training loss: 0.00074871
[50/200] Training loss: 0.00032982
[100/200] Training loss: 0.00025292
[150/200] Training loss: 0.00022329
[200/200] Training loss: 0.00020391
---batch_size---: 16 ---n_steps---: 8 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.29654065370859906 ---num_layers---: 2
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16578.21896344719 ----------
[1/100] Training loss: 0.12079719
[2/100] Training loss: 0.05007378
[3/100] Training loss: 0.04480721
[4/100] Training loss: 0.03927029
[5/100] Training loss: 0.03038682
[6/100] Training loss: 0.03344046
[7/100] Training loss: 0.03117548
[8/100] Training loss: 0.03100417
[9/100] Training loss: 0.02447113
[10/100] Training loss: 0.03076574
[50/100] Training loss: 0.01763509
[100/100] Training loss: 0.01562234
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.16320283903808583 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adam ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 19978.08319133745 ----------
[1/50] Training loss: 0.04042095
[2/50] Training loss: 0.00476394
[3/50] Training loss: 0.00422802
[4/50] Training loss: 0.00368802
[5/50] Training loss: 0.00329863
[6/50] Training loss: 0.00304330
[7/50] Training loss: 0.00147972
[8/50] Training loss: 0.00119593
[9/50] Training loss: 0.00155924
[10/50] Training loss: 0.00126624
[50/50] Training loss: 0.00059998
---batch_size---: 16 ---n_steps---: 2 ---network_epoch---: 50
---hidden_dim---: 64 ---dropout---: 0.29654065370859906 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adam ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 10033.652973867494 ----------
[1/200] Training loss: 0.42704059
[2/200] Training loss: 0.13315824
[3/200] Training loss: 0.04539596
[4/200] Training loss: 0.02055936
[5/200] Training loss: 0.01396524
[6/200] Training loss: 0.01247321
[7/200] Training loss: 0.01190894
[8/200] Training loss: 0.01192179
[9/200] Training loss: 0.01196761
[10/200] Training loss: 0.01185008
[50/200] Training loss: 0.01172899
[100/200] Training loss: 0.01181597
[150/200] Training loss: 0.01155151
[200/200] Training loss: 0.01125904
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 2
---loss---: MSELoss ---opt---: SGD ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 39046.69286892297 ----------
[1/100] Training loss: 0.01649780
[2/100] Training loss: 0.00337983
[3/100] Training loss: 0.00239868
[4/100] Training loss: 0.00231643
[5/100] Training loss: 0.00189717
[6/100] Training loss: 0.00149436
[7/100] Training loss: 0.00145736
[8/100] Training loss: 0.00154604
[9/100] Training loss: 0.00126179
[10/100] Training loss: 0.00125843
[50/100] Training loss: 0.00062376
[100/100] Training loss: 0.00039043
---batch_size---: 2 ---n_steps---: 1 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.29654065370859906 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adam ---learning_rate---: 0.0103728850286019
----FITNESS-----------RMSE---- 29811.073378863766 ----------
[1/50] Training loss: 0.12055406
[2/50] Training loss: 0.05516419
[3/50] Training loss: 0.05052326
[4/50] Training loss: 0.05486064
[5/50] Training loss: 0.04519056
[6/50] Training loss: 0.03984133
[7/50] Training loss: 0.03293730
[8/50] Training loss: 0.03604251
[9/50] Training loss: 0.03141825
[10/50] Training loss: 0.03114563
[50/50] Training loss: 0.02020962
---batch_size---: 16 ---n_steps---: 2 ---network_epoch---: 50
---hidden_dim---: 64 ---dropout---: 0.16320283903808583 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adam ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 6507.862936479225 ----------
[1/200] Training loss: 0.01969380
[2/200] Training loss: 0.00178112
[3/200] Training loss: 0.00145116
[4/200] Training loss: 0.00097444
[5/200] Training loss: 0.00076198
[6/200] Training loss: 0.00058331
[7/200] Training loss: 0.00049714
[8/200] Training loss: 0.00044129
[9/200] Training loss: 0.00047003
[10/200] Training loss: 0.00047036
[50/200] Training loss: 0.00031334
[100/200] Training loss: 0.00013878
[150/200] Training loss: 0.00015040
[200/200] Training loss: 0.00006716
---batch_size---: 16 ---n_steps---: 8 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adam ---learning_rate---: 0.005675417330366128
----FITNESS-----------RMSE---- 27071.26831162515 ----------
[1/200] Training loss: 0.19829155
[2/200] Training loss: 0.06167350
[3/200] Training loss: 0.05169322
[4/200] Training loss: 0.05209012
[5/200] Training loss: 0.04878231
[6/200] Training loss: 0.04789528
[7/200] Training loss: 0.04494937
[8/200] Training loss: 0.04337690
[9/200] Training loss: 0.04287030
[10/200] Training loss: 0.04009126
[50/200] Training loss: 0.01954127
[100/200] Training loss: 0.01402027
[150/200] Training loss: 0.01161094
[200/200] Training loss: 0.01055689
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 32 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10367.287784179622 ----------
[1/100] Training loss: 0.01731865
[2/100] Training loss: 0.00194911
[3/100] Training loss: 0.00132057
[4/100] Training loss: 0.00105857
[5/100] Training loss: 0.00083295
[6/100] Training loss: 0.00075001
[7/100] Training loss: 0.00047718
[8/100] Training loss: 0.00045741
[9/100] Training loss: 0.00057564
[10/100] Training loss: 0.00039919
[50/100] Training loss: 0.00029646
[100/100] Training loss: 0.00019452
---batch_size---: 16 ---n_steps---: 8 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.1048664913718335 ---num_layers---: 2
---loss---: SmoothL1Loss ---opt---: Adam ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 24667.52877772721 ----------
[1/100] Training loss: 0.09913640
[2/100] Training loss: 0.00478180
[3/100] Training loss: 0.00460589
[4/100] Training loss: 0.00386839
[5/100] Training loss: 0.00337853
[6/100] Training loss: 0.00302559
[7/100] Training loss: 0.00261763
[8/100] Training loss: 0.00259127
[9/100] Training loss: 0.00216365
[10/100] Training loss: 0.00202460
[50/100] Training loss: 0.00067774
[100/100] Training loss: 0.00049202
---batch_size---: 16 ---n_steps---: 8 ---network_epoch---: 100
---hidden_dim---: 32 ---dropout---: 0.29654065370859906 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15955.894710106357 ----------
[1/200] Training loss: 0.02372466
[2/200] Training loss: 0.00193864
[3/200] Training loss: 0.00148259
[4/200] Training loss: 0.00096090
[5/200] Training loss: 0.00063037
[6/200] Training loss: 0.00051809
[7/200] Training loss: 0.00050229
[8/200] Training loss: 0.00036077
[9/200] Training loss: 0.00037616
[10/200] Training loss: 0.00033523
[50/200] Training loss: 0.00022756
[100/200] Training loss: 0.00011301
[150/200] Training loss: 0.00012149
[200/200] Training loss: 0.00008852
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adam ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 11260.534978410218 ----------
[1/100] Training loss: 0.07425852
[2/100] Training loss: 0.00532487
[3/100] Training loss: 0.00462029
[4/100] Training loss: 0.00379098
[5/100] Training loss: 0.00278525
[6/100] Training loss: 0.00227732
[7/100] Training loss: 0.00202110
[8/100] Training loss: 0.00181117
[9/100] Training loss: 0.00203992
[10/100] Training loss: 0.00174441
[50/100] Training loss: 0.00077991
[100/100] Training loss: 0.00048260
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 64 ---dropout---: 0.5563728816518204 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11648.331039251932 ----------
[1/50] Training loss: 0.04051971
[2/50] Training loss: 0.00472440
[3/50] Training loss: 0.00399188
[4/50] Training loss: 0.00297334
[5/50] Training loss: 0.00169862
[6/50] Training loss: 0.00163489
[7/50] Training loss: 0.00120930
[8/50] Training loss: 0.00106435
[9/50] Training loss: 0.00113001
[10/50] Training loss: 0.00088109
[50/50] Training loss: 0.00033860
---batch_size---: 16 ---n_steps---: 2 ---network_epoch---: 50
---hidden_dim---: 80 ---dropout---: 0.29654065370859906 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adam ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 17297.56422158912 ----------
[1/100] Training loss: 0.02256530
[2/100] Training loss: 0.00239800
[3/100] Training loss: 0.00191306
[4/100] Training loss: 0.00153020
[5/100] Training loss: 0.00109219
[6/100] Training loss: 0.00094199
[7/100] Training loss: 0.00060266
[8/100] Training loss: 0.00057860
[9/100] Training loss: 0.00054665
[10/100] Training loss: 0.00048518
[50/100] Training loss: 0.00037311
[100/100] Training loss: 0.00022371
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 32 ---dropout---: 0.16320283903808583 ---num_layers---: 2
---loss---: SmoothL1Loss ---opt---: Adam ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 19066.440464858668 ----------
[1/50] Training loss: 0.12080142
[2/50] Training loss: 0.05861522
[3/50] Training loss: 0.05299020
[4/50] Training loss: 0.05138811
[5/50] Training loss: 0.04806783
[6/50] Training loss: 0.04346310
[7/50] Training loss: 0.04160984
[8/50] Training loss: 0.03537733
[9/50] Training loss: 0.03340159
[10/50] Training loss: 0.02944613
[50/50] Training loss: 0.01841938
---batch_size---: 8 ---n_steps---: 2 ---network_epoch---: 50
---hidden_dim---: 64 ---dropout---: 0.29654065370859906 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10346.801631422148 ----------
[1/200] Training loss: 0.02151839
[2/200] Training loss: 0.00305818
[3/200] Training loss: 0.00223827
[4/200] Training loss: 0.00174723
[5/200] Training loss: 0.00163115
[6/200] Training loss: 0.00152003
[7/200] Training loss: 0.00100017
[8/200] Training loss: 0.00115930
[9/200] Training loss: 0.00081810
[10/200] Training loss: 0.00064635
[50/200] Training loss: 0.00031001
[100/200] Training loss: 0.00023060
[150/200] Training loss: 0.00020528
[200/200] Training loss: 0.00017159
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.29654065370859906 ---num_layers---: 2
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8353.187176162162 ----------
[1/100] Training loss: 0.04154519
[2/100] Training loss: 0.00490522
[3/100] Training loss: 0.00443830
[4/100] Training loss: 0.00405796
[5/100] Training loss: 0.00328350
[6/100] Training loss: 0.00285282
[7/100] Training loss: 0.00218795
[8/100] Training loss: 0.00187827
[9/100] Training loss: 0.00169872
[10/100] Training loss: 0.00140150
[50/100] Training loss: 0.00054811
[100/100] Training loss: 0.00045044
---batch_size---: 16 ---n_steps---: 8 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 20859.46346385736 ----------
[1/100] Training loss: 0.03282285
[2/100] Training loss: 0.00681112
[3/100] Training loss: 0.00629955
[4/100] Training loss: 0.00582717
[5/100] Training loss: 0.00530350
[6/100] Training loss: 0.00556963
[7/100] Training loss: 0.00498838
[8/100] Training loss: 0.00490436
[9/100] Training loss: 0.00493510
[10/100] Training loss: 0.00439685
[50/100] Training loss: 0.00101551
[100/100] Training loss: 0.00073062
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 17029.98015266019 ----------
[1/200] Training loss: 0.02986145
[2/200] Training loss: 0.00162518
[3/200] Training loss: 0.00091769
[4/200] Training loss: 0.00061616
[5/200] Training loss: 0.00048425
[6/200] Training loss: 0.00044090
[7/200] Training loss: 0.00042391
[8/200] Training loss: 0.00037617
[9/200] Training loss: 0.00045042
[10/200] Training loss: 0.00035118
[50/200] Training loss: 0.00029052
[100/200] Training loss: 0.00016687
[150/200] Training loss: 0.00028672
[200/200] Training loss: 0.00010460
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adam ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 26976.691568834012 ----------
[1/200] Training loss: 0.04369733
[2/200] Training loss: 0.00517646
[3/200] Training loss: 0.00444621
[4/200] Training loss: 0.00388324
[5/200] Training loss: 0.00372049
[6/200] Training loss: 0.00309626
[7/200] Training loss: 0.00284910
[8/200] Training loss: 0.00218467
[9/200] Training loss: 0.00193787
[10/200] Training loss: 0.00180120
[50/200] Training loss: 0.00077302
[100/200] Training loss: 0.00057485
[150/200] Training loss: 0.00045026
[200/200] Training loss: 0.00040723
---batch_size---: 16 ---n_steps---: 8 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10034.057205338228 ----------
[1/200] Training loss: 0.30126416
[2/200] Training loss: 0.07516605
[3/200] Training loss: 0.07003098
[4/200] Training loss: 0.05759176
[5/200] Training loss: 0.05584906
[6/200] Training loss: 0.05534263
[7/200] Training loss: 0.05296667
[8/200] Training loss: 0.05399070
[9/200] Training loss: 0.05094546
[10/200] Training loss: 0.05102638
[50/200] Training loss: 0.02920217
[100/200] Training loss: 0.02030109
[150/200] Training loss: 0.01493684
[200/200] Training loss: 0.01433292
---batch_size---: 32 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 32 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12727.655557878677 ----------
[1/200] Training loss: 0.02523853
[2/200] Training loss: 0.00213632
[3/200] Training loss: 0.00186653
[4/200] Training loss: 0.00179257
[5/200] Training loss: 0.00130729
[6/200] Training loss: 0.00105949
[7/200] Training loss: 0.00076998
[8/200] Training loss: 0.00064032
[9/200] Training loss: 0.00053858
[10/200] Training loss: 0.00049045
[50/200] Training loss: 0.00019262
[100/200] Training loss: 0.00016292
[150/200] Training loss: 0.00011238
[200/200] Training loss: 0.00010747
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 16 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adam ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 10255.934867187876 ----------
[1/200] Training loss: 0.23123558
[2/200] Training loss: 0.06373251
[3/200] Training loss: 0.05445261
[4/200] Training loss: 0.05554366
[5/200] Training loss: 0.05328652
[6/200] Training loss: 0.05014464
[7/200] Training loss: 0.05022666
[8/200] Training loss: 0.04677352
[9/200] Training loss: 0.04508794
[10/200] Training loss: 0.04494687
[50/200] Training loss: 0.02127019
[100/200] Training loss: 0.01656837
[150/200] Training loss: 0.01311241
[200/200] Training loss: 0.01163843
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5688.383250098397 ----------
[1/100] Training loss: 0.25255779
[2/100] Training loss: 0.07798907
[3/100] Training loss: 0.06178498
[4/100] Training loss: 0.05760287
[5/100] Training loss: 0.05287074
[6/100] Training loss: 0.05405345
[7/100] Training loss: 0.05150532
[8/100] Training loss: 0.05146988
[9/100] Training loss: 0.04854450
[10/100] Training loss: 0.04807631
[50/100] Training loss: 0.02722247
[100/100] Training loss: 0.02005525
---batch_size---: 32 ---n_steps---: 1 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.16320283903808583 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 19219.299466942077 ----------
[1/200] Training loss: 0.10644845
[2/200] Training loss: 0.00922302
[3/200] Training loss: 0.00520759
[4/200] Training loss: 0.00470019
[5/200] Training loss: 0.00442468
[6/200] Training loss: 0.00380226
[7/200] Training loss: 0.00317841
[8/200] Training loss: 0.00270267
[9/200] Training loss: 0.00190542
[10/200] Training loss: 0.00155766
[50/200] Training loss: 0.00057504
[100/200] Training loss: 0.00043648
[150/200] Training loss: 0.00037911
[200/200] Training loss: 0.00034042
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adam ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 25186.843549758276 ----------
[1/100] Training loss: 0.06696642
[2/100] Training loss: 0.00510240
[3/100] Training loss: 0.00437825
[4/100] Training loss: 0.00417949
[5/100] Training loss: 0.00406836
[6/100] Training loss: 0.00377239
[7/100] Training loss: 0.00355838
[8/100] Training loss: 0.00344588
[9/100] Training loss: 0.00325210
[10/100] Training loss: 0.00321116
[50/100] Training loss: 0.00070992
[100/100] Training loss: 0.00049281
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 100
---hidden_dim---: 32 ---dropout---: 0.6625704538293802 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 30652.126060030485 ----------
[1/200] Training loss: 0.14672553
[2/200] Training loss: 0.06356968
[3/200] Training loss: 0.05346646
[4/200] Training loss: 0.04934732
[5/200] Training loss: 0.04959298
[6/200] Training loss: 0.03998506
[7/200] Training loss: 0.04410947
[8/200] Training loss: 0.04003311
[9/200] Training loss: 0.03739357
[10/200] Training loss: 0.03317517
[50/200] Training loss: 0.01766537
[100/200] Training loss: 0.01536113
[150/200] Training loss: 0.01393764
[200/200] Training loss: 0.01345305
---batch_size---: 16 ---n_steps---: 8 ---network_epoch---: 200
---hidden_dim---: 32 ---dropout---: 0.29654065370859906 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11799.02572249082 ----------
[1/50] Training loss: 0.02099185
[2/50] Training loss: 0.00292168
[3/50] Training loss: 0.00230267
[4/50] Training loss: 0.00230041
[5/50] Training loss: 0.00213323
[6/50] Training loss: 0.00201540
[7/50] Training loss: 0.00192209
[8/50] Training loss: 0.00190592
[9/50] Training loss: 0.00175036
[10/50] Training loss: 0.00177815
[50/50] Training loss: 0.00049174
---batch_size---: 16 ---n_steps---: 2 ---network_epoch---: 50
---hidden_dim---: 64 ---dropout---: 0.29654065370859906 ---num_layers---: 2
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 14124.72668762125 ----------
[1/100] Training loss: 0.05051006
[2/100] Training loss: 0.00507283
[3/100] Training loss: 0.00288740
[4/100] Training loss: 0.00236783
[5/100] Training loss: 0.00167369
[6/100] Training loss: 0.00131549
[7/100] Training loss: 0.00119461
[8/100] Training loss: 0.00140797
[9/100] Training loss: 0.00126257
[10/100] Training loss: 0.00101236
[50/100] Training loss: 0.00038874
[100/100] Training loss: 0.00036969
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 32 ---dropout---: 0.29654065370859906 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adam ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 28875.669481416357 ----------
[1/50] Training loss: 0.09157913
[2/50] Training loss: 0.05267970
[3/50] Training loss: 0.04492799
[4/50] Training loss: 0.03896951
[5/50] Training loss: 0.04144419
[6/50] Training loss: 0.03403683
[7/50] Training loss: 0.03520080
[8/50] Training loss: 0.03429403
[9/50] Training loss: 0.02708874
[10/50] Training loss: 0.02668364
[50/50] Training loss: 0.01418440
---batch_size---: 16 ---n_steps---: 2 ---network_epoch---: 50
---hidden_dim---: 64 ---dropout---: 0.16320283903808583 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adam ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 16259.588186666968 ----------
[1/50] Training loss: 0.03479929
[2/50] Training loss: 0.00574818
[3/50] Training loss: 0.00490308
[4/50] Training loss: 0.00478784
[5/50] Training loss: 0.00492936
[6/50] Training loss: 0.00444796
[7/50] Training loss: 0.00431563
[8/50] Training loss: 0.00425117
[9/50] Training loss: 0.00415821
[10/50] Training loss: 0.00371830
[50/50] Training loss: 0.00091497
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 50
---hidden_dim---: 64 ---dropout---: 0.16320283903808583 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 18959.97299576136 ----------
[1/200] Training loss: 0.24014737
[2/200] Training loss: 0.07190405
[3/200] Training loss: 0.06047819
[4/200] Training loss: 0.05895584
[5/200] Training loss: 0.05644495
[6/200] Training loss: 0.05417705
[7/200] Training loss: 0.05291825
[8/200] Training loss: 0.05223255
[9/200] Training loss: 0.05113902
[10/200] Training loss: 0.04952521
[50/200] Training loss: 0.02344997
[100/200] Training loss: 0.01895269
[150/200] Training loss: 0.01679025
[200/200] Training loss: 0.01385063
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 32 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 21779.34140418392 ----------
[1/50] Training loss: 0.02255478
[2/50] Training loss: 0.00252625
[3/50] Training loss: 0.00240617
[4/50] Training loss: 0.00243814
[5/50] Training loss: 0.00219857
[6/50] Training loss: 0.00221777
[7/50] Training loss: 0.00212089
[8/50] Training loss: 0.00210164
[9/50] Training loss: 0.00200110
[10/50] Training loss: 0.00201804
[50/50] Training loss: 0.00048172
---batch_size---: 16 ---n_steps---: 2 ---network_epoch---: 50
---hidden_dim---: 64 ---dropout---: 0.29654065370859906 ---num_layers---: 2
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 20719.911196720896 ----------
[1/200] Training loss: 0.19087920
[2/200] Training loss: 0.06269797
[3/200] Training loss: 0.05612561
[4/200] Training loss: 0.05174305
[5/200] Training loss: 0.04817381
[6/200] Training loss: 0.04797991
[7/200] Training loss: 0.04510266
[8/200] Training loss: 0.04253106
[9/200] Training loss: 0.04012099
[10/200] Training loss: 0.03977896
[50/200] Training loss: 0.01726945
[100/200] Training loss: 0.01416146
[150/200] Training loss: 0.01245317
[200/200] Training loss: 0.01138219
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7930.990858650639 ----------
[1/200] Training loss: 0.03328449
[2/200] Training loss: 0.00243404
[3/200] Training loss: 0.00233692
[4/200] Training loss: 0.00225586
[5/200] Training loss: 0.00231421
[6/200] Training loss: 0.00196492
[7/200] Training loss: 0.00191757
[8/200] Training loss: 0.00166781
[9/200] Training loss: 0.00164193
[10/200] Training loss: 0.00149966
[50/200] Training loss: 0.00037985
[100/200] Training loss: 0.00022847
[150/200] Training loss: 0.00017930
[200/200] Training loss: 0.00014538
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.29654065370859906 ---num_layers---: 2
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12546.444914795586 ----------
[1/200] Training loss: 0.27360365
[2/200] Training loss: 0.07608263
[3/200] Training loss: 0.06244198
[4/200] Training loss: 0.05520541
[5/200] Training loss: 0.05482649
[6/200] Training loss: 0.05300298
[7/200] Training loss: 0.05108995
[8/200] Training loss: 0.04962352
[9/200] Training loss: 0.04698908
[10/200] Training loss: 0.04840924
[50/200] Training loss: 0.02470320
[100/200] Training loss: 0.01739292
[150/200] Training loss: 0.01585855
[200/200] Training loss: 0.01412804
---batch_size---: 32 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13271.895117126265 ----------
[1/200] Training loss: 0.13513838
[2/200] Training loss: 0.00773849
[3/200] Training loss: 0.00516503
[4/200] Training loss: 0.00481219
[5/200] Training loss: 0.00448964
[6/200] Training loss: 0.00419274
[7/200] Training loss: 0.00413226
[8/200] Training loss: 0.00377247
[9/200] Training loss: 0.00352032
[10/200] Training loss: 0.00341047
[50/200] Training loss: 0.00081738
[100/200] Training loss: 0.00040779
[150/200] Training loss: 0.00033714
[200/200] Training loss: 0.00028142
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 19443.379953084288 ----------
[1/200] Training loss: 0.03759459
[2/200] Training loss: 0.00237948
[3/200] Training loss: 0.00181535
[4/200] Training loss: 0.00123148
[5/200] Training loss: 0.00082481
[6/200] Training loss: 0.00062099
[7/200] Training loss: 0.00055020
[8/200] Training loss: 0.00047685
[9/200] Training loss: 0.00047166
[10/200] Training loss: 0.00042609
[50/200] Training loss: 0.00013282
[100/200] Training loss: 0.00010595
[150/200] Training loss: 0.00013858
[200/200] Training loss: 0.00010245
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 16 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adam ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 42546.30606762472 ----------
[1/200] Training loss: 0.12408042
[2/200] Training loss: 0.05584462
[3/200] Training loss: 0.05369745
[4/200] Training loss: 0.05241441
[5/200] Training loss: 0.05082538
[6/200] Training loss: 0.05007190
[7/200] Training loss: 0.04795693
[8/200] Training loss: 0.04794868
[9/200] Training loss: 0.04687090
[10/200] Training loss: 0.04553686
[50/200] Training loss: 0.02050237
[100/200] Training loss: 0.01562488
[150/200] Training loss: 0.01381648
[200/200] Training loss: 0.01290962
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 17151.05594416857 ----------
[1/200] Training loss: 0.04200641
[2/200] Training loss: 0.00304915
[3/200] Training loss: 0.00261742
[4/200] Training loss: 0.00232923
[5/200] Training loss: 0.00217053
[6/200] Training loss: 0.00210028
[7/200] Training loss: 0.00185641
[8/200] Training loss: 0.00175112
[9/200] Training loss: 0.00167475
[10/200] Training loss: 0.00158123
[50/200] Training loss: 0.00049268
[100/200] Training loss: 0.00028373
[150/200] Training loss: 0.00021076
[200/200] Training loss: 0.00017686
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.29654065370859906 ---num_layers---: 2
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14647.813761787114 ----------
[1/200] Training loss: 0.15369673
[2/200] Training loss: 0.05487576
[3/200] Training loss: 0.05036088
[4/200] Training loss: 0.04625049
[5/200] Training loss: 0.04073867
[6/200] Training loss: 0.03753256
[7/200] Training loss: 0.03727336
[8/200] Training loss: 0.03502270
[9/200] Training loss: 0.03197913
[10/200] Training loss: 0.03018734
[50/200] Training loss: 0.01800943
[100/200] Training loss: 0.01498586
[150/200] Training loss: 0.01362525
[200/200] Training loss: 0.01314481
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.02534414379207697
----FITNESS-----------RMSE---- 19471.325378617657 ----------
[1/200] Training loss: 0.03768604
[2/200] Training loss: 0.00191062
[3/200] Training loss: 0.00162196
[4/200] Training loss: 0.00144253
[5/200] Training loss: 0.00122369
[6/200] Training loss: 0.00107231
[7/200] Training loss: 0.00092553
[8/200] Training loss: 0.00078863
[9/200] Training loss: 0.00071030
[10/200] Training loss: 0.00064046
[50/200] Training loss: 0.00020798
[100/200] Training loss: 0.00015800
[150/200] Training loss: 0.00015233
[200/200] Training loss: 0.00012855
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9349.836790019384 ----------
[1/200] Training loss: 0.07079212
[2/200] Training loss: 0.00782961
[3/200] Training loss: 0.00548129
[4/200] Training loss: 0.00438747
[5/200] Training loss: 0.00400152
[6/200] Training loss: 0.00370251
[7/200] Training loss: 0.00313234
[8/200] Training loss: 0.00259775
[9/200] Training loss: 0.00190845
[10/200] Training loss: 0.00177529
[50/200] Training loss: 0.00055206
[100/200] Training loss: 0.00037810
[150/200] Training loss: 0.00024085
[200/200] Training loss: 0.00025667
---batch_size---: 32 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 16 ---dropout---: 0.6625704538293802 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adam ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 21383.8645712135 ----------
[1/200] Training loss: 0.14596073
[2/200] Training loss: 0.05583891
[3/200] Training loss: 0.05167666
[4/200] Training loss: 0.04995550
[5/200] Training loss: 0.04749270
[6/200] Training loss: 0.04714309
[7/200] Training loss: 0.04539932
[8/200] Training loss: 0.04363712
[9/200] Training loss: 0.04459836
[10/200] Training loss: 0.04136179
[50/200] Training loss: 0.02105195
[100/200] Training loss: 0.01430009
[150/200] Training loss: 0.01218725
[200/200] Training loss: 0.01078354
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 32 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6645.648200138193 ----------
[1/100] Training loss: 0.13697807
[2/100] Training loss: 0.00592608
[3/100] Training loss: 0.00524355
[4/100] Training loss: 0.00511227
[5/100] Training loss: 0.00487087
[6/100] Training loss: 0.00454848
[7/100] Training loss: 0.00423432
[8/100] Training loss: 0.00374621
[9/100] Training loss: 0.00373231
[10/100] Training loss: 0.00364278
[50/100] Training loss: 0.00096503
[100/100] Training loss: 0.00055541
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 64 ---dropout---: 0.5563728816518204 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12734.927718679835 ----------
[1/200] Training loss: 0.11933013
[2/200] Training loss: 0.00693833
[3/200] Training loss: 0.00568987
[4/200] Training loss: 0.00494974
[5/200] Training loss: 0.00452953
[6/200] Training loss: 0.00409425
[7/200] Training loss: 0.00366074
[8/200] Training loss: 0.00302028
[9/200] Training loss: 0.00281289
[10/200] Training loss: 0.00245420
[50/200] Training loss: 0.00080385
[100/200] Training loss: 0.00048341
[150/200] Training loss: 0.00038663
[200/200] Training loss: 0.00037288
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.29654065370859906 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 21980.3170131825 ----------
[1/200] Training loss: 0.05061175
[2/200] Training loss: 0.00280375
[3/200] Training loss: 0.00220692
[4/200] Training loss: 0.00176615
[5/200] Training loss: 0.00133657
[6/200] Training loss: 0.00131518
[7/200] Training loss: 0.00097868
[8/200] Training loss: 0.00096650
[9/200] Training loss: 0.00081097
[10/200] Training loss: 0.00083950
[50/200] Training loss: 0.00038610
[100/200] Training loss: 0.00027637
[150/200] Training loss: 0.00024152
[200/200] Training loss: 0.00024320
---batch_size---: 16 ---n_steps---: 8 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 2
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17751.973862080802 ----------
[1/50] Training loss: 0.17304350
[2/50] Training loss: 0.05913756
[3/50] Training loss: 0.05563324
[4/50] Training loss: 0.05227629
[5/50] Training loss: 0.04924815
[6/50] Training loss: 0.04769741
[7/50] Training loss: 0.04432439
[8/50] Training loss: 0.04226124
[9/50] Training loss: 0.03740679
[10/50] Training loss: 0.03534436
[50/50] Training loss: 0.01828788
---batch_size---: 16 ---n_steps---: 8 ---network_epoch---: 50
---hidden_dim---: 32 ---dropout---: 0.29654065370859906 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 18175.72975151204 ----------
[1/200] Training loss: 0.12305328
[2/200] Training loss: 0.05298774
[3/200] Training loss: 0.05223225
[4/200] Training loss: 0.04661125
[5/200] Training loss: 0.04037560
[6/200] Training loss: 0.03760375
[7/200] Training loss: 0.03603610
[8/200] Training loss: 0.03351797
[9/200] Training loss: 0.03178695
[10/200] Training loss: 0.02975666
[50/200] Training loss: 0.01881539
[100/200] Training loss: 0.01408165
[150/200] Training loss: 0.01142382
[200/200] Training loss: 0.01063371
---batch_size---: 8 ---n_steps---: 2 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.29654065370859906 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5304.816302191811 ----------
[1/200] Training loss: 0.16099418
[2/200] Training loss: 0.06117058
[3/200] Training loss: 0.05251013
[4/200] Training loss: 0.05127812
[5/200] Training loss: 0.04737378
[6/200] Training loss: 0.04177313
[7/200] Training loss: 0.03669403
[8/200] Training loss: 0.03530442
[9/200] Training loss: 0.03257081
[10/200] Training loss: 0.02950421
[50/200] Training loss: 0.01669078
[100/200] Training loss: 0.01403236
[150/200] Training loss: 0.01279288
[200/200] Training loss: 0.01204531
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.29654065370859906 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 18188.924981977358 ----------
[1/200] Training loss: 0.06494115
[2/200] Training loss: 0.00349048
[3/200] Training loss: 0.00246768
[4/200] Training loss: 0.00217781
[5/200] Training loss: 0.00196888
[6/200] Training loss: 0.00167322
[7/200] Training loss: 0.00153249
[8/200] Training loss: 0.00144100
[9/200] Training loss: 0.00136085
[10/200] Training loss: 0.00131655
[50/200] Training loss: 0.00044773
[100/200] Training loss: 0.00034450
[150/200] Training loss: 0.00027347
[200/200] Training loss: 0.00020436
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10548.819839204763 ----------
[1/200] Training loss: 0.14781716
[2/200] Training loss: 0.05872810
[3/200] Training loss: 0.05158345
[4/200] Training loss: 0.05017883
[5/200] Training loss: 0.04719509
[6/200] Training loss: 0.04302610
[7/200] Training loss: 0.04098613
[8/200] Training loss: 0.03993712
[9/200] Training loss: 0.03803328
[10/200] Training loss: 0.03439585
[50/200] Training loss: 0.02070362
[100/200] Training loss: 0.01758092
[150/200] Training loss: 0.01659042
[200/200] Training loss: 0.01557899
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.29654065370859906 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17095.32333710012 ----------
[1/200] Training loss: 0.06394916
[2/200] Training loss: 0.00460672
[3/200] Training loss: 0.00266653
[4/200] Training loss: 0.00248576
[5/200] Training loss: 0.00237115
[6/200] Training loss: 0.00231352
[7/200] Training loss: 0.00223526
[8/200] Training loss: 0.00214280
[9/200] Training loss: 0.00202601
[10/200] Training loss: 0.00203274
[50/200] Training loss: 0.00047707
[100/200] Training loss: 0.00031216
[150/200] Training loss: 0.00016328
[200/200] Training loss: 0.00011474
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4365.538912894947 ----------
[1/200] Training loss: 0.02248202
[2/200] Training loss: 0.00241020
[3/200] Training loss: 0.00216870
[4/200] Training loss: 0.00201065
[5/200] Training loss: 0.00167326
[6/200] Training loss: 0.00144999
[7/200] Training loss: 0.00148548
[8/200] Training loss: 0.00122399
[9/200] Training loss: 0.00111654
[10/200] Training loss: 0.00084264
[50/200] Training loss: 0.00034380
[100/200] Training loss: 0.00026214
[150/200] Training loss: 0.00023118
[200/200] Training loss: 0.00020203
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 2
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12063.964190928287 ----------
[1/200] Training loss: 0.03103466
[2/200] Training loss: 0.00250635
[3/200] Training loss: 0.00227174
[4/200] Training loss: 0.00210335
[5/200] Training loss: 0.00193320
[6/200] Training loss: 0.00175364
[7/200] Training loss: 0.00160990
[8/200] Training loss: 0.00137402
[9/200] Training loss: 0.00133702
[10/200] Training loss: 0.00114763
[50/200] Training loss: 0.00036239
[100/200] Training loss: 0.00016882
[150/200] Training loss: 0.00012356
[200/200] Training loss: 0.00010856
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.29654065370859906 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14985.08965605478 ----------
[1/200] Training loss: 0.15036324
[2/200] Training loss: 0.05947482
[3/200] Training loss: 0.05191130
[4/200] Training loss: 0.04913077
[5/200] Training loss: 0.05067855
[6/200] Training loss: 0.04664709
[7/200] Training loss: 0.04056748
[8/200] Training loss: 0.03840727
[9/200] Training loss: 0.03913725
[10/200] Training loss: 0.03609847
[50/200] Training loss: 0.02123047
[100/200] Training loss: 0.01682401
[150/200] Training loss: 0.01392786
[200/200] Training loss: 0.01212137
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 32 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9034.381882563965 ----------
[1/200] Training loss: 0.18977507
[2/200] Training loss: 0.07060242
[3/200] Training loss: 0.05712239
[4/200] Training loss: 0.04865936
[5/200] Training loss: 0.04436237
[6/200] Training loss: 0.04090921
[7/200] Training loss: 0.03932401
[8/200] Training loss: 0.03605576
[9/200] Training loss: 0.03592956
[10/200] Training loss: 0.03342262
[50/200] Training loss: 0.01725155
[100/200] Training loss: 0.01372317
[150/200] Training loss: 0.01293460
[200/200] Training loss: 0.01221380
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5733.665668662587 ----------
[1/200] Training loss: 0.17960438
[2/200] Training loss: 0.06477712
[3/200] Training loss: 0.05442090
[4/200] Training loss: 0.05451073
[5/200] Training loss: 0.04989564
[6/200] Training loss: 0.04945072
[7/200] Training loss: 0.04938512
[8/200] Training loss: 0.04785944
[9/200] Training loss: 0.04486290
[10/200] Training loss: 0.04495776
[50/200] Training loss: 0.02126677
[100/200] Training loss: 0.01689867
[150/200] Training loss: 0.01464367
[200/200] Training loss: 0.01225722
---batch_size---: 16 ---n_steps---: 2 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8660.552869187972 ----------
[1/200] Training loss: 0.01045503
[2/200] Training loss: 0.00185660
[3/200] Training loss: 0.00148724
[4/200] Training loss: 0.00126993
[5/200] Training loss: 0.00102938
[6/200] Training loss: 0.00087041
[7/200] Training loss: 0.00070843
[8/200] Training loss: 0.00062386
[9/200] Training loss: 0.00057088
[10/200] Training loss: 0.00051693
[50/200] Training loss: 0.00020197
[100/200] Training loss: 0.00014880
[150/200] Training loss: 0.00013315
[200/200] Training loss: 0.00011396
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.29654065370859906 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6380.973279994205 ----------
[1/200] Training loss: 0.14120462
[2/200] Training loss: 0.00799996
[3/200] Training loss: 0.00461162
[4/200] Training loss: 0.00423139
[5/200] Training loss: 0.00388632
[6/200] Training loss: 0.00353770
[7/200] Training loss: 0.00307070
[8/200] Training loss: 0.00279980
[9/200] Training loss: 0.00253235
[10/200] Training loss: 0.00222905
[50/200] Training loss: 0.00079193
[100/200] Training loss: 0.00057653
[150/200] Training loss: 0.00045584
[200/200] Training loss: 0.00032728
---batch_size---: 16 ---n_steps---: 2 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.29654065370859906 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12867.994404723682 ----------
[1/50] Training loss: 0.11556097
[2/50] Training loss: 0.05911303
[3/50] Training loss: 0.04650188
[4/50] Training loss: 0.04186904
[5/50] Training loss: 0.03970301
[6/50] Training loss: 0.04216564
[7/50] Training loss: 0.03741136
[8/50] Training loss: 0.02998837
[9/50] Training loss: 0.03222671
[10/50] Training loss: 0.02898337
[50/50] Training loss: 0.01302637
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 50
---hidden_dim---: 32 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adam ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 20256.628742216708 ----------
[1/200] Training loss: 0.16632519
[2/200] Training loss: 0.05736361
[3/200] Training loss: 0.05089893
[4/200] Training loss: 0.04525698
[5/200] Training loss: 0.04180469
[6/200] Training loss: 0.03881657
[7/200] Training loss: 0.03627706
[8/200] Training loss: 0.03473926
[9/200] Training loss: 0.03296232
[10/200] Training loss: 0.03168021
[50/200] Training loss: 0.01772766
[100/200] Training loss: 0.01566302
[150/200] Training loss: 0.01438093
[200/200] Training loss: 0.01338603
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9134.830923448993 ----------
[1/200] Training loss: 0.03187059
[2/200] Training loss: 0.00222564
[3/200] Training loss: 0.00190304
[4/200] Training loss: 0.00176893
[5/200] Training loss: 0.00149008
[6/200] Training loss: 0.00130088
[7/200] Training loss: 0.00106715
[8/200] Training loss: 0.00102904
[9/200] Training loss: 0.00085546
[10/200] Training loss: 0.00074824
[50/200] Training loss: 0.00025915
[100/200] Training loss: 0.00021030
[150/200] Training loss: 0.00018437
[200/200] Training loss: 0.00015518
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16428.16897892154 ----------
[1/100] Training loss: 0.11882900
[2/100] Training loss: 0.00515822
[3/100] Training loss: 0.00499680
[4/100] Training loss: 0.00466910
[5/100] Training loss: 0.00409507
[6/100] Training loss: 0.00375521
[7/100] Training loss: 0.00329785
[8/100] Training loss: 0.00316913
[9/100] Training loss: 0.00304529
[10/100] Training loss: 0.00298291
[50/100] Training loss: 0.00085289
[100/100] Training loss: 0.00056206
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 19787.204754588252 ----------
[1/200] Training loss: 0.09095677
[2/200] Training loss: 0.00711235
[3/200] Training loss: 0.00588733
[4/200] Training loss: 0.00542206
[5/200] Training loss: 0.00509420
[6/200] Training loss: 0.00526220
[7/200] Training loss: 0.00465197
[8/200] Training loss: 0.00444569
[9/200] Training loss: 0.00450415
[10/200] Training loss: 0.00440256
[50/200] Training loss: 0.00107938
[100/200] Training loss: 0.00068213
[150/200] Training loss: 0.00039163
[200/200] Training loss: 0.00035599
---batch_size---: 32 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.5563728816518204 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6217.493707274661 ----------
[1/200] Training loss: 0.04908091
[2/200] Training loss: 0.00223172
[3/200] Training loss: 0.00214224
[4/200] Training loss: 0.00191220
[5/200] Training loss: 0.00172048
[6/200] Training loss: 0.00157039
[7/200] Training loss: 0.00135640
[8/200] Training loss: 0.00119824
[9/200] Training loss: 0.00109109
[10/200] Training loss: 0.00093917
[50/200] Training loss: 0.00027138
[100/200] Training loss: 0.00019409
[150/200] Training loss: 0.00016474
[200/200] Training loss: 0.00014356
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10575.452330751627 ----------
[1/200] Training loss: 0.17717886
[2/200] Training loss: 0.05625006
[3/200] Training loss: 0.05509574
[4/200] Training loss: 0.05131986
[5/200] Training loss: 0.04602680
[6/200] Training loss: 0.04469048
[7/200] Training loss: 0.04284843
[8/200] Training loss: 0.04068274
[9/200] Training loss: 0.03623505
[10/200] Training loss: 0.03473136
[50/200] Training loss: 0.01918434
[100/200] Training loss: 0.01681633
[150/200] Training loss: 0.01560221
[200/200] Training loss: 0.01485178
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11034.965156265787 ----------
[1/200] Training loss: 0.06572022
[2/200] Training loss: 0.00227428
[3/200] Training loss: 0.00159394
[4/200] Training loss: 0.00088560
[5/200] Training loss: 0.00060354
[6/200] Training loss: 0.00065688
[7/200] Training loss: 0.00052253
[8/200] Training loss: 0.00034488
[9/200] Training loss: 0.00033862
[10/200] Training loss: 0.00042759
[50/200] Training loss: 0.00016585
[100/200] Training loss: 0.00013378
[150/200] Training loss: 0.00012410
[200/200] Training loss: 0.00010625
---batch_size---: 16 ---n_steps---: 2 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adam ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 26515.571274253172 ----------
[1/200] Training loss: 0.10189581
[2/200] Training loss: 0.05129448
[3/200] Training loss: 0.04749926
[4/200] Training loss: 0.04595862
[5/200] Training loss: 0.04360748
[6/200] Training loss: 0.04198610
[7/200] Training loss: 0.04057353
[8/200] Training loss: 0.03966661
[9/200] Training loss: 0.03790633
[10/200] Training loss: 0.03767126
[50/200] Training loss: 0.02605387
[100/200] Training loss: 0.02284411
[150/200] Training loss: 0.02087274
[200/200] Training loss: 0.01969130
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 16 ---dropout---: 0.29654065370859906 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.004191505811302626
----FITNESS-----------RMSE---- 18547.387093604317 ----------
[1/200] Training loss: 0.16998611
[2/200] Training loss: 0.05661878
[3/200] Training loss: 0.05103884
[4/200] Training loss: 0.04713875
[5/200] Training loss: 0.04497753
[6/200] Training loss: 0.03967875
[7/200] Training loss: 0.03970698
[8/200] Training loss: 0.03808758
[9/200] Training loss: 0.03653940
[10/200] Training loss: 0.03606007
[50/200] Training loss: 0.01913587
[100/200] Training loss: 0.01492575
[150/200] Training loss: 0.01256536
[200/200] Training loss: 0.01150178
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4753.560349885126 ----------
[1/200] Training loss: 0.14078699
[2/200] Training loss: 0.05568642
[3/200] Training loss: 0.05319128
[4/200] Training loss: 0.05192250
[5/200] Training loss: 0.04884024
[6/200] Training loss: 0.04767290
[7/200] Training loss: 0.04893960
[8/200] Training loss: 0.04593335
[9/200] Training loss: 0.04466450
[10/200] Training loss: 0.04129125
[50/200] Training loss: 0.01824431
[100/200] Training loss: 0.01344065
[150/200] Training loss: 0.01176096
[200/200] Training loss: 0.01119037
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 32 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 19233.926692175988 ----------
[1/200] Training loss: 0.03237907
[2/200] Training loss: 0.00239601
[3/200] Training loss: 0.00197170
[4/200] Training loss: 0.00184299
[5/200] Training loss: 0.00163962
[6/200] Training loss: 0.00145774
[7/200] Training loss: 0.00130757
[8/200] Training loss: 0.00109140
[9/200] Training loss: 0.00097059
[10/200] Training loss: 0.00086498
[50/200] Training loss: 0.00026634
[100/200] Training loss: 0.00019570
[150/200] Training loss: 0.00016004
[200/200] Training loss: 0.00015590
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.29654065370859906 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15870.290230490431 ----------
[1/200] Training loss: 0.02573925
[2/200] Training loss: 0.00212656
[3/200] Training loss: 0.00169462
[4/200] Training loss: 0.00140890
[5/200] Training loss: 0.00107075
[6/200] Training loss: 0.00084416
[7/200] Training loss: 0.00072500
[8/200] Training loss: 0.00057409
[9/200] Training loss: 0.00047465
[10/200] Training loss: 0.00052533
[50/200] Training loss: 0.00019843
[100/200] Training loss: 0.00014274
[150/200] Training loss: 0.00012560
[200/200] Training loss: 0.00011848
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6346.989837710472 ----------
[1/200] Training loss: 0.06119080
[2/200] Training loss: 0.00607383
[3/200] Training loss: 0.00556384
[4/200] Training loss: 0.00494913
[5/200] Training loss: 0.00483316
[6/200] Training loss: 0.00472216
[7/200] Training loss: 0.00449179
[8/200] Training loss: 0.00422794
[9/200] Training loss: 0.00387664
[10/200] Training loss: 0.00360121
[50/200] Training loss: 0.00084786
[100/200] Training loss: 0.00058229
[150/200] Training loss: 0.00037942
[200/200] Training loss: 0.00031756
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17438.6650865254 ----------
[1/200] Training loss: 0.03111213
[2/200] Training loss: 0.00214150
[3/200] Training loss: 0.00197474
[4/200] Training loss: 0.00177970
[5/200] Training loss: 0.00170866
[6/200] Training loss: 0.00153824
[7/200] Training loss: 0.00147972
[8/200] Training loss: 0.00136653
[9/200] Training loss: 0.00135315
[10/200] Training loss: 0.00139443
[50/200] Training loss: 0.00035118
[100/200] Training loss: 0.00024635
[150/200] Training loss: 0.00015821
[200/200] Training loss: 0.00015036
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8833.099569233893 ----------
[1/200] Training loss: 0.15763183
[2/200] Training loss: 0.06476377
[3/200] Training loss: 0.05703637
[4/200] Training loss: 0.05256712
[5/200] Training loss: 0.05454941
[6/200] Training loss: 0.05243486
[7/200] Training loss: 0.04873069
[8/200] Training loss: 0.04544272
[9/200] Training loss: 0.04440279
[10/200] Training loss: 0.04197162
[50/200] Training loss: 0.02286464
[100/200] Training loss: 0.01907784
[150/200] Training loss: 0.01657898
[200/200] Training loss: 0.01516914
---batch_size---: 16 ---n_steps---: 2 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 23866.846628744235 ----------
[1/200] Training loss: 0.12718945
[2/200] Training loss: 0.00406969
[3/200] Training loss: 0.00356598
[4/200] Training loss: 0.00304534
[5/200] Training loss: 0.00279568
[6/200] Training loss: 0.00263840
[7/200] Training loss: 0.00202524
[8/200] Training loss: 0.00190569
[9/200] Training loss: 0.00170129
[10/200] Training loss: 0.00145720
[50/200] Training loss: 0.00048754
[100/200] Training loss: 0.00038795
[150/200] Training loss: 0.00030276
[200/200] Training loss: 0.00027479
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.29654065370859906 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.017812376333253388
----FITNESS-----------RMSE---- 17641.42216489362 ----------
[1/200] Training loss: 0.09058775
[2/200] Training loss: 0.00506316
[3/200] Training loss: 0.00456834
[4/200] Training loss: 0.00430584
[5/200] Training loss: 0.00401289
[6/200] Training loss: 0.00358233
[7/200] Training loss: 0.00338381
[8/200] Training loss: 0.00329000
[9/200] Training loss: 0.00293604
[10/200] Training loss: 0.00274205
[50/200] Training loss: 0.00097657
[100/200] Training loss: 0.00064937
[150/200] Training loss: 0.00041633
[200/200] Training loss: 0.00030895
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13553.737787046051 ----------
[1/200] Training loss: 0.18076928
[2/200] Training loss: 0.07983011
[3/200] Training loss: 0.06843895
[4/200] Training loss: 0.05833509
[5/200] Training loss: 0.05813197
[6/200] Training loss: 0.06084904
[7/200] Training loss: 0.05581072
[8/200] Training loss: 0.05392786
[9/200] Training loss: 0.05549730
[10/200] Training loss: 0.05091816
[50/200] Training loss: 0.02916998
[100/200] Training loss: 0.02388758
[150/200] Training loss: 0.01978417
[200/200] Training loss: 0.01716747
---batch_size---: 32 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 32 ---dropout---: 0.5563728816518204 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.00802650654681353
----FITNESS-----------RMSE---- 26177.952249937352 ----------
[1/200] Training loss: 0.03467220
[2/200] Training loss: 0.00251340
[3/200] Training loss: 0.00216080
[4/200] Training loss: 0.00189848
[5/200] Training loss: 0.00159640
[6/200] Training loss: 0.00139253
[7/200] Training loss: 0.00125627
[8/200] Training loss: 0.00106914
[9/200] Training loss: 0.00086197
[10/200] Training loss: 0.00079123
[50/200] Training loss: 0.00026145
[100/200] Training loss: 0.00021739
[150/200] Training loss: 0.00020331
[200/200] Training loss: 0.00016191
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 19454.609325298723 ----------
[1/200] Training loss: 0.09843204
[2/200] Training loss: 0.05054540
[3/200] Training loss: 0.04796312
[4/200] Training loss: 0.04402238
[5/200] Training loss: 0.03908263
[6/200] Training loss: 0.03869432
[7/200] Training loss: 0.03753313
[8/200] Training loss: 0.03382054
[9/200] Training loss: 0.03278831
[10/200] Training loss: 0.03078102
[50/200] Training loss: 0.01567952
[100/200] Training loss: 0.01301040
[150/200] Training loss: 0.01144869
[200/200] Training loss: 0.01076580
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.29654065370859906 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17117.41569279662 ----------
[1/200] Training loss: 0.03406401
[2/200] Training loss: 0.00260158
[3/200] Training loss: 0.00242281
[4/200] Training loss: 0.00222466
[5/200] Training loss: 0.00212993
[6/200] Training loss: 0.00209008
[7/200] Training loss: 0.00198502
[8/200] Training loss: 0.00182612
[9/200] Training loss: 0.00168882
[10/200] Training loss: 0.00156916
[50/200] Training loss: 0.00044853
[100/200] Training loss: 0.00026990
[150/200] Training loss: 0.00023894
[200/200] Training loss: 0.00015471
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.6625704538293802 ---num_layers---: 2
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17998.228357257834 ----------
[1/200] Training loss: 0.07954844
[2/200] Training loss: 0.00670124
[3/200] Training loss: 0.00499411
[4/200] Training loss: 0.00462699
[5/200] Training loss: 0.00464921
[6/200] Training loss: 0.00449616
[7/200] Training loss: 0.00405632
[8/200] Training loss: 0.00389148
[9/200] Training loss: 0.00401551
[10/200] Training loss: 0.00364741
[50/200] Training loss: 0.00106483
[100/200] Training loss: 0.00066689
[150/200] Training loss: 0.00049220
[200/200] Training loss: 0.00036420
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7360.87902359494 ----------
[1/200] Training loss: 0.16026337
[2/200] Training loss: 0.06435557
[3/200] Training loss: 0.05875006
[4/200] Training loss: 0.05356333
[5/200] Training loss: 0.05303394
[6/200] Training loss: 0.05040250
[7/200] Training loss: 0.04862582
[8/200] Training loss: 0.04768750
[9/200] Training loss: 0.04630978
[10/200] Training loss: 0.04575692
[50/200] Training loss: 0.02190272
[100/200] Training loss: 0.01685064
[150/200] Training loss: 0.01416136
[200/200] Training loss: 0.01169744
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 32 ---dropout---: 0.5563728816518204 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 33748.75452516729 ----------
[1/200] Training loss: 0.02091285
[2/200] Training loss: 0.00212969
[3/200] Training loss: 0.00161841
[4/200] Training loss: 0.00138036
[5/200] Training loss: 0.00133622
[6/200] Training loss: 0.00100367
[7/200] Training loss: 0.00094595
[8/200] Training loss: 0.00093032
[9/200] Training loss: 0.00077431
[10/200] Training loss: 0.00072237
[50/200] Training loss: 0.00035622
[100/200] Training loss: 0.00021954
[150/200] Training loss: 0.00015548
[200/200] Training loss: 0.00013226
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 3426.9347819881255 ----------
[1/200] Training loss: 0.13311200
[2/200] Training loss: 0.00536923
[3/200] Training loss: 0.00470047
[4/200] Training loss: 0.00435017
[5/200] Training loss: 0.00403615
[6/200] Training loss: 0.00368235
[7/200] Training loss: 0.00379352
[8/200] Training loss: 0.00332460
[9/200] Training loss: 0.00332643
[10/200] Training loss: 0.00286975
[50/200] Training loss: 0.00080632
[100/200] Training loss: 0.00053920
[150/200] Training loss: 0.00050633
[200/200] Training loss: 0.00042704
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12139.627671390914 ----------
[1/200] Training loss: 0.17210547
[2/200] Training loss: 0.06637544
[3/200] Training loss: 0.05612292
[4/200] Training loss: 0.05380126
[5/200] Training loss: 0.05346924
[6/200] Training loss: 0.04728026
[7/200] Training loss: 0.04848493
[8/200] Training loss: 0.08789400
[9/200] Training loss: 0.08530162
[10/200] Training loss: 0.08322241
[50/200] Training loss: 0.04972468
[100/200] Training loss: 0.03696223
[150/200] Training loss: 0.03007872
[200/200] Training loss: 0.02757327
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 24432.717736674324 ----------
[1/200] Training loss: 0.19139536
[2/200] Training loss: 0.00689997
[3/200] Training loss: 0.00481834
[4/200] Training loss: 0.00451103
[5/200] Training loss: 0.00470931
[6/200] Training loss: 0.00433638
[7/200] Training loss: 0.00431156
[8/200] Training loss: 0.00443555
[9/200] Training loss: 0.00447462
[10/200] Training loss: 0.00420514
[50/200] Training loss: 0.00135761
[100/200] Training loss: 0.00068596
[150/200] Training loss: 0.00045667
[200/200] Training loss: 0.00035731
---batch_size---: 32 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.5563728816518204 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 18621.00061758229 ----------
[1/200] Training loss: 0.15014089
[2/200] Training loss: 0.05878288
[3/200] Training loss: 0.05552849
[4/200] Training loss: 0.05295338
[5/200] Training loss: 0.04921106
[6/200] Training loss: 0.04664350
[7/200] Training loss: 0.04298596
[8/200] Training loss: 0.03978356
[9/200] Training loss: 0.04070410
[10/200] Training loss: 0.03791168
[50/200] Training loss: 0.01806655
[100/200] Training loss: 0.01550822
[150/200] Training loss: 0.01290432
[200/200] Training loss: 0.01255806
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8016.8874259278455 ----------
[1/200] Training loss: 0.16829172
[2/200] Training loss: 0.05966475
[3/200] Training loss: 0.05199767
[4/200] Training loss: 0.05008436
[5/200] Training loss: 0.04886452
[6/200] Training loss: 0.04564129
[7/200] Training loss: 0.05183176
[8/200] Training loss: 0.04335045
[9/200] Training loss: 0.04123358
[10/200] Training loss: 0.04069598
[50/200] Training loss: 0.02252245
[100/200] Training loss: 0.01664640
[150/200] Training loss: 0.01434907
[200/200] Training loss: 0.01272361
---batch_size---: 16 ---n_steps---: 2 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13219.731918613175 ----------
[1/200] Training loss: 0.17203315
[2/200] Training loss: 0.06451869
[3/200] Training loss: 0.05116878
[4/200] Training loss: 0.04456866
[5/200] Training loss: 0.03986173
[6/200] Training loss: 0.03732386
[7/200] Training loss: 0.03331009
[8/200] Training loss: 0.03207453
[9/200] Training loss: 0.03194614
[10/200] Training loss: 0.02881644
[50/200] Training loss: 0.01667543
[100/200] Training loss: 0.01455579
[150/200] Training loss: 0.01218327
[200/200] Training loss: 0.01147907
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13892.423258740715 ----------
[1/200] Training loss: 0.17972243
[2/200] Training loss: 0.00442235
[3/200] Training loss: 0.00396526
[4/200] Training loss: 0.00389543
[5/200] Training loss: 0.00346527
[6/200] Training loss: 0.00338884
[7/200] Training loss: 0.00285674
[8/200] Training loss: 0.00262960
[9/200] Training loss: 0.00206336
[10/200] Training loss: 0.00193409
[50/200] Training loss: 0.00067936
[100/200] Training loss: 0.00054683
[150/200] Training loss: 0.00050218
[200/200] Training loss: 0.00041461
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14583.560059189937 ----------
[1/200] Training loss: 0.04642363
[2/200] Training loss: 0.00463327
[3/200] Training loss: 0.00439449
[4/200] Training loss: 0.00390888
[5/200] Training loss: 0.00341381
[6/200] Training loss: 0.00327395
[7/200] Training loss: 0.00356083
[8/200] Training loss: 0.00293104
[9/200] Training loss: 0.00295334
[10/200] Training loss: 0.00312153
[50/200] Training loss: 0.00061987
[100/200] Training loss: 0.00042119
[150/200] Training loss: 0.00034110
[200/200] Training loss: 0.00034165
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11493.440564078277 ----------
[1/200] Training loss: 0.02237286
[2/200] Training loss: 0.00188048
[3/200] Training loss: 0.00169912
[4/200] Training loss: 0.00154621
[5/200] Training loss: 0.00145493
[6/200] Training loss: 0.00133203
[7/200] Training loss: 0.00129882
[8/200] Training loss: 0.00118292
[9/200] Training loss: 0.00116851
[10/200] Training loss: 0.00106124
[50/200] Training loss: 0.00035373
[100/200] Training loss: 0.00025348
[150/200] Training loss: 0.00018081
[200/200] Training loss: 0.00016724
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10561.312797185774 ----------
[1/200] Training loss: 0.03995652
[2/200] Training loss: 0.00295496
[3/200] Training loss: 0.00242553
[4/200] Training loss: 0.00228168
[5/200] Training loss: 0.00207107
[6/200] Training loss: 0.00174565
[7/200] Training loss: 0.00155902
[8/200] Training loss: 0.00150257
[9/200] Training loss: 0.00124596
[10/200] Training loss: 0.00109377
[50/200] Training loss: 0.00029801
[100/200] Training loss: 0.00023748
[150/200] Training loss: 0.00019364
[200/200] Training loss: 0.00017334
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 2
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 18038.031821681656 ----------
[1/200] Training loss: 0.16582790
[2/200] Training loss: 0.05318026
[3/200] Training loss: 0.05157948
[4/200] Training loss: 0.04731521
[5/200] Training loss: 0.04639448
[6/200] Training loss: 0.04122551
[7/200] Training loss: 0.03900301
[8/200] Training loss: 0.03650498
[9/200] Training loss: 0.03576108
[10/200] Training loss: 0.03453367
[50/200] Training loss: 0.01864751
[100/200] Training loss: 0.01562718
[150/200] Training loss: 0.01380441
[200/200] Training loss: 0.01288751
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11169.662841822936 ----------
[1/200] Training loss: 0.02215269
[2/200] Training loss: 0.00241903
[3/200] Training loss: 0.00215210
[4/200] Training loss: 0.00188311
[5/200] Training loss: 0.00169594
[6/200] Training loss: 0.00132701
[7/200] Training loss: 0.00087812
[8/200] Training loss: 0.00067676
[9/200] Training loss: 0.00062456
[10/200] Training loss: 0.00044281
[50/200] Training loss: 0.00017127
[100/200] Training loss: 0.00012940
[150/200] Training loss: 0.00011456
[200/200] Training loss: 0.00010257
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12931.60531411317 ----------
[1/200] Training loss: 0.16052357
[2/200] Training loss: 0.05834072
[3/200] Training loss: 0.05565904
[4/200] Training loss: 0.05222788
[5/200] Training loss: 0.04799453
[6/200] Training loss: 0.04824992
[7/200] Training loss: 0.04438493
[8/200] Training loss: 0.04402240
[9/200] Training loss: 0.04301730
[10/200] Training loss: 0.04172349
[50/200] Training loss: 0.02283847
[100/200] Training loss: 0.01926556
[150/200] Training loss: 0.01563198
[200/200] Training loss: 0.01316677
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7708.068499955096 ----------
[1/200] Training loss: 0.13201699
[2/200] Training loss: 0.05475110
[3/200] Training loss: 0.04975730
[4/200] Training loss: 0.04674767
[5/200] Training loss: 0.04212742
[6/200] Training loss: 0.04417939
[7/200] Training loss: 0.03911024
[8/200] Training loss: 0.03528612
[9/200] Training loss: 0.03478335
[10/200] Training loss: 0.03145405
[50/200] Training loss: 0.01659400
[100/200] Training loss: 0.01471645
[150/200] Training loss: 0.01358118
[200/200] Training loss: 0.01221572
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 32 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13993.455041554247 ----------
[1/200] Training loss: 0.14151796
[2/200] Training loss: 0.05645557
[3/200] Training loss: 0.05330169
[4/200] Training loss: 0.04925955
[5/200] Training loss: 0.04593638
[6/200] Training loss: 0.04514892
[7/200] Training loss: 0.03908232
[8/200] Training loss: 0.03919159
[9/200] Training loss: 0.03675039
[10/200] Training loss: 0.03496796
[50/200] Training loss: 0.01851751
[100/200] Training loss: 0.01426069
[150/200] Training loss: 0.01313667
[200/200] Training loss: 0.01235238
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10002.890782168923 ----------
[1/200] Training loss: 0.03489252
[2/200] Training loss: 0.00303788
[3/200] Training loss: 0.00211432
[4/200] Training loss: 0.00191092
[5/200] Training loss: 0.00187522
[6/200] Training loss: 0.00156141
[7/200] Training loss: 0.00139295
[8/200] Training loss: 0.00106933
[9/200] Training loss: 0.00096720
[10/200] Training loss: 0.00072219
[50/200] Training loss: 0.00028378
[100/200] Training loss: 0.00015364
[150/200] Training loss: 0.00011764
[200/200] Training loss: 0.00010496
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 32 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16511.7645332048 ----------
[1/200] Training loss: 0.02172197
[2/200] Training loss: 0.00192442
[3/200] Training loss: 0.00172195
[4/200] Training loss: 0.00134377
[5/200] Training loss: 0.00116895
[6/200] Training loss: 0.00094330
[7/200] Training loss: 0.00075094
[8/200] Training loss: 0.00068047
[9/200] Training loss: 0.00056297
[10/200] Training loss: 0.00048441
[50/200] Training loss: 0.00020740
[100/200] Training loss: 0.00016740
[150/200] Training loss: 0.00014211
[200/200] Training loss: 0.00012476
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7865.378566858686 ----------
[1/200] Training loss: 0.04346863
[2/200] Training loss: 0.00272325
[3/200] Training loss: 0.00239617
[4/200] Training loss: 0.00204351
[5/200] Training loss: 0.00175164
[6/200] Training loss: 0.00146196
[7/200] Training loss: 0.00132184
[8/200] Training loss: 0.00101892
[9/200] Training loss: 0.00093766
[10/200] Training loss: 0.00078814
[50/200] Training loss: 0.00025463
[100/200] Training loss: 0.00016440
[150/200] Training loss: 0.00013982
[200/200] Training loss: 0.00013019
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15874.984598417726 ----------
[1/200] Training loss: 0.03468283
[2/200] Training loss: 0.00209703
[3/200] Training loss: 0.00194215
[4/200] Training loss: 0.00163956
[5/200] Training loss: 0.00149314
[6/200] Training loss: 0.00133573
[7/200] Training loss: 0.00114926
[8/200] Training loss: 0.00097721
[9/200] Training loss: 0.00080938
[10/200] Training loss: 0.00070475
[50/200] Training loss: 0.00024968
[100/200] Training loss: 0.00018953
[150/200] Training loss: 0.00016086
[200/200] Training loss: 0.00014581
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16706.965254048984 ----------
[1/200] Training loss: 0.04017904
[2/200] Training loss: 0.00218697
[3/200] Training loss: 0.00185576
[4/200] Training loss: 0.00128464
[5/200] Training loss: 0.00114920
[6/200] Training loss: 0.00097417
[7/200] Training loss: 0.00089840
[8/200] Training loss: 0.00076421
[9/200] Training loss: 0.00065034
[10/200] Training loss: 0.00059184
[50/200] Training loss: 0.00026943
[100/200] Training loss: 0.00022567
[150/200] Training loss: 0.00019455
[200/200] Training loss: 0.00017420
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11173.74243483355 ----------
[1/200] Training loss: 0.04733653
[2/200] Training loss: 0.00258866
[3/200] Training loss: 0.00220740
[4/200] Training loss: 0.00199235
[5/200] Training loss: 0.00184565
[6/200] Training loss: 0.00166546
[7/200] Training loss: 0.00150853
[8/200] Training loss: 0.00137693
[9/200] Training loss: 0.00119710
[10/200] Training loss: 0.00107894
[50/200] Training loss: 0.00030719
[100/200] Training loss: 0.00023562
[150/200] Training loss: 0.00019553
[200/200] Training loss: 0.00018360
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8985.044462883865 ----------
[1/200] Training loss: 0.11065257
[2/200] Training loss: 0.05212713
[3/200] Training loss: 0.04710269
[4/200] Training loss: 0.04082124
[5/200] Training loss: 0.03721319
[6/200] Training loss: 0.03237541
[7/200] Training loss: 0.03171259
[8/200] Training loss: 0.02961968
[9/200] Training loss: 0.02740458
[10/200] Training loss: 0.02597641
[50/200] Training loss: 0.01544695
[100/200] Training loss: 0.01334008
[150/200] Training loss: 0.01188124
[200/200] Training loss: 0.01117471
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6397.602676002942 ----------
[1/200] Training loss: 0.04992853
[2/200] Training loss: 0.00522205
[3/200] Training loss: 0.00456240
[4/200] Training loss: 0.00398905
[5/200] Training loss: 0.00350778
[6/200] Training loss: 0.00313979
[7/200] Training loss: 0.00267070
[8/200] Training loss: 0.00226848
[9/200] Training loss: 0.00229164
[10/200] Training loss: 0.00181825
[50/200] Training loss: 0.00070617
[100/200] Training loss: 0.00042770
[150/200] Training loss: 0.00034344
[200/200] Training loss: 0.00032324
---batch_size---: 16 ---n_steps---: 2 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 2
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17767.4524904388 ----------
[1/200] Training loss: 0.14547902
[2/200] Training loss: 0.05810684
[3/200] Training loss: 0.04957887
[4/200] Training loss: 0.04941098
[5/200] Training loss: 0.04749411
[6/200] Training loss: 0.04378047
[7/200] Training loss: 0.04099681
[8/200] Training loss: 0.04008456
[9/200] Training loss: 0.03673475
[10/200] Training loss: 0.03383032
[50/200] Training loss: 0.01782323
[100/200] Training loss: 0.01566875
[150/200] Training loss: 0.01313520
[200/200] Training loss: 0.01147167
---batch_size---: 16 ---n_steps---: 2 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.29654065370859906 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13485.858667507975 ----------
[1/200] Training loss: 0.06409763
[2/200] Training loss: 0.00226657
[3/200] Training loss: 0.00207756
[4/200] Training loss: 0.00195196
[5/200] Training loss: 0.00189414
[6/200] Training loss: 0.00176230
[7/200] Training loss: 0.00157453
[8/200] Training loss: 0.00141993
[9/200] Training loss: 0.00122980
[10/200] Training loss: 0.00110499
[50/200] Training loss: 0.00032539
[100/200] Training loss: 0.00022807
[150/200] Training loss: 0.00020299
[200/200] Training loss: 0.00016955
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10806.235977434511 ----------
[1/200] Training loss: 0.03188964
[2/200] Training loss: 0.00498136
[3/200] Training loss: 0.00425415
[4/200] Training loss: 0.00348025
[5/200] Training loss: 0.00298870
[6/200] Training loss: 0.00267645
[7/200] Training loss: 0.00263437
[8/200] Training loss: 0.00225824
[9/200] Training loss: 0.00229409
[10/200] Training loss: 0.00174024
[50/200] Training loss: 0.00051131
[100/200] Training loss: 0.00036120
[150/200] Training loss: 0.00028376
[200/200] Training loss: 0.00024171
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14053.341239719472 ----------
[1/200] Training loss: 0.09260301
[2/200] Training loss: 0.05215370
[3/200] Training loss: 0.04611722
[4/200] Training loss: 0.04141644
[5/200] Training loss: 0.03741889
[6/200] Training loss: 0.03471372
[7/200] Training loss: 0.03210972
[8/200] Training loss: 0.02870853
[9/200] Training loss: 0.02854510
[10/200] Training loss: 0.02596297
[50/200] Training loss: 0.01563306
[100/200] Training loss: 0.01342476
[150/200] Training loss: 0.01229261
[200/200] Training loss: 0.01184128
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6154.193367127816 ----------
[1/200] Training loss: 0.15648894
[2/200] Training loss: 0.06146001
[3/200] Training loss: 0.05447215
[4/200] Training loss: 0.05159474
[5/200] Training loss: 0.04906483
[6/200] Training loss: 0.04512489
[7/200] Training loss: 0.04365017
[8/200] Training loss: 0.04202764
[9/200] Training loss: 0.04222658
[10/200] Training loss: 0.03843047
[50/200] Training loss: 0.01856966
[100/200] Training loss: 0.01520223
[150/200] Training loss: 0.01408661
[200/200] Training loss: 0.01241709
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10550.331558771033 ----------
[1/200] Training loss: 0.16179217
[2/200] Training loss: 0.00476480
[3/200] Training loss: 0.00479594
[4/200] Training loss: 0.00364660
[5/200] Training loss: 0.00346843
[6/200] Training loss: 0.00320119
[7/200] Training loss: 0.00291237
[8/200] Training loss: 0.00272731
[9/200] Training loss: 0.00248026
[10/200] Training loss: 0.00215286
[50/200] Training loss: 0.00075920
[100/200] Training loss: 0.00053063
[150/200] Training loss: 0.00042636
[200/200] Training loss: 0.00039247
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11566.863360479365 ----------
[1/200] Training loss: 0.07743848
[2/200] Training loss: 0.00430436
[3/200] Training loss: 0.00375416
[4/200] Training loss: 0.00344877
[5/200] Training loss: 0.00297666
[6/200] Training loss: 0.00260939
[7/200] Training loss: 0.00230810
[8/200] Training loss: 0.00202467
[9/200] Training loss: 0.00178902
[10/200] Training loss: 0.00150538
[50/200] Training loss: 0.00061859
[100/200] Training loss: 0.00045032
[150/200] Training loss: 0.00037458
[200/200] Training loss: 0.00031447
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14892.849290850962 ----------
[1/200] Training loss: 0.04247044
[2/200] Training loss: 0.00299351
[3/200] Training loss: 0.00255138
[4/200] Training loss: 0.00225567
[5/200] Training loss: 0.00228116
[6/200] Training loss: 0.00198340
[7/200] Training loss: 0.00195272
[8/200] Training loss: 0.00172833
[9/200] Training loss: 0.00159322
[10/200] Training loss: 0.00138828
[50/200] Training loss: 0.00029942
[100/200] Training loss: 0.00024101
[150/200] Training loss: 0.00019769
[200/200] Training loss: 0.00017037
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11929.699409457055 ----------
[1/200] Training loss: 0.03956489
[2/200] Training loss: 0.00265189
[3/200] Training loss: 0.00208495
[4/200] Training loss: 0.00183687
[5/200] Training loss: 0.00158063
[6/200] Training loss: 0.00136047
[7/200] Training loss: 0.00115537
[8/200] Training loss: 0.00100572
[9/200] Training loss: 0.00099358
[10/200] Training loss: 0.00082783
[50/200] Training loss: 0.00027528
[100/200] Training loss: 0.00022436
[150/200] Training loss: 0.00018369
[200/200] Training loss: 0.00016245
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13807.87427520978 ----------
[1/200] Training loss: 0.05945985
[2/200] Training loss: 0.00511005
[3/200] Training loss: 0.00412433
[4/200] Training loss: 0.00468123
[5/200] Training loss: 0.00405328
[6/200] Training loss: 0.00359068
[7/200] Training loss: 0.00334219
[8/200] Training loss: 0.00311603
[9/200] Training loss: 0.00280605
[10/200] Training loss: 0.00282594
[50/200] Training loss: 0.00069571
[100/200] Training loss: 0.00047774
[150/200] Training loss: 0.00034923
[200/200] Training loss: 0.00032639
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13391.67861024151 ----------
[1/200] Training loss: 0.10595083
[2/200] Training loss: 0.05684259
[3/200] Training loss: 0.04797275
[4/200] Training loss: 0.04147852
[5/200] Training loss: 0.03429080
[6/200] Training loss: 0.02958632
[7/200] Training loss: 0.02878364
[8/200] Training loss: 0.02675734
[9/200] Training loss: 0.02500740
[10/200] Training loss: 0.02309637
[50/200] Training loss: 0.01600702
[100/200] Training loss: 0.01433619
[150/200] Training loss: 0.01284903
[200/200] Training loss: 0.01163768
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8408.468588274562 ----------
[1/200] Training loss: 0.12387435
[2/200] Training loss: 0.05644367
[3/200] Training loss: 0.05331608
[4/200] Training loss: 0.04714428
[5/200] Training loss: 0.04726400
[6/200] Training loss: 0.04288322
[7/200] Training loss: 0.04189002
[8/200] Training loss: 0.04020812
[9/200] Training loss: 0.03734435
[10/200] Training loss: 0.03743283
[50/200] Training loss: 0.02014869
[100/200] Training loss: 0.01580494
[150/200] Training loss: 0.01262693
[200/200] Training loss: 0.01069602
---batch_size---: 8 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 32 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7909.717820504092 ----------
[1/200] Training loss: 0.02431462
[2/200] Training loss: 0.00227335
[3/200] Training loss: 0.00171737
[4/200] Training loss: 0.00153339
[5/200] Training loss: 0.00137583
[6/200] Training loss: 0.00113810
[7/200] Training loss: 0.00108302
[8/200] Training loss: 0.00090828
[9/200] Training loss: 0.00082870
[10/200] Training loss: 0.00069508
[50/200] Training loss: 0.00028836
[100/200] Training loss: 0.00021440
[150/200] Training loss: 0.00018593
[200/200] Training loss: 0.00015447
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14410.541141816986 ----------
[1/200] Training loss: 0.01931331
[2/200] Training loss: 0.00194239
[3/200] Training loss: 0.00141152
[4/200] Training loss: 0.00114031
[5/200] Training loss: 0.00084657
[6/200] Training loss: 0.00083234
[7/200] Training loss: 0.00062959
[8/200] Training loss: 0.00052433
[9/200] Training loss: 0.00048539
[10/200] Training loss: 0.00050547
[50/200] Training loss: 0.00022443
[100/200] Training loss: 0.00015780
[150/200] Training loss: 0.00013873
[200/200] Training loss: 0.00012536
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13706.615628958156 ----------
[1/200] Training loss: 0.14975059
[2/200] Training loss: 0.00582478
[3/200] Training loss: 0.00465158
[4/200] Training loss: 0.00444789
[5/200] Training loss: 0.00425974
[6/200] Training loss: 0.00405598
[7/200] Training loss: 0.00351584
[8/200] Training loss: 0.00328905
[9/200] Training loss: 0.00337113
[10/200] Training loss: 0.00266041
[50/200] Training loss: 0.00066820
[100/200] Training loss: 0.00050183
[150/200] Training loss: 0.00043912
[200/200] Training loss: 0.00036342
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10242.02011323938 ----------
[1/200] Training loss: 0.08817759
[2/200] Training loss: 0.00436013
[3/200] Training loss: 0.00400897
[4/200] Training loss: 0.00397296
[5/200] Training loss: 0.00391362
[6/200] Training loss: 0.00350312
[7/200] Training loss: 0.00297818
[8/200] Training loss: 0.00297721
[9/200] Training loss: 0.00269795
[10/200] Training loss: 0.00272350
[50/200] Training loss: 0.00069635
[100/200] Training loss: 0.00052002
[150/200] Training loss: 0.00040478
[200/200] Training loss: 0.00031689
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15507.219479971256 ----------
[1/200] Training loss: 0.02220471
[2/200] Training loss: 0.00193193
[3/200] Training loss: 0.00150257
[4/200] Training loss: 0.00119160
[5/200] Training loss: 0.00091603
[6/200] Training loss: 0.00073391
[7/200] Training loss: 0.00062360
[8/200] Training loss: 0.00050199
[9/200] Training loss: 0.00047062
[10/200] Training loss: 0.00046795
[50/200] Training loss: 0.00019089
[100/200] Training loss: 0.00014455
[150/200] Training loss: 0.00012186
[200/200] Training loss: 0.00011763
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8481.132471551191 ----------
[1/200] Training loss: 0.16916867
[2/200] Training loss: 0.06155189
[3/200] Training loss: 0.05281713
[4/200] Training loss: 0.05146161
[5/200] Training loss: 0.04996089
[6/200] Training loss: 0.04610479
[7/200] Training loss: 0.04268891
[8/200] Training loss: 0.03967084
[9/200] Training loss: 0.03651254
[10/200] Training loss: 0.03419059
[50/200] Training loss: 0.01762353
[100/200] Training loss: 0.01543684
[150/200] Training loss: 0.01363406
[200/200] Training loss: 0.01289450
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17567.25317173974 ----------
[1/200] Training loss: 0.14813663
[2/200] Training loss: 0.05529037
[3/200] Training loss: 0.05464690
[4/200] Training loss: 0.05168582
[5/200] Training loss: 0.05036343
[6/200] Training loss: 0.04587058
[7/200] Training loss: 0.04361468
[8/200] Training loss: 0.04366888
[9/200] Training loss: 0.04035578
[10/200] Training loss: 0.03868291
[50/200] Training loss: 0.01900612
[100/200] Training loss: 0.01670683
[150/200] Training loss: 0.01487408
[200/200] Training loss: 0.01366576
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5103.729028857234 ----------
[1/200] Training loss: 0.05249030
[2/200] Training loss: 0.00484644
[3/200] Training loss: 0.00481197
[4/200] Training loss: 0.00460132
[5/200] Training loss: 0.00438032
[6/200] Training loss: 0.00404371
[7/200] Training loss: 0.00377630
[8/200] Training loss: 0.00359537
[9/200] Training loss: 0.00347001
[10/200] Training loss: 0.00326715
[50/200] Training loss: 0.00054239
[100/200] Training loss: 0.00038370
[150/200] Training loss: 0.00029072
[200/200] Training loss: 0.00027001
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16655.479338644083 ----------
[1/200] Training loss: 0.21703665
[2/200] Training loss: 0.07152083
[3/200] Training loss: 0.06504839
[4/200] Training loss: 0.05611491
[5/200] Training loss: 0.05383374
[6/200] Training loss: 0.05363531
[7/200] Training loss: 0.05153191
[8/200] Training loss: 0.05022264
[9/200] Training loss: 0.05067289
[10/200] Training loss: 0.04841159
[50/200] Training loss: 0.02415265
[100/200] Training loss: 0.01644155
[150/200] Training loss: 0.01420174
[200/200] Training loss: 0.01434501
---batch_size---: 32 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14650.130920916714 ----------
[1/200] Training loss: 0.26271667
[2/200] Training loss: 0.06534096
[3/200] Training loss: 0.05822314
[4/200] Training loss: 0.05798729
[5/200] Training loss: 0.05502868
[6/200] Training loss: 0.05220335
[7/200] Training loss: 0.05238823
[8/200] Training loss: 0.05254602
[9/200] Training loss: 0.04956419
[10/200] Training loss: 0.04926204
[50/200] Training loss: 0.02420460
[100/200] Training loss: 0.01660598
[150/200] Training loss: 0.01580926
[200/200] Training loss: 0.01435469
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16520.105568669955 ----------
[1/200] Training loss: 0.13234774
[2/200] Training loss: 0.00467136
[3/200] Training loss: 0.00386315
[4/200] Training loss: 0.00376590
[5/200] Training loss: 0.00350849
[6/200] Training loss: 0.00322901
[7/200] Training loss: 0.00316201
[8/200] Training loss: 0.00281668
[9/200] Training loss: 0.00267947
[10/200] Training loss: 0.00249598
[50/200] Training loss: 0.00063884
[100/200] Training loss: 0.00041128
[150/200] Training loss: 0.00024792
[200/200] Training loss: 0.00020575
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 32 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9419.870912066683 ----------
[1/200] Training loss: 0.10755351
[2/200] Training loss: 0.00548735
[3/200] Training loss: 0.00464533
[4/200] Training loss: 0.00476460
[5/200] Training loss: 0.00450486
[6/200] Training loss: 0.00423242
[7/200] Training loss: 0.00378543
[8/200] Training loss: 0.00345351
[9/200] Training loss: 0.00334673
[10/200] Training loss: 0.00292325
[50/200] Training loss: 0.00056964
[100/200] Training loss: 0.00038127
[150/200] Training loss: 0.00034434
[200/200] Training loss: 0.00031580
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12073.104654561725 ----------
[1/200] Training loss: 0.02074287
[2/200] Training loss: 0.00209708
[3/200] Training loss: 0.00171181
[4/200] Training loss: 0.00140210
[5/200] Training loss: 0.00111727
[6/200] Training loss: 0.00082311
[7/200] Training loss: 0.00073406
[8/200] Training loss: 0.00057469
[9/200] Training loss: 0.00047657
[10/200] Training loss: 0.00044066
[50/200] Training loss: 0.00021153
[100/200] Training loss: 0.00015279
[150/200] Training loss: 0.00012832
[200/200] Training loss: 0.00011878
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10615.002967498407 ----------
[1/200] Training loss: 0.01780285
[2/200] Training loss: 0.00252613
[3/200] Training loss: 0.00234073
[4/200] Training loss: 0.00225902
[5/200] Training loss: 0.00209026
[6/200] Training loss: 0.00194047
[7/200] Training loss: 0.00186606
[8/200] Training loss: 0.00166276
[9/200] Training loss: 0.00143630
[10/200] Training loss: 0.00140804
[50/200] Training loss: 0.00025527
[100/200] Training loss: 0.00019502
[150/200] Training loss: 0.00014936
[200/200] Training loss: 0.00013103
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 32 ---dropout---: 0.2303155250222492 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12673.452568262526 ----------
[1/200] Training loss: 0.10542251
[2/200] Training loss: 0.05452042
[3/200] Training loss: 0.05155520
[4/200] Training loss: 0.04893176
[5/200] Training loss: 0.04684677
[6/200] Training loss: 0.04166939
[7/200] Training loss: 0.03799358
[8/200] Training loss: 0.03336976
[9/200] Training loss: 0.02965783
[10/200] Training loss: 0.03103690
[50/200] Training loss: 0.01421019
[100/200] Training loss: 0.01180419
[150/200] Training loss: 0.01036278
[200/200] Training loss: 0.00979600
---batch_size---: 8 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9550.440408693203 ----------
[1/200] Training loss: 0.10084817
[2/200] Training loss: 0.05220868
[3/200] Training loss: 0.04807579
[4/200] Training loss: 0.04321862
[5/200] Training loss: 0.03897433
[6/200] Training loss: 0.03849821
[7/200] Training loss: 0.03522479
[8/200] Training loss: 0.03403441
[9/200] Training loss: 0.03025622
[10/200] Training loss: 0.02938591
[50/200] Training loss: 0.01646055
[100/200] Training loss: 0.01302304
[150/200] Training loss: 0.01176135
[200/200] Training loss: 0.01100932
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8809.705102896463 ----------
[1/200] Training loss: 0.01472643
[2/200] Training loss: 0.00213746
[3/200] Training loss: 0.00188264
[4/200] Training loss: 0.00166249
[5/200] Training loss: 0.00159370
[6/200] Training loss: 0.00127768
[7/200] Training loss: 0.00114420
[8/200] Training loss: 0.00101798
[9/200] Training loss: 0.00091563
[10/200] Training loss: 0.00084727
[50/200] Training loss: 0.00022525
[100/200] Training loss: 0.00016605
[150/200] Training loss: 0.00014645
[200/200] Training loss: 0.00012648
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12896.106078968178 ----------
[1/200] Training loss: 0.15045655
[2/200] Training loss: 0.06001153
[3/200] Training loss: 0.05517548
[4/200] Training loss: 0.05091166
[5/200] Training loss: 0.04800829
[6/200] Training loss: 0.04350137
[7/200] Training loss: 0.04184314
[8/200] Training loss: 0.03919466
[9/200] Training loss: 0.03632076
[10/200] Training loss: 0.03446347
[50/200] Training loss: 0.01875797
[100/200] Training loss: 0.01574049
[150/200] Training loss: 0.01478557
[200/200] Training loss: 0.01350786
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 32 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6869.05204522429 ----------
[1/200] Training loss: 0.12167510
[2/200] Training loss: 0.00568973
[3/200] Training loss: 0.00490675
[4/200] Training loss: 0.00451227
[5/200] Training loss: 0.00435942
[6/200] Training loss: 0.00409825
[7/200] Training loss: 0.00380021
[8/200] Training loss: 0.00359226
[9/200] Training loss: 0.00344455
[10/200] Training loss: 0.00322467
[50/200] Training loss: 0.00080555
[100/200] Training loss: 0.00035873
[150/200] Training loss: 0.00025590
[200/200] Training loss: 0.00019868
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 3429.165350344016 ----------
[1/200] Training loss: 0.14635272
[2/200] Training loss: 0.06190447
[3/200] Training loss: 0.05600338
[4/200] Training loss: 0.05178150
[5/200] Training loss: 0.04706682
[6/200] Training loss: 0.04750725
[7/200] Training loss: 0.04397774
[8/200] Training loss: 0.04188690
[9/200] Training loss: 0.03980522
[10/200] Training loss: 0.03643671
[50/200] Training loss: 0.01585254
[100/200] Training loss: 0.01410564
[150/200] Training loss: 0.01291363
[200/200] Training loss: 0.01224905
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11876.797548160868 ----------
[1/200] Training loss: 0.13225670
[2/200] Training loss: 0.00584726
[3/200] Training loss: 0.00424333
[4/200] Training loss: 0.00398556
[5/200] Training loss: 0.00373923
[6/200] Training loss: 0.00332695
[7/200] Training loss: 0.00304947
[8/200] Training loss: 0.00280771
[9/200] Training loss: 0.00246865
[10/200] Training loss: 0.00235120
[50/200] Training loss: 0.00076140
[100/200] Training loss: 0.00060638
[150/200] Training loss: 0.00037204
[200/200] Training loss: 0.00026967
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 32 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8872.133001708215 ----------
[1/200] Training loss: 0.11836622
[2/200] Training loss: 0.05441270
[3/200] Training loss: 0.04833515
[4/200] Training loss: 0.04605319
[5/200] Training loss: 0.04223369
[6/200] Training loss: 0.04015597
[7/200] Training loss: 0.03456215
[8/200] Training loss: 0.03288914
[9/200] Training loss: 0.02865613
[10/200] Training loss: 0.02640627
[50/200] Training loss: 0.01653744
[100/200] Training loss: 0.01413585
[150/200] Training loss: 0.01362286
[200/200] Training loss: 0.01243294
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8489.621899707901 ----------
[1/200] Training loss: 0.11873908
[2/200] Training loss: 0.05343730
[3/200] Training loss: 0.04576887
[4/200] Training loss: 0.04279301
[5/200] Training loss: 0.03904941
[6/200] Training loss: 0.03899390
[7/200] Training loss: 0.03520596
[8/200] Training loss: 0.03541597
[9/200] Training loss: 0.03242020
[10/200] Training loss: 0.03134707
[50/200] Training loss: 0.01722076
[100/200] Training loss: 0.01500999
[150/200] Training loss: 0.01385559
[200/200] Training loss: 0.01273127
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6525.25003352362 ----------
[1/200] Training loss: 0.10345548
[2/200] Training loss: 0.05088842
[3/200] Training loss: 0.04446328
[4/200] Training loss: 0.03942315
[5/200] Training loss: 0.03677253
[6/200] Training loss: 0.03141245
[7/200] Training loss: 0.02892221
[8/200] Training loss: 0.02640820
[9/200] Training loss: 0.02601277
[10/200] Training loss: 0.02352026
[50/200] Training loss: 0.01760311
[100/200] Training loss: 0.01472933
[150/200] Training loss: 0.01355507
[200/200] Training loss: 0.01185248
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7849.433610140289 ----------
[1/200] Training loss: 0.15507424
[2/200] Training loss: 0.06085694
[3/200] Training loss: 0.05568426
[4/200] Training loss: 0.05519727
[5/200] Training loss: 0.05126596
[6/200] Training loss: 0.04895520
[7/200] Training loss: 0.04968091
[8/200] Training loss: 0.04879147
[9/200] Training loss: 0.04764140
[10/200] Training loss: 0.04653845
[50/200] Training loss: 0.01840581
[100/200] Training loss: 0.01359567
[150/200] Training loss: 0.01345412
[200/200] Training loss: 0.01184707
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 32 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 18789.63458931546 ----------
[1/200] Training loss: 0.10321762
[2/200] Training loss: 0.05186084
[3/200] Training loss: 0.04581094
[4/200] Training loss: 0.04045673
[5/200] Training loss: 0.03744326
[6/200] Training loss: 0.03432369
[7/200] Training loss: 0.03129857
[8/200] Training loss: 0.02895392
[9/200] Training loss: 0.02807782
[10/200] Training loss: 0.02626909
[50/200] Training loss: 0.01700599
[100/200] Training loss: 0.01499828
[150/200] Training loss: 0.01378295
[200/200] Training loss: 0.01264933
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11794.48074312727 ----------
[1/200] Training loss: 0.07470216
[2/200] Training loss: 0.00275418
[3/200] Training loss: 0.00255268
[4/200] Training loss: 0.00230241
[5/200] Training loss: 0.00225297
[6/200] Training loss: 0.00225202
[7/200] Training loss: 0.00207730
[8/200] Training loss: 0.00191489
[9/200] Training loss: 0.00179119
[10/200] Training loss: 0.00175741
[50/200] Training loss: 0.00035274
[100/200] Training loss: 0.00024275
[150/200] Training loss: 0.00020289
[200/200] Training loss: 0.00016523
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8409.722944306786 ----------
[1/200] Training loss: 0.17360215
[2/200] Training loss: 0.00599919
[3/200] Training loss: 0.00535205
[4/200] Training loss: 0.00600171
[5/200] Training loss: 0.00501796
[6/200] Training loss: 0.00456955
[7/200] Training loss: 0.00459150
[8/200] Training loss: 0.00407111
[9/200] Training loss: 0.00411495
[10/200] Training loss: 0.00366265
[50/200] Training loss: 0.00081933
[100/200] Training loss: 0.00052192
[150/200] Training loss: 0.00047988
[200/200] Training loss: 0.00042231
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12192.257871288648 ----------
[1/200] Training loss: 0.16329751
[2/200] Training loss: 0.05787947
[3/200] Training loss: 0.05103029
[4/200] Training loss: 0.04954909
[5/200] Training loss: 0.05029852
[6/200] Training loss: 0.04591597
[7/200] Training loss: 0.04584147
[8/200] Training loss: 0.04388697
[9/200] Training loss: 0.03813926
[10/200] Training loss: 0.03958140
[50/200] Training loss: 0.01912793
[100/200] Training loss: 0.01551443
[150/200] Training loss: 0.01466122
[200/200] Training loss: 0.01342476
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8817.483994881986 ----------
[1/200] Training loss: 0.10020092
[2/200] Training loss: 0.05089318
[3/200] Training loss: 0.04540113
[4/200] Training loss: 0.03985686
[5/200] Training loss: 0.03716279
[6/200] Training loss: 0.03401916
[7/200] Training loss: 0.03152052
[8/200] Training loss: 0.03006212
[9/200] Training loss: 0.02798723
[10/200] Training loss: 0.02710712
[50/200] Training loss: 0.01742017
[100/200] Training loss: 0.01446435
[150/200] Training loss: 0.01259782
[200/200] Training loss: 0.01197509
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4381.588981180229 ----------
[1/200] Training loss: 0.01911046
[2/200] Training loss: 0.00234044
[3/200] Training loss: 0.00201005
[4/200] Training loss: 0.00178433
[5/200] Training loss: 0.00156126
[6/200] Training loss: 0.00128373
[7/200] Training loss: 0.00111211
[8/200] Training loss: 0.00085810
[9/200] Training loss: 0.00077809
[10/200] Training loss: 0.00063844
[50/200] Training loss: 0.00021445
[100/200] Training loss: 0.00015514
[150/200] Training loss: 0.00012519
[200/200] Training loss: 0.00011712
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4614.022973501541 ----------
[1/200] Training loss: 0.01955888
[2/200] Training loss: 0.00195537
[3/200] Training loss: 0.00155511
[4/200] Training loss: 0.00139362
[5/200] Training loss: 0.00127571
[6/200] Training loss: 0.00115904
[7/200] Training loss: 0.00101350
[8/200] Training loss: 0.00093449
[9/200] Training loss: 0.00082483
[10/200] Training loss: 0.00075059
[50/200] Training loss: 0.00027649
[100/200] Training loss: 0.00022006
[150/200] Training loss: 0.00018085
[200/200] Training loss: 0.00015361
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11260.146357841002 ----------
[1/200] Training loss: 0.10310609
[2/200] Training loss: 0.05236198
[3/200] Training loss: 0.04736489
[4/200] Training loss: 0.04256790
[5/200] Training loss: 0.04120658
[6/200] Training loss: 0.03521757
[7/200] Training loss: 0.03326905
[8/200] Training loss: 0.03018843
[9/200] Training loss: 0.02868963
[10/200] Training loss: 0.02739620
[50/200] Training loss: 0.01694933
[100/200] Training loss: 0.01435060
[150/200] Training loss: 0.01272704
[200/200] Training loss: 0.01230433
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5526.921023499432 ----------
[1/200] Training loss: 0.10231505
[2/200] Training loss: 0.05420824
[3/200] Training loss: 0.05031089
[4/200] Training loss: 0.04770494
[5/200] Training loss: 0.04253744
[6/200] Training loss: 0.03857364
[7/200] Training loss: 0.03482795
[8/200] Training loss: 0.03199124
[9/200] Training loss: 0.02999838
[10/200] Training loss: 0.02726578
[50/200] Training loss: 0.01783430
[100/200] Training loss: 0.01615941
[150/200] Training loss: 0.01480051
[200/200] Training loss: 0.01362561
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11460.041884740212 ----------
[1/200] Training loss: 0.14790744
[2/200] Training loss: 0.05870441
[3/200] Training loss: 0.05180654
[4/200] Training loss: 0.04828589
[5/200] Training loss: 0.04655993
[6/200] Training loss: 0.04620228
[7/200] Training loss: 0.04389547
[8/200] Training loss: 0.03739940
[9/200] Training loss: 0.03905203
[10/200] Training loss: 0.03510773
[50/200] Training loss: 0.01964942
[100/200] Training loss: 0.01580686
[150/200] Training loss: 0.01375876
[200/200] Training loss: 0.01270657
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8019.19996009577 ----------
[1/200] Training loss: 0.05201117
[2/200] Training loss: 0.00419904
[3/200] Training loss: 0.00359432
[4/200] Training loss: 0.00309900
[5/200] Training loss: 0.00282987
[6/200] Training loss: 0.00245732
[7/200] Training loss: 0.00210924
[8/200] Training loss: 0.00178234
[9/200] Training loss: 0.00151273
[10/200] Training loss: 0.00145640
[50/200] Training loss: 0.00047378
[100/200] Training loss: 0.00035201
[150/200] Training loss: 0.00029311
[200/200] Training loss: 0.00025293
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10151.503533959883 ----------
[1/200] Training loss: 0.28927152
[2/200] Training loss: 0.07164697
[3/200] Training loss: 0.05760151
[4/200] Training loss: 0.05384118
[5/200] Training loss: 0.05383926
[6/200] Training loss: 0.05242082
[7/200] Training loss: 0.04870189
[8/200] Training loss: 0.04626803
[9/200] Training loss: 0.04392728
[10/200] Training loss: 0.04694793
[50/200] Training loss: 0.02352592
[100/200] Training loss: 0.01781457
[150/200] Training loss: 0.01462600
[200/200] Training loss: 0.01376641
---batch_size---: 32 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8309.124141568713 ----------
[1/200] Training loss: 0.10885988
[2/200] Training loss: 0.05359399
[3/200] Training loss: 0.04852630
[4/200] Training loss: 0.04652708
[5/200] Training loss: 0.04221376
[6/200] Training loss: 0.03933012
[7/200] Training loss: 0.03644001
[8/200] Training loss: 0.03155786
[9/200] Training loss: 0.03019979
[10/200] Training loss: 0.02699898
[50/200] Training loss: 0.01503347
[100/200] Training loss: 0.01139995
[150/200] Training loss: 0.01043815
[200/200] Training loss: 0.00912850
---batch_size---: 8 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8240.171357441543 ----------
[1/200] Training loss: 0.15630612
[2/200] Training loss: 0.06353055
[3/200] Training loss: 0.05695506
[4/200] Training loss: 0.05262730
[5/200] Training loss: 0.04812406
[6/200] Training loss: 0.04660896
[7/200] Training loss: 0.04326327
[8/200] Training loss: 0.03895255
[9/200] Training loss: 0.03688105
[10/200] Training loss: 0.03709118
[50/200] Training loss: 0.01917203
[100/200] Training loss: 0.01559249
[150/200] Training loss: 0.01340108
[200/200] Training loss: 0.01317359
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 32 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15622.610793334128 ----------
[1/200] Training loss: 0.15972464
[2/200] Training loss: 0.05558513
[3/200] Training loss: 0.04844509
[4/200] Training loss: 0.04911492
[5/200] Training loss: 0.04296000
[6/200] Training loss: 0.03984695
[7/200] Training loss: 0.03978986
[8/200] Training loss: 0.03773920
[9/200] Training loss: 0.03445733
[10/200] Training loss: 0.03284855
[50/200] Training loss: 0.01807837
[100/200] Training loss: 0.01545648
[150/200] Training loss: 0.01457497
[200/200] Training loss: 0.01298538
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9618.37699406714 ----------
[1/200] Training loss: 0.14905948
[2/200] Training loss: 0.05646605
[3/200] Training loss: 0.05093765
[4/200] Training loss: 0.05034433
[5/200] Training loss: 0.04941886
[6/200] Training loss: 0.04508225
[7/200] Training loss: 0.04326276
[8/200] Training loss: 0.03957586
[9/200] Training loss: 0.03639935
[10/200] Training loss: 0.03814627
[50/200] Training loss: 0.01740696
[100/200] Training loss: 0.01365825
[150/200] Training loss: 0.01251635
[200/200] Training loss: 0.01170673
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 32 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9955.236611954535 ----------
[1/200] Training loss: 0.05519519
[2/200] Training loss: 0.00419630
[3/200] Training loss: 0.00264896
[4/200] Training loss: 0.00250741
[5/200] Training loss: 0.00229918
[6/200] Training loss: 0.00214167
[7/200] Training loss: 0.00206009
[8/200] Training loss: 0.00192601
[9/200] Training loss: 0.00185445
[10/200] Training loss: 0.00166498
[50/200] Training loss: 0.00028508
[100/200] Training loss: 0.00020360
[150/200] Training loss: 0.00017882
[200/200] Training loss: 0.00016400
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7678.1870255939975 ----------
[1/200] Training loss: 0.22133204
[2/200] Training loss: 0.06768640
[3/200] Training loss: 0.06148041
[4/200] Training loss: 0.05047927
[5/200] Training loss: 0.04748626
[6/200] Training loss: 0.04343466
[7/200] Training loss: 0.04276112
[8/200] Training loss: 0.04646561
[9/200] Training loss: 0.03883975
[10/200] Training loss: 0.03633752
[50/200] Training loss: 0.01871800
[100/200] Training loss: 0.01660473
[150/200] Training loss: 0.01427133
[200/200] Training loss: 0.01323155
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 32 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10922.310012080778 ----------
[1/200] Training loss: 0.15111650
[2/200] Training loss: 0.05602138
[3/200] Training loss: 0.05605257
[4/200] Training loss: 0.05205558
[5/200] Training loss: 0.05014791
[6/200] Training loss: 0.04634908
[7/200] Training loss: 0.04678128
[8/200] Training loss: 0.04298573
[9/200] Training loss: 0.04304571
[10/200] Training loss: 0.04178221
[50/200] Training loss: 0.01663474
[100/200] Training loss: 0.01376915
[150/200] Training loss: 0.01226321
[200/200] Training loss: 0.01123219
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8319.010518084468 ----------
[1/200] Training loss: 0.04083153
[2/200] Training loss: 0.00455072
[3/200] Training loss: 0.00366852
[4/200] Training loss: 0.00301075
[5/200] Training loss: 0.00236729
[6/200] Training loss: 0.00189201
[7/200] Training loss: 0.00153377
[8/200] Training loss: 0.00130952
[9/200] Training loss: 0.00106334
[10/200] Training loss: 0.00101742
[50/200] Training loss: 0.00047177
[100/200] Training loss: 0.00032454
[150/200] Training loss: 0.00027278
[200/200] Training loss: 0.00022431
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7682.922881299799 ----------
[1/200] Training loss: 0.08402590
[2/200] Training loss: 0.00652123
[3/200] Training loss: 0.00477387
[4/200] Training loss: 0.00417879
[5/200] Training loss: 0.00398310
[6/200] Training loss: 0.00358482
[7/200] Training loss: 0.00335758
[8/200] Training loss: 0.00301457
[9/200] Training loss: 0.00269064
[10/200] Training loss: 0.00226496
[50/200] Training loss: 0.00061986
[100/200] Training loss: 0.00034352
[150/200] Training loss: 0.00025063
[200/200] Training loss: 0.00021165
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16056.946160462767 ----------
[1/200] Training loss: 0.10414061
[2/200] Training loss: 0.05301024
[3/200] Training loss: 0.04657721
[4/200] Training loss: 0.04147742
[5/200] Training loss: 0.03810215
[6/200] Training loss: 0.03477122
[7/200] Training loss: 0.03105372
[8/200] Training loss: 0.03083060
[9/200] Training loss: 0.02693851
[10/200] Training loss: 0.02634903
[50/200] Training loss: 0.01554165
[100/200] Training loss: 0.01258945
[150/200] Training loss: 0.01142661
[200/200] Training loss: 0.01095579
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9261.152844003818 ----------
[1/200] Training loss: 0.11228137
[2/200] Training loss: 0.05577516
[3/200] Training loss: 0.04857901
[4/200] Training loss: 0.04482024
[5/200] Training loss: 0.04058076
[6/200] Training loss: 0.03661415
[7/200] Training loss: 0.03181708
[8/200] Training loss: 0.03241214
[9/200] Training loss: 0.02792693
[10/200] Training loss: 0.02799515
[50/200] Training loss: 0.01523649
[100/200] Training loss: 0.01280084
[150/200] Training loss: 0.01153385
[200/200] Training loss: 0.01040646
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14869.587754877402 ----------
[1/200] Training loss: 0.14751817
[2/200] Training loss: 0.00601924
[3/200] Training loss: 0.00575950
[4/200] Training loss: 0.00516557
[5/200] Training loss: 0.00485387
[6/200] Training loss: 0.00481612
[7/200] Training loss: 0.00435925
[8/200] Training loss: 0.00418952
[9/200] Training loss: 0.00394709
[10/200] Training loss: 0.00376567
[50/200] Training loss: 0.00076841
[100/200] Training loss: 0.00053349
[150/200] Training loss: 0.00040131
[200/200] Training loss: 0.00036668
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13719.960349796933 ----------
[1/200] Training loss: 0.06242277
[2/200] Training loss: 0.00414618
[3/200] Training loss: 0.00373896
[4/200] Training loss: 0.00326305
[5/200] Training loss: 0.00282579
[6/200] Training loss: 0.00246710
[7/200] Training loss: 0.00211486
[8/200] Training loss: 0.00171660
[9/200] Training loss: 0.00148657
[10/200] Training loss: 0.00138655
[50/200] Training loss: 0.00053349
[100/200] Training loss: 0.00043605
[150/200] Training loss: 0.00035613
[200/200] Training loss: 0.00032973
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14636.638958449443 ----------
[1/200] Training loss: 0.10693170
[2/200] Training loss: 0.05377078
[3/200] Training loss: 0.05038169
[4/200] Training loss: 0.04922668
[5/200] Training loss: 0.04504306
[6/200] Training loss: 0.04381699
[7/200] Training loss: 0.04180965
[8/200] Training loss: 0.03990950
[9/200] Training loss: 0.03804482
[10/200] Training loss: 0.03493185
[50/200] Training loss: 0.01953920
[100/200] Training loss: 0.01370708
[150/200] Training loss: 0.01064279
[200/200] Training loss: 0.00989679
---batch_size---: 8 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16447.01456191974 ----------
[1/200] Training loss: 0.10996536
[2/200] Training loss: 0.05368754
[3/200] Training loss: 0.04930677
[4/200] Training loss: 0.04429423
[5/200] Training loss: 0.04061832
[6/200] Training loss: 0.03589265
[7/200] Training loss: 0.03514011
[8/200] Training loss: 0.02918104
[9/200] Training loss: 0.02764981
[10/200] Training loss: 0.02791168
[50/200] Training loss: 0.01641754
[100/200] Training loss: 0.01358588
[150/200] Training loss: 0.01147357
[200/200] Training loss: 0.01037418
---batch_size---: 8 ---n_steps---: 2 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13044.59857565575 ----------
[1/200] Training loss: 0.03581340
[2/200] Training loss: 0.00412694
[3/200] Training loss: 0.00350927
[4/200] Training loss: 0.00296908
[5/200] Training loss: 0.00258298
[6/200] Training loss: 0.00230799
[7/200] Training loss: 0.00208731
[8/200] Training loss: 0.00180559
[9/200] Training loss: 0.00158752
[10/200] Training loss: 0.00146380
[50/200] Training loss: 0.00051937
[100/200] Training loss: 0.00042648
[150/200] Training loss: 0.00032808
[200/200] Training loss: 0.00030383
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12646.726058549699 ----------
[1/200] Training loss: 0.10648567
[2/200] Training loss: 0.05107164
[3/200] Training loss: 0.04570400
[4/200] Training loss: 0.04139864
[5/200] Training loss: 0.03641733
[6/200] Training loss: 0.03225406
[7/200] Training loss: 0.03108375
[8/200] Training loss: 0.02922302
[9/200] Training loss: 0.02680819
[10/200] Training loss: 0.02780870
[50/200] Training loss: 0.01700108
[100/200] Training loss: 0.01315592
[150/200] Training loss: 0.01202593
[200/200] Training loss: 0.01102172
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4024.090207736402 ----------
[1/200] Training loss: 0.03843843
[2/200] Training loss: 0.00378069
[3/200] Training loss: 0.00245607
[4/200] Training loss: 0.00190879
[5/200] Training loss: 0.00169066
[6/200] Training loss: 0.00127413
[7/200] Training loss: 0.00101957
[8/200] Training loss: 0.00115672
[9/200] Training loss: 0.00096230
[10/200] Training loss: 0.00094614
[50/200] Training loss: 0.00044867
[100/200] Training loss: 0.00034989
[150/200] Training loss: 0.00027178
[200/200] Training loss: 0.00024635
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14920.968869346252 ----------
[1/200] Training loss: 0.10273464
[2/200] Training loss: 0.05168037
[3/200] Training loss: 0.04764021
[4/200] Training loss: 0.04144630
[5/200] Training loss: 0.03985323
[6/200] Training loss: 0.03464293
[7/200] Training loss: 0.03173929
[8/200] Training loss: 0.02863834
[9/200] Training loss: 0.02785646
[10/200] Training loss: 0.02780780
[50/200] Training loss: 0.01688061
[100/200] Training loss: 0.01428534
[150/200] Training loss: 0.01277491
[200/200] Training loss: 0.01151519
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 23262.211760707534 ----------
[1/200] Training loss: 0.13619268
[2/200] Training loss: 0.05722122
[3/200] Training loss: 0.05380317
[4/200] Training loss: 0.05101044
[5/200] Training loss: 0.04812424
[6/200] Training loss: 0.04870035
[7/200] Training loss: 0.04289223
[8/200] Training loss: 0.04048875
[9/200] Training loss: 0.03901515
[10/200] Training loss: 0.03481935
[50/200] Training loss: 0.01857385
[100/200] Training loss: 0.01576317
[150/200] Training loss: 0.01429794
[200/200] Training loss: 0.01296660
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 26348.630021312303 ----------
[1/200] Training loss: 0.05961288
[2/200] Training loss: 0.00299870
[3/200] Training loss: 0.00254830
[4/200] Training loss: 0.00248407
[5/200] Training loss: 0.00218798
[6/200] Training loss: 0.00216754
[7/200] Training loss: 0.00198078
[8/200] Training loss: 0.00194295
[9/200] Training loss: 0.00175647
[10/200] Training loss: 0.00164444
[50/200] Training loss: 0.00040631
[100/200] Training loss: 0.00025037
[150/200] Training loss: 0.00018488
[200/200] Training loss: 0.00017981
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11107.116637543697 ----------
[1/200] Training loss: 0.13017524
[2/200] Training loss: 0.05493262
[3/200] Training loss: 0.05198589
[4/200] Training loss: 0.04830452
[5/200] Training loss: 0.04879761
[6/200] Training loss: 0.04668185
[7/200] Training loss: 0.04378044
[8/200] Training loss: 0.04236637
[9/200] Training loss: 0.03925919
[10/200] Training loss: 0.03775190
[50/200] Training loss: 0.01982766
[100/200] Training loss: 0.01646834
[150/200] Training loss: 0.01241242
[200/200] Training loss: 0.01086380
---batch_size---: 8 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.4145459217106824 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7999.4872335669115 ----------
[1/200] Training loss: 0.11492969
[2/200] Training loss: 0.05494399
[3/200] Training loss: 0.05043164
[4/200] Training loss: 0.04664512
[5/200] Training loss: 0.04523589
[6/200] Training loss: 0.04229936
[7/200] Training loss: 0.03888314
[8/200] Training loss: 0.03602217
[9/200] Training loss: 0.03281208
[10/200] Training loss: 0.03006141
[50/200] Training loss: 0.01624822
[100/200] Training loss: 0.01434139
[150/200] Training loss: 0.01293959
[200/200] Training loss: 0.01182354
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8383.619266164227 ----------
[1/200] Training loss: 0.02968725
[2/200] Training loss: 0.00253538
[3/200] Training loss: 0.00237899
[4/200] Training loss: 0.00226261
[5/200] Training loss: 0.00223723
[6/200] Training loss: 0.00207354
[7/200] Training loss: 0.00189615
[8/200] Training loss: 0.00186510
[9/200] Training loss: 0.00164398
[10/200] Training loss: 0.00155648
[50/200] Training loss: 0.00041379
[100/200] Training loss: 0.00030196
[150/200] Training loss: 0.00020562
[200/200] Training loss: 0.00019173
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9155.605496088176 ----------
[1/200] Training loss: 0.16272586
[2/200] Training loss: 0.05491180
[3/200] Training loss: 0.05114043
[4/200] Training loss: 0.04934681
[5/200] Training loss: 0.04783892
[6/200] Training loss: 0.04585689
[7/200] Training loss: 0.04263415
[8/200] Training loss: 0.04129509
[9/200] Training loss: 0.04061227
[10/200] Training loss: 0.03889289
[50/200] Training loss: 0.01855924
[100/200] Training loss: 0.01405293
[150/200] Training loss: 0.01098907
[200/200] Training loss: 0.01006141
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 19833.066530418335 ----------
[1/200] Training loss: 0.11180878
[2/200] Training loss: 0.04792388
[3/200] Training loss: 0.04452725
[4/200] Training loss: 0.03899462
[5/200] Training loss: 0.03654026
[6/200] Training loss: 0.03358990
[7/200] Training loss: 0.03178961
[8/200] Training loss: 0.03072743
[9/200] Training loss: 0.02953598
[10/200] Training loss: 0.02881913
[50/200] Training loss: 0.01610053
[100/200] Training loss: 0.01352523
[150/200] Training loss: 0.01182142
[200/200] Training loss: 0.01062835
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.378630221872867 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6588.338181969714 ----------
[1/200] Training loss: 0.11188985
[2/200] Training loss: 0.05329056
[3/200] Training loss: 0.04785818
[4/200] Training loss: 0.04352893
[5/200] Training loss: 0.04028052
[6/200] Training loss: 0.03774664
[7/200] Training loss: 0.03433808
[8/200] Training loss: 0.03245319
[9/200] Training loss: 0.02998897
[10/200] Training loss: 0.02862215
[50/200] Training loss: 0.01738936
[100/200] Training loss: 0.01428954
[150/200] Training loss: 0.01204903
[200/200] Training loss: 0.01149177
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5982.5954233927605 ----------
[1/200] Training loss: 0.25405314
[2/200] Training loss: 0.06273431
[3/200] Training loss: 0.05701691
[4/200] Training loss: 0.05691787
[5/200] Training loss: 0.05551933
[6/200] Training loss: 0.05055000
[7/200] Training loss: 0.05036242
[8/200] Training loss: 0.05027525
[9/200] Training loss: 0.04898733
[10/200] Training loss: 0.04643129
[50/200] Training loss: 0.02461906
[100/200] Training loss: 0.02111801
[150/200] Training loss: 0.01891453
[200/200] Training loss: 0.01536373
---batch_size---: 32 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12738.411203913933 ----------
[1/200] Training loss: 0.04929108
[2/200] Training loss: 0.00479346
[3/200] Training loss: 0.00428987
[4/200] Training loss: 0.00369193
[5/200] Training loss: 0.00314314
[6/200] Training loss: 0.00268204
[7/200] Training loss: 0.00241892
[8/200] Training loss: 0.00203035
[9/200] Training loss: 0.00189480
[10/200] Training loss: 0.00174916
[50/200] Training loss: 0.00067114
[100/200] Training loss: 0.00045174
[150/200] Training loss: 0.00027337
[200/200] Training loss: 0.00020635
---batch_size---: 8 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12808.722652942408 ----------
[1/200] Training loss: 0.17692440
[2/200] Training loss: 0.06064856
[3/200] Training loss: 0.05433990
[4/200] Training loss: 0.04671191
[5/200] Training loss: 0.04708201
[6/200] Training loss: 0.04464958
[7/200] Training loss: 0.04325116
[8/200] Training loss: 0.04280170
[9/200] Training loss: 0.04007899
[10/200] Training loss: 0.04062642
[50/200] Training loss: 0.02040289
[100/200] Training loss: 0.01715669
[150/200] Training loss: 0.01531395
[200/200] Training loss: 0.01381733
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15956.723473194614 ----------
[1/200] Training loss: 0.14085132
[2/200] Training loss: 0.05671225
[3/200] Training loss: 0.05177737
[4/200] Training loss: 0.05093054
[5/200] Training loss: 0.04371307
[6/200] Training loss: 0.04406174
[7/200] Training loss: 0.04184736
[8/200] Training loss: 0.04007412
[9/200] Training loss: 0.04106939
[10/200] Training loss: 0.03928043
[50/200] Training loss: 0.01947884
[100/200] Training loss: 0.01674621
[150/200] Training loss: 0.01415439
[200/200] Training loss: 0.01279217
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8433.110932508833 ----------
[1/200] Training loss: 0.10650228
[2/200] Training loss: 0.05096839
[3/200] Training loss: 0.04494576
[4/200] Training loss: 0.03934662
[5/200] Training loss: 0.03635892
[6/200] Training loss: 0.03386073
[7/200] Training loss: 0.03171894
[8/200] Training loss: 0.02995101
[9/200] Training loss: 0.02797450
[10/200] Training loss: 0.02539771
[50/200] Training loss: 0.01814674
[100/200] Training loss: 0.01526910
[150/200] Training loss: 0.01310447
[200/200] Training loss: 0.01251327
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14127.384471302536 ----------
[1/200] Training loss: 0.13844273
[2/200] Training loss: 0.05592346
[3/200] Training loss: 0.05130712
[4/200] Training loss: 0.04603884
[5/200] Training loss: 0.04283544
[6/200] Training loss: 0.03769181
[7/200] Training loss: 0.03570553
[8/200] Training loss: 0.03510945
[9/200] Training loss: 0.03356146
[10/200] Training loss: 0.02849891
[50/200] Training loss: 0.01765303
[100/200] Training loss: 0.01473627
[150/200] Training loss: 0.01321640
[200/200] Training loss: 0.01253598
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4478.846056742741 ----------
[1/200] Training loss: 0.14478266
[2/200] Training loss: 0.05567657
[3/200] Training loss: 0.05047518
[4/200] Training loss: 0.04544953
[5/200] Training loss: 0.04324047
[6/200] Training loss: 0.04162495
[7/200] Training loss: 0.03874479
[8/200] Training loss: 0.03523546
[9/200] Training loss: 0.03660530
[10/200] Training loss: 0.03199433
[50/200] Training loss: 0.01790860
[100/200] Training loss: 0.01555675
[150/200] Training loss: 0.01419088
[200/200] Training loss: 0.01321966
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13776.944799192599 ----------
[1/200] Training loss: 0.22157475
[2/200] Training loss: 0.06795605
[3/200] Training loss: 0.05643707
[4/200] Training loss: 0.05488038
[5/200] Training loss: 0.05365262
[6/200] Training loss: 0.04984753
[7/200] Training loss: 0.04805681
[8/200] Training loss: 0.04842332
[9/200] Training loss: 0.04687790
[10/200] Training loss: 0.04850602
[50/200] Training loss: 0.02772602
[100/200] Training loss: 0.02051336
[150/200] Training loss: 0.01681693
[200/200] Training loss: 0.01543992
---batch_size---: 32 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.4145459217106824 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6342.537662481793 ----------
[1/200] Training loss: 0.01556189
[2/200] Training loss: 0.00198600
[3/200] Training loss: 0.00165335
[4/200] Training loss: 0.00135471
[5/200] Training loss: 0.00104100
[6/200] Training loss: 0.00084990
[7/200] Training loss: 0.00067806
[8/200] Training loss: 0.00058902
[9/200] Training loss: 0.00056181
[10/200] Training loss: 0.00049567
[50/200] Training loss: 0.00025531
[100/200] Training loss: 0.00018391
[150/200] Training loss: 0.00014496
[200/200] Training loss: 0.00012811
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11004.812038376667 ----------
[1/200] Training loss: 0.10784553
[2/200] Training loss: 0.05665805
[3/200] Training loss: 0.05173900
[4/200] Training loss: 0.05103769
[5/200] Training loss: 0.04912094
[6/200] Training loss: 0.04640499
[7/200] Training loss: 0.04385556
[8/200] Training loss: 0.04169234
[9/200] Training loss: 0.03751423
[10/200] Training loss: 0.03515274
[50/200] Training loss: 0.01947070
[100/200] Training loss: 0.01470222
[150/200] Training loss: 0.01104209
[200/200] Training loss: 0.00991135
---batch_size---: 8 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11362.692990660269 ----------
[1/200] Training loss: 0.16098418
[2/200] Training loss: 0.05742203
[3/200] Training loss: 0.05109234
[4/200] Training loss: 0.04746674
[5/200] Training loss: 0.04418051
[6/200] Training loss: 0.04330546
[7/200] Training loss: 0.04112997
[8/200] Training loss: 0.03967320
[9/200] Training loss: 0.03613989
[10/200] Training loss: 0.03598685
[50/200] Training loss: 0.01806136
[100/200] Training loss: 0.01583328
[150/200] Training loss: 0.01514035
[200/200] Training loss: 0.01405657
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6645.320458789027 ----------
[1/200] Training loss: 0.02538083
[2/200] Training loss: 0.00217189
[3/200] Training loss: 0.00203280
[4/200] Training loss: 0.00177690
[5/200] Training loss: 0.00161741
[6/200] Training loss: 0.00152266
[7/200] Training loss: 0.00139559
[8/200] Training loss: 0.00124186
[9/200] Training loss: 0.00105896
[10/200] Training loss: 0.00090995
[50/200] Training loss: 0.00026814
[100/200] Training loss: 0.00018531
[150/200] Training loss: 0.00017475
[200/200] Training loss: 0.00015480
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15885.71484069886 ----------
[1/200] Training loss: 0.13461233
[2/200] Training loss: 0.05700835
[3/200] Training loss: 0.04917245
[4/200] Training loss: 0.04655685
[5/200] Training loss: 0.04430170
[6/200] Training loss: 0.04215523
[7/200] Training loss: 0.03967275
[8/200] Training loss: 0.03925092
[9/200] Training loss: 0.03383014
[10/200] Training loss: 0.03170100
[50/200] Training loss: 0.01785803
[100/200] Training loss: 0.01554159
[150/200] Training loss: 0.01268135
[200/200] Training loss: 0.01238438
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8809.625190665038 ----------
[1/200] Training loss: 0.09683546
[2/200] Training loss: 0.04952699
[3/200] Training loss: 0.04233417
[4/200] Training loss: 0.03677709
[5/200] Training loss: 0.03485095
[6/200] Training loss: 0.03072554
[7/200] Training loss: 0.02943891
[8/200] Training loss: 0.02596751
[9/200] Training loss: 0.02538635
[10/200] Training loss: 0.02323798
[50/200] Training loss: 0.01646992
[100/200] Training loss: 0.01457390
[150/200] Training loss: 0.01325848
[200/200] Training loss: 0.01251713
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8896.223018787243 ----------
[1/200] Training loss: 0.03722179
[2/200] Training loss: 0.00447936
[3/200] Training loss: 0.00354271
[4/200] Training loss: 0.00282455
[5/200] Training loss: 0.00229648
[6/200] Training loss: 0.00173451
[7/200] Training loss: 0.00142560
[8/200] Training loss: 0.00116307
[9/200] Training loss: 0.00105209
[10/200] Training loss: 0.00092653
[50/200] Training loss: 0.00050766
[100/200] Training loss: 0.00039645
[150/200] Training loss: 0.00033536
[200/200] Training loss: 0.00028401
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12748.974546997888 ----------
[1/200] Training loss: 0.15691341
[2/200] Training loss: 0.06191189
[3/200] Training loss: 0.05133698
[4/200] Training loss: 0.04884392
[5/200] Training loss: 0.04533099
[6/200] Training loss: 0.04269564
[7/200] Training loss: 0.04051992
[8/200] Training loss: 0.04088670
[9/200] Training loss: 0.03820112
[10/200] Training loss: 0.03447481
[50/200] Training loss: 0.01999350
[100/200] Training loss: 0.01610472
[150/200] Training loss: 0.01336790
[200/200] Training loss: 0.01240892
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4315.949953370637 ----------
[1/200] Training loss: 0.01484331
[2/200] Training loss: 0.00214964
[3/200] Training loss: 0.00185410
[4/200] Training loss: 0.00171622
[5/200] Training loss: 0.00148241
[6/200] Training loss: 0.00136116
[7/200] Training loss: 0.00125762
[8/200] Training loss: 0.00103673
[9/200] Training loss: 0.00097516
[10/200] Training loss: 0.00083158
[50/200] Training loss: 0.00018228
[100/200] Training loss: 0.00011119
[150/200] Training loss: 0.00009639
[200/200] Training loss: 0.00008343
---batch_size---: 8 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9592.717654554417 ----------
[1/200] Training loss: 0.12274558
[2/200] Training loss: 0.05477718
[3/200] Training loss: 0.05051710
[4/200] Training loss: 0.04686462
[5/200] Training loss: 0.04445128
[6/200] Training loss: 0.04137236
[7/200] Training loss: 0.03686396
[8/200] Training loss: 0.03477211
[9/200] Training loss: 0.03249497
[10/200] Training loss: 0.02966182
[50/200] Training loss: 0.01764929
[100/200] Training loss: 0.01566836
[150/200] Training loss: 0.01451889
[200/200] Training loss: 0.01365409
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15906.17993108339 ----------
[1/200] Training loss: 0.15815809
[2/200] Training loss: 0.06239171
[3/200] Training loss: 0.04923940
[4/200] Training loss: 0.04442117
[5/200] Training loss: 0.04252663
[6/200] Training loss: 0.03719834
[7/200] Training loss: 0.03837824
[8/200] Training loss: 0.03213941
[9/200] Training loss: 0.03042103
[10/200] Training loss: 0.02887020
[50/200] Training loss: 0.01830511
[100/200] Training loss: 0.01756432
[150/200] Training loss: 0.01568028
[200/200] Training loss: 0.01462430
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11984.791696145578 ----------
[1/200] Training loss: 0.11736918
[2/200] Training loss: 0.05366554
[3/200] Training loss: 0.04576415
[4/200] Training loss: 0.04135421
[5/200] Training loss: 0.03337238
[6/200] Training loss: 0.03139152
[7/200] Training loss: 0.02831519
[8/200] Training loss: 0.02755446
[9/200] Training loss: 0.02617872
[10/200] Training loss: 0.02468938
[50/200] Training loss: 0.01644504
[100/200] Training loss: 0.01440154
[150/200] Training loss: 0.01319078
[200/200] Training loss: 0.01244836
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5249.963237966529 ----------
[1/200] Training loss: 0.13199371
[2/200] Training loss: 0.05784014
[3/200] Training loss: 0.05580274
[4/200] Training loss: 0.05154036
[5/200] Training loss: 0.05263467
[6/200] Training loss: 0.05182046
[7/200] Training loss: 0.04863168
[8/200] Training loss: 0.04707707
[9/200] Training loss: 0.04763812
[10/200] Training loss: 0.04559084
[50/200] Training loss: 0.02113238
[100/200] Training loss: 0.01368097
[150/200] Training loss: 0.01206942
[200/200] Training loss: 0.01090905
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13779.702173849768 ----------
[1/200] Training loss: 0.09454169
[2/200] Training loss: 0.04792147
[3/200] Training loss: 0.04252950
[4/200] Training loss: 0.03864448
[5/200] Training loss: 0.03635194
[6/200] Training loss: 0.03301300
[7/200] Training loss: 0.03132756
[8/200] Training loss: 0.03030054
[9/200] Training loss: 0.02882241
[10/200] Training loss: 0.02889944
[50/200] Training loss: 0.01551655
[100/200] Training loss: 0.01367734
[150/200] Training loss: 0.01200934
[200/200] Training loss: 0.01107426
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16228.845430282463 ----------
[1/200] Training loss: 0.17238969
[2/200] Training loss: 0.05422929
[3/200] Training loss: 0.05437742
[4/200] Training loss: 0.04996487
[5/200] Training loss: 0.04631312
[6/200] Training loss: 0.04554806
[7/200] Training loss: 0.04566682
[8/200] Training loss: 0.04365005
[9/200] Training loss: 0.04218356
[10/200] Training loss: 0.03993387
[50/200] Training loss: 0.01954270
[100/200] Training loss: 0.01474087
[150/200] Training loss: 0.01194470
[200/200] Training loss: 0.01054790
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4508.917386690512 ----------
[1/200] Training loss: 0.11695290
[2/200] Training loss: 0.05249827
[3/200] Training loss: 0.04962874
[4/200] Training loss: 0.04757720
[5/200] Training loss: 0.04438626
[6/200] Training loss: 0.04171412
[7/200] Training loss: 0.03871890
[8/200] Training loss: 0.03606855
[9/200] Training loss: 0.03332862
[10/200] Training loss: 0.03236755
[50/200] Training loss: 0.01855676
[100/200] Training loss: 0.01581306
[150/200] Training loss: 0.01406142
[200/200] Training loss: 0.01267500
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13203.526195679699 ----------
[1/200] Training loss: 0.11234401
[2/200] Training loss: 0.05503466
[3/200] Training loss: 0.04542423
[4/200] Training loss: 0.03845143
[5/200] Training loss: 0.03638609
[6/200] Training loss: 0.03207766
[7/200] Training loss: 0.03053914
[8/200] Training loss: 0.02837110
[9/200] Training loss: 0.02679394
[10/200] Training loss: 0.02729543
[50/200] Training loss: 0.01756600
[100/200] Training loss: 0.01585314
[150/200] Training loss: 0.01439901
[200/200] Training loss: 0.01320353
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.378630221872867 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11297.582042189382 ----------
[1/200] Training loss: 0.10900686
[2/200] Training loss: 0.05392870
[3/200] Training loss: 0.05061189
[4/200] Training loss: 0.04800684
[5/200] Training loss: 0.04635652
[6/200] Training loss: 0.04508347
[7/200] Training loss: 0.04233225
[8/200] Training loss: 0.04001825
[9/200] Training loss: 0.03488938
[10/200] Training loss: 0.03399394
[50/200] Training loss: 0.01499062
[100/200] Training loss: 0.01152049
[150/200] Training loss: 0.01071638
[200/200] Training loss: 0.00939631
---batch_size---: 8 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.4145459217106824 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17490.12704356375 ----------
[1/200] Training loss: 0.10271859
[2/200] Training loss: 0.05494832
[3/200] Training loss: 0.05067809
[4/200] Training loss: 0.04658074
[5/200] Training loss: 0.04302656
[6/200] Training loss: 0.04019209
[7/200] Training loss: 0.03695791
[8/200] Training loss: 0.03439249
[9/200] Training loss: 0.03141009
[10/200] Training loss: 0.03031099
[50/200] Training loss: 0.01601105
[100/200] Training loss: 0.01341391
[150/200] Training loss: 0.01272447
[200/200] Training loss: 0.01179977
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7898.322353512802 ----------
[1/200] Training loss: 0.11441902
[2/200] Training loss: 0.05584557
[3/200] Training loss: 0.05159757
[4/200] Training loss: 0.04972316
[5/200] Training loss: 0.04794969
[6/200] Training loss: 0.04523179
[7/200] Training loss: 0.04188099
[8/200] Training loss: 0.03986828
[9/200] Training loss: 0.03716373
[10/200] Training loss: 0.03309212
[50/200] Training loss: 0.01755883
[100/200] Training loss: 0.01580137
[150/200] Training loss: 0.01404031
[200/200] Training loss: 0.01309689
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.378630221872867 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11507.08581700858 ----------
[1/200] Training loss: 0.10487649
[2/200] Training loss: 0.04944186
[3/200] Training loss: 0.04664539
[4/200] Training loss: 0.04195200
[5/200] Training loss: 0.03820151
[6/200] Training loss: 0.03589742
[7/200] Training loss: 0.03391966
[8/200] Training loss: 0.03050337
[9/200] Training loss: 0.02925594
[10/200] Training loss: 0.02691097
[50/200] Training loss: 0.01730272
[100/200] Training loss: 0.01426877
[150/200] Training loss: 0.01284919
[200/200] Training loss: 0.01201361
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5490.269756578451 ----------
[1/200] Training loss: 0.10196708
[2/200] Training loss: 0.04886542
[3/200] Training loss: 0.04470094
[4/200] Training loss: 0.04182932
[5/200] Training loss: 0.03717003
[6/200] Training loss: 0.03400732
[7/200] Training loss: 0.03243756
[8/200] Training loss: 0.02957012
[9/200] Training loss: 0.02950082
[10/200] Training loss: 0.02633650
[50/200] Training loss: 0.01681797
[100/200] Training loss: 0.01428275
[150/200] Training loss: 0.01275644
[200/200] Training loss: 0.01183728
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.7319789082087752 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6847.802859311883 ----------
[1/200] Training loss: 0.10970370
[2/200] Training loss: 0.05359228
[3/200] Training loss: 0.04552436
[4/200] Training loss: 0.03855882
[5/200] Training loss: 0.03293301
[6/200] Training loss: 0.02870845
[7/200] Training loss: 0.02778350
[8/200] Training loss: 0.02531411
[9/200] Training loss: 0.02508472
[10/200] Training loss: 0.02289423
[50/200] Training loss: 0.01690135
[100/200] Training loss: 0.01522917
[150/200] Training loss: 0.01401967
[200/200] Training loss: 0.01286055
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.378630221872867 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16180.922099806303 ----------
[1/200] Training loss: 0.13691859
[2/200] Training loss: 0.05424822
[3/200] Training loss: 0.05319725
[4/200] Training loss: 0.04681060
[5/200] Training loss: 0.04243540
[6/200] Training loss: 0.03899350
[7/200] Training loss: 0.03818825
[8/200] Training loss: 0.03189030
[9/200] Training loss: 0.03295362
[10/200] Training loss: 0.03093345
[50/200] Training loss: 0.01892691
[100/200] Training loss: 0.01601833
[150/200] Training loss: 0.01375465
[200/200] Training loss: 0.01342876
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8890.1626531802 ----------
[1/200] Training loss: 0.10119494
[2/200] Training loss: 0.05138505
[3/200] Training loss: 0.04747417
[4/200] Training loss: 0.04130065
[5/200] Training loss: 0.03734862
[6/200] Training loss: 0.03337636
[7/200] Training loss: 0.03097083
[8/200] Training loss: 0.02893508
[9/200] Training loss: 0.02871258
[10/200] Training loss: 0.02765225
[50/200] Training loss: 0.01814085
[100/200] Training loss: 0.01572644
[150/200] Training loss: 0.01376346
[200/200] Training loss: 0.01258286
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9575.078067566865 ----------
[1/200] Training loss: 0.15672032
[2/200] Training loss: 0.06325435
[3/200] Training loss: 0.05630280
[4/200] Training loss: 0.05631820
[5/200] Training loss: 0.05439281
[6/200] Training loss: 0.05014527
[7/200] Training loss: 0.05126285
[8/200] Training loss: 0.04883536
[9/200] Training loss: 0.04697377
[10/200] Training loss: 0.04628528
[50/200] Training loss: 0.01987579
[100/200] Training loss: 0.01624562
[150/200] Training loss: 0.01549347
[200/200] Training loss: 0.01460052
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9331.88984075573 ----------
[1/200] Training loss: 0.14969758
[2/200] Training loss: 0.05730668
[3/200] Training loss: 0.05545211
[4/200] Training loss: 0.05093285
[5/200] Training loss: 0.04856509
[6/200] Training loss: 0.04569229
[7/200] Training loss: 0.04353211
[8/200] Training loss: 0.04240153
[9/200] Training loss: 0.03786377
[10/200] Training loss: 0.03812612
[50/200] Training loss: 0.01820014
[100/200] Training loss: 0.01678122
[150/200] Training loss: 0.01458491
[200/200] Training loss: 0.01314715
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 21211.270211847284 ----------
[1/200] Training loss: 0.11328755
[2/200] Training loss: 0.05446233
[3/200] Training loss: 0.05112418
[4/200] Training loss: 0.04415831
[5/200] Training loss: 0.04075982
[6/200] Training loss: 0.03811301
[7/200] Training loss: 0.03484054
[8/200] Training loss: 0.03209975
[9/200] Training loss: 0.02969080
[10/200] Training loss: 0.02853177
[50/200] Training loss: 0.01867827
[100/200] Training loss: 0.01591709
[150/200] Training loss: 0.01401874
[200/200] Training loss: 0.01257383
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14782.64766542178 ----------
[1/200] Training loss: 0.13903273
[2/200] Training loss: 0.05160237
[3/200] Training loss: 0.04739150
[4/200] Training loss: 0.04339339
[5/200] Training loss: 0.03809306
[6/200] Training loss: 0.03529631
[7/200] Training loss: 0.03616305
[8/200] Training loss: 0.03140839
[9/200] Training loss: 0.03216917
[10/200] Training loss: 0.03020858
[50/200] Training loss: 0.01734896
[100/200] Training loss: 0.01584637
[150/200] Training loss: 0.01415741
[200/200] Training loss: 0.01322317
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6655.802280717179 ----------
[1/200] Training loss: 0.13941551
[2/200] Training loss: 0.05909822
[3/200] Training loss: 0.05204476
[4/200] Training loss: 0.04951405
[5/200] Training loss: 0.04547237
[6/200] Training loss: 0.04209010
[7/200] Training loss: 0.04317888
[8/200] Training loss: 0.03920095
[9/200] Training loss: 0.03893994
[10/200] Training loss: 0.03741585
[50/200] Training loss: 0.01963262
[100/200] Training loss: 0.01672049
[150/200] Training loss: 0.01466927
[200/200] Training loss: 0.01316204
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8638.541543570882 ----------
[1/200] Training loss: 0.11626226
[2/200] Training loss: 0.05380869
[3/200] Training loss: 0.04704648
[4/200] Training loss: 0.04080795
[5/200] Training loss: 0.03824994
[6/200] Training loss: 0.03562916
[7/200] Training loss: 0.03408204
[8/200] Training loss: 0.03179656
[9/200] Training loss: 0.03147064
[10/200] Training loss: 0.03012253
[50/200] Training loss: 0.01680376
[100/200] Training loss: 0.01484804
[150/200] Training loss: 0.01318729
[200/200] Training loss: 0.01235376
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12651.437862946646 ----------
[1/200] Training loss: 0.10619787
[2/200] Training loss: 0.05275890
[3/200] Training loss: 0.04702763
[4/200] Training loss: 0.04473234
[5/200] Training loss: 0.04150950
[6/200] Training loss: 0.03684603
[7/200] Training loss: 0.03492843
[8/200] Training loss: 0.03319378
[9/200] Training loss: 0.03266085
[10/200] Training loss: 0.03075818
[50/200] Training loss: 0.01820880
[100/200] Training loss: 0.01632264
[150/200] Training loss: 0.01484061
[200/200] Training loss: 0.01322171
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9994.305178450375 ----------
[1/200] Training loss: 0.17438382
[2/200] Training loss: 0.06187108
[3/200] Training loss: 0.05281700
[4/200] Training loss: 0.05003243
[5/200] Training loss: 0.05096838
[6/200] Training loss: 0.04424785
[7/200] Training loss: 0.04297376
[8/200] Training loss: 0.04101040
[9/200] Training loss: 0.03929443
[10/200] Training loss: 0.03825221
[50/200] Training loss: 0.01946585
[100/200] Training loss: 0.01701622
[150/200] Training loss: 0.01608540
[200/200] Training loss: 0.01448162
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6689.931240304342 ----------
[1/200] Training loss: 0.11829977
[2/200] Training loss: 0.05303215
[3/200] Training loss: 0.04677843
[4/200] Training loss: 0.04241091
[5/200] Training loss: 0.03677464
[6/200] Training loss: 0.03219445
[7/200] Training loss: 0.02965836
[8/200] Training loss: 0.02804137
[9/200] Training loss: 0.02790125
[10/200] Training loss: 0.02560225
[50/200] Training loss: 0.01705799
[100/200] Training loss: 0.01406543
[150/200] Training loss: 0.01282579
[200/200] Training loss: 0.01181738
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.4145459217106824 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5200.851276473882 ----------
[1/200] Training loss: 0.26743866
[2/200] Training loss: 0.06542262
[3/200] Training loss: 0.05447089
[4/200] Training loss: 0.05540446
[5/200] Training loss: 0.05034170
[6/200] Training loss: 0.05731068
[7/200] Training loss: 0.04994608
[8/200] Training loss: 0.04853043
[9/200] Training loss: 0.04656885
[10/200] Training loss: 0.04687919
[50/200] Training loss: 0.02320748
[100/200] Training loss: 0.01730903
[150/200] Training loss: 0.01380307
[200/200] Training loss: 0.01472803
---batch_size---: 32 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.378630221872867 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9058.502304465126 ----------
[1/200] Training loss: 0.10849366
[2/200] Training loss: 0.05072068
[3/200] Training loss: 0.04676034
[4/200] Training loss: 0.04117102
[5/200] Training loss: 0.03408653
[6/200] Training loss: 0.03482329
[7/200] Training loss: 0.03065278
[8/200] Training loss: 0.02982136
[9/200] Training loss: 0.02641150
[10/200] Training loss: 0.02585918
[50/200] Training loss: 0.01656347
[100/200] Training loss: 0.01414193
[150/200] Training loss: 0.01271407
[200/200] Training loss: 0.01184081
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8800.682246280683 ----------
[1/200] Training loss: 0.09642881
[2/200] Training loss: 0.05047983
[3/200] Training loss: 0.04566722
[4/200] Training loss: 0.04283808
[5/200] Training loss: 0.03871925
[6/200] Training loss: 0.03713891
[7/200] Training loss: 0.03355836
[8/200] Training loss: 0.03315388
[9/200] Training loss: 0.03045030
[10/200] Training loss: 0.02904894
[50/200] Training loss: 0.01620640
[100/200] Training loss: 0.01375496
[150/200] Training loss: 0.01233891
[200/200] Training loss: 0.01107608
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11174.173794961309 ----------
[1/200] Training loss: 0.17004628
[2/200] Training loss: 0.06241362
[3/200] Training loss: 0.05866028
[4/200] Training loss: 0.05325862
[5/200] Training loss: 0.05188705
[6/200] Training loss: 0.04917976
[7/200] Training loss: 0.04854785
[8/200] Training loss: 0.04617814
[9/200] Training loss: 0.04292426
[10/200] Training loss: 0.04130263
[50/200] Training loss: 0.01979460
[100/200] Training loss: 0.01738982
[150/200] Training loss: 0.01546223
[200/200] Training loss: 0.01463500
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 27986.677973635957 ----------
[1/200] Training loss: 0.16449551
[2/200] Training loss: 0.05975274
[3/200] Training loss: 0.05386381
[4/200] Training loss: 0.05173647
[5/200] Training loss: 0.04974468
[6/200] Training loss: 0.04902715
[7/200] Training loss: 0.04827232
[8/200] Training loss: 0.04682699
[9/200] Training loss: 0.04561302
[10/200] Training loss: 0.04267906
[50/200] Training loss: 0.02138917
[100/200] Training loss: 0.01633866
[150/200] Training loss: 0.01323116
[200/200] Training loss: 0.01153763
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11477.025747117586 ----------
[1/200] Training loss: 0.19091399
[2/200] Training loss: 0.06459050
[3/200] Training loss: 0.05482428
[4/200] Training loss: 0.05404354
[5/200] Training loss: 0.05279586
[6/200] Training loss: 0.04840474
[7/200] Training loss: 0.04901674
[8/200] Training loss: 0.04833311
[9/200] Training loss: 0.04464524
[10/200] Training loss: 0.04267341
[50/200] Training loss: 0.01826551
[100/200] Training loss: 0.01617482
[150/200] Training loss: 0.01500414
[200/200] Training loss: 0.01408417
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 19258.244987537157 ----------
[1/200] Training loss: 0.11692387
[2/200] Training loss: 0.05382676
[3/200] Training loss: 0.05085324
[4/200] Training loss: 0.04703341
[5/200] Training loss: 0.04207895
[6/200] Training loss: 0.03806165
[7/200] Training loss: 0.03676243
[8/200] Training loss: 0.03391653
[9/200] Training loss: 0.03054732
[10/200] Training loss: 0.02718761
[50/200] Training loss: 0.01690358
[100/200] Training loss: 0.01511561
[150/200] Training loss: 0.01336725
[200/200] Training loss: 0.01239836
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9868.543154893736 ----------
[1/200] Training loss: 0.15005047
[2/200] Training loss: 0.06038618
[3/200] Training loss: 0.05561889
[4/200] Training loss: 0.05126955
[5/200] Training loss: 0.04648811
[6/200] Training loss: 0.04517753
[7/200] Training loss: 0.04354465
[8/200] Training loss: 0.04128235
[9/200] Training loss: 0.03950124
[10/200] Training loss: 0.03449206
[50/200] Training loss: 0.01843350
[100/200] Training loss: 0.01510694
[150/200] Training loss: 0.01359157
[200/200] Training loss: 0.01201976
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10107.476440734354 ----------
[1/200] Training loss: 0.10437922
[2/200] Training loss: 0.05305647
[3/200] Training loss: 0.04912138
[4/200] Training loss: 0.04487759
[5/200] Training loss: 0.04193344
[6/200] Training loss: 0.03929397
[7/200] Training loss: 0.03663175
[8/200] Training loss: 0.03464959
[9/200] Training loss: 0.03309934
[10/200] Training loss: 0.03091097
[50/200] Training loss: 0.01608216
[100/200] Training loss: 0.01321415
[150/200] Training loss: 0.01200398
[200/200] Training loss: 0.01109014
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.7319789082087752 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11984.26167938601 ----------
[1/200] Training loss: 0.02813280
[2/200] Training loss: 0.00432554
[3/200] Training loss: 0.00338211
[4/200] Training loss: 0.00272607
[5/200] Training loss: 0.00208817
[6/200] Training loss: 0.00170246
[7/200] Training loss: 0.00133690
[8/200] Training loss: 0.00114006
[9/200] Training loss: 0.00107894
[10/200] Training loss: 0.00096632
[50/200] Training loss: 0.00046878
[100/200] Training loss: 0.00033766
[150/200] Training loss: 0.00029510
[200/200] Training loss: 0.00025636
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14036.027358195053 ----------
[1/200] Training loss: 0.13618911
[2/200] Training loss: 0.05578581
[3/200] Training loss: 0.04951242
[4/200] Training loss: 0.04509856
[5/200] Training loss: 0.04324682
[6/200] Training loss: 0.04118017
[7/200] Training loss: 0.04175318
[8/200] Training loss: 0.03878874
[9/200] Training loss: 0.03850885
[10/200] Training loss: 0.03453302
[50/200] Training loss: 0.01809258
[100/200] Training loss: 0.01408394
[150/200] Training loss: 0.01324676
[200/200] Training loss: 0.01182995
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15298.319384821327 ----------
[1/200] Training loss: 0.16487352
[2/200] Training loss: 0.05770277
[3/200] Training loss: 0.05629823
[4/200] Training loss: 0.04858196
[5/200] Training loss: 0.04737709
[6/200] Training loss: 0.04668147
[7/200] Training loss: 0.04038600
[8/200] Training loss: 0.03952810
[9/200] Training loss: 0.03730937
[10/200] Training loss: 0.03396946
[50/200] Training loss: 0.02032420
[100/200] Training loss: 0.01654122
[150/200] Training loss: 0.01480223
[200/200] Training loss: 0.01435999
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12454.312666703048 ----------
[1/200] Training loss: 0.16299523
[2/200] Training loss: 0.06307959
[3/200] Training loss: 0.05234021
[4/200] Training loss: 0.04572813
[5/200] Training loss: 0.04256999
[6/200] Training loss: 0.04208275
[7/200] Training loss: 0.03764580
[8/200] Training loss: 0.03909290
[9/200] Training loss: 0.03550538
[10/200] Training loss: 0.03381267
[50/200] Training loss: 0.01768315
[100/200] Training loss: 0.01549785
[150/200] Training loss: 0.01473146
[200/200] Training loss: 0.01389420
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7280.274994806172 ----------
[1/200] Training loss: 0.15559245
[2/200] Training loss: 0.05280849
[3/200] Training loss: 0.05162882
[4/200] Training loss: 0.04401794
[5/200] Training loss: 0.03910154
[6/200] Training loss: 0.03828676
[7/200] Training loss: 0.03738559
[8/200] Training loss: 0.03636660
[9/200] Training loss: 0.03143531
[10/200] Training loss: 0.02880538
[50/200] Training loss: 0.01746246
[100/200] Training loss: 0.01463420
[150/200] Training loss: 0.01352962
[200/200] Training loss: 0.01195506
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7385.953425252558 ----------
[1/200] Training loss: 0.11122164
[2/200] Training loss: 0.05448699
[3/200] Training loss: 0.05013251
[4/200] Training loss: 0.04805461
[5/200] Training loss: 0.04586915
[6/200] Training loss: 0.04375510
[7/200] Training loss: 0.04083070
[8/200] Training loss: 0.03704870
[9/200] Training loss: 0.03610037
[10/200] Training loss: 0.03283634
[50/200] Training loss: 0.01591960
[100/200] Training loss: 0.01210500
[150/200] Training loss: 0.01061199
[200/200] Training loss: 0.00959152
---batch_size---: 8 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.4145459217106824 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15391.330026998967 ----------
[1/200] Training loss: 0.15453604
[2/200] Training loss: 0.06241962
[3/200] Training loss: 0.05011875
[4/200] Training loss: 0.04946906
[5/200] Training loss: 0.04772476
[6/200] Training loss: 0.04227222
[7/200] Training loss: 0.04053154
[8/200] Training loss: 0.03698327
[9/200] Training loss: 0.03663136
[10/200] Training loss: 0.03495500
[50/200] Training loss: 0.01737248
[100/200] Training loss: 0.01536887
[150/200] Training loss: 0.01421365
[200/200] Training loss: 0.01310279
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11901.433863194805 ----------
[1/200] Training loss: 0.12512243
[2/200] Training loss: 0.05318527
[3/200] Training loss: 0.04481279
[4/200] Training loss: 0.03854129
[5/200] Training loss: 0.03471224
[6/200] Training loss: 0.03081759
[7/200] Training loss: 0.03013835
[8/200] Training loss: 0.02778417
[9/200] Training loss: 0.02513817
[10/200] Training loss: 0.02407323
[50/200] Training loss: 0.01746302
[100/200] Training loss: 0.01511779
[150/200] Training loss: 0.01382440
[200/200] Training loss: 0.01205589
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6835.749263979772 ----------
[1/200] Training loss: 0.15786251
[2/200] Training loss: 0.05439925
[3/200] Training loss: 0.04688002
[4/200] Training loss: 0.04380445
[5/200] Training loss: 0.04342449
[6/200] Training loss: 0.03842182
[7/200] Training loss: 0.03673632
[8/200] Training loss: 0.03603702
[9/200] Training loss: 0.03311216
[10/200] Training loss: 0.03143752
[50/200] Training loss: 0.01826140
[100/200] Training loss: 0.01586469
[150/200] Training loss: 0.01342987
[200/200] Training loss: 0.01327196
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4237.28261979302 ----------
[1/200] Training loss: 0.13082987
[2/200] Training loss: 0.05583165
[3/200] Training loss: 0.04892502
[4/200] Training loss: 0.04523370
[5/200] Training loss: 0.03904567
[6/200] Training loss: 0.03624476
[7/200] Training loss: 0.03378726
[8/200] Training loss: 0.02946683
[9/200] Training loss: 0.02714618
[10/200] Training loss: 0.02704862
[50/200] Training loss: 0.01627670
[100/200] Training loss: 0.01327869
[150/200] Training loss: 0.01204703
[200/200] Training loss: 0.01154765
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9859.733059266868 ----------
[1/200] Training loss: 0.09963401
[2/200] Training loss: 0.05164660
[3/200] Training loss: 0.04665482
[4/200] Training loss: 0.04235401
[5/200] Training loss: 0.03706020
[6/200] Training loss: 0.03304519
[7/200] Training loss: 0.03035145
[8/200] Training loss: 0.02916744
[9/200] Training loss: 0.02830654
[10/200] Training loss: 0.02630736
[50/200] Training loss: 0.01526424
[100/200] Training loss: 0.01293109
[150/200] Training loss: 0.01133097
[200/200] Training loss: 0.01069599
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14254.21986641149 ----------
[1/200] Training loss: 0.16431749
[2/200] Training loss: 0.05951187
[3/200] Training loss: 0.05607710
[4/200] Training loss: 0.05318245
[5/200] Training loss: 0.05080828
[6/200] Training loss: 0.04790443
[7/200] Training loss: 0.04663505
[8/200] Training loss: 0.04281563
[9/200] Training loss: 0.03894343
[10/200] Training loss: 0.03772710
[50/200] Training loss: 0.01773550
[100/200] Training loss: 0.01536951
[150/200] Training loss: 0.01366286
[200/200] Training loss: 0.01316936
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12148.612431055655 ----------
[1/200] Training loss: 0.13816864
[2/200] Training loss: 0.05394322
[3/200] Training loss: 0.05189570
[4/200] Training loss: 0.04796000
[5/200] Training loss: 0.04660699
[6/200] Training loss: 0.04195344
[7/200] Training loss: 0.03935638
[8/200] Training loss: 0.03708580
[9/200] Training loss: 0.03660234
[10/200] Training loss: 0.03407619
[50/200] Training loss: 0.01681639
[100/200] Training loss: 0.01423163
[150/200] Training loss: 0.01280731
[200/200] Training loss: 0.01186856
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16958.190469504698 ----------
[1/200] Training loss: 0.14429866
[2/200] Training loss: 0.05864295
[3/200] Training loss: 0.05405853
[4/200] Training loss: 0.05075519
[5/200] Training loss: 0.05051335
[6/200] Training loss: 0.04911640
[7/200] Training loss: 0.04523028
[8/200] Training loss: 0.04008718
[9/200] Training loss: 0.04029896
[10/200] Training loss: 0.03762803
[50/200] Training loss: 0.01831396
[100/200] Training loss: 0.01527849
[150/200] Training loss: 0.01372006
[200/200] Training loss: 0.01272889
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 18472.292765111753 ----------
[1/200] Training loss: 0.11211629
[2/200] Training loss: 0.05881606
[3/200] Training loss: 0.05175837
[4/200] Training loss: 0.04867363
[5/200] Training loss: 0.04647945
[6/200] Training loss: 0.04243226
[7/200] Training loss: 0.03831353
[8/200] Training loss: 0.03800540
[9/200] Training loss: 0.03410040
[10/200] Training loss: 0.03351656
[50/200] Training loss: 0.01714652
[100/200] Training loss: 0.01447775
[150/200] Training loss: 0.01353313
[200/200] Training loss: 0.01200504
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.7319789082087752 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12858.560417091798 ----------
[1/200] Training loss: 0.10373835
[2/200] Training loss: 0.05469295
[3/200] Training loss: 0.04822142
[4/200] Training loss: 0.04550839
[5/200] Training loss: 0.03945784
[6/200] Training loss: 0.03779413
[7/200] Training loss: 0.03408539
[8/200] Training loss: 0.03260477
[9/200] Training loss: 0.02974122
[10/200] Training loss: 0.02864006
[50/200] Training loss: 0.01794311
[100/200] Training loss: 0.01557943
[150/200] Training loss: 0.01374111
[200/200] Training loss: 0.01254456
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14020.284733199964 ----------
[1/200] Training loss: 0.10884741
[2/200] Training loss: 0.05230380
[3/200] Training loss: 0.04632973
[4/200] Training loss: 0.04206858
[5/200] Training loss: 0.03911771
[6/200] Training loss: 0.03525163
[7/200] Training loss: 0.03392703
[8/200] Training loss: 0.03142984
[9/200] Training loss: 0.02963757
[10/200] Training loss: 0.02882319
[50/200] Training loss: 0.01653321
[100/200] Training loss: 0.01332726
[150/200] Training loss: 0.01208433
[200/200] Training loss: 0.01092948
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6297.411849323498 ----------
[1/200] Training loss: 0.09863610
[2/200] Training loss: 0.05077507
[3/200] Training loss: 0.04900550
[4/200] Training loss: 0.04529886
[5/200] Training loss: 0.04219940
[6/200] Training loss: 0.03953726
[7/200] Training loss: 0.03581707
[8/200] Training loss: 0.03288675
[9/200] Training loss: 0.03088559
[10/200] Training loss: 0.02800253
[50/200] Training loss: 0.01674256
[100/200] Training loss: 0.01454453
[150/200] Training loss: 0.01321415
[200/200] Training loss: 0.01248274
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.7319789082087752 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17269.173460244125 ----------
[1/200] Training loss: 0.14531029
[2/200] Training loss: 0.05649329
[3/200] Training loss: 0.05106967
[4/200] Training loss: 0.04736682
[5/200] Training loss: 0.04605725
[6/200] Training loss: 0.04665266
[7/200] Training loss: 0.04202039
[8/200] Training loss: 0.04183239
[9/200] Training loss: 0.04003355
[10/200] Training loss: 0.03725344
[50/200] Training loss: 0.01815769
[100/200] Training loss: 0.01413129
[150/200] Training loss: 0.01251221
[200/200] Training loss: 0.01117932
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12035.347938468583 ----------
[1/200] Training loss: 0.11880892
[2/200] Training loss: 0.05440337
[3/200] Training loss: 0.04929438
[4/200] Training loss: 0.04806824
[5/200] Training loss: 0.04357633
[6/200] Training loss: 0.04046594
[7/200] Training loss: 0.03603029
[8/200] Training loss: 0.03229315
[9/200] Training loss: 0.03067862
[10/200] Training loss: 0.02941904
[50/200] Training loss: 0.01670791
[100/200] Training loss: 0.01431005
[150/200] Training loss: 0.01332841
[200/200] Training loss: 0.01219406
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5069.715573875915 ----------
[1/200] Training loss: 0.14194834
[2/200] Training loss: 0.05627562
[3/200] Training loss: 0.05447841
[4/200] Training loss: 0.05135492
[5/200] Training loss: 0.04649782
[6/200] Training loss: 0.04355211
[7/200] Training loss: 0.04313649
[8/200] Training loss: 0.04279214
[9/200] Training loss: 0.03937186
[10/200] Training loss: 0.03785792
[50/200] Training loss: 0.01797035
[100/200] Training loss: 0.01562903
[150/200] Training loss: 0.01380560
[200/200] Training loss: 0.01209364
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17806.1265860939 ----------
[1/200] Training loss: 0.15449061
[2/200] Training loss: 0.05835845
[3/200] Training loss: 0.04993855
[4/200] Training loss: 0.04745038
[5/200] Training loss: 0.04678336
[6/200] Training loss: 0.04328470
[7/200] Training loss: 0.04212760
[8/200] Training loss: 0.04075342
[9/200] Training loss: 0.03743505
[10/200] Training loss: 0.03819815
[50/200] Training loss: 0.01870517
[100/200] Training loss: 0.01622316
[150/200] Training loss: 0.01487098
[200/200] Training loss: 0.01437984
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5587.092624970523 ----------
[1/200] Training loss: 0.11235048
[2/200] Training loss: 0.05256068
[3/200] Training loss: 0.04636956
[4/200] Training loss: 0.04039306
[5/200] Training loss: 0.03785023
[6/200] Training loss: 0.03392633
[7/200] Training loss: 0.02782242
[8/200] Training loss: 0.02704300
[9/200] Training loss: 0.02605035
[10/200] Training loss: 0.02585150
[50/200] Training loss: 0.01648547
[100/200] Training loss: 0.01487131
[150/200] Training loss: 0.01327185
[200/200] Training loss: 0.01255498
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6870.298392355313 ----------
[1/200] Training loss: 0.16148077
[2/200] Training loss: 0.05546764
[3/200] Training loss: 0.05342155
[4/200] Training loss: 0.05220746
[5/200] Training loss: 0.05063401
[6/200] Training loss: 0.04910906
[7/200] Training loss: 0.04588712
[8/200] Training loss: 0.04539374
[9/200] Training loss: 0.04183767
[10/200] Training loss: 0.04163143
[50/200] Training loss: 0.01910560
[100/200] Training loss: 0.01580102
[150/200] Training loss: 0.01417258
[200/200] Training loss: 0.01272937
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8523.729700078482 ----------
[1/200] Training loss: 0.14925635
[2/200] Training loss: 0.05721413
[3/200] Training loss: 0.05053433
[4/200] Training loss: 0.04603703
[5/200] Training loss: 0.04026530
[6/200] Training loss: 0.03890708
[7/200] Training loss: 0.03491879
[8/200] Training loss: 0.03340714
[9/200] Training loss: 0.03089957
[10/200] Training loss: 0.02880684
[50/200] Training loss: 0.01766027
[100/200] Training loss: 0.01543621
[150/200] Training loss: 0.01433091
[200/200] Training loss: 0.01373940
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8724.352124943147 ----------
[1/200] Training loss: 0.14675913
[2/200] Training loss: 0.05469738
[3/200] Training loss: 0.04992692
[4/200] Training loss: 0.04465029
[5/200] Training loss: 0.03960369
[6/200] Training loss: 0.04033788
[7/200] Training loss: 0.03739242
[8/200] Training loss: 0.03626538
[9/200] Training loss: 0.03111695
[10/200] Training loss: 0.02993658
[50/200] Training loss: 0.01761001
[100/200] Training loss: 0.01519870
[150/200] Training loss: 0.01333220
[200/200] Training loss: 0.01306740
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17610.757621408568 ----------
[1/200] Training loss: 0.10111599
[2/200] Training loss: 0.05315940
[3/200] Training loss: 0.04374421
[4/200] Training loss: 0.03888101
[5/200] Training loss: 0.03344321
[6/200] Training loss: 0.03143843
[7/200] Training loss: 0.02812214
[8/200] Training loss: 0.02638901
[9/200] Training loss: 0.02650599
[10/200] Training loss: 0.02525384
[50/200] Training loss: 0.01683502
[100/200] Training loss: 0.01414244
[150/200] Training loss: 0.01335955
[200/200] Training loss: 0.01232138
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.7319789082087752 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16197.55635890797 ----------
[1/200] Training loss: 0.10508990
[2/200] Training loss: 0.05332462
[3/200] Training loss: 0.04937003
[4/200] Training loss: 0.04328803
[5/200] Training loss: 0.03570148
[6/200] Training loss: 0.03419319
[7/200] Training loss: 0.03121649
[8/200] Training loss: 0.02925394
[9/200] Training loss: 0.02606559
[10/200] Training loss: 0.02702941
[50/200] Training loss: 0.01578618
[100/200] Training loss: 0.01358988
[150/200] Training loss: 0.01191671
[200/200] Training loss: 0.01130609
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5499.825815423612 ----------
[1/200] Training loss: 0.13741059
[2/200] Training loss: 0.05917239
[3/200] Training loss: 0.05249454
[4/200] Training loss: 0.04900488
[5/200] Training loss: 0.04646130
[6/200] Training loss: 0.04595540
[7/200] Training loss: 0.04371593
[8/200] Training loss: 0.04213325
[9/200] Training loss: 0.03857088
[10/200] Training loss: 0.03647634
[50/200] Training loss: 0.01621957
[100/200] Training loss: 0.01415476
[150/200] Training loss: 0.01274374
[200/200] Training loss: 0.01178179
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10797.62418312473 ----------
[1/200] Training loss: 0.10045471
[2/200] Training loss: 0.05489337
[3/200] Training loss: 0.05058911
[4/200] Training loss: 0.04664661
[5/200] Training loss: 0.04014023
[6/200] Training loss: 0.03571204
[7/200] Training loss: 0.03176522
[8/200] Training loss: 0.02896707
[9/200] Training loss: 0.02843425
[10/200] Training loss: 0.02579724
[50/200] Training loss: 0.01671743
[100/200] Training loss: 0.01490487
[150/200] Training loss: 0.01338652
[200/200] Training loss: 0.01213802
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 20468.91027876179 ----------
[1/200] Training loss: 0.16311434
[2/200] Training loss: 0.06131526
[3/200] Training loss: 0.05281844
[4/200] Training loss: 0.04709515
[5/200] Training loss: 0.04229074
[6/200] Training loss: 0.04003623
[7/200] Training loss: 0.03662955
[8/200] Training loss: 0.03862863
[9/200] Training loss: 0.03392346
[10/200] Training loss: 0.03136670
[50/200] Training loss: 0.01937644
[100/200] Training loss: 0.01720250
[150/200] Training loss: 0.01561932
[200/200] Training loss: 0.01464729
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8900.039101037702 ----------
[1/200] Training loss: 0.22451008
[2/200] Training loss: 0.06654730
[3/200] Training loss: 0.06115890
[4/200] Training loss: 0.05380416
[5/200] Training loss: 0.05164104
[6/200] Training loss: 0.05687813
[7/200] Training loss: 0.05265220
[8/200] Training loss: 0.05168785
[9/200] Training loss: 0.04940599
[10/200] Training loss: 0.05092922
[50/200] Training loss: 0.02556775
[100/200] Training loss: 0.01715216
[150/200] Training loss: 0.01460044
[200/200] Training loss: 0.01270348
---batch_size---: 32 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.378630221872867 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16283.766149143754 ----------
[1/200] Training loss: 0.11770154
[2/200] Training loss: 0.05502709
[3/200] Training loss: 0.05010126
[4/200] Training loss: 0.04609822
[5/200] Training loss: 0.04190267
[6/200] Training loss: 0.03858817
[7/200] Training loss: 0.03748779
[8/200] Training loss: 0.03530249
[9/200] Training loss: 0.03404282
[10/200] Training loss: 0.03061881
[50/200] Training loss: 0.01623066
[100/200] Training loss: 0.01395764
[150/200] Training loss: 0.01294687
[200/200] Training loss: 0.01177061
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4160.5651058480025 ----------
[1/200] Training loss: 0.11899930
[2/200] Training loss: 0.05507889
[3/200] Training loss: 0.05238614
[4/200] Training loss: 0.04867887
[5/200] Training loss: 0.04605649
[6/200] Training loss: 0.04200394
[7/200] Training loss: 0.03884717
[8/200] Training loss: 0.03539440
[9/200] Training loss: 0.03357229
[10/200] Training loss: 0.03134504
[50/200] Training loss: 0.01569132
[100/200] Training loss: 0.01139436
[150/200] Training loss: 0.01037249
[200/200] Training loss: 0.00965854
---batch_size---: 8 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12775.99436443207 ----------
[1/200] Training loss: 0.13841950
[2/200] Training loss: 0.05377276
[3/200] Training loss: 0.04845750
[4/200] Training loss: 0.04426390
[5/200] Training loss: 0.04272136
[6/200] Training loss: 0.03895017
[7/200] Training loss: 0.03597000
[8/200] Training loss: 0.03591769
[9/200] Training loss: 0.03349662
[10/200] Training loss: 0.03188707
[50/200] Training loss: 0.01696883
[100/200] Training loss: 0.01507325
[150/200] Training loss: 0.01353439
[200/200] Training loss: 0.01309984
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7466.652797606167 ----------
[1/200] Training loss: 0.13666855
[2/200] Training loss: 0.05705722
[3/200] Training loss: 0.05012112
[4/200] Training loss: 0.04712272
[5/200] Training loss: 0.04541421
[6/200] Training loss: 0.03996009
[7/200] Training loss: 0.03788755
[8/200] Training loss: 0.03474127
[9/200] Training loss: 0.03503171
[10/200] Training loss: 0.03279644
[50/200] Training loss: 0.01810964
[100/200] Training loss: 0.01572743
[150/200] Training loss: 0.01485008
[200/200] Training loss: 0.01360180
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 24308.846126461864 ----------
[1/200] Training loss: 0.10364873
[2/200] Training loss: 0.04921573
[3/200] Training loss: 0.04125779
[4/200] Training loss: 0.03789144
[5/200] Training loss: 0.03448751
[6/200] Training loss: 0.02901518
[7/200] Training loss: 0.02836210
[8/200] Training loss: 0.02611925
[9/200] Training loss: 0.02502357
[10/200] Training loss: 0.02432916
[50/200] Training loss: 0.01656693
[100/200] Training loss: 0.01414928
[150/200] Training loss: 0.01192878
[200/200] Training loss: 0.01084799
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13146.468423116528 ----------
[1/200] Training loss: 0.12271806
[2/200] Training loss: 0.05736720
[3/200] Training loss: 0.05232966
[4/200] Training loss: 0.04869516
[5/200] Training loss: 0.04207215
[6/200] Training loss: 0.03899732
[7/200] Training loss: 0.03639806
[8/200] Training loss: 0.03385313
[9/200] Training loss: 0.03147260
[10/200] Training loss: 0.02857367
[50/200] Training loss: 0.01681336
[100/200] Training loss: 0.01440492
[150/200] Training loss: 0.01324339
[200/200] Training loss: 0.01228391
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13676.742887105833 ----------
[1/200] Training loss: 0.10855338
[2/200] Training loss: 0.05304780
[3/200] Training loss: 0.04485609
[4/200] Training loss: 0.04084119
[5/200] Training loss: 0.03712133
[6/200] Training loss: 0.03381824
[7/200] Training loss: 0.03193728
[8/200] Training loss: 0.02960051
[9/200] Training loss: 0.02624769
[10/200] Training loss: 0.02524553
[50/200] Training loss: 0.01660897
[100/200] Training loss: 0.01459719
[150/200] Training loss: 0.01311573
[200/200] Training loss: 0.01234512
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15214.017746801796 ----------
[1/200] Training loss: 0.13391726
[2/200] Training loss: 0.05575855
[3/200] Training loss: 0.05288680
[4/200] Training loss: 0.04589677
[5/200] Training loss: 0.04042949
[6/200] Training loss: 0.03640538
[7/200] Training loss: 0.03131032
[8/200] Training loss: 0.03038337
[9/200] Training loss: 0.02779020
[10/200] Training loss: 0.02703087
[50/200] Training loss: 0.01745202
[100/200] Training loss: 0.01509951
[150/200] Training loss: 0.01346113
[200/200] Training loss: 0.01285542
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10971.856360707608 ----------
[1/200] Training loss: 0.01062689
[2/200] Training loss: 0.00177714
[3/200] Training loss: 0.00158808
[4/200] Training loss: 0.00125305
[5/200] Training loss: 0.00107874
[6/200] Training loss: 0.00088236
[7/200] Training loss: 0.00073589
[8/200] Training loss: 0.00060587
[9/200] Training loss: 0.00058370
[10/200] Training loss: 0.00050035
[50/200] Training loss: 0.00018486
[100/200] Training loss: 0.00012926
[150/200] Training loss: 0.00010994
[200/200] Training loss: 0.00010440
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6064.976834250894 ----------
[1/200] Training loss: 0.12320717
[2/200] Training loss: 0.05464805
[3/200] Training loss: 0.04769887
[4/200] Training loss: 0.04234453
[5/200] Training loss: 0.03939563
[6/200] Training loss: 0.03562677
[7/200] Training loss: 0.03161554
[8/200] Training loss: 0.02937660
[9/200] Training loss: 0.02757712
[10/200] Training loss: 0.02772049
[50/200] Training loss: 0.01493471
[100/200] Training loss: 0.01246313
[150/200] Training loss: 0.01147359
[200/200] Training loss: 0.01066715
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16663.03309724853 ----------
[1/200] Training loss: 0.15556267
[2/200] Training loss: 0.06188533
[3/200] Training loss: 0.05540183
[4/200] Training loss: 0.05087317
[5/200] Training loss: 0.04898798
[6/200] Training loss: 0.04718255
[7/200] Training loss: 0.04602834
[8/200] Training loss: 0.04576229
[9/200] Training loss: 0.04288494
[10/200] Training loss: 0.04138663
[50/200] Training loss: 0.01961756
[100/200] Training loss: 0.01613275
[150/200] Training loss: 0.01426894
[200/200] Training loss: 0.01287454
---batch_size---: 16 ---n_steps---: 2 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10975.327603310983 ----------
[1/200] Training loss: 0.15257194
[2/200] Training loss: 0.05586411
[3/200] Training loss: 0.04703698
[4/200] Training loss: 0.04607575
[5/200] Training loss: 0.04210012
[6/200] Training loss: 0.03870082
[7/200] Training loss: 0.03808369
[8/200] Training loss: 0.03498336
[9/200] Training loss: 0.03391022
[10/200] Training loss: 0.03373515
[50/200] Training loss: 0.01820995
[100/200] Training loss: 0.01580713
[150/200] Training loss: 0.01467850
[200/200] Training loss: 0.01364726
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10108.39492699014 ----------
[1/200] Training loss: 0.10602800
[2/200] Training loss: 0.05289290
[3/200] Training loss: 0.04810328
[4/200] Training loss: 0.03965600
[5/200] Training loss: 0.03615212
[6/200] Training loss: 0.03250784
[7/200] Training loss: 0.02795119
[8/200] Training loss: 0.02912714
[9/200] Training loss: 0.02739826
[10/200] Training loss: 0.02406275
[50/200] Training loss: 0.01757543
[100/200] Training loss: 0.01405602
[150/200] Training loss: 0.01199453
[200/200] Training loss: 0.01098074
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15154.697225612923 ----------
[1/200] Training loss: 0.10716040
[2/200] Training loss: 0.05414167
[3/200] Training loss: 0.05154531
[4/200] Training loss: 0.04820730
[5/200] Training loss: 0.04478372
[6/200] Training loss: 0.04034676
[7/200] Training loss: 0.03638242
[8/200] Training loss: 0.03268249
[9/200] Training loss: 0.02979998
[10/200] Training loss: 0.02638392
[50/200] Training loss: 0.01653320
[100/200] Training loss: 0.01334165
[150/200] Training loss: 0.01163109
[200/200] Training loss: 0.01090630
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13450.215165565196 ----------
[1/200] Training loss: 0.11321115
[2/200] Training loss: 0.05085857
[3/200] Training loss: 0.04377362
[4/200] Training loss: 0.04031776
[5/200] Training loss: 0.03532380
[6/200] Training loss: 0.03515066
[7/200] Training loss: 0.03208745
[8/200] Training loss: 0.03061245
[9/200] Training loss: 0.02976916
[10/200] Training loss: 0.02766494
[50/200] Training loss: 0.01714618
[100/200] Training loss: 0.01374070
[150/200] Training loss: 0.01213162
[200/200] Training loss: 0.01138850
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6502.451537689458 ----------
[1/200] Training loss: 0.15788401
[2/200] Training loss: 0.06039539
[3/200] Training loss: 0.05672179
[4/200] Training loss: 0.05162672
[5/200] Training loss: 0.05028724
[6/200] Training loss: 0.04841519
[7/200] Training loss: 0.04314441
[8/200] Training loss: 0.04299134
[9/200] Training loss: 0.04052435
[10/200] Training loss: 0.03711929
[50/200] Training loss: 0.01799475
[100/200] Training loss: 0.01495112
[150/200] Training loss: 0.01308832
[200/200] Training loss: 0.01229748
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12455.776491250957 ----------
[1/200] Training loss: 0.11704563
[2/200] Training loss: 0.05550504
[3/200] Training loss: 0.04705230
[4/200] Training loss: 0.04351776
[5/200] Training loss: 0.04038744
[6/200] Training loss: 0.03751197
[7/200] Training loss: 0.03280803
[8/200] Training loss: 0.03026655
[9/200] Training loss: 0.03046484
[10/200] Training loss: 0.02682735
[50/200] Training loss: 0.01606798
[100/200] Training loss: 0.01348385
[150/200] Training loss: 0.01143910
[200/200] Training loss: 0.01089048
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11797.26544585651 ----------
[1/200] Training loss: 0.11708558
[2/200] Training loss: 0.05031235
[3/200] Training loss: 0.04550890
[4/200] Training loss: 0.04037722
[5/200] Training loss: 0.03516115
[6/200] Training loss: 0.03269678
[7/200] Training loss: 0.03148376
[8/200] Training loss: 0.03043871
[9/200] Training loss: 0.02705279
[10/200] Training loss: 0.02676600
[50/200] Training loss: 0.01682792
[100/200] Training loss: 0.01520563
[150/200] Training loss: 0.01285369
[200/200] Training loss: 0.01151558
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6303.8769023514415 ----------
[1/200] Training loss: 0.14927439
[2/200] Training loss: 0.05551666
[3/200] Training loss: 0.04998109
[4/200] Training loss: 0.04487329
[5/200] Training loss: 0.04058535
[6/200] Training loss: 0.03745782
[7/200] Training loss: 0.03645900
[8/200] Training loss: 0.03361332
[9/200] Training loss: 0.03159000
[10/200] Training loss: 0.03151623
[50/200] Training loss: 0.01743451
[100/200] Training loss: 0.01549978
[150/200] Training loss: 0.01467163
[200/200] Training loss: 0.01260897
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15936.641051363364 ----------
[1/200] Training loss: 0.12162055
[2/200] Training loss: 0.05474479
[3/200] Training loss: 0.04994982
[4/200] Training loss: 0.04352058
[5/200] Training loss: 0.04060981
[6/200] Training loss: 0.03545742
[7/200] Training loss: 0.03190532
[8/200] Training loss: 0.02919152
[9/200] Training loss: 0.02774824
[10/200] Training loss: 0.02665626
[50/200] Training loss: 0.01493688
[100/200] Training loss: 0.01214410
[150/200] Training loss: 0.01069638
[200/200] Training loss: 0.01019493
---batch_size---: 8 ---n_steps---: 8 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15155.692263964718 ----------
[1/200] Training loss: 0.12501207
[2/200] Training loss: 0.05715139
[3/200] Training loss: 0.05037133
[4/200] Training loss: 0.04617049
[5/200] Training loss: 0.04394906
[6/200] Training loss: 0.04204628
[7/200] Training loss: 0.04159974
[8/200] Training loss: 0.04118855
[9/200] Training loss: 0.03619534
[10/200] Training loss: 0.03758248
[50/200] Training loss: 0.01783359
[100/200] Training loss: 0.01457759
[150/200] Training loss: 0.01304559
[200/200] Training loss: 0.01184149
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15313.495747215919 ----------
[1/200] Training loss: 0.12577380
[2/200] Training loss: 0.05163548
[3/200] Training loss: 0.04482902
[4/200] Training loss: 0.03985555
[5/200] Training loss: 0.03794782
[6/200] Training loss: 0.03257493
[7/200] Training loss: 0.02977313
[8/200] Training loss: 0.02776283
[9/200] Training loss: 0.02696424
[10/200] Training loss: 0.02537390
[50/200] Training loss: 0.01608796
[100/200] Training loss: 0.01446413
[150/200] Training loss: 0.01267107
[200/200] Training loss: 0.01175225
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 3920.898111402539 ----------
[1/200] Training loss: 0.16409382
[2/200] Training loss: 0.06400258
[3/200] Training loss: 0.05399070
[4/200] Training loss: 0.04950597
[5/200] Training loss: 0.04727072
[6/200] Training loss: 0.04523153
[7/200] Training loss: 0.04333034
[8/200] Training loss: 0.04093084
[9/200] Training loss: 0.03737052
[10/200] Training loss: 0.03687126
[50/200] Training loss: 0.01864206
[100/200] Training loss: 0.01642375
[150/200] Training loss: 0.01444828
[200/200] Training loss: 0.01389241
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13207.892791812023 ----------
[1/200] Training loss: 0.15728248
[2/200] Training loss: 0.05784993
[3/200] Training loss: 0.05413925
[4/200] Training loss: 0.05041287
[5/200] Training loss: 0.04693827
[6/200] Training loss: 0.04132018
[7/200] Training loss: 0.03791898
[8/200] Training loss: 0.03651386
[9/200] Training loss: 0.03090564
[10/200] Training loss: 0.03209317
[50/200] Training loss: 0.01773845
[100/200] Training loss: 0.01548793
[150/200] Training loss: 0.01491060
[200/200] Training loss: 0.01380998
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 18133.08225316369 ----------
[1/200] Training loss: 0.10262821
[2/200] Training loss: 0.05508501
[3/200] Training loss: 0.04903446
[4/200] Training loss: 0.04544872
[5/200] Training loss: 0.04183198
[6/200] Training loss: 0.03885775
[7/200] Training loss: 0.03514787
[8/200] Training loss: 0.03467448
[9/200] Training loss: 0.03236501
[10/200] Training loss: 0.03086006
[50/200] Training loss: 0.01630670
[100/200] Training loss: 0.01322703
[150/200] Training loss: 0.01250016
[200/200] Training loss: 0.01115644
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14786.545235449692 ----------
[1/200] Training loss: 0.12369856
[2/200] Training loss: 0.05637771
[3/200] Training loss: 0.04678306
[4/200] Training loss: 0.04374222
[5/200] Training loss: 0.04180195
[6/200] Training loss: 0.04282703
[7/200] Training loss: 0.03840807
[8/200] Training loss: 0.03502498
[9/200] Training loss: 0.03365200
[10/200] Training loss: 0.03278128
[50/200] Training loss: 0.01817442
[100/200] Training loss: 0.01470239
[150/200] Training loss: 0.01364766
[200/200] Training loss: 0.01271710
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5346.420110690891 ----------
[1/200] Training loss: 0.11070423
[2/200] Training loss: 0.05554605
[3/200] Training loss: 0.05055252
[4/200] Training loss: 0.04712569
[5/200] Training loss: 0.04314395
[6/200] Training loss: 0.03798888
[7/200] Training loss: 0.03387709
[8/200] Training loss: 0.02893706
[9/200] Training loss: 0.02709186
[10/200] Training loss: 0.02721507
[50/200] Training loss: 0.01612285
[100/200] Training loss: 0.01379020
[150/200] Training loss: 0.01225740
[200/200] Training loss: 0.01157852
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 24249.901608047814 ----------
[1/100] Training loss: 0.10587885
[2/100] Training loss: 0.05157879
[3/100] Training loss: 0.04472365
[4/100] Training loss: 0.04156114
[5/100] Training loss: 0.03979945
[6/100] Training loss: 0.03616323
[7/100] Training loss: 0.03366628
[8/100] Training loss: 0.03069064
[9/100] Training loss: 0.02934423
[10/100] Training loss: 0.02758424
[50/100] Training loss: 0.01505658
[100/100] Training loss: 0.01236473
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6638.6649260224 ----------
[1/200] Training loss: 0.11092539
[2/200] Training loss: 0.05258178
[3/200] Training loss: 0.04910318
[4/200] Training loss: 0.04467852
[5/200] Training loss: 0.03969736
[6/200] Training loss: 0.03852305
[7/200] Training loss: 0.03317929
[8/200] Training loss: 0.03208445
[9/200] Training loss: 0.02963909
[10/200] Training loss: 0.02831232
[50/200] Training loss: 0.01716438
[100/200] Training loss: 0.01485733
[150/200] Training loss: 0.01307800
[200/200] Training loss: 0.01248119
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15848.55122716269 ----------
[1/200] Training loss: 0.10901544
[2/200] Training loss: 0.05742702
[3/200] Training loss: 0.05144676
[4/200] Training loss: 0.04671220
[5/200] Training loss: 0.04183127
[6/200] Training loss: 0.03782736
[7/200] Training loss: 0.03455422
[8/200] Training loss: 0.03217491
[9/200] Training loss: 0.02807004
[10/200] Training loss: 0.02685049
[50/200] Training loss: 0.01762151
[100/200] Training loss: 0.01580560
[150/200] Training loss: 0.01394219
[200/200] Training loss: 0.01213151
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13459.246338484187 ----------
[1/200] Training loss: 0.02222340
[2/200] Training loss: 0.00223780
[3/200] Training loss: 0.00182557
[4/200] Training loss: 0.00147045
[5/200] Training loss: 0.00104756
[6/200] Training loss: 0.00084059
[7/200] Training loss: 0.00065562
[8/200] Training loss: 0.00050860
[9/200] Training loss: 0.00051014
[10/200] Training loss: 0.00044635
[50/200] Training loss: 0.00020948
[100/200] Training loss: 0.00014950
[150/200] Training loss: 0.00012433
[200/200] Training loss: 0.00011677
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11490.19860576831 ----------
[1/200] Training loss: 0.11404874
[2/200] Training loss: 0.05224056
[3/200] Training loss: 0.04773185
[4/200] Training loss: 0.04249612
[5/200] Training loss: 0.03812571
[6/200] Training loss: 0.03490613
[7/200] Training loss: 0.03201003
[8/200] Training loss: 0.02940263
[9/200] Training loss: 0.02761332
[10/200] Training loss: 0.02613578
[50/200] Training loss: 0.01592438
[100/200] Training loss: 0.01316299
[150/200] Training loss: 0.01205780
[200/200] Training loss: 0.01114591
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5477.477886765039 ----------
[1/200] Training loss: 0.11783987
[2/200] Training loss: 0.05406664
[3/200] Training loss: 0.04958009
[4/200] Training loss: 0.04321819
[5/200] Training loss: 0.03872676
[6/200] Training loss: 0.03522661
[7/200] Training loss: 0.03299796
[8/200] Training loss: 0.03006168
[9/200] Training loss: 0.02813253
[10/200] Training loss: 0.02763491
[50/200] Training loss: 0.01675352
[100/200] Training loss: 0.01489641
[150/200] Training loss: 0.01398894
[200/200] Training loss: 0.01317238
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9812.051773202178 ----------
[1/200] Training loss: 0.13332082
[2/200] Training loss: 0.05995437
[3/200] Training loss: 0.05345745
[4/200] Training loss: 0.05073046
[5/200] Training loss: 0.04786812
[6/200] Training loss: 0.04602966
[7/200] Training loss: 0.04225115
[8/200] Training loss: 0.03836180
[9/200] Training loss: 0.03604521
[10/200] Training loss: 0.03036692
[50/200] Training loss: 0.01748755
[100/200] Training loss: 0.01554325
[150/200] Training loss: 0.01453137
[200/200] Training loss: 0.01325465
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.7319789082087752 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12776.510947829223 ----------
[1/200] Training loss: 0.10128910
[2/200] Training loss: 0.04819182
[3/200] Training loss: 0.04150306
[4/200] Training loss: 0.03879170
[5/200] Training loss: 0.03498050
[6/200] Training loss: 0.03358151
[7/200] Training loss: 0.03087379
[8/200] Training loss: 0.02798931
[9/200] Training loss: 0.02767514
[10/200] Training loss: 0.02581757
[50/200] Training loss: 0.01610846
[100/200] Training loss: 0.01297098
[150/200] Training loss: 0.01162234
[200/200] Training loss: 0.01076692
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6544.7273434422 ----------
[1/200] Training loss: 0.12651164
[2/200] Training loss: 0.05592685
[3/200] Training loss: 0.04893415
[4/200] Training loss: 0.04676852
[5/200] Training loss: 0.04400323
[6/200] Training loss: 0.04331534
[7/200] Training loss: 0.03862559
[8/200] Training loss: 0.03841742
[9/200] Training loss: 0.04063268
[10/200] Training loss: 0.03492644
[50/200] Training loss: 0.01864544
[100/200] Training loss: 0.01500792
[150/200] Training loss: 0.01310842
[200/200] Training loss: 0.01221135
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 3266.070115597643 ----------
[1/200] Training loss: 0.15950088
[2/200] Training loss: 0.05812345
[3/200] Training loss: 0.05022985
[4/200] Training loss: 0.04642795
[5/200] Training loss: 0.04316435
[6/200] Training loss: 0.03962823
[7/200] Training loss: 0.03812909
[8/200] Training loss: 0.03648301
[9/200] Training loss: 0.03463989
[10/200] Training loss: 0.03378673
[50/200] Training loss: 0.01892115
[100/200] Training loss: 0.01668312
[150/200] Training loss: 0.01419163
[200/200] Training loss: 0.01369644
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9989.568959669881 ----------
[1/200] Training loss: 0.13517768
[2/200] Training loss: 0.05517208
[3/200] Training loss: 0.04566226
[4/200] Training loss: 0.04349477
[5/200] Training loss: 0.04378718
[6/200] Training loss: 0.03819140
[7/200] Training loss: 0.03901938
[8/200] Training loss: 0.03714794
[9/200] Training loss: 0.03189369
[10/200] Training loss: 0.03304084
[50/200] Training loss: 0.01946225
[100/200] Training loss: 0.01496843
[150/200] Training loss: 0.01340700
[200/200] Training loss: 0.01193552
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7019.184852958355 ----------
[1/200] Training loss: 0.09836919
[2/200] Training loss: 0.04864203
[3/200] Training loss: 0.04270248
[4/200] Training loss: 0.03625295
[5/200] Training loss: 0.03396190
[6/200] Training loss: 0.03061281
[7/200] Training loss: 0.02722231
[8/200] Training loss: 0.02747021
[9/200] Training loss: 0.02477001
[10/200] Training loss: 0.02489866
[50/200] Training loss: 0.01578011
[100/200] Training loss: 0.01298099
[150/200] Training loss: 0.01205700
[200/200] Training loss: 0.01106859
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12145.122477768597 ----------
[1/200] Training loss: 0.16626475
[2/200] Training loss: 0.05860145
[3/200] Training loss: 0.05538139
[4/200] Training loss: 0.05129656
[5/200] Training loss: 0.04809304
[6/200] Training loss: 0.04606541
[7/200] Training loss: 0.04534215
[8/200] Training loss: 0.04109198
[9/200] Training loss: 0.03820257
[10/200] Training loss: 0.03908095
[50/200] Training loss: 0.01690838
[100/200] Training loss: 0.01541582
[150/200] Training loss: 0.01378675
[200/200] Training loss: 0.01232366
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12381.329815492358 ----------
[1/200] Training loss: 0.10784506
[2/200] Training loss: 0.05020391
[3/200] Training loss: 0.04738566
[4/200] Training loss: 0.04217917
[5/200] Training loss: 0.03610539
[6/200] Training loss: 0.03440273
[7/200] Training loss: 0.02968186
[8/200] Training loss: 0.02931254
[9/200] Training loss: 0.02683842
[10/200] Training loss: 0.02658185
[50/200] Training loss: 0.01648390
[100/200] Training loss: 0.01407783
[150/200] Training loss: 0.01250009
[200/200] Training loss: 0.01159294
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8173.028569630722 ----------
[1/200] Training loss: 0.08756654
[2/200] Training loss: 0.05085724
[3/200] Training loss: 0.04588764
[4/200] Training loss: 0.04034027
[5/200] Training loss: 0.03516973
[6/200] Training loss: 0.03082178
[7/200] Training loss: 0.02856773
[8/200] Training loss: 0.02661462
[9/200] Training loss: 0.02492451
[10/200] Training loss: 0.02392644
[50/200] Training loss: 0.01500305
[100/200] Training loss: 0.01270372
[150/200] Training loss: 0.01141942
[200/200] Training loss: 0.01088876
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15120.948118421675 ----------
[1/200] Training loss: 0.11583485
[2/200] Training loss: 0.05458449
[3/200] Training loss: 0.05211779
[4/200] Training loss: 0.04823201
[5/200] Training loss: 0.04379651
[6/200] Training loss: 0.03958744
[7/200] Training loss: 0.03671056
[8/200] Training loss: 0.03275675
[9/200] Training loss: 0.02904216
[10/200] Training loss: 0.02803922
[50/200] Training loss: 0.01632218
[100/200] Training loss: 0.01372891
[150/200] Training loss: 0.01211038
[200/200] Training loss: 0.01156911
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.7319789082087752 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7485.565843675413 ----------
[1/200] Training loss: 0.10382430
[2/200] Training loss: 0.05528820
[3/200] Training loss: 0.05178429
[4/200] Training loss: 0.04852905
[5/200] Training loss: 0.04365038
[6/200] Training loss: 0.04018402
[7/200] Training loss: 0.03700357
[8/200] Training loss: 0.03534437
[9/200] Training loss: 0.03192692
[10/200] Training loss: 0.03052339
[50/200] Training loss: 0.01667301
[100/200] Training loss: 0.01349607
[150/200] Training loss: 0.01258959
[200/200] Training loss: 0.01158755
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16786.458351897818 ----------
[1/200] Training loss: 0.09731759
[2/200] Training loss: 0.05326937
[3/200] Training loss: 0.04585052
[4/200] Training loss: 0.04150279
[5/200] Training loss: 0.03743653
[6/200] Training loss: 0.03549549
[7/200] Training loss: 0.03254966
[8/200] Training loss: 0.03073239
[9/200] Training loss: 0.02875723
[10/200] Training loss: 0.02749772
[50/200] Training loss: 0.01777449
[100/200] Training loss: 0.01660904
[150/200] Training loss: 0.01488766
[200/200] Training loss: 0.01381340
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9309.765625406475 ----------
[1/200] Training loss: 0.18202440
[2/200] Training loss: 0.05849463
[3/200] Training loss: 0.05251542
[4/200] Training loss: 0.04942799
[5/200] Training loss: 0.04447880
[6/200] Training loss: 0.04290885
[7/200] Training loss: 0.03899342
[8/200] Training loss: 0.03714305
[9/200] Training loss: 0.03549108
[10/200] Training loss: 0.03260254
[50/200] Training loss: 0.01937222
[100/200] Training loss: 0.01696110
[150/200] Training loss: 0.01599110
[200/200] Training loss: 0.01515603
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.7319789082087752 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5631.610604436354 ----------
[1/200] Training loss: 0.11052542
[2/200] Training loss: 0.05389001
[3/200] Training loss: 0.04774782
[4/200] Training loss: 0.04352959
[5/200] Training loss: 0.03679919
[6/200] Training loss: 0.03513192
[7/200] Training loss: 0.03089336
[8/200] Training loss: 0.03043516
[9/200] Training loss: 0.02897375
[10/200] Training loss: 0.02831536
[50/200] Training loss: 0.01775772
[100/200] Training loss: 0.01415164
[150/200] Training loss: 0.01282383
[200/200] Training loss: 0.01200494
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6966131684157987 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7296.450369871641 ----------
[1/200] Training loss: 0.15437882
[2/200] Training loss: 0.06198260
[3/200] Training loss: 0.05299414
[4/200] Training loss: 0.05211177
[5/200] Training loss: 0.04957024
[6/200] Training loss: 0.04806491
[7/200] Training loss: 0.04455807
[8/200] Training loss: 0.04149890
[9/200] Training loss: 0.03814431
[10/200] Training loss: 0.03566360
[50/200] Training loss: 0.01790469
[100/200] Training loss: 0.01615628
[150/200] Training loss: 0.01473045
[200/200] Training loss: 0.01394902
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16340.623243928 ----------
[1/200] Training loss: 0.16847389
[2/200] Training loss: 0.06062870
[3/200] Training loss: 0.04899291
[4/200] Training loss: 0.04728942
[5/200] Training loss: 0.04582854
[6/200] Training loss: 0.04257463
[7/200] Training loss: 0.04101132
[8/200] Training loss: 0.03931920
[9/200] Training loss: 0.03858541
[10/200] Training loss: 0.03745527
[50/200] Training loss: 0.01899156
[100/200] Training loss: 0.01549167
[150/200] Training loss: 0.01453428
[200/200] Training loss: 0.01411133
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9847.07631736446 ----------
[1/200] Training loss: 0.10601738
[2/200] Training loss: 0.05142827
[3/200] Training loss: 0.04441522
[4/200] Training loss: 0.04111435
[5/200] Training loss: 0.03710675
[6/200] Training loss: 0.03430562
[7/200] Training loss: 0.03257567
[8/200] Training loss: 0.03203551
[9/200] Training loss: 0.02820593
[10/200] Training loss: 0.02702814
[50/200] Training loss: 0.01539762
[100/200] Training loss: 0.01325811
[150/200] Training loss: 0.01254620
[200/200] Training loss: 0.01109409
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 3735.0621146106796 ----------
[1/200] Training loss: 0.10568023
[2/200] Training loss: 0.04687967
[3/200] Training loss: 0.04167168
[4/200] Training loss: 0.03631510
[5/200] Training loss: 0.03217419
[6/200] Training loss: 0.02981394
[7/200] Training loss: 0.02729341
[8/200] Training loss: 0.02697690
[9/200] Training loss: 0.02582901
[10/200] Training loss: 0.02486455
[50/200] Training loss: 0.01509471
[100/200] Training loss: 0.01122756
[150/200] Training loss: 0.01006828
[200/200] Training loss: 0.00913247
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18814.23291021986 ----------
[1/200] Training loss: 0.11571341
[2/200] Training loss: 0.05139624
[3/200] Training loss: 0.04257895
[4/200] Training loss: 0.03763882
[5/200] Training loss: 0.03394257
[6/200] Training loss: 0.03061823
[7/200] Training loss: 0.02910461
[8/200] Training loss: 0.02721616
[9/200] Training loss: 0.02648808
[10/200] Training loss: 0.02488796
[50/200] Training loss: 0.01737550
[100/200] Training loss: 0.01506244
[150/200] Training loss: 0.01312596
[200/200] Training loss: 0.01152051
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9300.729433759483 ----------
[1/200] Training loss: 0.14363698
[2/200] Training loss: 0.06174842
[3/200] Training loss: 0.05266919
[4/200] Training loss: 0.05119249
[5/200] Training loss: 0.05038766
[6/200] Training loss: 0.04643408
[7/200] Training loss: 0.04418649
[8/200] Training loss: 0.04167472
[9/200] Training loss: 0.03856840
[10/200] Training loss: 0.03791324
[50/200] Training loss: 0.01772602
[100/200] Training loss: 0.01449397
[150/200] Training loss: 0.01344017
[200/200] Training loss: 0.01261117
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10433.668194839243 ----------
[1/200] Training loss: 0.09605001
[2/200] Training loss: 0.05069607
[3/200] Training loss: 0.04593170
[4/200] Training loss: 0.04215937
[5/200] Training loss: 0.03570040
[6/200] Training loss: 0.03302715
[7/200] Training loss: 0.02915269
[8/200] Training loss: 0.02823630
[9/200] Training loss: 0.02590350
[10/200] Training loss: 0.02387730
[50/200] Training loss: 0.01620689
[100/200] Training loss: 0.01364347
[150/200] Training loss: 0.01237312
[200/200] Training loss: 0.01102650
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11407.53645622051 ----------
[1/200] Training loss: 0.10570066
[2/200] Training loss: 0.05070563
[3/200] Training loss: 0.04753003
[4/200] Training loss: 0.04408379
[5/200] Training loss: 0.03971484
[6/200] Training loss: 0.03612600
[7/200] Training loss: 0.03292162
[8/200] Training loss: 0.03199167
[9/200] Training loss: 0.02830555
[10/200] Training loss: 0.02778202
[50/200] Training loss: 0.01492242
[100/200] Training loss: 0.01326476
[150/200] Training loss: 0.01204796
[200/200] Training loss: 0.01100539
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.7319789082087752 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12916.505409746089 ----------
[1/200] Training loss: 0.14308387
[2/200] Training loss: 0.06075562
[3/200] Training loss: 0.04931513
[4/200] Training loss: 0.04892922
[5/200] Training loss: 0.04456964
[6/200] Training loss: 0.04293806
[7/200] Training loss: 0.03628199
[8/200] Training loss: 0.03498344
[9/200] Training loss: 0.03103854
[10/200] Training loss: 0.02835328
[50/200] Training loss: 0.01823685
[100/200] Training loss: 0.01581318
[150/200] Training loss: 0.01462458
[200/200] Training loss: 0.01308451
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17035.333633363334 ----------
[1/200] Training loss: 0.10428988
[2/200] Training loss: 0.04943886
[3/200] Training loss: 0.04521749
[4/200] Training loss: 0.03836507
[5/200] Training loss: 0.03274512
[6/200] Training loss: 0.03296152
[7/200] Training loss: 0.03128418
[8/200] Training loss: 0.02824441
[9/200] Training loss: 0.02720084
[10/200] Training loss: 0.02674609
[50/200] Training loss: 0.01450220
[100/200] Training loss: 0.01198516
[150/200] Training loss: 0.01128255
[200/200] Training loss: 0.01074157
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7252.395742097917 ----------
[1/200] Training loss: 0.11771043
[2/200] Training loss: 0.05228508
[3/200] Training loss: 0.04701865
[4/200] Training loss: 0.04435666
[5/200] Training loss: 0.03958870
[6/200] Training loss: 0.03592379
[7/200] Training loss: 0.03320955
[8/200] Training loss: 0.02952672
[9/200] Training loss: 0.02896526
[10/200] Training loss: 0.02652509
[50/200] Training loss: 0.01505010
[100/200] Training loss: 0.01257763
[150/200] Training loss: 0.01195849
[200/200] Training loss: 0.01115403
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11118.254539270092 ----------
[1/100] Training loss: 0.11268502
[2/100] Training loss: 0.05530590
[3/100] Training loss: 0.05040107
[4/100] Training loss: 0.04861407
[5/100] Training loss: 0.04547428
[6/100] Training loss: 0.03740263
[7/100] Training loss: 0.03617061
[8/100] Training loss: 0.03321851
[9/100] Training loss: 0.02980314
[10/100] Training loss: 0.03020694
[50/100] Training loss: 0.01628145
[100/100] Training loss: 0.01366856
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15752.165057540504 ----------
[1/200] Training loss: 0.14503088
[2/200] Training loss: 0.06148081
[3/200] Training loss: 0.05194860
[4/200] Training loss: 0.04837219
[5/200] Training loss: 0.04871044
[6/200] Training loss: 0.04964153
[7/200] Training loss: 0.04470975
[8/200] Training loss: 0.04008638
[9/200] Training loss: 0.04087872
[10/200] Training loss: 0.03936305
[50/200] Training loss: 0.01830825
[100/200] Training loss: 0.01544067
[150/200] Training loss: 0.01440416
[200/200] Training loss: 0.01372947
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15163.244507690299 ----------
[1/200] Training loss: 0.11418246
[2/200] Training loss: 0.05609972
[3/200] Training loss: 0.05101901
[4/200] Training loss: 0.04578579
[5/200] Training loss: 0.04374638
[6/200] Training loss: 0.03877772
[7/200] Training loss: 0.03827589
[8/200] Training loss: 0.03468154
[9/200] Training loss: 0.03250097
[10/200] Training loss: 0.03158767
[50/200] Training loss: 0.01641537
[100/200] Training loss: 0.01407648
[150/200] Training loss: 0.01301655
[200/200] Training loss: 0.01220026
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7653.959236891715 ----------
[1/200] Training loss: 0.16510734
[2/200] Training loss: 0.06190414
[3/200] Training loss: 0.04828632
[4/200] Training loss: 0.04872998
[5/200] Training loss: 0.04697350
[6/200] Training loss: 0.04497170
[7/200] Training loss: 0.04334946
[8/200] Training loss: 0.04261522
[9/200] Training loss: 0.04059477
[10/200] Training loss: 0.03761409
[50/200] Training loss: 0.01857407
[100/200] Training loss: 0.01598254
[150/200] Training loss: 0.01409057
[200/200] Training loss: 0.01247974
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9453.731961505995 ----------
[1/200] Training loss: 0.13079537
[2/200] Training loss: 0.05766829
[3/200] Training loss: 0.05301102
[4/200] Training loss: 0.05069393
[5/200] Training loss: 0.04719373
[6/200] Training loss: 0.04444836
[7/200] Training loss: 0.04381805
[8/200] Training loss: 0.03834167
[9/200] Training loss: 0.03830449
[10/200] Training loss: 0.03821730
[50/200] Training loss: 0.01806744
[100/200] Training loss: 0.01465530
[150/200] Training loss: 0.01278809
[200/200] Training loss: 0.01217881
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9818.765299160583 ----------
[1/200] Training loss: 0.22011749
[2/200] Training loss: 0.06142870
[3/200] Training loss: 0.06200016
[4/200] Training loss: 0.05971612
[5/200] Training loss: 0.06030885
[6/200] Training loss: 0.05921616
[7/200] Training loss: 0.06204681
[8/200] Training loss: 0.06008855
[9/200] Training loss: 0.06054970
[10/200] Training loss: 0.05903828
[50/200] Training loss: 0.05626240
[100/200] Training loss: 0.05646276
[150/200] Training loss: 0.04659632
[200/200] Training loss: 0.04369343
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 2
---loss---: L1Loss ---opt---: Rprop ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 30956.760554037304 ----------
[1/200] Training loss: 0.10922838
[2/200] Training loss: 0.05229814
[3/200] Training loss: 0.04840744
[4/200] Training loss: 0.04205042
[5/200] Training loss: 0.03849003
[6/200] Training loss: 0.03694312
[7/200] Training loss: 0.03310655
[8/200] Training loss: 0.03014636
[9/200] Training loss: 0.02902507
[10/200] Training loss: 0.02708515
[50/200] Training loss: 0.01694987
[100/200] Training loss: 0.01500324
[150/200] Training loss: 0.01358459
[200/200] Training loss: 0.01249354
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6966131684157987 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11788.384791819446 ----------
[1/200] Training loss: 0.14685332
[2/200] Training loss: 0.05765215
[3/200] Training loss: 0.05588443
[4/200] Training loss: 0.05245785
[5/200] Training loss: 0.04611268
[6/200] Training loss: 0.04432233
[7/200] Training loss: 0.04280206
[8/200] Training loss: 0.03921457
[9/200] Training loss: 0.03518023
[10/200] Training loss: 0.03196721
[50/200] Training loss: 0.01785676
[100/200] Training loss: 0.01443493
[150/200] Training loss: 0.01389805
[200/200] Training loss: 0.01265406
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5945.5711247953295 ----------
[1/200] Training loss: 0.12314893
[2/200] Training loss: 0.05537958
[3/200] Training loss: 0.04851068
[4/200] Training loss: 0.04579787
[5/200] Training loss: 0.04013232
[6/200] Training loss: 0.03741336
[7/200] Training loss: 0.03431875
[8/200] Training loss: 0.03094008
[9/200] Training loss: 0.03196249
[10/200] Training loss: 0.02957904
[50/200] Training loss: 0.01796543
[100/200] Training loss: 0.01428281
[150/200] Training loss: 0.01231076
[200/200] Training loss: 0.01101646
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 32 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9690.426203217276 ----------
[1/200] Training loss: 0.16252927
[2/200] Training loss: 0.05605469
[3/200] Training loss: 0.04996020
[4/200] Training loss: 0.04727699
[5/200] Training loss: 0.04576315
[6/200] Training loss: 0.04222657
[7/200] Training loss: 0.04059189
[8/200] Training loss: 0.04166992
[9/200] Training loss: 0.03700190
[10/200] Training loss: 0.03675257
[50/200] Training loss: 0.01748671
[100/200] Training loss: 0.01561488
[150/200] Training loss: 0.01311115
[200/200] Training loss: 0.01270797
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.7319789082087752 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11796.880265561738 ----------
[1/200] Training loss: 0.11203412
[2/200] Training loss: 0.05498957
[3/200] Training loss: 0.04824537
[4/200] Training loss: 0.04485179
[5/200] Training loss: 0.04038372
[6/200] Training loss: 0.03797283
[7/200] Training loss: 0.03532521
[8/200] Training loss: 0.03193649
[9/200] Training loss: 0.02899703
[10/200] Training loss: 0.02747805
[50/200] Training loss: 0.01672855
[100/200] Training loss: 0.01400435
[150/200] Training loss: 0.01209172
[200/200] Training loss: 0.01163296
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14179.47587183673 ----------
[1/200] Training loss: 0.16804736
[2/200] Training loss: 0.05908356
[3/200] Training loss: 0.05261563
[4/200] Training loss: 0.05112742
[5/200] Training loss: 0.04509603
[6/200] Training loss: 0.04268429
[7/200] Training loss: 0.03992439
[8/200] Training loss: 0.03862412
[9/200] Training loss: 0.03543791
[10/200] Training loss: 0.03615366
[50/200] Training loss: 0.01758732
[100/200] Training loss: 0.01501040
[150/200] Training loss: 0.01407212
[200/200] Training loss: 0.01270991
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9447.357302441778 ----------
[1/100] Training loss: 0.15769581
[2/100] Training loss: 0.06208209
[3/100] Training loss: 0.05417440
[4/100] Training loss: 0.04905829
[5/100] Training loss: 0.04504120
[6/100] Training loss: 0.04002324
[7/100] Training loss: 0.03796595
[8/100] Training loss: 0.03428071
[9/100] Training loss: 0.03627966
[10/100] Training loss: 0.03283611
[50/100] Training loss: 0.01755150
[100/100] Training loss: 0.01456807
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6654.293350912627 ----------
[1/200] Training loss: 0.10836458
[2/200] Training loss: 0.05218023
[3/200] Training loss: 0.04947428
[4/200] Training loss: 0.04537215
[5/200] Training loss: 0.04296521
[6/200] Training loss: 0.03990731
[7/200] Training loss: 0.03836244
[8/200] Training loss: 0.03425072
[9/200] Training loss: 0.03274902
[10/200] Training loss: 0.03106735
[50/200] Training loss: 0.01411610
[100/200] Training loss: 0.01237494
[150/200] Training loss: 0.01126513
[200/200] Training loss: 0.01048714
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4811.564610394419 ----------
[1/200] Training loss: 0.10836375
[2/200] Training loss: 0.05122354
[3/200] Training loss: 0.04548867
[4/200] Training loss: 0.04310774
[5/200] Training loss: 0.03890730
[6/200] Training loss: 0.03593546
[7/200] Training loss: 0.03446934
[8/200] Training loss: 0.02996226
[9/200] Training loss: 0.02959909
[10/200] Training loss: 0.02839590
[50/200] Training loss: 0.01508003
[100/200] Training loss: 0.01263154
[150/200] Training loss: 0.01127360
[200/200] Training loss: 0.01040114
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14332.4069157975 ----------
[1/200] Training loss: 0.10431067
[2/200] Training loss: 0.05577385
[3/200] Training loss: 0.05036123
[4/200] Training loss: 0.04814771
[5/200] Training loss: 0.04610545
[6/200] Training loss: 0.04102144
[7/200] Training loss: 0.03829322
[8/200] Training loss: 0.03492835
[9/200] Training loss: 0.03117218
[10/200] Training loss: 0.03023394
[50/200] Training loss: 0.01688356
[100/200] Training loss: 0.01508807
[150/200] Training loss: 0.01375328
[200/200] Training loss: 0.01336828
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.7319789082087752 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8828.173990129555 ----------
[1/200] Training loss: 0.10003031
[2/200] Training loss: 0.05228940
[3/200] Training loss: 0.04833232
[4/200] Training loss: 0.04271768
[5/200] Training loss: 0.03911813
[6/200] Training loss: 0.03554688
[7/200] Training loss: 0.03295519
[8/200] Training loss: 0.02995749
[9/200] Training loss: 0.02879505
[10/200] Training loss: 0.02620746
[50/200] Training loss: 0.01682716
[100/200] Training loss: 0.01343625
[150/200] Training loss: 0.01246051
[200/200] Training loss: 0.01177990
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12777.868992911142 ----------
[1/200] Training loss: 0.11350940
[2/200] Training loss: 0.05832201
[3/200] Training loss: 0.05115581
[4/200] Training loss: 0.04704007
[5/200] Training loss: 0.04216937
[6/200] Training loss: 0.04030224
[7/200] Training loss: 0.03525240
[8/200] Training loss: 0.03332746
[9/200] Training loss: 0.03127471
[10/200] Training loss: 0.02860216
[50/200] Training loss: 0.01748722
[100/200] Training loss: 0.01493216
[150/200] Training loss: 0.01331517
[200/200] Training loss: 0.01243815
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 18282.79978559083 ----------
[1/200] Training loss: 0.12826382
[2/200] Training loss: 0.05558301
[3/200] Training loss: 0.05059926
[4/200] Training loss: 0.04640745
[5/200] Training loss: 0.04200439
[6/200] Training loss: 0.03946550
[7/200] Training loss: 0.03652026
[8/200] Training loss: 0.03483010
[9/200] Training loss: 0.03070229
[10/200] Training loss: 0.03129223
[50/200] Training loss: 0.01676761
[100/200] Training loss: 0.01456698
[150/200] Training loss: 0.01207942
[200/200] Training loss: 0.01140101
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6966131684157987 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12962.383114227106 ----------
[1/200] Training loss: 0.10252399
[2/200] Training loss: 0.04944243
[3/200] Training loss: 0.04119201
[4/200] Training loss: 0.03785955
[5/200] Training loss: 0.03417923
[6/200] Training loss: 0.03138485
[7/200] Training loss: 0.02991201
[8/200] Training loss: 0.02891534
[9/200] Training loss: 0.02872522
[10/200] Training loss: 0.02616119
[50/200] Training loss: 0.01667715
[100/200] Training loss: 0.01317931
[150/200] Training loss: 0.01221277
[200/200] Training loss: 0.01089945
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5241.065731318393 ----------
[1/100] Training loss: 0.11215737
[2/100] Training loss: 0.05453987
[3/100] Training loss: 0.04808293
[4/100] Training loss: 0.04286066
[5/100] Training loss: 0.03893636
[6/100] Training loss: 0.03643096
[7/100] Training loss: 0.03313999
[8/100] Training loss: 0.03188271
[9/100] Training loss: 0.02953432
[10/100] Training loss: 0.02770706
[50/100] Training loss: 0.01739652
[100/100] Training loss: 0.01408496
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16706.17610346545 ----------
[1/200] Training loss: 0.10588700
[2/200] Training loss: 0.05386830
[3/200] Training loss: 0.04874215
[4/200] Training loss: 0.04375508
[5/200] Training loss: 0.03595558
[6/200] Training loss: 0.03226100
[7/200] Training loss: 0.02912517
[8/200] Training loss: 0.02898892
[9/200] Training loss: 0.02675861
[10/200] Training loss: 0.02535891
[50/200] Training loss: 0.01625470
[100/200] Training loss: 0.01396600
[150/200] Training loss: 0.01262014
[200/200] Training loss: 0.01131787
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 23270.75417772273 ----------
[1/100] Training loss: 0.11472358
[2/100] Training loss: 0.05032063
[3/100] Training loss: 0.04170715
[4/100] Training loss: 0.03886310
[5/100] Training loss: 0.03326728
[6/100] Training loss: 0.03328413
[7/100] Training loss: 0.02911067
[8/100] Training loss: 0.02656174
[9/100] Training loss: 0.02764587
[10/100] Training loss: 0.02503431
[50/100] Training loss: 0.01819874
[100/100] Training loss: 0.01621190
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13928.186098699285 ----------
[1/200] Training loss: 0.11831052
[2/200] Training loss: 0.05775441
[3/200] Training loss: 0.05047699
[4/200] Training loss: 0.04931120
[5/200] Training loss: 0.04195915
[6/200] Training loss: 0.03830548
[7/200] Training loss: 0.03473926
[8/200] Training loss: 0.03214183
[9/200] Training loss: 0.02909828
[10/200] Training loss: 0.02749667
[50/200] Training loss: 0.01716837
[100/200] Training loss: 0.01369693
[150/200] Training loss: 0.01242814
[200/200] Training loss: 0.01177408
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13005.68429572239 ----------
[1/200] Training loss: 0.10229552
[2/200] Training loss: 0.04966959
[3/200] Training loss: 0.04282008
[4/200] Training loss: 0.03964408
[5/200] Training loss: 0.03962915
[6/200] Training loss: 0.03525567
[7/200] Training loss: 0.03209127
[8/200] Training loss: 0.02931202
[9/200] Training loss: 0.02939440
[10/200] Training loss: 0.02652838
[50/200] Training loss: 0.01661167
[100/200] Training loss: 0.01403776
[150/200] Training loss: 0.01262843
[200/200] Training loss: 0.01156193
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10645.349407135493 ----------
[1/200] Training loss: 0.08851579
[2/200] Training loss: 0.05470423
[3/200] Training loss: 0.04714395
[4/200] Training loss: 0.04352658
[5/200] Training loss: 0.04036818
[6/200] Training loss: 0.03630646
[7/200] Training loss: 0.03448635
[8/200] Training loss: 0.03128948
[9/200] Training loss: 0.02857910
[10/200] Training loss: 0.02748120
[50/200] Training loss: 0.01495423
[100/200] Training loss: 0.01261593
[150/200] Training loss: 0.01146721
[200/200] Training loss: 0.01070393
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9236.059765939153 ----------
[1/200] Training loss: 0.14796885
[2/200] Training loss: 0.05676007
[3/200] Training loss: 0.05262500
[4/200] Training loss: 0.04657572
[5/200] Training loss: 0.04273170
[6/200] Training loss: 0.04262536
[7/200] Training loss: 0.03754334
[8/200] Training loss: 0.03714570
[9/200] Training loss: 0.03282572
[10/200] Training loss: 0.03291107
[50/200] Training loss: 0.01599373
[100/200] Training loss: 0.01401799
[150/200] Training loss: 0.01297831
[200/200] Training loss: 0.01201669
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6608.517836852679 ----------
[1/200] Training loss: 0.11862277
[2/200] Training loss: 0.05835760
[3/200] Training loss: 0.05284442
[4/200] Training loss: 0.04809928
[5/200] Training loss: 0.04117056
[6/200] Training loss: 0.03577156
[7/200] Training loss: 0.03276304
[8/200] Training loss: 0.02935264
[9/200] Training loss: 0.02827617
[10/200] Training loss: 0.02573163
[50/200] Training loss: 0.01642678
[100/200] Training loss: 0.01423615
[150/200] Training loss: 0.01226771
[200/200] Training loss: 0.01099346
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 18052.94878960221 ----------
[1/100] Training loss: 0.10053775
[2/100] Training loss: 0.05523164
[3/100] Training loss: 0.04559083
[4/100] Training loss: 0.04249604
[5/100] Training loss: 0.03803810
[6/100] Training loss: 0.03613203
[7/100] Training loss: 0.03112292
[8/100] Training loss: 0.02862845
[9/100] Training loss: 0.02819193
[10/100] Training loss: 0.02531331
[50/100] Training loss: 0.01449741
[100/100] Training loss: 0.01196210
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 19111.260764271938 ----------
[1/200] Training loss: 0.11657668
[2/200] Training loss: 0.05208572
[3/200] Training loss: 0.04520421
[4/200] Training loss: 0.04032974
[5/200] Training loss: 0.03428524
[6/200] Training loss: 0.03390792
[7/200] Training loss: 0.03275585
[8/200] Training loss: 0.02814585
[9/200] Training loss: 0.02803085
[10/200] Training loss: 0.02497500
[50/200] Training loss: 0.01770564
[100/200] Training loss: 0.01505454
[150/200] Training loss: 0.01424482
[200/200] Training loss: 0.01319743
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6804.587276242402 ----------
[1/200] Training loss: 0.11439310
[2/200] Training loss: 0.05758078
[3/200] Training loss: 0.05281005
[4/200] Training loss: 0.04696294
[5/200] Training loss: 0.04474355
[6/200] Training loss: 0.03881190
[7/200] Training loss: 0.03965725
[8/200] Training loss: 0.03410185
[9/200] Training loss: 0.03270799
[10/200] Training loss: 0.03044525
[50/200] Training loss: 0.01594484
[100/200] Training loss: 0.01409368
[150/200] Training loss: 0.01284584
[200/200] Training loss: 0.01207395
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9227.657991061436 ----------
[1/200] Training loss: 0.12647651
[2/200] Training loss: 0.05543606
[3/200] Training loss: 0.05123999
[4/200] Training loss: 0.04698018
[5/200] Training loss: 0.04430709
[6/200] Training loss: 0.04043068
[7/200] Training loss: 0.03865353
[8/200] Training loss: 0.03458015
[9/200] Training loss: 0.03395361
[10/200] Training loss: 0.02952087
[50/200] Training loss: 0.01752629
[100/200] Training loss: 0.01504265
[150/200] Training loss: 0.01338732
[200/200] Training loss: 0.01231167
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7221.810299364004 ----------
[1/200] Training loss: 0.10769742
[2/200] Training loss: 0.05272680
[3/200] Training loss: 0.04670083
[4/200] Training loss: 0.04341591
[5/200] Training loss: 0.03879895
[6/200] Training loss: 0.03602888
[7/200] Training loss: 0.03225408
[8/200] Training loss: 0.02891373
[9/200] Training loss: 0.02680864
[10/200] Training loss: 0.02531135
[50/200] Training loss: 0.01743183
[100/200] Training loss: 0.01505694
[150/200] Training loss: 0.01319390
[200/200] Training loss: 0.01226168
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.7319789082087752 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13410.588652255352 ----------
[1/200] Training loss: 0.10580104
[2/200] Training loss: 0.04931592
[3/200] Training loss: 0.04379940
[4/200] Training loss: 0.03858327
[5/200] Training loss: 0.03590538
[6/200] Training loss: 0.03335232
[7/200] Training loss: 0.02867923
[8/200] Training loss: 0.02850509
[9/200] Training loss: 0.02529239
[10/200] Training loss: 0.02437511
[50/200] Training loss: 0.01633403
[100/200] Training loss: 0.01412680
[150/200] Training loss: 0.01267740
[200/200] Training loss: 0.01109913
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17672.03349928921 ----------
[1/100] Training loss: 0.12944408
[2/100] Training loss: 0.05764665
[3/100] Training loss: 0.05032498
[4/100] Training loss: 0.04547006
[5/100] Training loss: 0.04407665
[6/100] Training loss: 0.04194057
[7/100] Training loss: 0.03801583
[8/100] Training loss: 0.03534849
[9/100] Training loss: 0.03324573
[10/100] Training loss: 0.02959903
[50/100] Training loss: 0.01567069
[100/100] Training loss: 0.01365781
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6565.798352066563 ----------
[1/200] Training loss: 0.17419599
[2/200] Training loss: 0.06023202
[3/200] Training loss: 0.05657604
[4/200] Training loss: 0.05183954
[5/200] Training loss: 0.05035653
[6/200] Training loss: 0.04789050
[7/200] Training loss: 0.04763833
[8/200] Training loss: 0.04693198
[9/200] Training loss: 0.04324134
[10/200] Training loss: 0.04190078
[50/200] Training loss: 0.02230426
[100/200] Training loss: 0.01699885
[150/200] Training loss: 0.01479874
[200/200] Training loss: 0.01306288
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 3654.8622409059412 ----------
[1/200] Training loss: 0.11728220
[2/200] Training loss: 0.05253607
[3/200] Training loss: 0.04907263
[4/200] Training loss: 0.04536481
[5/200] Training loss: 0.04054643
[6/200] Training loss: 0.03840925
[7/200] Training loss: 0.03719280
[8/200] Training loss: 0.03331034
[9/200] Training loss: 0.03229286
[10/200] Training loss: 0.03095595
[50/200] Training loss: 0.01853151
[100/200] Training loss: 0.01506212
[150/200] Training loss: 0.01343566
[200/200] Training loss: 0.01243556
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9596.904084130465 ----------
[1/200] Training loss: 0.11568622
[2/200] Training loss: 0.05034058
[3/200] Training loss: 0.04558473
[4/200] Training loss: 0.04244882
[5/200] Training loss: 0.03835333
[6/200] Training loss: 0.03839856
[7/200] Training loss: 0.03609457
[8/200] Training loss: 0.03258364
[9/200] Training loss: 0.03344137
[10/200] Training loss: 0.02991756
[50/200] Training loss: 0.01664031
[100/200] Training loss: 0.01410645
[150/200] Training loss: 0.01284297
[200/200] Training loss: 0.01172146
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4448.495026410617 ----------
[1/200] Training loss: 0.09119235
[2/200] Training loss: 0.04937581
[3/200] Training loss: 0.04693818
[4/200] Training loss: 0.04091157
[5/200] Training loss: 0.03881678
[6/200] Training loss: 0.03402048
[7/200] Training loss: 0.03381507
[8/200] Training loss: 0.03192729
[9/200] Training loss: 0.02991635
[10/200] Training loss: 0.02786187
[50/200] Training loss: 0.01616220
[100/200] Training loss: 0.01263562
[150/200] Training loss: 0.01177409
[200/200] Training loss: 0.01051166
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13722.648140938396 ----------
[1/200] Training loss: 0.10473250
[2/200] Training loss: 0.04831763
[3/200] Training loss: 0.04497848
[4/200] Training loss: 0.03958062
[5/200] Training loss: 0.03740218
[6/200] Training loss: 0.03508166
[7/200] Training loss: 0.03208544
[8/200] Training loss: 0.03090403
[9/200] Training loss: 0.02957306
[10/200] Training loss: 0.02914276
[50/200] Training loss: 0.01590201
[100/200] Training loss: 0.01310274
[150/200] Training loss: 0.01155867
[200/200] Training loss: 0.01075074
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21177.142394572504 ----------
[1/200] Training loss: 0.12155042
[2/200] Training loss: 0.05364205
[3/200] Training loss: 0.04726717
[4/200] Training loss: 0.04359932
[5/200] Training loss: 0.04063785
[6/200] Training loss: 0.03467385
[7/200] Training loss: 0.03368607
[8/200] Training loss: 0.03085048
[9/200] Training loss: 0.02902908
[10/200] Training loss: 0.02859446
[50/200] Training loss: 0.01655812
[100/200] Training loss: 0.01464401
[150/200] Training loss: 0.01319548
[200/200] Training loss: 0.01164199
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9818.108575484384 ----------
[1/200] Training loss: 0.12410391
[2/200] Training loss: 0.05943134
[3/200] Training loss: 0.05384558
[4/200] Training loss: 0.04890325
[5/200] Training loss: 0.04442893
[6/200] Training loss: 0.04126542
[7/200] Training loss: 0.04079567
[8/200] Training loss: 0.03610202
[9/200] Training loss: 0.03426123
[10/200] Training loss: 0.03228705
[50/200] Training loss: 0.01713946
[100/200] Training loss: 0.01473221
[150/200] Training loss: 0.01369040
[200/200] Training loss: 0.01278829
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8605.73948013766 ----------
[1/200] Training loss: 0.10895627
[2/200] Training loss: 0.05489088
[3/200] Training loss: 0.04714367
[4/200] Training loss: 0.04053999
[5/200] Training loss: 0.03631360
[6/200] Training loss: 0.03343141
[7/200] Training loss: 0.03152559
[8/200] Training loss: 0.02835703
[9/200] Training loss: 0.02867430
[10/200] Training loss: 0.02699889
[50/200] Training loss: 0.01608982
[100/200] Training loss: 0.01468288
[150/200] Training loss: 0.01307777
[200/200] Training loss: 0.01234601
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8048.816310489387 ----------
[1/100] Training loss: 0.10712942
[2/100] Training loss: 0.05265512
[3/100] Training loss: 0.04975298
[4/100] Training loss: 0.04396851
[5/100] Training loss: 0.04172054
[6/100] Training loss: 0.03924695
[7/100] Training loss: 0.03547764
[8/100] Training loss: 0.03237846
[9/100] Training loss: 0.03086491
[10/100] Training loss: 0.02888206
[50/100] Training loss: 0.01533893
[100/100] Training loss: 0.01235769
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14223.823958415684 ----------
[1/200] Training loss: 0.11610637
[2/200] Training loss: 0.05219186
[3/200] Training loss: 0.04582978
[4/200] Training loss: 0.04175257
[5/200] Training loss: 0.03596695
[6/200] Training loss: 0.03306849
[7/200] Training loss: 0.03059901
[8/200] Training loss: 0.03042633
[9/200] Training loss: 0.02800103
[10/200] Training loss: 0.02728601
[50/200] Training loss: 0.01604877
[100/200] Training loss: 0.01340512
[150/200] Training loss: 0.01219323
[200/200] Training loss: 0.01147877
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4872.309513977945 ----------
[1/200] Training loss: 0.11598062
[2/200] Training loss: 0.05209859
[3/200] Training loss: 0.04733719
[4/200] Training loss: 0.04549579
[5/200] Training loss: 0.04080539
[6/200] Training loss: 0.03708561
[7/200] Training loss: 0.03519052
[8/200] Training loss: 0.03112702
[9/200] Training loss: 0.02861860
[10/200] Training loss: 0.02824228
[50/200] Training loss: 0.01627722
[100/200] Training loss: 0.01413186
[150/200] Training loss: 0.01300868
[200/200] Training loss: 0.01181258
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5950.503508107528 ----------
[1/200] Training loss: 0.11036354
[2/200] Training loss: 0.05439043
[3/200] Training loss: 0.04910229
[4/200] Training loss: 0.04543593
[5/200] Training loss: 0.04351688
[6/200] Training loss: 0.03934940
[7/200] Training loss: 0.03666516
[8/200] Training loss: 0.03611068
[9/200] Training loss: 0.03256408
[10/200] Training loss: 0.03061816
[50/200] Training loss: 0.01723603
[100/200] Training loss: 0.01494660
[150/200] Training loss: 0.01338679
[200/200] Training loss: 0.01246921
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8298.769547348571 ----------
[1/100] Training loss: 0.08959088
[2/100] Training loss: 0.04785871
[3/100] Training loss: 0.04556993
[4/100] Training loss: 0.04220315
[5/100] Training loss: 0.03932911
[6/100] Training loss: 0.03624310
[7/100] Training loss: 0.03413242
[8/100] Training loss: 0.03236883
[9/100] Training loss: 0.03051509
[10/100] Training loss: 0.02967176
[50/100] Training loss: 0.01569675
[100/100] Training loss: 0.01284306
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5409.505892408289 ----------
[1/200] Training loss: 0.11478750
[2/200] Training loss: 0.05569750
[3/200] Training loss: 0.04873840
[4/200] Training loss: 0.04525851
[5/200] Training loss: 0.04183471
[6/200] Training loss: 0.03773857
[7/200] Training loss: 0.03243888
[8/200] Training loss: 0.02892694
[9/200] Training loss: 0.02763078
[10/200] Training loss: 0.02630253
[50/200] Training loss: 0.01704397
[100/200] Training loss: 0.01470859
[150/200] Training loss: 0.01298600
[200/200] Training loss: 0.01151026
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9548.722637086072 ----------
[1/200] Training loss: 0.12041089
[2/200] Training loss: 0.05340863
[3/200] Training loss: 0.04568134
[4/200] Training loss: 0.03967607
[5/200] Training loss: 0.03740355
[6/200] Training loss: 0.03312781
[7/200] Training loss: 0.02929754
[8/200] Training loss: 0.02912156
[9/200] Training loss: 0.02687778
[10/200] Training loss: 0.02641908
[50/200] Training loss: 0.01846523
[100/200] Training loss: 0.01507747
[150/200] Training loss: 0.01251098
[200/200] Training loss: 0.01131278
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9090.44729372543 ----------
[1/100] Training loss: 0.11418622
[2/100] Training loss: 0.05437468
[3/100] Training loss: 0.05114495
[4/100] Training loss: 0.04777604
[5/100] Training loss: 0.04348924
[6/100] Training loss: 0.04078614
[7/100] Training loss: 0.03781825
[8/100] Training loss: 0.03477304
[9/100] Training loss: 0.03042425
[10/100] Training loss: 0.03073719
[50/100] Training loss: 0.01640569
[100/100] Training loss: 0.01367739
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9933.829070403819 ----------
[1/200] Training loss: 0.10620806
[2/200] Training loss: 0.05213230
[3/200] Training loss: 0.04549912
[4/200] Training loss: 0.04183944
[5/200] Training loss: 0.03850954
[6/200] Training loss: 0.03553557
[7/200] Training loss: 0.03110868
[8/200] Training loss: 0.02776223
[9/200] Training loss: 0.02511828
[10/200] Training loss: 0.02428908
[50/200] Training loss: 0.01607563
[100/200] Training loss: 0.01431528
[150/200] Training loss: 0.01315864
[200/200] Training loss: 0.01188011
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6601.542547011267 ----------
[1/200] Training loss: 0.11206806
[2/200] Training loss: 0.05337697
[3/200] Training loss: 0.04933399
[4/200] Training loss: 0.04374110
[5/200] Training loss: 0.03952433
[6/200] Training loss: 0.03447471
[7/200] Training loss: 0.03064408
[8/200] Training loss: 0.02801203
[9/200] Training loss: 0.02653529
[10/200] Training loss: 0.02442759
[50/200] Training loss: 0.01510222
[100/200] Training loss: 0.01273439
[150/200] Training loss: 0.01128987
[200/200] Training loss: 0.01060627
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 3764.2684548262496 ----------
[1/200] Training loss: 0.10844888
[2/200] Training loss: 0.05351709
[3/200] Training loss: 0.04727072
[4/200] Training loss: 0.04295110
[5/200] Training loss: 0.03857720
[6/200] Training loss: 0.03408033
[7/200] Training loss: 0.03168284
[8/200] Training loss: 0.02899563
[9/200] Training loss: 0.02784888
[10/200] Training loss: 0.02599034
[50/200] Training loss: 0.01692059
[100/200] Training loss: 0.01442677
[150/200] Training loss: 0.01295222
[200/200] Training loss: 0.01183254
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13534.75880834232 ----------
[1/200] Training loss: 0.11745740
[2/200] Training loss: 0.05416140
[3/200] Training loss: 0.04480179
[4/200] Training loss: 0.03871771
[5/200] Training loss: 0.03582390
[6/200] Training loss: 0.03285301
[7/200] Training loss: 0.03056576
[8/200] Training loss: 0.03036742
[9/200] Training loss: 0.02936519
[10/200] Training loss: 0.02800728
[50/200] Training loss: 0.01781705
[100/200] Training loss: 0.01508059
[150/200] Training loss: 0.01373568
[200/200] Training loss: 0.01247707
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17719.812188620963 ----------
[1/100] Training loss: 0.14774264
[2/100] Training loss: 0.05436443
[3/100] Training loss: 0.05162757
[4/100] Training loss: 0.04962919
[5/100] Training loss: 0.04581712
[6/100] Training loss: 0.04424254
[7/100] Training loss: 0.03950834
[8/100] Training loss: 0.03800260
[9/100] Training loss: 0.03672316
[10/100] Training loss: 0.03647496
[50/100] Training loss: 0.01785193
[100/100] Training loss: 0.01543483
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5133.48108012487 ----------
[1/200] Training loss: 0.09685403
[2/200] Training loss: 0.05136474
[3/200] Training loss: 0.04546149
[4/200] Training loss: 0.04279030
[5/200] Training loss: 0.03976688
[6/200] Training loss: 0.03733856
[7/200] Training loss: 0.03493661
[8/200] Training loss: 0.03327491
[9/200] Training loss: 0.03238677
[10/200] Training loss: 0.03166972
[50/200] Training loss: 0.01580048
[100/200] Training loss: 0.01315291
[150/200] Training loss: 0.01170541
[200/200] Training loss: 0.01103798
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10678.88982994019 ----------
[1/200] Training loss: 0.10951922
[2/200] Training loss: 0.05859660
[3/200] Training loss: 0.05155227
[4/200] Training loss: 0.04987217
[5/200] Training loss: 0.04782176
[6/200] Training loss: 0.04493351
[7/200] Training loss: 0.04173970
[8/200] Training loss: 0.03789318
[9/200] Training loss: 0.03431253
[10/200] Training loss: 0.03069828
[50/200] Training loss: 0.01765560
[100/200] Training loss: 0.01515856
[150/200] Training loss: 0.01384084
[200/200] Training loss: 0.01313295
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11368.75718801312 ----------
[1/200] Training loss: 0.10303432
[2/200] Training loss: 0.05311324
[3/200] Training loss: 0.04973093
[4/200] Training loss: 0.04589713
[5/200] Training loss: 0.04299879
[6/200] Training loss: 0.03863423
[7/200] Training loss: 0.03663364
[8/200] Training loss: 0.03258847
[9/200] Training loss: 0.03149016
[10/200] Training loss: 0.02875319
[50/200] Training loss: 0.01636865
[100/200] Training loss: 0.01413832
[150/200] Training loss: 0.01248016
[200/200] Training loss: 0.01129017
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10834.785092469532 ----------
[1/200] Training loss: 0.16258225
[2/200] Training loss: 0.05818952
[3/200] Training loss: 0.05327786
[4/200] Training loss: 0.04845184
[5/200] Training loss: 0.04587293
[6/200] Training loss: 0.03959206
[7/200] Training loss: 0.04484610
[8/200] Training loss: 0.03473667
[9/200] Training loss: 0.03132705
[10/200] Training loss: 0.03293863
[50/200] Training loss: 0.01900302
[100/200] Training loss: 0.01588879
[150/200] Training loss: 0.01396630
[200/200] Training loss: 0.01296586
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5441.935133755271 ----------
[1/200] Training loss: 0.15128405
[2/200] Training loss: 0.05786138
[3/200] Training loss: 0.05288881
[4/200] Training loss: 0.05045545
[5/200] Training loss: 0.04703932
[6/200] Training loss: 0.04438117
[7/200] Training loss: 0.04068995
[8/200] Training loss: 0.03793927
[9/200] Training loss: 0.03825926
[10/200] Training loss: 0.03524579
[50/200] Training loss: 0.01948553
[100/200] Training loss: 0.01624005
[150/200] Training loss: 0.01502695
[200/200] Training loss: 0.01378683
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8595.43506752276 ----------
[1/200] Training loss: 0.15095482
[2/200] Training loss: 0.05902109
[3/200] Training loss: 0.05035434
[4/200] Training loss: 0.04811233
[5/200] Training loss: 0.04634395
[6/200] Training loss: 0.04198222
[7/200] Training loss: 0.03744635
[8/200] Training loss: 0.03659402
[9/200] Training loss: 0.03721787
[10/200] Training loss: 0.03239007
[50/200] Training loss: 0.01850611
[100/200] Training loss: 0.01486238
[150/200] Training loss: 0.01309404
[200/200] Training loss: 0.01252242
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8287.575278692797 ----------
[1/200] Training loss: 0.10132372
[2/200] Training loss: 0.05361183
[3/200] Training loss: 0.04635800
[4/200] Training loss: 0.04342344
[5/200] Training loss: 0.03759733
[6/200] Training loss: 0.03399279
[7/200] Training loss: 0.03066935
[8/200] Training loss: 0.02855230
[9/200] Training loss: 0.02773893
[10/200] Training loss: 0.02612464
[50/200] Training loss: 0.01594753
[100/200] Training loss: 0.01381220
[150/200] Training loss: 0.01203189
[200/200] Training loss: 0.01152943
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16013.021201509726 ----------
[1/200] Training loss: 0.11550425
[2/200] Training loss: 0.05413944
[3/200] Training loss: 0.05095364
[4/200] Training loss: 0.04695585
[5/200] Training loss: 0.04486321
[6/200] Training loss: 0.04088225
[7/200] Training loss: 0.03589364
[8/200] Training loss: 0.03527712
[9/200] Training loss: 0.03051350
[10/200] Training loss: 0.02880613
[50/200] Training loss: 0.01703829
[100/200] Training loss: 0.01467103
[150/200] Training loss: 0.01391635
[200/200] Training loss: 0.01305660
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11339.888182870234 ----------
[1/200] Training loss: 0.14721379
[2/200] Training loss: 0.05763575
[3/200] Training loss: 0.04965189
[4/200] Training loss: 0.04712137
[5/200] Training loss: 0.04437941
[6/200] Training loss: 0.03868184
[7/200] Training loss: 0.03780134
[8/200] Training loss: 0.03725434
[9/200] Training loss: 0.03252475
[10/200] Training loss: 0.03354353
[50/200] Training loss: 0.01868230
[100/200] Training loss: 0.01625711
[150/200] Training loss: 0.01518238
[200/200] Training loss: 0.01413154
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11135.725571331219 ----------
[1/200] Training loss: 0.15697611
[2/200] Training loss: 0.05965513
[3/200] Training loss: 0.05545516
[4/200] Training loss: 0.05073879
[5/200] Training loss: 0.04869537
[6/200] Training loss: 0.04600651
[7/200] Training loss: 0.04626965
[8/200] Training loss: 0.04332605
[9/200] Training loss: 0.04191676
[10/200] Training loss: 0.04217656
[50/200] Training loss: 0.02230707
[100/200] Training loss: 0.01730594
[150/200] Training loss: 0.01547537
[200/200] Training loss: 0.01353608
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4338.897094884828 ----------
[1/200] Training loss: 0.14765989
[2/200] Training loss: 0.05581309
[3/200] Training loss: 0.05242907
[4/200] Training loss: 0.04761482
[5/200] Training loss: 0.04798865
[6/200] Training loss: 0.04379210
[7/200] Training loss: 0.03819925
[8/200] Training loss: 0.03936316
[9/200] Training loss: 0.03801427
[10/200] Training loss: 0.03586840
[50/200] Training loss: 0.01760895
[100/200] Training loss: 0.01466750
[150/200] Training loss: 0.01393268
[200/200] Training loss: 0.01225957
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 22784.154493858226 ----------
[1/200] Training loss: 0.15573392
[2/200] Training loss: 0.05657574
[3/200] Training loss: 0.05500171
[4/200] Training loss: 0.05007748
[5/200] Training loss: 0.04877510
[6/200] Training loss: 0.04786120
[7/200] Training loss: 0.04455614
[8/200] Training loss: 0.04107508
[9/200] Training loss: 0.03831422
[10/200] Training loss: 0.03746126
[50/200] Training loss: 0.01688254
[100/200] Training loss: 0.01460696
[150/200] Training loss: 0.01306205
[200/200] Training loss: 0.01250006
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17585.879790331786 ----------
[1/200] Training loss: 0.12147005
[2/200] Training loss: 0.05459155
[3/200] Training loss: 0.04855163
[4/200] Training loss: 0.04402915
[5/200] Training loss: 0.04069088
[6/200] Training loss: 0.03957322
[7/200] Training loss: 0.03395731
[8/200] Training loss: 0.03079465
[9/200] Training loss: 0.03060261
[10/200] Training loss: 0.02749746
[50/200] Training loss: 0.01803030
[100/200] Training loss: 0.01631060
[150/200] Training loss: 0.01484206
[200/200] Training loss: 0.01349958
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9984.445903504109 ----------
[1/200] Training loss: 0.10658705
[2/200] Training loss: 0.05150418
[3/200] Training loss: 0.04558927
[4/200] Training loss: 0.04485736
[5/200] Training loss: 0.04104712
[6/200] Training loss: 0.03889326
[7/200] Training loss: 0.03686673
[8/200] Training loss: 0.03510265
[9/200] Training loss: 0.03276759
[10/200] Training loss: 0.03197921
[50/200] Training loss: 0.01588565
[100/200] Training loss: 0.01335856
[150/200] Training loss: 0.01147829
[200/200] Training loss: 0.01065145
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10375.855049103182 ----------
[1/200] Training loss: 0.10235765
[2/200] Training loss: 0.04928422
[3/200] Training loss: 0.04407739
[4/200] Training loss: 0.04016455
[5/200] Training loss: 0.03588752
[6/200] Training loss: 0.03491567
[7/200] Training loss: 0.03166628
[8/200] Training loss: 0.02991885
[9/200] Training loss: 0.02631634
[10/200] Training loss: 0.02528005
[50/200] Training loss: 0.01605870
[100/200] Training loss: 0.01329866
[150/200] Training loss: 0.01158008
[200/200] Training loss: 0.01079230
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14415.160352906241 ----------
[1/100] Training loss: 0.06938739
[2/100] Training loss: 0.04136877
[3/100] Training loss: 0.03143719
[4/100] Training loss: 0.02852850
[5/100] Training loss: 0.02572479
[6/100] Training loss: 0.02360488
[7/100] Training loss: 0.02391549
[8/100] Training loss: 0.02250095
[9/100] Training loss: 0.02195475
[10/100] Training loss: 0.02191560
[50/100] Training loss: 0.01739338
[100/100] Training loss: 0.01537929
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7581.003627488909 ----------
[1/100] Training loss: 0.18537471
[2/100] Training loss: 0.06718830
[3/100] Training loss: 0.05375043
[4/100] Training loss: 0.05493652
[5/100] Training loss: 0.05100308
[6/100] Training loss: 0.04827363
[7/100] Training loss: 0.04396917
[8/100] Training loss: 0.04584935
[9/100] Training loss: 0.04275339
[10/100] Training loss: 0.03991320
[50/100] Training loss: 0.01788910
[100/100] Training loss: 0.01546953
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 21850.60914482706 ----------
[1/150] Training loss: 0.11429056
[2/150] Training loss: 0.05480256
[3/150] Training loss: 0.04877450
[4/150] Training loss: 0.04516898
[5/150] Training loss: 0.03904373
[6/150] Training loss: 0.03695006
[7/150] Training loss: 0.03477534
[8/150] Training loss: 0.03249941
[9/150] Training loss: 0.03081989
[10/150] Training loss: 0.02958467
[50/150] Training loss: 0.01808785
[100/150] Training loss: 0.01611406
[150/150] Training loss: 0.01463161
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 150
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6312.00063371353 ----------
[1/200] Training loss: 0.16631130
[2/200] Training loss: 0.05859211
[3/200] Training loss: 0.05300124
[4/200] Training loss: 0.05008634
[5/200] Training loss: 0.04730332
[6/200] Training loss: 0.04552633
[7/200] Training loss: 0.04157081
[8/200] Training loss: 0.04006268
[9/200] Training loss: 0.03738292
[10/200] Training loss: 0.03643928
[50/200] Training loss: 0.01942045
[100/200] Training loss: 0.01600244
[150/200] Training loss: 0.01466904
[200/200] Training loss: 0.01341609
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15860.412604973428 ----------
[1/200] Training loss: 0.11600817
[2/200] Training loss: 0.05446236
[3/200] Training loss: 0.04470602
[4/200] Training loss: 0.04077967
[5/200] Training loss: 0.03616184
[6/200] Training loss: 0.03449252
[7/200] Training loss: 0.03100109
[8/200] Training loss: 0.02984263
[9/200] Training loss: 0.02800128
[10/200] Training loss: 0.02626006
[50/200] Training loss: 0.01638682
[100/200] Training loss: 0.01458186
[150/200] Training loss: 0.01323331
[200/200] Training loss: 0.01226993
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8338.123050183416 ----------
[1/200] Training loss: 0.12432049
[2/200] Training loss: 0.05541829
[3/200] Training loss: 0.04873424
[4/200] Training loss: 0.04778602
[5/200] Training loss: 0.04387047
[6/200] Training loss: 0.04030972
[7/200] Training loss: 0.03831442
[8/200] Training loss: 0.03806499
[9/200] Training loss: 0.03428783
[10/200] Training loss: 0.03247752
[50/200] Training loss: 0.01768789
[100/200] Training loss: 0.01587985
[150/200] Training loss: 0.01389718
[200/200] Training loss: 0.01256585
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10995.20768335005 ----------
[1/200] Training loss: 0.12104308
[2/200] Training loss: 0.05031752
[3/200] Training loss: 0.04591198
[4/200] Training loss: 0.04340065
[5/200] Training loss: 0.03941679
[6/200] Training loss: 0.03510161
[7/200] Training loss: 0.03337703
[8/200] Training loss: 0.03219045
[9/200] Training loss: 0.03072484
[10/200] Training loss: 0.02959096
[50/200] Training loss: 0.01787906
[100/200] Training loss: 0.01540444
[150/200] Training loss: 0.01310585
[200/200] Training loss: 0.01216110
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10961.513764074742 ----------
[1/200] Training loss: 0.10028508
[2/200] Training loss: 0.04827238
[3/200] Training loss: 0.04293745
[4/200] Training loss: 0.03899168
[5/200] Training loss: 0.03521614
[6/200] Training loss: 0.03202060
[7/200] Training loss: 0.03073101
[8/200] Training loss: 0.02895259
[9/200] Training loss: 0.02682496
[10/200] Training loss: 0.02749844
[50/200] Training loss: 0.01541023
[100/200] Training loss: 0.01282467
[150/200] Training loss: 0.01252265
[200/200] Training loss: 0.01139386
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9634.242263925067 ----------
[1/200] Training loss: 0.10945797
[2/200] Training loss: 0.05132859
[3/200] Training loss: 0.04750989
[4/200] Training loss: 0.04298472
[5/200] Training loss: 0.03985953
[6/200] Training loss: 0.03702420
[7/200] Training loss: 0.03387453
[8/200] Training loss: 0.03228523
[9/200] Training loss: 0.03016602
[10/200] Training loss: 0.02830973
[50/200] Training loss: 0.01770458
[100/200] Training loss: 0.01416790
[150/200] Training loss: 0.01309442
[200/200] Training loss: 0.01209627
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6583.684075044914 ----------
[1/100] Training loss: 0.15030882
[2/100] Training loss: 0.05722475
[3/100] Training loss: 0.05181063
[4/100] Training loss: 0.04752064
[5/100] Training loss: 0.04630653
[6/100] Training loss: 0.04388583
[7/100] Training loss: 0.04022639
[8/100] Training loss: 0.03767305
[9/100] Training loss: 0.03419191
[10/100] Training loss: 0.03177151
[50/100] Training loss: 0.01776708
[100/100] Training loss: 0.01563339
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8676.410317637128 ----------
[1/200] Training loss: 0.10035556
[2/200] Training loss: 0.05254209
[3/200] Training loss: 0.04482297
[4/200] Training loss: 0.04154101
[5/200] Training loss: 0.03655906
[6/200] Training loss: 0.03037365
[7/200] Training loss: 0.02715871
[8/200] Training loss: 0.02666671
[9/200] Training loss: 0.02581329
[10/200] Training loss: 0.02345370
[50/200] Training loss: 0.01716338
[100/200] Training loss: 0.01392745
[150/200] Training loss: 0.01213972
[200/200] Training loss: 0.01074359
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8176.895009720988 ----------
[1/200] Training loss: 0.16717781
[2/200] Training loss: 0.05938732
[3/200] Training loss: 0.05266029
[4/200] Training loss: 0.05022149
[5/200] Training loss: 0.04942666
[6/200] Training loss: 0.04363706
[7/200] Training loss: 0.03809590
[8/200] Training loss: 0.03560976
[9/200] Training loss: 0.03271064
[10/200] Training loss: 0.02983633
[50/200] Training loss: 0.01569323
[100/200] Training loss: 0.01321482
[150/200] Training loss: 0.01271829
[200/200] Training loss: 0.01136182
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10910.88007449445 ----------
[1/200] Training loss: 0.11315505
[2/200] Training loss: 0.05393078
[3/200] Training loss: 0.04936094
[4/200] Training loss: 0.04358640
[5/200] Training loss: 0.03989514
[6/200] Training loss: 0.03690258
[7/200] Training loss: 0.03415577
[8/200] Training loss: 0.02990329
[9/200] Training loss: 0.02709142
[10/200] Training loss: 0.02602993
[50/200] Training loss: 0.01630317
[100/200] Training loss: 0.01475878
[150/200] Training loss: 0.01324763
[200/200] Training loss: 0.01246420
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14827.211201031703 ----------
[1/200] Training loss: 0.14493291
[2/200] Training loss: 0.05428506
[3/200] Training loss: 0.05186913
[4/200] Training loss: 0.04872447
[5/200] Training loss: 0.04639087
[6/200] Training loss: 0.04625270
[7/200] Training loss: 0.04190551
[8/200] Training loss: 0.03970690
[9/200] Training loss: 0.03749076
[10/200] Training loss: 0.03631325
[50/200] Training loss: 0.01680712
[100/200] Training loss: 0.01523819
[150/200] Training loss: 0.01300943
[200/200] Training loss: 0.01232418
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6250.119998848022 ----------
[1/100] Training loss: 0.11482325
[2/100] Training loss: 0.04669041
[3/100] Training loss: 0.04284456
[4/100] Training loss: 0.03899704
[5/100] Training loss: 0.03442221
[6/100] Training loss: 0.03271616
[7/100] Training loss: 0.02947838
[8/100] Training loss: 0.02883147
[9/100] Training loss: 0.02712966
[10/100] Training loss: 0.02671180
[50/100] Training loss: 0.01705372
[100/100] Training loss: 0.01285898
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4798.870283723035 ----------
[1/100] Training loss: 0.16195517
[2/100] Training loss: 0.05961122
[3/100] Training loss: 0.05020491
[4/100] Training loss: 0.04521287
[5/100] Training loss: 0.04716897
[6/100] Training loss: 0.04235256
[7/100] Training loss: 0.04013610
[8/100] Training loss: 0.03536124
[9/100] Training loss: 0.03447732
[10/100] Training loss: 0.03104507
[50/100] Training loss: 0.01891270
[100/100] Training loss: 0.01725016
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10741.141093943417 ----------
[1/200] Training loss: 0.11235283
[2/200] Training loss: 0.04826264
[3/200] Training loss: 0.04304777
[4/200] Training loss: 0.03788647
[5/200] Training loss: 0.03464542
[6/200] Training loss: 0.03328933
[7/200] Training loss: 0.03121735
[8/200] Training loss: 0.02755280
[9/200] Training loss: 0.02668474
[10/200] Training loss: 0.02555347
[50/200] Training loss: 0.01571187
[100/200] Training loss: 0.01372059
[150/200] Training loss: 0.01205126
[200/200] Training loss: 0.01111298
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 32 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5476.549826304879 ----------
[1/200] Training loss: 0.12395481
[2/200] Training loss: 0.05550129
[3/200] Training loss: 0.04606048
[4/200] Training loss: 0.04439047
[5/200] Training loss: 0.04035373
[6/200] Training loss: 0.03530902
[7/200] Training loss: 0.03267433
[8/200] Training loss: 0.03023520
[9/200] Training loss: 0.02758745
[10/200] Training loss: 0.02562602
[50/200] Training loss: 0.01728747
[100/200] Training loss: 0.01503194
[150/200] Training loss: 0.01344733
[200/200] Training loss: 0.01228835
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12667.607193152146 ----------
[1/200] Training loss: 0.08850192
[2/200] Training loss: 0.05050646
[3/200] Training loss: 0.04318522
[4/200] Training loss: 0.04029337
[5/200] Training loss: 0.03573136
[6/200] Training loss: 0.03475852
[7/200] Training loss: 0.03200551
[8/200] Training loss: 0.03116380
[9/200] Training loss: 0.02828089
[10/200] Training loss: 0.02822021
[50/200] Training loss: 0.01690948
[100/200] Training loss: 0.01378157
[150/200] Training loss: 0.01206659
[200/200] Training loss: 0.01140247
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5211.373715250135 ----------
[1/200] Training loss: 0.10668353
[2/200] Training loss: 0.05458316
[3/200] Training loss: 0.04928762
[4/200] Training loss: 0.04675706
[5/200] Training loss: 0.03938299
[6/200] Training loss: 0.03626562
[7/200] Training loss: 0.03198390
[8/200] Training loss: 0.02806270
[9/200] Training loss: 0.02570925
[10/200] Training loss: 0.02441109
[50/200] Training loss: 0.01640166
[100/200] Training loss: 0.01393837
[150/200] Training loss: 0.01303154
[200/200] Training loss: 0.01164517
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8377.749101041401 ----------
[1/200] Training loss: 0.10054535
[2/200] Training loss: 0.05076350
[3/200] Training loss: 0.04226424
[4/200] Training loss: 0.03837296
[5/200] Training loss: 0.03581478
[6/200] Training loss: 0.03143279
[7/200] Training loss: 0.03027484
[8/200] Training loss: 0.02790392
[9/200] Training loss: 0.02736864
[10/200] Training loss: 0.02558873
[50/200] Training loss: 0.01533225
[100/200] Training loss: 0.01328825
[150/200] Training loss: 0.01217136
[200/200] Training loss: 0.01105103
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16362.138735507653 ----------
[1/200] Training loss: 0.09542638
[2/200] Training loss: 0.05114644
[3/200] Training loss: 0.04497095
[4/200] Training loss: 0.03946524
[5/200] Training loss: 0.03712519
[6/200] Training loss: 0.03460543
[7/200] Training loss: 0.03257554
[8/200] Training loss: 0.03022133
[9/200] Training loss: 0.02847231
[10/200] Training loss: 0.02714203
[50/200] Training loss: 0.01667498
[100/200] Training loss: 0.01384341
[150/200] Training loss: 0.01264516
[200/200] Training loss: 0.01166653
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11062.797837798538 ----------
[1/100] Training loss: 0.10721332
[2/100] Training loss: 0.05445317
[3/100] Training loss: 0.04950660
[4/100] Training loss: 0.04708042
[5/100] Training loss: 0.04232614
[6/100] Training loss: 0.03722105
[7/100] Training loss: 0.03630240
[8/100] Training loss: 0.03163069
[9/100] Training loss: 0.03056912
[10/100] Training loss: 0.02739929
[50/100] Training loss: 0.01549933
[100/100] Training loss: 0.01321166
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10815.972263278045 ----------
[1/200] Training loss: 0.10999746
[2/200] Training loss: 0.05241691
[3/200] Training loss: 0.04507848
[4/200] Training loss: 0.03968933
[5/200] Training loss: 0.03669177
[6/200] Training loss: 0.03353538
[7/200] Training loss: 0.03240783
[8/200] Training loss: 0.03168109
[9/200] Training loss: 0.02903813
[10/200] Training loss: 0.02766177
[50/200] Training loss: 0.01556781
[100/200] Training loss: 0.01334855
[150/200] Training loss: 0.01170768
[200/200] Training loss: 0.01100768
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11570.38599183277 ----------
[1/100] Training loss: 0.17173738
[2/100] Training loss: 0.05331192
[3/100] Training loss: 0.05157725
[4/100] Training loss: 0.04801445
[5/100] Training loss: 0.04683140
[6/100] Training loss: 0.04620752
[7/100] Training loss: 0.03837172
[8/100] Training loss: 0.03954997
[9/100] Training loss: 0.03694531
[10/100] Training loss: 0.03733622
[50/100] Training loss: 0.02012290
[100/100] Training loss: 0.01554774
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 3791.036006159794 ----------
[1/150] Training loss: 0.11587454
[2/150] Training loss: 0.05238804
[3/150] Training loss: 0.04613051
[4/150] Training loss: 0.04274931
[5/150] Training loss: 0.04040598
[6/150] Training loss: 0.03662242
[7/150] Training loss: 0.03253378
[8/150] Training loss: 0.03111089
[9/150] Training loss: 0.02874541
[10/150] Training loss: 0.02797328
[50/150] Training loss: 0.01654957
[100/150] Training loss: 0.01444688
[150/150] Training loss: 0.01297438
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 150
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5617.863472887179 ----------
[1/200] Training loss: 0.11761129
[2/200] Training loss: 0.05330608
[3/200] Training loss: 0.04665386
[4/200] Training loss: 0.04396298
[5/200] Training loss: 0.04061784
[6/200] Training loss: 0.03534471
[7/200] Training loss: 0.03236290
[8/200] Training loss: 0.03084602
[9/200] Training loss: 0.02763447
[10/200] Training loss: 0.02723486
[50/200] Training loss: 0.01690772
[100/200] Training loss: 0.01510779
[150/200] Training loss: 0.01359496
[200/200] Training loss: 0.01299363
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10158.959001787536 ----------
[1/200] Training loss: 0.10360077
[2/200] Training loss: 0.04649589
[3/200] Training loss: 0.04296215
[4/200] Training loss: 0.04066129
[5/200] Training loss: 0.03843227
[6/200] Training loss: 0.03432392
[7/200] Training loss: 0.03123173
[8/200] Training loss: 0.02870831
[9/200] Training loss: 0.02943895
[10/200] Training loss: 0.02866602
[50/200] Training loss: 0.01744791
[100/200] Training loss: 0.01391686
[150/200] Training loss: 0.01269006
[200/200] Training loss: 0.01150182
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4828.696925672598 ----------
[1/200] Training loss: 0.10624886
[2/200] Training loss: 0.05181512
[3/200] Training loss: 0.03740078
[4/200] Training loss: 0.03336872
[5/200] Training loss: 0.03284004
[6/200] Training loss: 0.02569155
[7/200] Training loss: 0.03537756
[8/200] Training loss: 0.02488600
[9/200] Training loss: 0.02422568
[10/200] Training loss: 0.02795494
[50/200] Training loss: 0.01577486
[100/200] Training loss: 0.01342424
[150/200] Training loss: 0.01269147
[200/200] Training loss: 0.00923407
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adamax ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 32092.747591940457 ----------
[1/150] Training loss: 0.12373964
[2/150] Training loss: 0.05686296
[3/150] Training loss: 0.04968652
[4/150] Training loss: 0.04717908
[5/150] Training loss: 0.04506539
[6/150] Training loss: 0.04083647
[7/150] Training loss: 0.03968746
[8/150] Training loss: 0.03682066
[9/150] Training loss: 0.03529043
[10/150] Training loss: 0.03431448
[50/150] Training loss: 0.01844007
[100/150] Training loss: 0.01653919
[150/150] Training loss: 0.01463600
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 150
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12265.096493709294 ----------
[1/100] Training loss: 0.09228069
[2/100] Training loss: 0.04820195
[3/100] Training loss: 0.04427115
[4/100] Training loss: 0.03911690
[5/100] Training loss: 0.03446777
[6/100] Training loss: 0.03409757
[7/100] Training loss: 0.03043002
[8/100] Training loss: 0.02928598
[9/100] Training loss: 0.02657855
[10/100] Training loss: 0.02572371
[50/100] Training loss: 0.01739519
[100/100] Training loss: 0.01300815
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9959.099959333675 ----------
[1/100] Training loss: 0.11527501
[2/100] Training loss: 0.05355788
[3/100] Training loss: 0.04663194
[4/100] Training loss: 0.04320358
[5/100] Training loss: 0.03949723
[6/100] Training loss: 0.03736997
[7/100] Training loss: 0.03464819
[8/100] Training loss: 0.03184698
[9/100] Training loss: 0.03089302
[10/100] Training loss: 0.03024413
[50/100] Training loss: 0.01738116
[100/100] Training loss: 0.01582179
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10575.779120235067 ----------
[1/200] Training loss: 0.07466366
[2/200] Training loss: 0.04488126
[3/200] Training loss: 0.03618244
[4/200] Training loss: 0.02916805
[5/200] Training loss: 0.02616754
[6/200] Training loss: 0.02397916
[7/200] Training loss: 0.02271944
[8/200] Training loss: 0.02205218
[9/200] Training loss: 0.02121731
[10/200] Training loss: 0.02062817
[50/200] Training loss: 0.01522397
[100/200] Training loss: 0.01392346
[150/200] Training loss: 0.01275845
[200/200] Training loss: 0.01206265
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 22225.020854883354 ----------
[1/200] Training loss: 0.61215305
[2/200] Training loss: 0.13317812
[3/200] Training loss: 0.13445600
[4/200] Training loss: 0.11458072
[5/200] Training loss: 0.11340200
[6/200] Training loss: 0.09068534
[7/200] Training loss: 0.08563686
[8/200] Training loss: 0.07423255
[9/200] Training loss: 0.08618165
[10/200] Training loss: 0.07861577
[50/200] Training loss: 0.05083021
[100/200] Training loss: 0.04338600
[150/200] Training loss: 0.04132049
[200/200] Training loss: 0.03633612
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: RMSprop ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 39318.74748768073 ----------
[1/200] Training loss: 0.15095527
[2/200] Training loss: 0.06093315
[3/200] Training loss: 0.05656886
[4/200] Training loss: 0.05278581
[5/200] Training loss: 0.04908886
[6/200] Training loss: 0.04999488
[7/200] Training loss: 0.04607848
[8/200] Training loss: 0.04562697
[9/200] Training loss: 0.03968290
[10/200] Training loss: 0.03590126
[50/200] Training loss: 0.01913936
[100/200] Training loss: 0.01587858
[150/200] Training loss: 0.01478659
[200/200] Training loss: 0.01313386
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 23958.651047168743 ----------
[1/200] Training loss: 0.15255488
[2/200] Training loss: 0.05818159
[3/200] Training loss: 0.05389889
[4/200] Training loss: 0.04847648
[5/200] Training loss: 0.04662367
[6/200] Training loss: 0.04524252
[7/200] Training loss: 0.04122243
[8/200] Training loss: 0.04101261
[9/200] Training loss: 0.03726348
[10/200] Training loss: 0.03517798
[50/200] Training loss: 0.01978712
[100/200] Training loss: 0.01724357
[150/200] Training loss: 0.01577076
[200/200] Training loss: 0.01506761
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13090.117188169095 ----------
[1/200] Training loss: 0.11918942
[2/200] Training loss: 0.05744420
[3/200] Training loss: 0.05415652
[4/200] Training loss: 0.04957144
[5/200] Training loss: 0.04642595
[6/200] Training loss: 0.04272751
[7/200] Training loss: 0.03748817
[8/200] Training loss: 0.03276507
[9/200] Training loss: 0.03019221
[10/200] Training loss: 0.02934707
[50/200] Training loss: 0.01704526
[100/200] Training loss: 0.01488186
[150/200] Training loss: 0.01334588
[200/200] Training loss: 0.01266294
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15629.838642801147 ----------
[1/200] Training loss: 0.11026898
[2/200] Training loss: 0.05219779
[3/200] Training loss: 0.04615472
[4/200] Training loss: 0.04219165
[5/200] Training loss: 0.03701313
[6/200] Training loss: 0.03417159
[7/200] Training loss: 0.03320216
[8/200] Training loss: 0.03042651
[9/200] Training loss: 0.02970109
[10/200] Training loss: 0.02775638
[50/200] Training loss: 0.01915275
[100/200] Training loss: 0.01631557
[150/200] Training loss: 0.01453600
[200/200] Training loss: 0.01367854
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8486.114776504028 ----------
[1/200] Training loss: 0.15085470
[2/200] Training loss: 0.05593002
[3/200] Training loss: 0.05299134
[4/200] Training loss: 0.04608865
[5/200] Training loss: 0.04386150
[6/200] Training loss: 0.03976626
[7/200] Training loss: 0.03851933
[8/200] Training loss: 0.03536023
[9/200] Training loss: 0.03421828
[10/200] Training loss: 0.03220550
[50/200] Training loss: 0.01970550
[100/200] Training loss: 0.01627491
[150/200] Training loss: 0.01442965
[200/200] Training loss: 0.01265649
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8164.444377910845 ----------
[1/150] Training loss: 0.10724412
[2/150] Training loss: 0.05042598
[3/150] Training loss: 0.04525363
[4/150] Training loss: 0.04100043
[5/150] Training loss: 0.03716498
[6/150] Training loss: 0.03445884
[7/150] Training loss: 0.03108621
[8/150] Training loss: 0.02898908
[9/150] Training loss: 0.02823813
[10/150] Training loss: 0.02702129
[50/150] Training loss: 0.01782731
[100/150] Training loss: 0.01492228
[150/150] Training loss: 0.01328211
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 150
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13816.776469205834 ----------
[1/200] Training loss: 0.12102970
[2/200] Training loss: 0.05881447
[3/200] Training loss: 0.05341102
[4/200] Training loss: 0.05042827
[5/200] Training loss: 0.04898875
[6/200] Training loss: 0.04805018
[7/200] Training loss: 0.04554693
[8/200] Training loss: 0.04403748
[9/200] Training loss: 0.04065803
[10/200] Training loss: 0.03832940
[50/200] Training loss: 0.01793136
[100/200] Training loss: 0.01495260
[150/200] Training loss: 0.01343430
[200/200] Training loss: 0.01266177
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10934.439537534605 ----------
[1/200] Training loss: 0.13985414
[2/200] Training loss: 0.06032186
[3/200] Training loss: 0.05088461
[4/200] Training loss: 0.04864261
[5/200] Training loss: 0.04657681
[6/200] Training loss: 0.04420300
[7/200] Training loss: 0.04171945
[8/200] Training loss: 0.04030277
[9/200] Training loss: 0.03666811
[10/200] Training loss: 0.03445956
[50/200] Training loss: 0.01775735
[100/200] Training loss: 0.01627450
[150/200] Training loss: 0.01484362
[200/200] Training loss: 0.01320405
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9025.57433075591 ----------
[1/200] Training loss: 0.16494678
[2/200] Training loss: 0.05718496
[3/200] Training loss: 0.05064700
[4/200] Training loss: 0.04451712
[5/200] Training loss: 0.04338087
[6/200] Training loss: 0.03913263
[7/200] Training loss: 0.03422645
[8/200] Training loss: 0.03366014
[9/200] Training loss: 0.03107526
[10/200] Training loss: 0.02887298
[50/200] Training loss: 0.01781523
[100/200] Training loss: 0.01591097
[150/200] Training loss: 0.01499835
[200/200] Training loss: 0.01374040
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6485.311711860888 ----------
[1/100] Training loss: 0.14169541
[2/100] Training loss: 0.06291706
[3/100] Training loss: 0.05228633
[4/100] Training loss: 0.05042214
[5/100] Training loss: 0.04881925
[6/100] Training loss: 0.04705650
[7/100] Training loss: 0.03836446
[8/100] Training loss: 0.03446275
[9/100] Training loss: 0.03119198
[10/100] Training loss: 0.02995523
[50/100] Training loss: 0.01864619
[100/100] Training loss: 0.01501153
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16032.328339951126 ----------
[1/200] Training loss: 0.16026621
[2/200] Training loss: 0.06012523
[3/200] Training loss: 0.05361458
[4/200] Training loss: 0.05163135
[5/200] Training loss: 0.04964046
[6/200] Training loss: 0.04752251
[7/200] Training loss: 0.04607758
[8/200] Training loss: 0.04377683
[9/200] Training loss: 0.04263495
[10/200] Training loss: 0.03955341
[50/200] Training loss: 0.01793186
[100/200] Training loss: 0.01539112
[150/200] Training loss: 0.01396611
[200/200] Training loss: 0.01362700
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15935.38251815751 ----------
[1/200] Training loss: 0.12155355
[2/200] Training loss: 0.05683797
[3/200] Training loss: 0.05240196
[4/200] Training loss: 0.04892067
[5/200] Training loss: 0.04611762
[6/200] Training loss: 0.04156717
[7/200] Training loss: 0.03924353
[8/200] Training loss: 0.03635495
[9/200] Training loss: 0.03301599
[10/200] Training loss: 0.03243616
[50/200] Training loss: 0.01783553
[100/200] Training loss: 0.01496333
[150/200] Training loss: 0.01371731
[200/200] Training loss: 0.01236611
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 32 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9766.870532570809 ----------
[1/100] Training loss: 0.11936634
[2/100] Training loss: 0.05804028
[3/100] Training loss: 0.04968136
[4/100] Training loss: 0.04732133
[5/100] Training loss: 0.04606120
[6/100] Training loss: 0.04471795
[7/100] Training loss: 0.03932265
[8/100] Training loss: 0.03749673
[9/100] Training loss: 0.03753816
[10/100] Training loss: 0.03456894
[50/100] Training loss: 0.01742292
[100/100] Training loss: 0.01471572
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16481.18442345695 ----------
[1/200] Training loss: 0.11237990
[2/200] Training loss: 0.05199769
[3/200] Training loss: 0.04377408
[4/200] Training loss: 0.04145970
[5/200] Training loss: 0.03480893
[6/200] Training loss: 0.03179769
[7/200] Training loss: 0.03018342
[8/200] Training loss: 0.02785511
[9/200] Training loss: 0.02727479
[10/200] Training loss: 0.02599186
[50/200] Training loss: 0.01598559
[100/200] Training loss: 0.01358558
[150/200] Training loss: 0.01266485
[200/200] Training loss: 0.01132290
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8156.529899411882 ----------
[1/150] Training loss: 0.13373671
[2/150] Training loss: 0.05635793
[3/150] Training loss: 0.05264405
[4/150] Training loss: 0.05098569
[5/150] Training loss: 0.04775264
[6/150] Training loss: 0.04480705
[7/150] Training loss: 0.04341651
[8/150] Training loss: 0.04351687
[9/150] Training loss: 0.03755943
[10/150] Training loss: 0.03661878
[50/150] Training loss: 0.01766985
[100/150] Training loss: 0.01496975
[150/150] Training loss: 0.01345543
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 150
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14625.397635620031 ----------
[1/200] Training loss: 0.14375354
[2/200] Training loss: 0.06081170
[3/200] Training loss: 0.05109725
[4/200] Training loss: 0.05135983
[5/200] Training loss: 0.04418845
[6/200] Training loss: 0.04295702
[7/200] Training loss: 0.04111732
[8/200] Training loss: 0.03790383
[9/200] Training loss: 0.03636825
[10/200] Training loss: 0.03207103
[50/200] Training loss: 0.01920064
[100/200] Training loss: 0.01724090
[150/200] Training loss: 0.01601457
[200/200] Training loss: 0.01471919
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12795.947483480853 ----------
[1/200] Training loss: 0.11642423
[2/200] Training loss: 0.05644937
[3/200] Training loss: 0.05008278
[4/200] Training loss: 0.04521808
[5/200] Training loss: 0.04133857
[6/200] Training loss: 0.03749389
[7/200] Training loss: 0.03418657
[8/200] Training loss: 0.03232857
[9/200] Training loss: 0.03106333
[10/200] Training loss: 0.02844200
[50/200] Training loss: 0.01500930
[100/200] Training loss: 0.01236467
[150/200] Training loss: 0.01125725
[200/200] Training loss: 0.01080608
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17998.92796807632 ----------
[1/200] Training loss: 0.11720779
[2/200] Training loss: 0.05508349
[3/200] Training loss: 0.04790712
[4/200] Training loss: 0.04388512
[5/200] Training loss: 0.03821464
[6/200] Training loss: 0.03187222
[7/200] Training loss: 0.03028695
[8/200] Training loss: 0.02743073
[9/200] Training loss: 0.02619727
[10/200] Training loss: 0.02434999
[50/200] Training loss: 0.01598018
[100/200] Training loss: 0.01381462
[150/200] Training loss: 0.01253605
[200/200] Training loss: 0.01104460
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15125.29272444008 ----------
[1/200] Training loss: 0.13949235
[2/200] Training loss: 0.05387626
[3/200] Training loss: 0.04645748
[4/200] Training loss: 0.04319994
[5/200] Training loss: 0.04224281
[6/200] Training loss: 0.03928400
[7/200] Training loss: 0.03841698
[8/200] Training loss: 0.03604987
[9/200] Training loss: 0.03576045
[10/200] Training loss: 0.03405840
[50/200] Training loss: 0.02066845
[100/200] Training loss: 0.01774882
[150/200] Training loss: 0.01425396
[200/200] Training loss: 0.01276159
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5573.879080137996 ----------
[1/200] Training loss: 0.14221703
[2/200] Training loss: 0.05770520
[3/200] Training loss: 0.05069082
[4/200] Training loss: 0.04814600
[5/200] Training loss: 0.04598304
[6/200] Training loss: 0.04476609
[7/200] Training loss: 0.03910802
[8/200] Training loss: 0.03946852
[9/200] Training loss: 0.03944274
[10/200] Training loss: 0.03544835
[50/200] Training loss: 0.01834828
[100/200] Training loss: 0.01421268
[150/200] Training loss: 0.01325595
[200/200] Training loss: 0.01214552
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5733.510617414081 ----------
[1/200] Training loss: 0.16393783
[2/200] Training loss: 0.05720429
[3/200] Training loss: 0.05724284
[4/200] Training loss: 0.05084855
[5/200] Training loss: 0.04844422
[6/200] Training loss: 0.04777454
[7/200] Training loss: 0.04452751
[8/200] Training loss: 0.04050195
[9/200] Training loss: 0.04186223
[10/200] Training loss: 0.03820632
[50/200] Training loss: 0.01910750
[100/200] Training loss: 0.01718553
[150/200] Training loss: 0.01544474
[200/200] Training loss: 0.01456743
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11587.217094712603 ----------
[1/200] Training loss: 0.15435221
[2/200] Training loss: 0.06029604
[3/200] Training loss: 0.05560016
[4/200] Training loss: 0.05188198
[5/200] Training loss: 0.04677504
[6/200] Training loss: 0.04655474
[7/200] Training loss: 0.04240335
[8/200] Training loss: 0.04033111
[9/200] Training loss: 0.03714612
[10/200] Training loss: 0.03544487
[50/200] Training loss: 0.01851224
[100/200] Training loss: 0.01542998
[150/200] Training loss: 0.01422448
[200/200] Training loss: 0.01365210
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11415.546942656756 ----------
[1/200] Training loss: 0.09448874
[2/200] Training loss: 0.05118483
[3/200] Training loss: 0.04579188
[4/200] Training loss: 0.04200007
[5/200] Training loss: 0.03759919
[6/200] Training loss: 0.03326885
[7/200] Training loss: 0.02986846
[8/200] Training loss: 0.02936269
[9/200] Training loss: 0.02584757
[10/200] Training loss: 0.02511121
[50/200] Training loss: 0.01732441
[100/200] Training loss: 0.01503808
[150/200] Training loss: 0.01422520
[200/200] Training loss: 0.01271741
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17355.602207932745 ----------
[1/200] Training loss: 0.14726901
[2/200] Training loss: 0.05252699
[3/200] Training loss: 0.04951466
[4/200] Training loss: 0.04623830
[5/200] Training loss: 0.04362933
[6/200] Training loss: 0.04123581
[7/200] Training loss: 0.03576996
[8/200] Training loss: 0.03461492
[9/200] Training loss: 0.02999877
[10/200] Training loss: 0.03091253
[50/200] Training loss: 0.01921055
[100/200] Training loss: 0.01697757
[150/200] Training loss: 0.01440085
[200/200] Training loss: 0.01387277
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12958.465958592475 ----------
[1/200] Training loss: 0.11097882
[2/200] Training loss: 0.05242853
[3/200] Training loss: 0.04785953
[4/200] Training loss: 0.04363904
[5/200] Training loss: 0.04058895
[6/200] Training loss: 0.03737735
[7/200] Training loss: 0.03327194
[8/200] Training loss: 0.03271788
[9/200] Training loss: 0.03186173
[10/200] Training loss: 0.03017558
[50/200] Training loss: 0.01729873
[100/200] Training loss: 0.01473656
[150/200] Training loss: 0.01312619
[200/200] Training loss: 0.01210248
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 19124.649643849687 ----------
[1/200] Training loss: 0.16949892
[2/200] Training loss: 0.05807232
[3/200] Training loss: 0.05192476
[4/200] Training loss: 0.04461481
[5/200] Training loss: 0.04229120
[6/200] Training loss: 0.03802669
[7/200] Training loss: 0.03455089
[8/200] Training loss: 0.03468565
[9/200] Training loss: 0.03366339
[10/200] Training loss: 0.03185320
[50/200] Training loss: 0.01852988
[100/200] Training loss: 0.01590714
[150/200] Training loss: 0.01525872
[200/200] Training loss: 0.01434269
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7908.372525368289 ----------
[1/200] Training loss: 0.10368374
[2/200] Training loss: 0.05060659
[3/200] Training loss: 0.04386659
[4/200] Training loss: 0.04011489
[5/200] Training loss: 0.03535402
[6/200] Training loss: 0.03353049
[7/200] Training loss: 0.03132915
[8/200] Training loss: 0.02968777
[9/200] Training loss: 0.02706572
[10/200] Training loss: 0.02627064
[50/200] Training loss: 0.01650865
[100/200] Training loss: 0.01420511
[150/200] Training loss: 0.01247621
[200/200] Training loss: 0.01077286
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7494.963108648368 ----------
[1/200] Training loss: 0.12274626
[2/200] Training loss: 0.04848425
[3/200] Training loss: 0.04417740
[4/200] Training loss: 0.03908229
[5/200] Training loss: 0.03536321
[6/200] Training loss: 0.03190024
[7/200] Training loss: 0.03148362
[8/200] Training loss: 0.02829497
[9/200] Training loss: 0.02800447
[10/200] Training loss: 0.02656612
[50/200] Training loss: 0.01639573
[100/200] Training loss: 0.01321817
[150/200] Training loss: 0.01180862
[200/200] Training loss: 0.01141727
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6146.222579763932 ----------
[1/200] Training loss: 0.10105478
[2/200] Training loss: 0.05295981
[3/200] Training loss: 0.04767391
[4/200] Training loss: 0.04185589
[5/200] Training loss: 0.03814041
[6/200] Training loss: 0.03427231
[7/200] Training loss: 0.03235285
[8/200] Training loss: 0.02850010
[9/200] Training loss: 0.02812979
[10/200] Training loss: 0.02612973
[50/200] Training loss: 0.01517116
[100/200] Training loss: 0.01344529
[150/200] Training loss: 0.01188374
[200/200] Training loss: 0.01069676
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6024.885393100852 ----------
[1/200] Training loss: 0.15056806
[2/200] Training loss: 0.05987244
[3/200] Training loss: 0.05529260
[4/200] Training loss: 0.05370080
[5/200] Training loss: 0.04654506
[6/200] Training loss: 0.04455940
[7/200] Training loss: 0.04107324
[8/200] Training loss: 0.03991446
[9/200] Training loss: 0.03607241
[10/200] Training loss: 0.03295337
[50/200] Training loss: 0.01775308
[100/200] Training loss: 0.01605246
[150/200] Training loss: 0.01514948
[200/200] Training loss: 0.01445680
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 24712.277110780382 ----------
[1/200] Training loss: 0.09044245
[2/200] Training loss: 0.05171050
[3/200] Training loss: 0.04345218
[4/200] Training loss: 0.03549649
[5/200] Training loss: 0.03117000
[6/200] Training loss: 0.02785363
[7/200] Training loss: 0.02590792
[8/200] Training loss: 0.02529668
[9/200] Training loss: 0.02322931
[10/200] Training loss: 0.02284008
[50/200] Training loss: 0.01588424
[100/200] Training loss: 0.01365359
[150/200] Training loss: 0.01244921
[200/200] Training loss: 0.01149089
---batch_size---: 4 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7560.251318574006 ----------
[1/200] Training loss: 0.10799345
[2/200] Training loss: 0.05476491
[3/200] Training loss: 0.05052830
[4/200] Training loss: 0.04412272
[5/200] Training loss: 0.03945028
[6/200] Training loss: 0.03507117
[7/200] Training loss: 0.03337207
[8/200] Training loss: 0.03117059
[9/200] Training loss: 0.02841280
[10/200] Training loss: 0.02674550
[50/200] Training loss: 0.01720374
[100/200] Training loss: 0.01467187
[150/200] Training loss: 0.01272554
[200/200] Training loss: 0.01176566
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14527.310005641099 ----------
[1/200] Training loss: 0.14577865
[2/200] Training loss: 0.06157288
[3/200] Training loss: 0.05179918
[4/200] Training loss: 0.04874324
[5/200] Training loss: 0.04639004
[6/200] Training loss: 0.04266343
[7/200] Training loss: 0.03906736
[8/200] Training loss: 0.03679103
[9/200] Training loss: 0.03552979
[10/200] Training loss: 0.03607623
[50/200] Training loss: 0.01844191
[100/200] Training loss: 0.01553643
[150/200] Training loss: 0.01462904
[200/200] Training loss: 0.01410079
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 20739.809063730554 ----------
[1/200] Training loss: 0.10945632
[2/200] Training loss: 0.05468321
[3/200] Training loss: 0.04864187
[4/200] Training loss: 0.04424248
[5/200] Training loss: 0.03963244
[6/200] Training loss: 0.03610584
[7/200] Training loss: 0.03285046
[8/200] Training loss: 0.03025751
[9/200] Training loss: 0.02942082
[10/200] Training loss: 0.02655821
[50/200] Training loss: 0.01719767
[100/200] Training loss: 0.01475891
[150/200] Training loss: 0.01239357
[200/200] Training loss: 0.01156399
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9723.84409582959 ----------
[1/200] Training loss: 0.18905477
[2/200] Training loss: 0.06447231
[3/200] Training loss: 0.05385866
[4/200] Training loss: 0.05013377
[5/200] Training loss: 0.04976323
[6/200] Training loss: 0.04750813
[7/200] Training loss: 0.04394401
[8/200] Training loss: 0.04183414
[9/200] Training loss: 0.04264341
[10/200] Training loss: 0.03776800
[50/200] Training loss: 0.02078081
[100/200] Training loss: 0.01533797
[150/200] Training loss: 0.01290875
[200/200] Training loss: 0.01231304
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 3974.4098933049167 ----------
[1/200] Training loss: 0.11335938
[2/200] Training loss: 0.05259984
[3/200] Training loss: 0.04447070
[4/200] Training loss: 0.03807928
[5/200] Training loss: 0.03487696
[6/200] Training loss: 0.03052781
[7/200] Training loss: 0.02807514
[8/200] Training loss: 0.02559827
[9/200] Training loss: 0.02475205
[10/200] Training loss: 0.02352709
[50/200] Training loss: 0.01764455
[100/200] Training loss: 0.01532426
[150/200] Training loss: 0.01439806
[200/200] Training loss: 0.01299473
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14873.690597830788 ----------
[1/200] Training loss: 0.14476165
[2/200] Training loss: 0.06214732
[3/200] Training loss: 0.05363003
[4/200] Training loss: 0.05087849
[5/200] Training loss: 0.05007057
[6/200] Training loss: 0.04726844
[7/200] Training loss: 0.04586902
[8/200] Training loss: 0.04546439
[9/200] Training loss: 0.03891336
[10/200] Training loss: 0.04433898
[50/200] Training loss: 0.01842020
[100/200] Training loss: 0.01553177
[150/200] Training loss: 0.01410222
[200/200] Training loss: 0.01329322
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 19508.876338733608 ----------
[1/200] Training loss: 0.13970585
[2/200] Training loss: 0.05918172
[3/200] Training loss: 0.05175307
[4/200] Training loss: 0.05095362
[5/200] Training loss: 0.04562157
[6/200] Training loss: 0.04475254
[7/200] Training loss: 0.04218810
[8/200] Training loss: 0.03953860
[9/200] Training loss: 0.03773230
[10/200] Training loss: 0.03612467
[50/200] Training loss: 0.01742045
[100/200] Training loss: 0.01410235
[150/200] Training loss: 0.01314194
[200/200] Training loss: 0.01224419
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 3447.062227462684 ----------
[1/100] Training loss: 0.17002084
[2/100] Training loss: 0.05754526
[3/100] Training loss: 0.04821556
[4/100] Training loss: 0.04581181
[5/100] Training loss: 0.04169104
[6/100] Training loss: 0.04452278
[7/100] Training loss: 0.03765085
[8/100] Training loss: 0.03445001
[9/100] Training loss: 0.03266888
[10/100] Training loss: 0.03032038
[50/100] Training loss: 0.01796088
[100/100] Training loss: 0.01416073
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7994.19089089071 ----------
[1/200] Training loss: 0.16555813
[2/200] Training loss: 0.06566113
[3/200] Training loss: 0.05080638
[4/200] Training loss: 0.04865767
[5/200] Training loss: 0.04338709
[6/200] Training loss: 0.04133598
[7/200] Training loss: 0.03704888
[8/200] Training loss: 0.03637655
[9/200] Training loss: 0.03307382
[10/200] Training loss: 0.03284314
[50/200] Training loss: 0.01786293
[100/200] Training loss: 0.01458970
[150/200] Training loss: 0.01333618
[200/200] Training loss: 0.01274838
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4865.564304374159 ----------
[1/200] Training loss: 0.15029740
[2/200] Training loss: 0.06075829
[3/200] Training loss: 0.05543056
[4/200] Training loss: 0.05093002
[5/200] Training loss: 0.04705433
[6/200] Training loss: 0.04361512
[7/200] Training loss: 0.03980558
[8/200] Training loss: 0.03470494
[9/200] Training loss: 0.03541740
[10/200] Training loss: 0.03188421
[50/200] Training loss: 0.01828827
[100/200] Training loss: 0.01518872
[150/200] Training loss: 0.01362435
[200/200] Training loss: 0.01284986
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10760.69737517044 ----------
[1/200] Training loss: 0.09917374
[2/200] Training loss: 0.04860904
[3/200] Training loss: 0.04732656
[4/200] Training loss: 0.04053771
[5/200] Training loss: 0.03931443
[6/200] Training loss: 0.03526387
[7/200] Training loss: 0.03413869
[8/200] Training loss: 0.03224107
[9/200] Training loss: 0.03055151
[10/200] Training loss: 0.02920530
[50/200] Training loss: 0.01924210
[100/200] Training loss: 0.01439426
[150/200] Training loss: 0.01265882
[200/200] Training loss: 0.01206911
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5882.5130684087735 ----------
[1/200] Training loss: 0.15222411
[2/200] Training loss: 0.05849112
[3/200] Training loss: 0.05320764
[4/200] Training loss: 0.05259342
[5/200] Training loss: 0.04783325
[6/200] Training loss: 0.04561251
[7/200] Training loss: 0.04176970
[8/200] Training loss: 0.04226229
[9/200] Training loss: 0.03872398
[10/200] Training loss: 0.03593407
[50/200] Training loss: 0.01966476
[100/200] Training loss: 0.01597926
[150/200] Training loss: 0.01440320
[200/200] Training loss: 0.01343281
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6119629369982093 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9107.629329304087 ----------
[1/100] Training loss: 0.16702357
[2/100] Training loss: 0.05558765
[3/100] Training loss: 0.05120236
[4/100] Training loss: 0.04628415
[5/100] Training loss: 0.04067677
[6/100] Training loss: 0.04008732
[7/100] Training loss: 0.03856382
[8/100] Training loss: 0.03711970
[9/100] Training loss: 0.03692155
[10/100] Training loss: 0.03359253
[50/100] Training loss: 0.02007192
[100/100] Training loss: 0.01703668
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9285.680588949848 ----------
[1/200] Training loss: 0.17627841
[2/200] Training loss: 0.05822843
[3/200] Training loss: 0.05283227
[4/200] Training loss: 0.04444166
[5/200] Training loss: 0.03861451
[6/200] Training loss: 0.03639898
[7/200] Training loss: 0.03142689
[8/200] Training loss: 0.03056800
[9/200] Training loss: 0.02852068
[10/200] Training loss: 0.02620659
[50/200] Training loss: 0.01726117
[100/200] Training loss: 0.01508824
[150/200] Training loss: 0.01347122
[200/200] Training loss: 0.01263176
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12840.897164918033 ----------
[1/200] Training loss: 0.13486203
[2/200] Training loss: 0.05544068
[3/200] Training loss: 0.04900709
[4/200] Training loss: 0.04712551
[5/200] Training loss: 0.04438113
[6/200] Training loss: 0.03991834
[7/200] Training loss: 0.03819671
[8/200] Training loss: 0.03623118
[9/200] Training loss: 0.03404956
[10/200] Training loss: 0.03390206
[50/200] Training loss: 0.02131521
[100/200] Training loss: 0.01562599
[150/200] Training loss: 0.01385876
[200/200] Training loss: 0.01291771
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4526.908879135961 ----------
[1/200] Training loss: 0.15776842
[2/200] Training loss: 0.05684769
[3/200] Training loss: 0.05295507
[4/200] Training loss: 0.05189273
[5/200] Training loss: 0.04840180
[6/200] Training loss: 0.04492054
[7/200] Training loss: 0.04466035
[8/200] Training loss: 0.04246435
[9/200] Training loss: 0.04337975
[10/200] Training loss: 0.03960030
[50/200] Training loss: 0.02163902
[100/200] Training loss: 0.01493744
[150/200] Training loss: 0.01298060
[200/200] Training loss: 0.01153758
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 3649.624638233362 ----------
[1/100] Training loss: 0.16674580
[2/100] Training loss: 0.06139832
[3/100] Training loss: 0.05475495
[4/100] Training loss: 0.04987383
[5/100] Training loss: 0.04925243
[6/100] Training loss: 0.04478939
[7/100] Training loss: 0.04462013
[8/100] Training loss: 0.04275103
[9/100] Training loss: 0.03986537
[10/100] Training loss: 0.03911211
[50/100] Training loss: 0.01971418
[100/100] Training loss: 0.01578854
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8378.537819930158 ----------
[1/200] Training loss: 0.17504652
[2/200] Training loss: 0.05808413
[3/200] Training loss: 0.05468867
[4/200] Training loss: 0.04925096
[5/200] Training loss: 0.04931699
[6/200] Training loss: 0.04668575
[7/200] Training loss: 0.04290587
[8/200] Training loss: 0.04046125
[9/200] Training loss: 0.03836549
[10/200] Training loss: 0.03651904
[50/200] Training loss: 0.01830345
[100/200] Training loss: 0.01620325
[150/200] Training loss: 0.01457575
[200/200] Training loss: 0.01377541
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6432.652951932041 ----------
[1/200] Training loss: 0.13734392
[2/200] Training loss: 0.06277110
[3/200] Training loss: 0.05461931
[4/200] Training loss: 0.05250911
[5/200] Training loss: 0.05234103
[6/200] Training loss: 0.04939883
[7/200] Training loss: 0.04832834
[8/200] Training loss: 0.04824804
[9/200] Training loss: 0.04397304
[10/200] Training loss: 0.04045956
[50/200] Training loss: 0.01754770
[100/200] Training loss: 0.01529830
[150/200] Training loss: 0.01351238
[200/200] Training loss: 0.01290629
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13312.302280221855 ----------
[1/100] Training loss: 0.12184162
[2/100] Training loss: 0.05276353
[3/100] Training loss: 0.05064737
[4/100] Training loss: 0.04630818
[5/100] Training loss: 0.04620048
[6/100] Training loss: 0.04440029
[7/100] Training loss: 0.04080525
[8/100] Training loss: 0.03784056
[9/100] Training loss: 0.03763594
[10/100] Training loss: 0.03659992
[50/100] Training loss: 0.02308008
[100/100] Training loss: 0.01698173
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.011705725335286247
----FITNESS-----------RMSE---- 11865.009397383552 ----------
[1/200] Training loss: 0.11498876
[2/200] Training loss: 0.05676813
[3/200] Training loss: 0.05285170
[4/200] Training loss: 0.04919901
[5/200] Training loss: 0.04665561
[6/200] Training loss: 0.04215836
[7/200] Training loss: 0.04073171
[8/200] Training loss: 0.03523747
[9/200] Training loss: 0.03299202
[10/200] Training loss: 0.03399248
[50/200] Training loss: 0.01806484
[100/200] Training loss: 0.01490941
[150/200] Training loss: 0.01345359
[200/200] Training loss: 0.01241812
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11325.373989409798 ----------
[1/200] Training loss: 0.13296847
[2/200] Training loss: 0.05376926
[3/200] Training loss: 0.04782000
[4/200] Training loss: 0.04338616
[5/200] Training loss: 0.04323117
[6/200] Training loss: 0.03915190
[7/200] Training loss: 0.03965892
[8/200] Training loss: 0.03625972
[9/200] Training loss: 0.03511393
[10/200] Training loss: 0.03386618
[50/200] Training loss: 0.01915064
[100/200] Training loss: 0.01491099
[150/200] Training loss: 0.01401125
[200/200] Training loss: 0.01229526
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 23240.074354442157 ----------
[1/200] Training loss: 0.17026982
[2/200] Training loss: 0.06242079
[3/200] Training loss: 0.05409337
[4/200] Training loss: 0.04912088
[5/200] Training loss: 0.04600283
[6/200] Training loss: 0.04188165
[7/200] Training loss: 0.04292104
[8/200] Training loss: 0.03793678
[9/200] Training loss: 0.03772046
[10/200] Training loss: 0.03367493
[50/200] Training loss: 0.01753968
[100/200] Training loss: 0.01526967
[150/200] Training loss: 0.01487235
[200/200] Training loss: 0.01395317
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 24006.201865351377 ----------
[1/200] Training loss: 0.10681045
[2/200] Training loss: 0.05321531
[3/200] Training loss: 0.04710964
[4/200] Training loss: 0.04384533
[5/200] Training loss: 0.04334037
[6/200] Training loss: 0.03726823
[7/200] Training loss: 0.03477726
[8/200] Training loss: 0.03404771
[9/200] Training loss: 0.03277419
[10/200] Training loss: 0.03064459
[50/200] Training loss: 0.01731728
[100/200] Training loss: 0.01492142
[150/200] Training loss: 0.01320367
[200/200] Training loss: 0.01279517
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8705.521006809415 ----------
[1/200] Training loss: 0.13817746
[2/200] Training loss: 0.05778673
[3/200] Training loss: 0.04722654
[4/200] Training loss: 0.04610390
[5/200] Training loss: 0.04700296
[6/200] Training loss: 0.04470224
[7/200] Training loss: 0.03884909
[8/200] Training loss: 0.04251223
[9/200] Training loss: 0.03684180
[10/200] Training loss: 0.03534245
[50/200] Training loss: 0.01807805
[100/200] Training loss: 0.01562040
[150/200] Training loss: 0.01395840
[200/200] Training loss: 0.01271621
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9365.488134635589 ----------
[1/200] Training loss: 0.11535977
[2/200] Training loss: 0.05445279
[3/200] Training loss: 0.05066764
[4/200] Training loss: 0.04515386
[5/200] Training loss: 0.04335452
[6/200] Training loss: 0.03865174
[7/200] Training loss: 0.03663493
[8/200] Training loss: 0.03310475
[9/200] Training loss: 0.03254011
[10/200] Training loss: 0.02812179
[50/200] Training loss: 0.01608941
[100/200] Training loss: 0.01395490
[150/200] Training loss: 0.01236875
[200/200] Training loss: 0.01123950
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17157.451559016565 ----------
[1/200] Training loss: 0.19926699
[2/200] Training loss: 0.06536964
[3/200] Training loss: 0.05720562
[4/200] Training loss: 0.05283520
[5/200] Training loss: 0.04910785
[6/200] Training loss: 0.04631634
[7/200] Training loss: 0.04692729
[8/200] Training loss: 0.04136872
[9/200] Training loss: 0.04481998
[10/200] Training loss: 0.04156941
[50/200] Training loss: 0.02108785
[100/200] Training loss: 0.01642439
[150/200] Training loss: 0.01496365
[200/200] Training loss: 0.01286549
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9416.171196404619 ----------
[1/200] Training loss: 0.15243186
[2/200] Training loss: 0.05720531
[3/200] Training loss: 0.05154188
[4/200] Training loss: 0.04786932
[5/200] Training loss: 0.04657265
[6/200] Training loss: 0.04400210
[7/200] Training loss: 0.04102635
[8/200] Training loss: 0.03876895
[9/200] Training loss: 0.03400307
[10/200] Training loss: 0.03594672
[50/200] Training loss: 0.01810268
[100/200] Training loss: 0.01504302
[150/200] Training loss: 0.01410866
[200/200] Training loss: 0.01289669
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14881.776238070508 ----------
[1/100] Training loss: 0.10783410
[2/100] Training loss: 0.05190918
[3/100] Training loss: 0.04636024
[4/100] Training loss: 0.04316234
[5/100] Training loss: 0.03821136
[6/100] Training loss: 0.03397116
[7/100] Training loss: 0.03290442
[8/100] Training loss: 0.02920652
[9/100] Training loss: 0.02823051
[10/100] Training loss: 0.02674780
[50/100] Training loss: 0.01575904
[100/100] Training loss: 0.01331306
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6888.5583397398905 ----------
[1/200] Training loss: 0.13700676
[2/200] Training loss: 0.05749039
[3/200] Training loss: 0.05018915
[4/200] Training loss: 0.04589161
[5/200] Training loss: 0.04317982
[6/200] Training loss: 0.04199269
[7/200] Training loss: 0.03907299
[8/200] Training loss: 0.03540322
[9/200] Training loss: 0.03490887
[10/200] Training loss: 0.03167375
[50/200] Training loss: 0.01804249
[100/200] Training loss: 0.01427360
[150/200] Training loss: 0.01305587
[200/200] Training loss: 0.01130042
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4720.213766345757 ----------
[1/200] Training loss: 0.11500951
[2/200] Training loss: 0.05490863
[3/200] Training loss: 0.04940807
[4/200] Training loss: 0.04480880
[5/200] Training loss: 0.03971384
[6/200] Training loss: 0.03598669
[7/200] Training loss: 0.03302133
[8/200] Training loss: 0.03183577
[9/200] Training loss: 0.02976124
[10/200] Training loss: 0.02771534
[50/200] Training loss: 0.01662652
[100/200] Training loss: 0.01389463
[150/200] Training loss: 0.01243971
[200/200] Training loss: 0.01136855
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 19168.390647104414 ----------
[1/200] Training loss: 0.13849608
[2/200] Training loss: 0.06182548
[3/200] Training loss: 0.05715633
[4/200] Training loss: 0.05494644
[5/200] Training loss: 0.05254875
[6/200] Training loss: 0.05064128
[7/200] Training loss: 0.05056615
[8/200] Training loss: 0.04737554
[9/200] Training loss: 0.04662029
[10/200] Training loss: 0.04382503
[50/200] Training loss: 0.01935940
[100/200] Training loss: 0.01560839
[150/200] Training loss: 0.01419552
[200/200] Training loss: 0.01300698
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9794.065141706991 ----------
[1/200] Training loss: 0.08494231
[2/200] Training loss: 0.04813990
[3/200] Training loss: 0.04116517
[4/200] Training loss: 0.03343549
[5/200] Training loss: 0.03024076
[6/200] Training loss: 0.02657675
[7/200] Training loss: 0.02511663
[8/200] Training loss: 0.02458709
[9/200] Training loss: 0.02267783
[10/200] Training loss: 0.02230260
[50/200] Training loss: 0.01359827
[100/200] Training loss: 0.01168638
[150/200] Training loss: 0.01074218
[200/200] Training loss: 0.01020497
---batch_size---: 4 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16615.162232130024 ----------
[1/200] Training loss: 0.15104586
[2/200] Training loss: 0.05265116
[3/200] Training loss: 0.05520622
[4/200] Training loss: 0.04734025
[5/200] Training loss: 0.04409101
[6/200] Training loss: 0.04322453
[7/200] Training loss: 0.04256426
[8/200] Training loss: 0.03937335
[9/200] Training loss: 0.03605388
[10/200] Training loss: 0.03647135
[50/200] Training loss: 0.02016613
[100/200] Training loss: 0.01750889
[150/200] Training loss: 0.01576428
[200/200] Training loss: 0.01492083
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13042.04278477877 ----------
[1/100] Training loss: 0.11631712
[2/100] Training loss: 0.05243421
[3/100] Training loss: 0.04751695
[4/100] Training loss: 0.04499607
[5/100] Training loss: 0.04031389
[6/100] Training loss: 0.03777013
[7/100] Training loss: 0.03528391
[8/100] Training loss: 0.03336170
[9/100] Training loss: 0.02993659
[10/100] Training loss: 0.02877571
[50/100] Training loss: 0.01598333
[100/100] Training loss: 0.01391987
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16158.780151979296 ----------
[1/100] Training loss: 0.15527315
[2/100] Training loss: 0.05640028
[3/100] Training loss: 0.04823860
[4/100] Training loss: 0.04705475
[5/100] Training loss: 0.04418096
[6/100] Training loss: 0.03985710
[7/100] Training loss: 0.04013896
[8/100] Training loss: 0.03679023
[9/100] Training loss: 0.03586657
[10/100] Training loss: 0.03213636
[50/100] Training loss: 0.01856253
[100/100] Training loss: 0.01599181
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6669.218544927134 ----------
[1/100] Training loss: 0.24183690
[2/100] Training loss: 0.08224267
[3/100] Training loss: 0.08065261
[4/100] Training loss: 0.08110816
[5/100] Training loss: 0.07961821
[6/100] Training loss: 0.07914058
[7/100] Training loss: 0.07682504
[8/100] Training loss: 0.07533981
[9/100] Training loss: 0.07239800
[10/100] Training loss: 0.06665916
[50/100] Training loss: 0.03282222
[100/100] Training loss: 0.02444039
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: SGD ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13741.6470628524 ----------
[1/200] Training loss: 0.09665589
[2/200] Training loss: 0.04932132
[3/200] Training loss: 0.04521606
[4/200] Training loss: 0.04241187
[5/200] Training loss: 0.03876014
[6/200] Training loss: 0.03442462
[7/200] Training loss: 0.03178407
[8/200] Training loss: 0.02820638
[9/200] Training loss: 0.02767956
[10/200] Training loss: 0.02608713
[50/200] Training loss: 0.01532104
[100/200] Training loss: 0.01348338
[150/200] Training loss: 0.01193040
[200/200] Training loss: 0.01092008
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8141.3443607306035 ----------
[1/200] Training loss: 0.16447646
[2/200] Training loss: 0.05691705
[3/200] Training loss: 0.05278450
[4/200] Training loss: 0.04764254
[5/200] Training loss: 0.04586389
[6/200] Training loss: 0.04350634
[7/200] Training loss: 0.04222032
[8/200] Training loss: 0.04122334
[9/200] Training loss: 0.03800721
[10/200] Training loss: 0.03902855
[50/200] Training loss: 0.01709305
[100/200] Training loss: 0.01478679
[150/200] Training loss: 0.01349828
[200/200] Training loss: 0.01250616
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9435.013089551068 ----------
[1/200] Training loss: 0.10983106
[2/200] Training loss: 0.05643385
[3/200] Training loss: 0.05110875
[4/200] Training loss: 0.04714367
[5/200] Training loss: 0.04519746
[6/200] Training loss: 0.03984038
[7/200] Training loss: 0.03839834
[8/200] Training loss: 0.03426327
[9/200] Training loss: 0.03160011
[10/200] Training loss: 0.02911437
[50/200] Training loss: 0.01617682
[100/200] Training loss: 0.01368954
[150/200] Training loss: 0.01246567
[200/200] Training loss: 0.01150026
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 16 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6011.610765843045 ----------
[1/200] Training loss: 0.12951432
[2/200] Training loss: 0.05686856
[3/200] Training loss: 0.05305811
[4/200] Training loss: 0.04919316
[5/200] Training loss: 0.04590566
[6/200] Training loss: 0.04565323
[7/200] Training loss: 0.04359162
[8/200] Training loss: 0.04157624
[9/200] Training loss: 0.03878404
[10/200] Training loss: 0.03637101
[50/200] Training loss: 0.01843990
[100/200] Training loss: 0.01557577
[150/200] Training loss: 0.01412898
[200/200] Training loss: 0.01305232
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6845.87496234046 ----------
[1/100] Training loss: 0.15089869
[2/100] Training loss: 0.05188379
[3/100] Training loss: 0.04998458
[4/100] Training loss: 0.04559997
[5/100] Training loss: 0.04456447
[6/100] Training loss: 0.04111305
[7/100] Training loss: 0.04007027
[8/100] Training loss: 0.03824395
[9/100] Training loss: 0.03822146
[10/100] Training loss: 0.03428621
[50/100] Training loss: 0.01808064
[100/100] Training loss: 0.01572428
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14434.81236455812 ----------
[1/100] Training loss: 0.16940153
[2/100] Training loss: 0.06055836
[3/100] Training loss: 0.05424071
[4/100] Training loss: 0.04934838
[5/100] Training loss: 0.04743648
[6/100] Training loss: 0.04526925
[7/100] Training loss: 0.04503334
[8/100] Training loss: 0.03862437
[9/100] Training loss: 0.03874859
[10/100] Training loss: 0.03857091
[50/100] Training loss: 0.01941270
[100/100] Training loss: 0.01573324
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 20332.580357642757 ----------
[1/200] Training loss: 0.16025150
[2/200] Training loss: 0.06288245
[3/200] Training loss: 0.05360617
[4/200] Training loss: 0.04493590
[5/200] Training loss: 0.04560486
[6/200] Training loss: 0.04224706
[7/200] Training loss: 0.03871866
[8/200] Training loss: 0.03658488
[9/200] Training loss: 0.03459481
[10/200] Training loss: 0.03203437
[50/200] Training loss: 0.01813294
[100/200] Training loss: 0.01527957
[150/200] Training loss: 0.01375592
[200/200] Training loss: 0.01287496
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8566.526017003625 ----------
[1/200] Training loss: 0.15801103
[2/200] Training loss: 0.06075153
[3/200] Training loss: 0.05143251
[4/200] Training loss: 0.04706487
[5/200] Training loss: 0.04357392
[6/200] Training loss: 0.04351370
[7/200] Training loss: 0.03987845
[8/200] Training loss: 0.03562059
[9/200] Training loss: 0.03372166
[10/200] Training loss: 0.03359134
[50/200] Training loss: 0.01869701
[100/200] Training loss: 0.01620320
[150/200] Training loss: 0.01475908
[200/200] Training loss: 0.01319796
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6881.905840681054 ----------
[1/100] Training loss: 0.16292817
[2/100] Training loss: 0.06003033
[3/100] Training loss: 0.05792114
[4/100] Training loss: 0.05335511
[5/100] Training loss: 0.05035099
[6/100] Training loss: 0.05021323
[7/100] Training loss: 0.04957794
[8/100] Training loss: 0.04346298
[9/100] Training loss: 0.04225371
[10/100] Training loss: 0.04084347
[50/100] Training loss: 0.02037051
[100/100] Training loss: 0.01579284
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12985.387479778954 ----------
[1/200] Training loss: 0.16229491
[2/200] Training loss: 0.05701315
[3/200] Training loss: 0.05067106
[4/200] Training loss: 0.04621876
[5/200] Training loss: 0.04442899
[6/200] Training loss: 0.03863612
[7/200] Training loss: 0.03810654
[8/200] Training loss: 0.03586234
[9/200] Training loss: 0.03429010
[10/200] Training loss: 0.03198728
[50/200] Training loss: 0.01814744
[100/200] Training loss: 0.01631879
[150/200] Training loss: 0.01488298
[200/200] Training loss: 0.01376853
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8107.004132230351 ----------
[1/200] Training loss: 0.12460983
[2/200] Training loss: 0.05526888
[3/200] Training loss: 0.05024752
[4/200] Training loss: 0.04357348
[5/200] Training loss: 0.03977693
[6/200] Training loss: 0.03619827
[7/200] Training loss: 0.03348274
[8/200] Training loss: 0.03252254
[9/200] Training loss: 0.02952858
[10/200] Training loss: 0.02781885
[50/200] Training loss: 0.01655165
[100/200] Training loss: 0.01416286
[150/200] Training loss: 0.01209367
[200/200] Training loss: 0.01092939
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 19196.501347901914 ----------
[1/200] Training loss: 0.08755923
[2/200] Training loss: 0.04781464
[3/200] Training loss: 0.04359455
[4/200] Training loss: 0.03917819
[5/200] Training loss: 0.03688795
[6/200] Training loss: 0.03405543
[7/200] Training loss: 0.03075448
[8/200] Training loss: 0.03024216
[9/200] Training loss: 0.03038940
[10/200] Training loss: 0.03002316
[50/200] Training loss: 0.01581416
[100/200] Training loss: 0.01309740
[150/200] Training loss: 0.01165925
[200/200] Training loss: 0.01079829
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13022.064352474994 ----------
[1/200] Training loss: 0.17304128
[2/200] Training loss: 0.06353095
[3/200] Training loss: 0.05501595
[4/200] Training loss: 0.05055840
[5/200] Training loss: 0.04594827
[6/200] Training loss: 0.04626175
[7/200] Training loss: 0.04503421
[8/200] Training loss: 0.04147534
[9/200] Training loss: 0.03773909
[10/200] Training loss: 0.03541580
[50/200] Training loss: 0.02041840
[100/200] Training loss: 0.01694296
[150/200] Training loss: 0.01554005
[200/200] Training loss: 0.01475494
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6119629369982093 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8547.30460437675 ----------
[1/200] Training loss: 0.16245553
[2/200] Training loss: 0.06475536
[3/200] Training loss: 0.05336884
[4/200] Training loss: 0.05083570
[5/200] Training loss: 0.04970361
[6/200] Training loss: 0.04726285
[7/200] Training loss: 0.04378769
[8/200] Training loss: 0.04462179
[9/200] Training loss: 0.04267191
[10/200] Training loss: 0.04205235
[50/200] Training loss: 0.02042131
[100/200] Training loss: 0.01593517
[150/200] Training loss: 0.01481105
[200/200] Training loss: 0.01318205
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6119629369982093 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4940.292703878992 ----------
[1/100] Training loss: 0.15816537
[2/100] Training loss: 0.06443509
[3/100] Training loss: 0.05495003
[4/100] Training loss: 0.04673725
[5/100] Training loss: 0.04096715
[6/100] Training loss: 0.03738824
[7/100] Training loss: 0.03619124
[8/100] Training loss: 0.03326391
[9/100] Training loss: 0.03114202
[10/100] Training loss: 0.02855420
[50/100] Training loss: 0.01757729
[100/100] Training loss: 0.01564673
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15467.393316263733 ----------
[1/100] Training loss: 0.08716084
[2/100] Training loss: 0.04757130
[3/100] Training loss: 0.04020591
[4/100] Training loss: 0.03554358
[5/100] Training loss: 0.02929120
[6/100] Training loss: 0.02678621
[7/100] Training loss: 0.02581564
[8/100] Training loss: 0.02304986
[9/100] Training loss: 0.02234746
[10/100] Training loss: 0.02182292
[50/100] Training loss: 0.01502548
[100/100] Training loss: 0.01265965
---batch_size---: 4 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.6119629369982093 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13931.182290100149 ----------
[1/200] Training loss: 0.13598144
[2/200] Training loss: 0.05349769
[3/200] Training loss: 0.05307178
[4/200] Training loss: 0.04831443
[5/200] Training loss: 0.04514449
[6/200] Training loss: 0.04472493
[7/200] Training loss: 0.03931407
[8/200] Training loss: 0.03820366
[9/200] Training loss: 0.03533622
[10/200] Training loss: 0.03285498
[50/200] Training loss: 0.02086200
[100/200] Training loss: 0.01646469
[150/200] Training loss: 0.01381597
[200/200] Training loss: 0.01277904
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9801.298689459474 ----------
[1/200] Training loss: 0.15317277
[2/200] Training loss: 0.06000895
[3/200] Training loss: 0.05577889
[4/200] Training loss: 0.05073790
[5/200] Training loss: 0.05259475
[6/200] Training loss: 0.05000013
[7/200] Training loss: 0.04732588
[8/200] Training loss: 0.04538560
[9/200] Training loss: 0.04106003
[10/200] Training loss: 0.04045487
[50/200] Training loss: 0.01915502
[100/200] Training loss: 0.01675105
[150/200] Training loss: 0.01436297
[200/200] Training loss: 0.01375041
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7272.574784765022 ----------
[1/100] Training loss: 0.17453823
[2/100] Training loss: 0.06171245
[3/100] Training loss: 0.05005900
[4/100] Training loss: 0.04928214
[5/100] Training loss: 0.04352004
[6/100] Training loss: 0.04411436
[7/100] Training loss: 0.03972296
[8/100] Training loss: 0.03674783
[9/100] Training loss: 0.03610523
[10/100] Training loss: 0.03172909
[50/100] Training loss: 0.01922504
[100/100] Training loss: 0.01697537
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7542.356660885243 ----------
[1/100] Training loss: 0.15343593
[2/100] Training loss: 0.05629917
[3/100] Training loss: 0.05351339
[4/100] Training loss: 0.04883182
[5/100] Training loss: 0.04783044
[6/100] Training loss: 0.04325048
[7/100] Training loss: 0.04289962
[8/100] Training loss: 0.03918888
[9/100] Training loss: 0.03824516
[10/100] Training loss: 0.03585183
[50/100] Training loss: 0.01941151
[100/100] Training loss: 0.01599671
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.6119629369982093 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10176.137184609885 ----------
[1/200] Training loss: 0.12957801
[2/200] Training loss: 0.05521767
[3/200] Training loss: 0.04925130
[4/200] Training loss: 0.04609400
[5/200] Training loss: 0.04562983
[6/200] Training loss: 0.04082160
[7/200] Training loss: 0.03905781
[8/200] Training loss: 0.03752568
[9/200] Training loss: 0.03449930
[10/200] Training loss: 0.03252754
[50/200] Training loss: 0.01807095
[100/200] Training loss: 0.01458473
[150/200] Training loss: 0.01289518
[200/200] Training loss: 0.01194211
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4236.968727758089 ----------
[1/200] Training loss: 0.14775561
[2/200] Training loss: 0.05495458
[3/200] Training loss: 0.05227311
[4/200] Training loss: 0.04514061
[5/200] Training loss: 0.04341683
[6/200] Training loss: 0.04046425
[7/200] Training loss: 0.03787288
[8/200] Training loss: 0.03664206
[9/200] Training loss: 0.03358340
[10/200] Training loss: 0.03356247
[50/200] Training loss: 0.01864456
[100/200] Training loss: 0.01677346
[150/200] Training loss: 0.01522136
[200/200] Training loss: 0.01435682
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10671.572705088975 ----------
[1/100] Training loss: 0.12885518
[2/100] Training loss: 0.05583500
[3/100] Training loss: 0.05449046
[4/100] Training loss: 0.05213115
[5/100] Training loss: 0.04982116
[6/100] Training loss: 0.04592418
[7/100] Training loss: 0.04339510
[8/100] Training loss: 0.04277108
[9/100] Training loss: 0.04153501
[10/100] Training loss: 0.03517821
[50/100] Training loss: 0.01656146
[100/100] Training loss: 0.01410654
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11377.246767122528 ----------
[1/200] Training loss: 0.15912040
[2/200] Training loss: 0.05592463
[3/200] Training loss: 0.04978189
[4/200] Training loss: 0.04849792
[5/200] Training loss: 0.04528594
[6/200] Training loss: 0.04296169
[7/200] Training loss: 0.03873465
[8/200] Training loss: 0.03723158
[9/200] Training loss: 0.03648238
[10/200] Training loss: 0.03560280
[50/200] Training loss: 0.01656901
[100/200] Training loss: 0.01435550
[150/200] Training loss: 0.01314216
[200/200] Training loss: 0.01252878
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5762.703532197366 ----------
[1/200] Training loss: 0.17671688
[2/200] Training loss: 0.05793356
[3/200] Training loss: 0.05341194
[4/200] Training loss: 0.04703211
[5/200] Training loss: 0.04687673
[6/200] Training loss: 0.04329030
[7/200] Training loss: 0.04091893
[8/200] Training loss: 0.04046962
[9/200] Training loss: 0.03744164
[10/200] Training loss: 0.03683582
[50/200] Training loss: 0.01873307
[100/200] Training loss: 0.01337298
[150/200] Training loss: 0.01269205
[200/200] Training loss: 0.01189874
---batch_size---: 16 ---n_steps---: 2 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6845.685648640318 ----------
[1/200] Training loss: 0.16888775
[2/200] Training loss: 0.06249109
[3/200] Training loss: 0.04953024
[4/200] Training loss: 0.04618953
[5/200] Training loss: 0.04447094
[6/200] Training loss: 0.04128103
[7/200] Training loss: 0.03910272
[8/200] Training loss: 0.03702430
[9/200] Training loss: 0.03672181
[10/200] Training loss: 0.03447400
[50/200] Training loss: 0.01813746
[100/200] Training loss: 0.01617961
[150/200] Training loss: 0.01391766
[200/200] Training loss: 0.01348098
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12593.543107481706 ----------
[1/200] Training loss: 0.12194622
[2/200] Training loss: 0.05292546
[3/200] Training loss: 0.04659930
[4/200] Training loss: 0.04322013
[5/200] Training loss: 0.04124677
[6/200] Training loss: 0.03688178
[7/200] Training loss: 0.03581861
[8/200] Training loss: 0.03129935
[9/200] Training loss: 0.03261591
[10/200] Training loss: 0.02907546
[50/200] Training loss: 0.01653846
[100/200] Training loss: 0.01355333
[150/200] Training loss: 0.01194022
[200/200] Training loss: 0.01080829
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14291.135154353555 ----------
[1/100] Training loss: 0.16251022
[2/100] Training loss: 0.05913278
[3/100] Training loss: 0.05008589
[4/100] Training loss: 0.04710639
[5/100] Training loss: 0.04248499
[6/100] Training loss: 0.04267794
[7/100] Training loss: 0.03915188
[8/100] Training loss: 0.03487195
[9/100] Training loss: 0.03705481
[10/100] Training loss: 0.03219902
[50/100] Training loss: 0.01702562
[100/100] Training loss: 0.01425287
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5908.555153334866 ----------
[1/200] Training loss: 0.14158840
[2/200] Training loss: 0.05598224
[3/200] Training loss: 0.05052380
[4/200] Training loss: 0.04765282
[5/200] Training loss: 0.04582733
[6/200] Training loss: 0.04447324
[7/200] Training loss: 0.03914228
[8/200] Training loss: 0.03911805
[9/200] Training loss: 0.03940104
[10/200] Training loss: 0.03481779
[50/200] Training loss: 0.01884130
[100/200] Training loss: 0.01523017
[150/200] Training loss: 0.01404821
[200/200] Training loss: 0.01320316
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11444.0279622168 ----------
[1/200] Training loss: 0.11427870
[2/200] Training loss: 0.05328784
[3/200] Training loss: 0.04975349
[4/200] Training loss: 0.04627911
[5/200] Training loss: 0.04243880
[6/200] Training loss: 0.03866839
[7/200] Training loss: 0.03575477
[8/200] Training loss: 0.03114925
[9/200] Training loss: 0.02912200
[10/200] Training loss: 0.02659890
[50/200] Training loss: 0.01750941
[100/200] Training loss: 0.01530280
[150/200] Training loss: 0.01375761
[200/200] Training loss: 0.01243709
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 42705.66351199803 ----------
[1/200] Training loss: 0.15978263
[2/200] Training loss: 0.05625516
[3/200] Training loss: 0.04817873
[4/200] Training loss: 0.04542361
[5/200] Training loss: 0.04620132
[6/200] Training loss: 0.03935269
[7/200] Training loss: 0.03991189
[8/200] Training loss: 0.03634837
[9/200] Training loss: 0.03379930
[10/200] Training loss: 0.03335942
[50/200] Training loss: 0.01839630
[100/200] Training loss: 0.01560925
[150/200] Training loss: 0.01441499
[200/200] Training loss: 0.01307455
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5758.589931571791 ----------
[1/100] Training loss: 0.15072171
[2/100] Training loss: 0.05625187
[3/100] Training loss: 0.04957274
[4/100] Training loss: 0.04997898
[5/100] Training loss: 0.04590560
[6/100] Training loss: 0.04066714
[7/100] Training loss: 0.04301294
[8/100] Training loss: 0.03840778
[9/100] Training loss: 0.03627448
[10/100] Training loss: 0.03366900
[50/100] Training loss: 0.01944224
[100/100] Training loss: 0.01724157
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12246.348680321003 ----------
[1/100] Training loss: 0.17783245
[2/100] Training loss: 0.05982509
[3/100] Training loss: 0.05454427
[4/100] Training loss: 0.05468077
[5/100] Training loss: 0.04942560
[6/100] Training loss: 0.04523351
[7/100] Training loss: 0.04112926
[8/100] Training loss: 0.04008339
[9/100] Training loss: 0.03716292
[10/100] Training loss: 0.03602418
[50/100] Training loss: 0.01979735
[100/100] Training loss: 0.01529013
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5508.376530339951 ----------
[1/200] Training loss: 0.17154448
[2/200] Training loss: 0.05937328
[3/200] Training loss: 0.05385885
[4/200] Training loss: 0.05295188
[5/200] Training loss: 0.05009768
[6/200] Training loss: 0.04407193
[7/200] Training loss: 0.04184255
[8/200] Training loss: 0.03923025
[9/200] Training loss: 0.03730097
[10/200] Training loss: 0.03583462
[50/200] Training loss: 0.01970760
[100/200] Training loss: 0.01764953
[150/200] Training loss: 0.01633526
[200/200] Training loss: 0.01457937
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12824.380530848264 ----------
[1/150] Training loss: 0.13378476
[2/150] Training loss: 0.05854643
[3/150] Training loss: 0.05130706
[4/150] Training loss: 0.04593109
[5/150] Training loss: 0.04481173
[6/150] Training loss: 0.03694344
[7/150] Training loss: 0.03553086
[8/150] Training loss: 0.03435136
[9/150] Training loss: 0.03262300
[10/150] Training loss: 0.03193152
[50/150] Training loss: 0.01843433
[100/150] Training loss: 0.01478395
[150/150] Training loss: 0.01385607
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 150
---hidden_dim---: 48 ---dropout---: 0.6119629369982093 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7414.3112964050815 ----------
[1/100] Training loss: 0.14499760
[2/100] Training loss: 0.05434404
[3/100] Training loss: 0.05075120
[4/100] Training loss: 0.04297971
[5/100] Training loss: 0.04056597
[6/100] Training loss: 0.04042657
[7/100] Training loss: 0.03775380
[8/100] Training loss: 0.03457107
[9/100] Training loss: 0.03408715
[10/100] Training loss: 0.03163879
[50/100] Training loss: 0.01986491
[100/100] Training loss: 0.01539801
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6204.909024312927 ----------
[1/100] Training loss: 0.17483228
[2/100] Training loss: 0.06392692
[3/100] Training loss: 0.05399813
[4/100] Training loss: 0.05119739
[5/100] Training loss: 0.05018334
[6/100] Training loss: 0.04727622
[7/100] Training loss: 0.04400932
[8/100] Training loss: 0.04195040
[9/100] Training loss: 0.04282404
[10/100] Training loss: 0.03685748
[50/100] Training loss: 0.01897432
[100/100] Training loss: 0.01570439
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8308.072700692983 ----------
[1/200] Training loss: 0.16839674
[2/200] Training loss: 0.06529172
[3/200] Training loss: 0.05461486
[4/200] Training loss: 0.05308578
[5/200] Training loss: 0.04890445
[6/200] Training loss: 0.05015126
[7/200] Training loss: 0.04758743
[8/200] Training loss: 0.04400025
[9/200] Training loss: 0.04428038
[10/200] Training loss: 0.04204857
[50/200] Training loss: 0.01910786
[100/200] Training loss: 0.01566655
[150/200] Training loss: 0.01403332
[200/200] Training loss: 0.01327638
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6119629369982093 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 22182.798380727352 ----------
[1/200] Training loss: 0.14366612
[2/200] Training loss: 0.05712744
[3/200] Training loss: 0.05075862
[4/200] Training loss: 0.04937974
[5/200] Training loss: 0.04464066
[6/200] Training loss: 0.03988562
[7/200] Training loss: 0.04053809
[8/200] Training loss: 0.03509339
[9/200] Training loss: 0.03167747
[10/200] Training loss: 0.02999669
[50/200] Training loss: 0.01796275
[100/200] Training loss: 0.01567901
[150/200] Training loss: 0.01365209
[200/200] Training loss: 0.01279423
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6638.6640222261585 ----------
[1/200] Training loss: 0.15488843
[2/200] Training loss: 0.06082536
[3/200] Training loss: 0.05129599
[4/200] Training loss: 0.05016740
[5/200] Training loss: 0.04677709
[6/200] Training loss: 0.04374684
[7/200] Training loss: 0.04287044
[8/200] Training loss: 0.03904960
[9/200] Training loss: 0.03709083
[10/200] Training loss: 0.03407962
[50/200] Training loss: 0.01817515
[100/200] Training loss: 0.01658861
[150/200] Training loss: 0.01478049
[200/200] Training loss: 0.01424245
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11626.820717633862 ----------
[1/200] Training loss: 0.16182463
[2/200] Training loss: 0.06553083
[3/200] Training loss: 0.05638265
[4/200] Training loss: 0.05397960
[5/200] Training loss: 0.05180451
[6/200] Training loss: 0.05011741
[7/200] Training loss: 0.04744062
[8/200] Training loss: 0.04665562
[9/200] Training loss: 0.04404418
[10/200] Training loss: 0.04336198
[50/200] Training loss: 0.01819036
[100/200] Training loss: 0.01539881
[150/200] Training loss: 0.01423788
[200/200] Training loss: 0.01336544
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12134.431671899594 ----------
[1/200] Training loss: 0.11886678
[2/200] Training loss: 0.05811996
[3/200] Training loss: 0.05254668
[4/200] Training loss: 0.04812803
[5/200] Training loss: 0.04474059
[6/200] Training loss: 0.04129040
[7/200] Training loss: 0.03911909
[8/200] Training loss: 0.03677259
[9/200] Training loss: 0.03485601
[10/200] Training loss: 0.03275116
[50/200] Training loss: 0.01704186
[100/200] Training loss: 0.01462675
[150/200] Training loss: 0.01318791
[200/200] Training loss: 0.01214082
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19332.24787757492 ----------
[1/200] Training loss: 0.15661073
[2/200] Training loss: 0.05718793
[3/200] Training loss: 0.04956634
[4/200] Training loss: 0.04696485
[5/200] Training loss: 0.04030965
[6/200] Training loss: 0.03561442
[7/200] Training loss: 0.03448257
[8/200] Training loss: 0.03455306
[9/200] Training loss: 0.03307594
[10/200] Training loss: 0.03037000
[50/200] Training loss: 0.01926669
[100/200] Training loss: 0.01741919
[150/200] Training loss: 0.01610624
[200/200] Training loss: 0.01479904
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12954.678537115462 ----------
[1/200] Training loss: 0.14737143
[2/200] Training loss: 0.05814748
[3/200] Training loss: 0.04821929
[4/200] Training loss: 0.04760354
[5/200] Training loss: 0.04778157
[6/200] Training loss: 0.04314301
[7/200] Training loss: 0.03991790
[8/200] Training loss: 0.03566845
[9/200] Training loss: 0.03554329
[10/200] Training loss: 0.03481624
[50/200] Training loss: 0.01934188
[100/200] Training loss: 0.01566371
[150/200] Training loss: 0.01437236
[200/200] Training loss: 0.01226139
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13410.804599277404 ----------
[1/200] Training loss: 0.17244911
[2/200] Training loss: 0.05552052
[3/200] Training loss: 0.05327159
[4/200] Training loss: 0.04499886
[5/200] Training loss: 0.04438760
[6/200] Training loss: 0.04216522
[7/200] Training loss: 0.04058442
[8/200] Training loss: 0.03808287
[9/200] Training loss: 0.03659826
[10/200] Training loss: 0.03498729
[50/200] Training loss: 0.01720430
[100/200] Training loss: 0.01612945
[150/200] Training loss: 0.01464805
[200/200] Training loss: 0.01406418
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7592.471270936756 ----------
[1/100] Training loss: 0.15638637
[2/100] Training loss: 0.05575374
[3/100] Training loss: 0.04828988
[4/100] Training loss: 0.04845347
[5/100] Training loss: 0.04527058
[6/100] Training loss: 0.04426695
[7/100] Training loss: 0.03847640
[8/100] Training loss: 0.03658305
[9/100] Training loss: 0.03604556
[10/100] Training loss: 0.03423479
[50/100] Training loss: 0.01841487
[100/100] Training loss: 0.01523292
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6724.638280234856 ----------
[1/100] Training loss: 0.15464852
[2/100] Training loss: 0.06428075
[3/100] Training loss: 0.05513510
[4/100] Training loss: 0.05424139
[5/100] Training loss: 0.05243464
[6/100] Training loss: 0.05017083
[7/100] Training loss: 0.04763437
[8/100] Training loss: 0.04716912
[9/100] Training loss: 0.04375664
[10/100] Training loss: 0.04297888
[50/100] Training loss: 0.01856496
[100/100] Training loss: 0.01700864
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14162.032057582697 ----------
[1/200] Training loss: 0.14770190
[2/200] Training loss: 0.05902646
[3/200] Training loss: 0.05222600
[4/200] Training loss: 0.04773577
[5/200] Training loss: 0.04473380
[6/200] Training loss: 0.04434852
[7/200] Training loss: 0.04139799
[8/200] Training loss: 0.03928956
[9/200] Training loss: 0.03728865
[10/200] Training loss: 0.03691880
[50/200] Training loss: 0.01886173
[100/200] Training loss: 0.01347496
[150/200] Training loss: 0.01224189
[200/200] Training loss: 0.01117281
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6774.095363958201 ----------
[1/100] Training loss: 0.07962061
[2/100] Training loss: 0.05315829
[3/100] Training loss: 0.04782588
[4/100] Training loss: 0.04028676
[5/100] Training loss: 0.03061036
[6/100] Training loss: 0.02543627
[7/100] Training loss: 0.02399805
[8/100] Training loss: 0.02285395
[9/100] Training loss: 0.02214977
[10/100] Training loss: 0.02208981
[50/100] Training loss: 0.01649271
[100/100] Training loss: 0.01222006
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 18072.07348369301 ----------
[1/200] Training loss: 0.16995641
[2/200] Training loss: 0.05999988
[3/200] Training loss: 0.05400321
[4/200] Training loss: 0.04942934
[5/200] Training loss: 0.04686391
[6/200] Training loss: 0.04218811
[7/200] Training loss: 0.03855856
[8/200] Training loss: 0.03554667
[9/200] Training loss: 0.03496881
[10/200] Training loss: 0.03378678
[50/200] Training loss: 0.01797960
[100/200] Training loss: 0.01615214
[150/200] Training loss: 0.01521272
[200/200] Training loss: 0.01416511
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14622.701255240087 ----------
[1/100] Training loss: 0.15785054
[2/100] Training loss: 0.05581031
[3/100] Training loss: 0.05339713
[4/100] Training loss: 0.05087172
[5/100] Training loss: 0.04456247
[6/100] Training loss: 0.04342251
[7/100] Training loss: 0.04199067
[8/100] Training loss: 0.04142981
[9/100] Training loss: 0.03789820
[10/100] Training loss: 0.03775223
[50/100] Training loss: 0.02266153
[100/100] Training loss: 0.01657416
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16002.825250561227 ----------
[1/200] Training loss: 0.15785374
[2/200] Training loss: 0.05872383
[3/200] Training loss: 0.05696084
[4/200] Training loss: 0.05127629
[5/200] Training loss: 0.04805455
[6/200] Training loss: 0.04628578
[7/200] Training loss: 0.04111118
[8/200] Training loss: 0.03955714
[9/200] Training loss: 0.03732404
[10/200] Training loss: 0.03459137
[50/200] Training loss: 0.01767407
[100/200] Training loss: 0.01552258
[150/200] Training loss: 0.01410011
[200/200] Training loss: 0.01316877
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8400.250472456164 ----------
[1/200] Training loss: 0.12941853
[2/200] Training loss: 0.05733027
[3/200] Training loss: 0.05039034
[4/200] Training loss: 0.04553594
[5/200] Training loss: 0.04536725
[6/200] Training loss: 0.04246076
[7/200] Training loss: 0.04188139
[8/200] Training loss: 0.03620753
[9/200] Training loss: 0.03688917
[10/200] Training loss: 0.03711357
[50/200] Training loss: 0.01909658
[100/200] Training loss: 0.01601099
[150/200] Training loss: 0.01424184
[200/200] Training loss: 0.01358805
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7148.657216568718 ----------
[1/200] Training loss: 0.15829243
[2/200] Training loss: 0.05861070
[3/200] Training loss: 0.05047504
[4/200] Training loss: 0.04617375
[5/200] Training loss: 0.04515253
[6/200] Training loss: 0.04098804
[7/200] Training loss: 0.03917199
[8/200] Training loss: 0.03507846
[9/200] Training loss: 0.03533098
[10/200] Training loss: 0.03680583
[50/200] Training loss: 0.02128355
[100/200] Training loss: 0.01804623
[150/200] Training loss: 0.01626295
[200/200] Training loss: 0.01376328
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10853.351187536502 ----------
[1/200] Training loss: 0.08795329
[2/200] Training loss: 0.00413820
[3/200] Training loss: 0.00390695
[4/200] Training loss: 0.00348886
[5/200] Training loss: 0.00333682
[6/200] Training loss: 0.00290281
[7/200] Training loss: 0.00280168
[8/200] Training loss: 0.00232856
[9/200] Training loss: 0.00201525
[10/200] Training loss: 0.00186412
[50/200] Training loss: 0.00059517
[100/200] Training loss: 0.00042485
[150/200] Training loss: 0.00033594
[200/200] Training loss: 0.00027090
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8102.093309756436 ----------
[1/200] Training loss: 0.15816027
[2/200] Training loss: 0.05674167
[3/200] Training loss: 0.05254684
[4/200] Training loss: 0.04692466
[5/200] Training loss: 0.04406763
[6/200] Training loss: 0.03937524
[7/200] Training loss: 0.03809790
[8/200] Training loss: 0.03647509
[9/200] Training loss: 0.03402699
[10/200] Training loss: 0.03173373
[50/200] Training loss: 0.01865235
[100/200] Training loss: 0.01721045
[150/200] Training loss: 0.01608969
[200/200] Training loss: 0.01495473
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7256.232631331495 ----------
[1/100] Training loss: 0.11365521
[2/100] Training loss: 0.05076439
[3/100] Training loss: 0.04917104
[4/100] Training loss: 0.04168318
[5/100] Training loss: 0.03923886
[6/100] Training loss: 0.03407050
[7/100] Training loss: 0.03118584
[8/100] Training loss: 0.02842857
[9/100] Training loss: 0.02827346
[10/100] Training loss: 0.02547609
[50/100] Training loss: 0.01635244
[100/100] Training loss: 0.01362478
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8451.11116954451 ----------
[1/100] Training loss: 0.14880300
[2/100] Training loss: 0.05493947
[3/100] Training loss: 0.05203260
[4/100] Training loss: 0.04757467
[5/100] Training loss: 0.04327950
[6/100] Training loss: 0.04237395
[7/100] Training loss: 0.03889128
[8/100] Training loss: 0.03603222
[9/100] Training loss: 0.03369419
[10/100] Training loss: 0.03179331
[50/100] Training loss: 0.01615540
[100/100] Training loss: 0.01338045
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16028.064387192859 ----------
[1/100] Training loss: 0.16182661
[2/100] Training loss: 0.06386124
[3/100] Training loss: 0.05623862
[4/100] Training loss: 0.04967833
[5/100] Training loss: 0.05114960
[6/100] Training loss: 0.04678982
[7/100] Training loss: 0.04536339
[8/100] Training loss: 0.04209976
[9/100] Training loss: 0.04230359
[10/100] Training loss: 0.03822548
[50/100] Training loss: 0.01916015
[100/100] Training loss: 0.01667132
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.22046588632636643 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10269.674970513915 ----------
[1/100] Training loss: 0.13855308
[2/100] Training loss: 0.05756231
[3/100] Training loss: 0.05328068
[4/100] Training loss: 0.04867134
[5/100] Training loss: 0.04649246
[6/100] Training loss: 0.04202437
[7/100] Training loss: 0.03928455
[8/100] Training loss: 0.03746678
[9/100] Training loss: 0.03470241
[10/100] Training loss: 0.03387281
[50/100] Training loss: 0.01866782
[100/100] Training loss: 0.01534644
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 18498.89769688994 ----------
[1/200] Training loss: 0.15119567
[2/200] Training loss: 0.05987790
[3/200] Training loss: 0.05160992
[4/200] Training loss: 0.04707651
[5/200] Training loss: 0.04772045
[6/200] Training loss: 0.04560775
[7/200] Training loss: 0.04123312
[8/200] Training loss: 0.03723494
[9/200] Training loss: 0.03753276
[10/200] Training loss: 0.03329809
[50/200] Training loss: 0.01813665
[100/200] Training loss: 0.01565031
[150/200] Training loss: 0.01441107
[200/200] Training loss: 0.01355843
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12969.493436522493 ----------
[1/200] Training loss: 0.11812603
[2/200] Training loss: 0.05198154
[3/200] Training loss: 0.04832133
[4/200] Training loss: 0.04327178
[5/200] Training loss: 0.04243076
[6/200] Training loss: 0.03815401
[7/200] Training loss: 0.03563687
[8/200] Training loss: 0.03432572
[9/200] Training loss: 0.03262284
[10/200] Training loss: 0.03241726
[50/200] Training loss: 0.01719461
[100/200] Training loss: 0.01513915
[150/200] Training loss: 0.01374695
[200/200] Training loss: 0.01294097
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8864.571732464012 ----------
[1/150] Training loss: 0.14682626
[2/150] Training loss: 0.05480933
[3/150] Training loss: 0.05044634
[4/150] Training loss: 0.04669484
[5/150] Training loss: 0.04391707
[6/150] Training loss: 0.03921071
[7/150] Training loss: 0.03869156
[8/150] Training loss: 0.03508057
[9/150] Training loss: 0.03505144
[10/150] Training loss: 0.03113671
[50/150] Training loss: 0.01878525
[100/150] Training loss: 0.01591527
[150/150] Training loss: 0.01449790
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 150
---hidden_dim---: 48 ---dropout---: 0.6119629369982093 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7018.206323555898 ----------
[1/200] Training loss: 0.19837465
[2/200] Training loss: 0.06539829
[3/200] Training loss: 0.05807537
[4/200] Training loss: 0.05239340
[5/200] Training loss: 0.05320220
[6/200] Training loss: 0.05115186
[7/200] Training loss: 0.04908721
[8/200] Training loss: 0.04520870
[9/200] Training loss: 0.04493903
[10/200] Training loss: 0.04538115
[50/200] Training loss: 0.01837024
[100/200] Training loss: 0.01610040
[150/200] Training loss: 0.01466227
[200/200] Training loss: 0.01334799
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7614.351449729648 ----------
[1/100] Training loss: 0.14915577
[2/100] Training loss: 0.05815163
[3/100] Training loss: 0.05084770
[4/100] Training loss: 0.04345619
[5/100] Training loss: 0.04336865
[6/100] Training loss: 0.03730191
[7/100] Training loss: 0.03519579
[8/100] Training loss: 0.03048071
[9/100] Training loss: 0.02956416
[10/100] Training loss: 0.02645365
[50/100] Training loss: 0.01740055
[100/100] Training loss: 0.01573280
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14820.261806054574 ----------
[1/100] Training loss: 0.13068228
[2/100] Training loss: 0.05770529
[3/100] Training loss: 0.05222410
[4/100] Training loss: 0.04848434
[5/100] Training loss: 0.04638598
[6/100] Training loss: 0.03840094
[7/100] Training loss: 0.03265063
[8/100] Training loss: 0.03091304
[9/100] Training loss: 0.02866081
[10/100] Training loss: 0.02710442
[50/100] Training loss: 0.01767036
[100/100] Training loss: 0.01502212
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9598.586979342324 ----------
[1/200] Training loss: 0.14516118
[2/200] Training loss: 0.05580651
[3/200] Training loss: 0.05093000
[4/200] Training loss: 0.04893676
[5/200] Training loss: 0.04751099
[6/200] Training loss: 0.04304162
[7/200] Training loss: 0.04058794
[8/200] Training loss: 0.03988110
[9/200] Training loss: 0.03790311
[10/200] Training loss: 0.03677208
[50/200] Training loss: 0.01681966
[100/200] Training loss: 0.01506743
[150/200] Training loss: 0.01343395
[200/200] Training loss: 0.01227883
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6630.1185509762945 ----------
[1/200] Training loss: 0.13690778
[2/200] Training loss: 0.05577797
[3/200] Training loss: 0.04646875
[4/200] Training loss: 0.04243236
[5/200] Training loss: 0.04413614
[6/200] Training loss: 0.03961542
[7/200] Training loss: 0.03588824
[8/200] Training loss: 0.03728823
[9/200] Training loss: 0.03312525
[10/200] Training loss: 0.03293114
[50/200] Training loss: 0.01863361
[100/200] Training loss: 0.01451127
[150/200] Training loss: 0.01360703
[200/200] Training loss: 0.01219997
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5631.838420977648 ----------
[1/100] Training loss: 0.14279101
[2/100] Training loss: 0.05768014
[3/100] Training loss: 0.05094969
[4/100] Training loss: 0.04270035
[5/100] Training loss: 0.04159027
[6/100] Training loss: 0.03565369
[7/100] Training loss: 0.03095285
[8/100] Training loss: 0.03012194
[9/100] Training loss: 0.03008729
[10/100] Training loss: 0.02868100
[50/100] Training loss: 0.01822736
[100/100] Training loss: 0.01621212
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13189.425461330755 ----------
[1/100] Training loss: 0.15752344
[2/100] Training loss: 0.05976264
[3/100] Training loss: 0.05630575
[4/100] Training loss: 0.05016192
[5/100] Training loss: 0.05026928
[6/100] Training loss: 0.04925218
[7/100] Training loss: 0.04819065
[8/100] Training loss: 0.04454581
[9/100] Training loss: 0.04318815
[10/100] Training loss: 0.04344429
[50/100] Training loss: 0.01879312
[100/100] Training loss: 0.01749126
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10518.211254771411 ----------
[1/200] Training loss: 0.16058087
[2/200] Training loss: 0.05777422
[3/200] Training loss: 0.05261938
[4/200] Training loss: 0.04878665
[5/200] Training loss: 0.04424569
[6/200] Training loss: 0.04277616
[7/200] Training loss: 0.04130259
[8/200] Training loss: 0.03792319
[9/200] Training loss: 0.03559160
[10/200] Training loss: 0.03778357
[50/200] Training loss: 0.01684686
[100/200] Training loss: 0.01425901
[150/200] Training loss: 0.01251610
[200/200] Training loss: 0.01147677
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5476.558043150825 ----------
[1/200] Training loss: 0.16773665
[2/200] Training loss: 0.05994421
[3/200] Training loss: 0.05638410
[4/200] Training loss: 0.05380237
[5/200] Training loss: 0.05258340
[6/200] Training loss: 0.04932948
[7/200] Training loss: 0.04689025
[8/200] Training loss: 0.04723030
[9/200] Training loss: 0.04136199
[10/200] Training loss: 0.03811271
[50/200] Training loss: 0.01790039
[100/200] Training loss: 0.01577198
[150/200] Training loss: 0.01437639
[200/200] Training loss: 0.01287408
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11920.157045945325 ----------
[1/100] Training loss: 0.14977488
[2/100] Training loss: 0.05645760
[3/100] Training loss: 0.05261581
[4/100] Training loss: 0.04938196
[5/100] Training loss: 0.05062581
[6/100] Training loss: 0.04572001
[7/100] Training loss: 0.04418734
[8/100] Training loss: 0.04207728
[9/100] Training loss: 0.04066715
[10/100] Training loss: 0.03780524
[50/100] Training loss: 0.01884714
[100/100] Training loss: 0.01638810
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12070.406124070556 ----------
[1/150] Training loss: 0.14894547
[2/150] Training loss: 0.05344053
[3/150] Training loss: 0.04961679
[4/150] Training loss: 0.04399075
[5/150] Training loss: 0.04436976
[6/150] Training loss: 0.04200009
[7/150] Training loss: 0.03846189
[8/150] Training loss: 0.03755551
[9/150] Training loss: 0.03742931
[10/150] Training loss: 0.03348548
[50/150] Training loss: 0.01771161
[100/150] Training loss: 0.01435581
[150/150] Training loss: 0.01278553
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 150
---hidden_dim---: 48 ---dropout---: 0.6119629369982093 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10668.139106704599 ----------
[1/200] Training loss: 0.18513232
[2/200] Training loss: 0.05913103
[3/200] Training loss: 0.05246029
[4/200] Training loss: 0.05308528
[5/200] Training loss: 0.04911864
[6/200] Training loss: 0.04592269
[7/200] Training loss: 0.04405556
[8/200] Training loss: 0.04119000
[9/200] Training loss: 0.03736988
[10/200] Training loss: 0.03425679
[50/200] Training loss: 0.01802158
[100/200] Training loss: 0.01518929
[150/200] Training loss: 0.01437642
[200/200] Training loss: 0.01334831
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6256.211313566702 ----------
[1/200] Training loss: 0.17386198
[2/200] Training loss: 0.05248703
[3/200] Training loss: 0.05248793
[4/200] Training loss: 0.04735144
[5/200] Training loss: 0.04251534
[6/200] Training loss: 0.04210492
[7/200] Training loss: 0.04013563
[8/200] Training loss: 0.03792086
[9/200] Training loss: 0.03808552
[10/200] Training loss: 0.03438071
[50/200] Training loss: 0.01774142
[100/200] Training loss: 0.01498885
[150/200] Training loss: 0.01411656
[200/200] Training loss: 0.01221887
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7455.652620663063 ----------
[1/200] Training loss: 0.15193164
[2/200] Training loss: 0.06032461
[3/200] Training loss: 0.05342719
[4/200] Training loss: 0.05309117
[5/200] Training loss: 0.04687481
[6/200] Training loss: 0.04670746
[7/200] Training loss: 0.04247796
[8/200] Training loss: 0.04131287
[9/200] Training loss: 0.03653146
[10/200] Training loss: 0.03640174
[50/200] Training loss: 0.01764245
[100/200] Training loss: 0.01449419
[150/200] Training loss: 0.01327426
[200/200] Training loss: 0.01204751
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15430.722082909795 ----------
[1/100] Training loss: 0.16572081
[2/100] Training loss: 0.05909293
[3/100] Training loss: 0.05118438
[4/100] Training loss: 0.04645866
[5/100] Training loss: 0.04315829
[6/100] Training loss: 0.04313668
[7/100] Training loss: 0.03981203
[8/100] Training loss: 0.03827063
[9/100] Training loss: 0.03526033
[10/100] Training loss: 0.03434671
[50/100] Training loss: 0.01939069
[100/100] Training loss: 0.01593524
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11536.82764021375 ----------
[1/200] Training loss: 0.15582360
[2/200] Training loss: 0.06037044
[3/200] Training loss: 0.05352580
[4/200] Training loss: 0.05129223
[5/200] Training loss: 0.04814864
[6/200] Training loss: 0.04594697
[7/200] Training loss: 0.04481079
[8/200] Training loss: 0.04317389
[9/200] Training loss: 0.03876801
[10/200] Training loss: 0.03823179
[50/200] Training loss: 0.01810331
[100/200] Training loss: 0.01598828
[150/200] Training loss: 0.01530359
[200/200] Training loss: 0.01341131
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6440.598729931869 ----------
[1/150] Training loss: 0.15107078
[2/150] Training loss: 0.05744014
[3/150] Training loss: 0.04961359
[4/150] Training loss: 0.04729354
[5/150] Training loss: 0.04527411
[6/150] Training loss: 0.04321184
[7/150] Training loss: 0.03910456
[8/150] Training loss: 0.04002553
[9/150] Training loss: 0.03776532
[10/150] Training loss: 0.03397105
[50/150] Training loss: 0.01736663
[100/150] Training loss: 0.01466937
[150/150] Training loss: 0.01255499
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 150
---hidden_dim---: 80 ---dropout---: 0.6119629369982093 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 3737.7964096510127 ----------
[1/200] Training loss: 0.15087627
[2/200] Training loss: 0.05900583
[3/200] Training loss: 0.05095423
[4/200] Training loss: 0.04866736
[5/200] Training loss: 0.04399679
[6/200] Training loss: 0.03999039
[7/200] Training loss: 0.03982053
[8/200] Training loss: 0.03767848
[9/200] Training loss: 0.03660145
[10/200] Training loss: 0.03490445
[50/200] Training loss: 0.01825412
[100/200] Training loss: 0.01564292
[150/200] Training loss: 0.01424995
[200/200] Training loss: 0.01304068
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10390.681979543018 ----------
[1/100] Training loss: 0.15790727
[2/100] Training loss: 0.06043533
[3/100] Training loss: 0.05280100
[4/100] Training loss: 0.04673550
[5/100] Training loss: 0.04503367
[6/100] Training loss: 0.04326794
[7/100] Training loss: 0.04007529
[8/100] Training loss: 0.03859320
[9/100] Training loss: 0.03560447
[10/100] Training loss: 0.03259007
[50/100] Training loss: 0.01702605
[100/100] Training loss: 0.01494065
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8105.491225089322 ----------
[1/200] Training loss: 0.11216359
[2/200] Training loss: 0.04940544
[3/200] Training loss: 0.04270674
[4/200] Training loss: 0.03945660
[5/200] Training loss: 0.03617925
[6/200] Training loss: 0.03141774
[7/200] Training loss: 0.03063761
[8/200] Training loss: 0.02871448
[9/200] Training loss: 0.02583510
[10/200] Training loss: 0.02353688
[50/200] Training loss: 0.01549678
[100/200] Training loss: 0.01395802
[150/200] Training loss: 0.01244850
[200/200] Training loss: 0.01145102
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5159.161753618509 ----------
[1/200] Training loss: 0.08234787
[2/200] Training loss: 0.00545392
[3/200] Training loss: 0.00489846
[4/200] Training loss: 0.00441789
[5/200] Training loss: 0.00390998
[6/200] Training loss: 0.00368450
[7/200] Training loss: 0.00325074
[8/200] Training loss: 0.00279610
[9/200] Training loss: 0.00241956
[10/200] Training loss: 0.00228143
[50/200] Training loss: 0.00044525
[100/200] Training loss: 0.00036130
[150/200] Training loss: 0.00028618
[200/200] Training loss: 0.00026397
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14967.647243304473 ----------
[1/200] Training loss: 0.15780145
[2/200] Training loss: 0.05369563
[3/200] Training loss: 0.04744057
[4/200] Training loss: 0.04618394
[5/200] Training loss: 0.04228732
[6/200] Training loss: 0.03983982
[7/200] Training loss: 0.03967107
[8/200] Training loss: 0.03844344
[9/200] Training loss: 0.03735404
[10/200] Training loss: 0.03190606
[50/200] Training loss: 0.01941231
[100/200] Training loss: 0.01680896
[150/200] Training loss: 0.01504756
[200/200] Training loss: 0.01353232
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9758.205162836042 ----------
[1/200] Training loss: 0.17373116
[2/200] Training loss: 0.06201777
[3/200] Training loss: 0.05356204
[4/200] Training loss: 0.05196814
[5/200] Training loss: 0.04839727
[6/200] Training loss: 0.04758509
[7/200] Training loss: 0.04684528
[8/200] Training loss: 0.03993187
[9/200] Training loss: 0.03809643
[10/200] Training loss: 0.04199825
[50/200] Training loss: 0.02103582
[100/200] Training loss: 0.01562210
[150/200] Training loss: 0.01411201
[200/200] Training loss: 0.01324619
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6692.945838717059 ----------
[1/200] Training loss: 0.08804316
[2/200] Training loss: 0.00482421
[3/200] Training loss: 0.00442618
[4/200] Training loss: 0.00396373
[5/200] Training loss: 0.00380708
[6/200] Training loss: 0.00331500
[7/200] Training loss: 0.00279575
[8/200] Training loss: 0.00257667
[9/200] Training loss: 0.00214338
[10/200] Training loss: 0.00184961
[50/200] Training loss: 0.00062679
[100/200] Training loss: 0.00050398
[150/200] Training loss: 0.00040229
[200/200] Training loss: 0.00035286
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13644.28671642457 ----------
[1/200] Training loss: 0.16709209
[2/200] Training loss: 0.05879497
[3/200] Training loss: 0.05460512
[4/200] Training loss: 0.04797108
[5/200] Training loss: 0.04448925
[6/200] Training loss: 0.04276176
[7/200] Training loss: 0.04114144
[8/200] Training loss: 0.03759566
[9/200] Training loss: 0.03572729
[10/200] Training loss: 0.03266401
[50/200] Training loss: 0.01935551
[100/200] Training loss: 0.01682421
[150/200] Training loss: 0.01546462
[200/200] Training loss: 0.01470699
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8137.607756583012 ----------
[1/100] Training loss: 0.18135815
[2/100] Training loss: 0.05886745
[3/100] Training loss: 0.05310909
[4/100] Training loss: 0.05059086
[5/100] Training loss: 0.04642224
[6/100] Training loss: 0.04327715
[7/100] Training loss: 0.04151287
[8/100] Training loss: 0.03910115
[9/100] Training loss: 0.03786318
[10/100] Training loss: 0.03627148
[50/100] Training loss: 0.01795405
[100/100] Training loss: 0.01602899
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11220.245986608315 ----------
[1/200] Training loss: 0.11320100
[2/200] Training loss: 0.05356013
[3/200] Training loss: 0.04744316
[4/200] Training loss: 0.04149600
[5/200] Training loss: 0.03754551
[6/200] Training loss: 0.03343865
[7/200] Training loss: 0.03213786
[8/200] Training loss: 0.02972786
[9/200] Training loss: 0.02870720
[10/200] Training loss: 0.02733548
[50/200] Training loss: 0.01900437
[100/200] Training loss: 0.01720647
[150/200] Training loss: 0.01560245
[200/200] Training loss: 0.01263822
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8299.923855072406 ----------
[1/200] Training loss: 0.15472601
[2/200] Training loss: 0.05638482
[3/200] Training loss: 0.05238479
[4/200] Training loss: 0.04891244
[5/200] Training loss: 0.04653683
[6/200] Training loss: 0.04464574
[7/200] Training loss: 0.04297019
[8/200] Training loss: 0.04129612
[9/200] Training loss: 0.04130884
[10/200] Training loss: 0.03841221
[50/200] Training loss: 0.01979480
[100/200] Training loss: 0.01747070
[150/200] Training loss: 0.01642144
[200/200] Training loss: 0.01541052
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8301.753549702617 ----------
[1/200] Training loss: 0.15411474
[2/200] Training loss: 0.05649290
[3/200] Training loss: 0.05050405
[4/200] Training loss: 0.04592612
[5/200] Training loss: 0.04370244
[6/200] Training loss: 0.04212048
[7/200] Training loss: 0.04160984
[8/200] Training loss: 0.03840949
[9/200] Training loss: 0.03418490
[10/200] Training loss: 0.03230967
[50/200] Training loss: 0.01736802
[100/200] Training loss: 0.01449462
[150/200] Training loss: 0.01326059
[200/200] Training loss: 0.01212904
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6275.519102034508 ----------
[1/200] Training loss: 0.10004552
[2/200] Training loss: 0.04830138
[3/200] Training loss: 0.03984133
[4/200] Training loss: 0.03632760
[5/200] Training loss: 0.03373614
[6/200] Training loss: 0.03073551
[7/200] Training loss: 0.02760264
[8/200] Training loss: 0.02707213
[9/200] Training loss: 0.02492860
[10/200] Training loss: 0.02293304
[50/200] Training loss: 0.01492333
[100/200] Training loss: 0.01224911
[150/200] Training loss: 0.01106052
[200/200] Training loss: 0.01001695
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 3919.5988315132454 ----------
[1/100] Training loss: 0.10921614
[2/100] Training loss: 0.05418242
[3/100] Training loss: 0.05109619
[4/100] Training loss: 0.04711455
[5/100] Training loss: 0.04428285
[6/100] Training loss: 0.04161819
[7/100] Training loss: 0.03750755
[8/100] Training loss: 0.03506882
[9/100] Training loss: 0.03357326
[10/100] Training loss: 0.03017042
[50/100] Training loss: 0.01629112
[100/100] Training loss: 0.01345531
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17356.956415224417 ----------
[1/200] Training loss: 0.15952827
[2/200] Training loss: 0.06172241
[3/200] Training loss: 0.05262986
[4/200] Training loss: 0.04932650
[5/200] Training loss: 0.04626312
[6/200] Training loss: 0.04192580
[7/200] Training loss: 0.04023882
[8/200] Training loss: 0.03727397
[9/200] Training loss: 0.03657193
[10/200] Training loss: 0.03385977
[50/200] Training loss: 0.01911536
[100/200] Training loss: 0.01643743
[150/200] Training loss: 0.01561819
[200/200] Training loss: 0.01411987
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6447.606067371052 ----------
[1/200] Training loss: 0.15044523
[2/200] Training loss: 0.05923184
[3/200] Training loss: 0.05163798
[4/200] Training loss: 0.04608448
[5/200] Training loss: 0.04446740
[6/200] Training loss: 0.04103725
[7/200] Training loss: 0.04027416
[8/200] Training loss: 0.03846753
[9/200] Training loss: 0.03412876
[10/200] Training loss: 0.03500541
[50/200] Training loss: 0.01737634
[100/200] Training loss: 0.01374773
[150/200] Training loss: 0.01326626
[200/200] Training loss: 0.01183979
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13088.721251520334 ----------
[1/200] Training loss: 0.11673266
[2/200] Training loss: 0.05431266
[3/200] Training loss: 0.04627629
[4/200] Training loss: 0.03947509
[5/200] Training loss: 0.03519677
[6/200] Training loss: 0.03047303
[7/200] Training loss: 0.02781130
[8/200] Training loss: 0.02845122
[9/200] Training loss: 0.02562034
[10/200] Training loss: 0.02413660
[50/200] Training loss: 0.01765745
[100/200] Training loss: 0.01521615
[150/200] Training loss: 0.01356455
[200/200] Training loss: 0.01225778
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10216.215737737726 ----------
[1/200] Training loss: 0.16780157
[2/200] Training loss: 0.06060648
[3/200] Training loss: 0.05275261
[4/200] Training loss: 0.05324529
[5/200] Training loss: 0.04874874
[6/200] Training loss: 0.04865180
[7/200] Training loss: 0.04637218
[8/200] Training loss: 0.04383515
[9/200] Training loss: 0.03718678
[10/200] Training loss: 0.03884076
[50/200] Training loss: 0.01924990
[100/200] Training loss: 0.01637041
[150/200] Training loss: 0.01505801
[200/200] Training loss: 0.01388762
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11235.641503714864 ----------
[1/200] Training loss: 0.15000848
[2/200] Training loss: 0.05723725
[3/200] Training loss: 0.04979851
[4/200] Training loss: 0.04711130
[5/200] Training loss: 0.04362433
[6/200] Training loss: 0.04069625
[7/200] Training loss: 0.03880100
[8/200] Training loss: 0.03545555
[9/200] Training loss: 0.03163105
[10/200] Training loss: 0.02900794
[50/200] Training loss: 0.01830017
[100/200] Training loss: 0.01548143
[150/200] Training loss: 0.01288062
[200/200] Training loss: 0.01219585
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8446.832305663467 ----------
[1/200] Training loss: 0.11631683
[2/200] Training loss: 0.05269626
[3/200] Training loss: 0.04927138
[4/200] Training loss: 0.04382991
[5/200] Training loss: 0.04011142
[6/200] Training loss: 0.03860312
[7/200] Training loss: 0.03408014
[8/200] Training loss: 0.03107167
[9/200] Training loss: 0.02881560
[10/200] Training loss: 0.02869536
[50/200] Training loss: 0.01684006
[100/200] Training loss: 0.01425030
[150/200] Training loss: 0.01256310
[200/200] Training loss: 0.01153403
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 22468.691105625177 ----------
[1/200] Training loss: 0.16651579
[2/200] Training loss: 0.06138899
[3/200] Training loss: 0.05469737
[4/200] Training loss: 0.05168373
[5/200] Training loss: 0.05175070
[6/200] Training loss: 0.04913921
[7/200] Training loss: 0.04684951
[8/200] Training loss: 0.04651707
[9/200] Training loss: 0.04529128
[10/200] Training loss: 0.04360475
[50/200] Training loss: 0.02094581
[100/200] Training loss: 0.01508334
[150/200] Training loss: 0.01322629
[200/200] Training loss: 0.01287688
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6697.666757909055 ----------
[1/200] Training loss: 0.09666803
[2/200] Training loss: 0.04823764
[3/200] Training loss: 0.04052618
[4/200] Training loss: 0.03726856
[5/200] Training loss: 0.03243452
[6/200] Training loss: 0.03111715
[7/200] Training loss: 0.02857133
[8/200] Training loss: 0.02686054
[9/200] Training loss: 0.02459439
[10/200] Training loss: 0.02323347
[50/200] Training loss: 0.01527334
[100/200] Training loss: 0.01257489
[150/200] Training loss: 0.01074132
[200/200] Training loss: 0.01053050
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 20436.416124164236 ----------
[1/100] Training loss: 0.14312487
[2/100] Training loss: 0.05756768
[3/100] Training loss: 0.05368848
[4/100] Training loss: 0.05127538
[5/100] Training loss: 0.04858790
[6/100] Training loss: 0.04624324
[7/100] Training loss: 0.04495174
[8/100] Training loss: 0.04211148
[9/100] Training loss: 0.03887694
[10/100] Training loss: 0.03803897
[50/100] Training loss: 0.01793079
[100/100] Training loss: 0.01563159
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 20400.218038050476 ----------
[1/200] Training loss: 0.14805968
[2/200] Training loss: 0.05581605
[3/200] Training loss: 0.04849326
[4/200] Training loss: 0.04542674
[5/200] Training loss: 0.04391207
[6/200] Training loss: 0.04213982
[7/200] Training loss: 0.04035375
[8/200] Training loss: 0.03883883
[9/200] Training loss: 0.03689617
[10/200] Training loss: 0.03582399
[50/200] Training loss: 0.01855169
[100/200] Training loss: 0.01698546
[150/200] Training loss: 0.01503210
[200/200] Training loss: 0.01348928
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13724.672090800566 ----------
[1/200] Training loss: 0.16251189
[2/200] Training loss: 0.05574525
[3/200] Training loss: 0.05076887
[4/200] Training loss: 0.04506982
[5/200] Training loss: 0.04258857
[6/200] Training loss: 0.04061173
[7/200] Training loss: 0.03981960
[8/200] Training loss: 0.03594979
[9/200] Training loss: 0.03296968
[10/200] Training loss: 0.03014896
[50/200] Training loss: 0.01938831
[100/200] Training loss: 0.01625055
[150/200] Training loss: 0.01385178
[200/200] Training loss: 0.01216115
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12833.002454608975 ----------
[1/200] Training loss: 0.16072580
[2/200] Training loss: 0.05677500
[3/200] Training loss: 0.05030105
[4/200] Training loss: 0.04498913
[5/200] Training loss: 0.04216203
[6/200] Training loss: 0.03940124
[7/200] Training loss: 0.03904446
[8/200] Training loss: 0.03679770
[9/200] Training loss: 0.03454211
[10/200] Training loss: 0.03413412
[50/200] Training loss: 0.02024992
[100/200] Training loss: 0.01525863
[150/200] Training loss: 0.01324832
[200/200] Training loss: 0.01234298
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7179.851530498385 ----------
[1/100] Training loss: 0.15378526
[2/100] Training loss: 0.06170854
[3/100] Training loss: 0.05138911
[4/100] Training loss: 0.05003109
[5/100] Training loss: 0.04634727
[6/100] Training loss: 0.04415367
[7/100] Training loss: 0.04452734
[8/100] Training loss: 0.03971715
[9/100] Training loss: 0.04091406
[10/100] Training loss: 0.03661223
[50/100] Training loss: 0.02068645
[100/100] Training loss: 0.01655517
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7009.928958270547 ----------
[1/100] Training loss: 0.14112718
[2/100] Training loss: 0.06013804
[3/100] Training loss: 0.05207542
[4/100] Training loss: 0.04716447
[5/100] Training loss: 0.04624751
[6/100] Training loss: 0.04347641
[7/100] Training loss: 0.04395236
[8/100] Training loss: 0.03897456
[9/100] Training loss: 0.03901440
[10/100] Training loss: 0.03649589
[50/100] Training loss: 0.01681897
[100/100] Training loss: 0.01510473
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13017.705481381887 ----------
[1/200] Training loss: 0.12094408
[2/200] Training loss: 0.05311307
[3/200] Training loss: 0.04813041
[4/200] Training loss: 0.04718311
[5/200] Training loss: 0.04595921
[6/200] Training loss: 0.04102007
[7/200] Training loss: 0.03736374
[8/200] Training loss: 0.03792123
[9/200] Training loss: 0.03571995
[10/200] Training loss: 0.03593367
[50/200] Training loss: 0.01662915
[100/200] Training loss: 0.01461816
[150/200] Training loss: 0.01302308
[200/200] Training loss: 0.01236737
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17224.77378661328 ----------
[1/200] Training loss: 0.15060944
[2/200] Training loss: 0.05788516
[3/200] Training loss: 0.05455091
[4/200] Training loss: 0.04975059
[5/200] Training loss: 0.05012838
[6/200] Training loss: 0.04711451
[7/200] Training loss: 0.04464734
[8/200] Training loss: 0.04301941
[9/200] Training loss: 0.04071041
[10/200] Training loss: 0.03680425
[50/200] Training loss: 0.02057135
[100/200] Training loss: 0.01754633
[150/200] Training loss: 0.01535398
[200/200] Training loss: 0.01372693
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9296.247845233043 ----------
[1/200] Training loss: 0.13297818
[2/200] Training loss: 0.05689152
[3/200] Training loss: 0.05071179
[4/200] Training loss: 0.04713696
[5/200] Training loss: 0.04283878
[6/200] Training loss: 0.04298013
[7/200] Training loss: 0.03776649
[8/200] Training loss: 0.03545220
[9/200] Training loss: 0.03348887
[10/200] Training loss: 0.03107632
[50/200] Training loss: 0.01798938
[100/200] Training loss: 0.01595166
[150/200] Training loss: 0.01437448
[200/200] Training loss: 0.01253416
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 18103.812637121497 ----------
[1/200] Training loss: 0.11079063
[2/200] Training loss: 0.05582861
[3/200] Training loss: 0.04830746
[4/200] Training loss: 0.04282921
[5/200] Training loss: 0.03998436
[6/200] Training loss: 0.03412252
[7/200] Training loss: 0.03158811
[8/200] Training loss: 0.02920121
[9/200] Training loss: 0.02877263
[10/200] Training loss: 0.02677661
[50/200] Training loss: 0.01718721
[100/200] Training loss: 0.01412539
[150/200] Training loss: 0.01283785
[200/200] Training loss: 0.01219350
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10020.037524879834 ----------
[1/200] Training loss: 0.11335894
[2/200] Training loss: 0.05120947
[3/200] Training loss: 0.04458989
[4/200] Training loss: 0.04145645
[5/200] Training loss: 0.03643387
[6/200] Training loss: 0.03417509
[7/200] Training loss: 0.02998906
[8/200] Training loss: 0.02822565
[9/200] Training loss: 0.02597496
[10/200] Training loss: 0.02441859
[50/200] Training loss: 0.01574842
[100/200] Training loss: 0.01315224
[150/200] Training loss: 0.01166815
[200/200] Training loss: 0.01082691
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11829.252554578417 ----------
[1/200] Training loss: 0.12724600
[2/200] Training loss: 0.05627354
[3/200] Training loss: 0.04385433
[4/200] Training loss: 0.04163205
[5/200] Training loss: 0.03781688
[6/200] Training loss: 0.03383673
[7/200] Training loss: 0.03234250
[8/200] Training loss: 0.03137134
[9/200] Training loss: 0.03008188
[10/200] Training loss: 0.02886026
[50/200] Training loss: 0.01695909
[100/200] Training loss: 0.01419086
[150/200] Training loss: 0.01302791
[200/200] Training loss: 0.01204382
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11858.566523825719 ----------
[1/200] Training loss: 0.16180278
[2/200] Training loss: 0.05678533
[3/200] Training loss: 0.04943895
[4/200] Training loss: 0.04665629
[5/200] Training loss: 0.04561251
[6/200] Training loss: 0.04373224
[7/200] Training loss: 0.03890492
[8/200] Training loss: 0.03797024
[9/200] Training loss: 0.03602178
[10/200] Training loss: 0.03555856
[50/200] Training loss: 0.01757289
[100/200] Training loss: 0.01451896
[150/200] Training loss: 0.01356612
[200/200] Training loss: 0.01252427
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6468.422991734538 ----------
[1/150] Training loss: 0.15740735
[2/150] Training loss: 0.05796407
[3/150] Training loss: 0.05363609
[4/150] Training loss: 0.04966336
[5/150] Training loss: 0.04716964
[6/150] Training loss: 0.04496986
[7/150] Training loss: 0.04390840
[8/150] Training loss: 0.04076929
[9/150] Training loss: 0.03864555
[10/150] Training loss: 0.03685515
[50/150] Training loss: 0.01754015
[100/150] Training loss: 0.01492607
[150/150] Training loss: 0.01327346
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 150
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10153.908804002525 ----------
[1/200] Training loss: 0.13110329
[2/200] Training loss: 0.05723526
[3/200] Training loss: 0.05212145
[4/200] Training loss: 0.04733770
[5/200] Training loss: 0.04547432
[6/200] Training loss: 0.04044165
[7/200] Training loss: 0.03784089
[8/200] Training loss: 0.03576653
[9/200] Training loss: 0.03132651
[10/200] Training loss: 0.02949281
[50/200] Training loss: 0.02527145
[100/200] Training loss: 0.01719410
[150/200] Training loss: 0.01427689
[200/200] Training loss: 0.01284302
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13725.213003811636 ----------
[1/200] Training loss: 0.12114230
[2/200] Training loss: 0.05553494
[3/200] Training loss: 0.04736873
[4/200] Training loss: 0.04570827
[5/200] Training loss: 0.04102654
[6/200] Training loss: 0.03972517
[7/200] Training loss: 0.03521704
[8/200] Training loss: 0.03513186
[9/200] Training loss: 0.03078927
[10/200] Training loss: 0.02947577
[50/200] Training loss: 0.01678143
[100/200] Training loss: 0.01422489
[150/200] Training loss: 0.01293047
[200/200] Training loss: 0.01161062
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 28083.76157141347 ----------
[1/200] Training loss: 0.12243730
[2/200] Training loss: 0.05922712
[3/200] Training loss: 0.05160478
[4/200] Training loss: 0.04840302
[5/200] Training loss: 0.04556505
[6/200] Training loss: 0.04313995
[7/200] Training loss: 0.03961396
[8/200] Training loss: 0.03935827
[9/200] Training loss: 0.03696366
[10/200] Training loss: 0.03643481
[50/200] Training loss: 0.01868202
[100/200] Training loss: 0.01512005
[150/200] Training loss: 0.01370209
[200/200] Training loss: 0.01185513
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.012730333065230859
----FITNESS-----------RMSE---- 18268.05211291012 ----------
[1/200] Training loss: 0.16344545
[2/200] Training loss: 0.06147277
[3/200] Training loss: 0.05368803
[4/200] Training loss: 0.05116668
[5/200] Training loss: 0.04729447
[6/200] Training loss: 0.04602596
[7/200] Training loss: 0.04388307
[8/200] Training loss: 0.04104643
[9/200] Training loss: 0.03810173
[10/200] Training loss: 0.03787788
[50/200] Training loss: 0.01923915
[100/200] Training loss: 0.01640351
[150/200] Training loss: 0.01530587
[200/200] Training loss: 0.01398027
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7349.197507211246 ----------
[1/200] Training loss: 0.16185107
[2/200] Training loss: 0.06346302
[3/200] Training loss: 0.05015070
[4/200] Training loss: 0.04750978
[5/200] Training loss: 0.04789742
[6/200] Training loss: 0.04126896
[7/200] Training loss: 0.04208277
[8/200] Training loss: 0.03795587
[9/200] Training loss: 0.03668033
[10/200] Training loss: 0.03517781
[50/200] Training loss: 0.01899208
[100/200] Training loss: 0.01546748
[150/200] Training loss: 0.01541918
[200/200] Training loss: 0.01437258
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5905.784283226065 ----------
[1/200] Training loss: 0.17450687
[2/200] Training loss: 0.06244793
[3/200] Training loss: 0.05521348
[4/200] Training loss: 0.05451966
[5/200] Training loss: 0.05160797
[6/200] Training loss: 0.04883218
[7/200] Training loss: 0.04552557
[8/200] Training loss: 0.04365823
[9/200] Training loss: 0.04325699
[10/200] Training loss: 0.04064380
[50/200] Training loss: 0.01833009
[100/200] Training loss: 0.01501315
[150/200] Training loss: 0.01464187
[200/200] Training loss: 0.01341521
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15956.995707212558 ----------
[1/200] Training loss: 0.15559031
[2/200] Training loss: 0.05780746
[3/200] Training loss: 0.04941183
[4/200] Training loss: 0.04601098
[5/200] Training loss: 0.04154247
[6/200] Training loss: 0.04417118
[7/200] Training loss: 0.04037197
[8/200] Training loss: 0.03734345
[9/200] Training loss: 0.03724254
[10/200] Training loss: 0.03646035
[50/200] Training loss: 0.01897005
[100/200] Training loss: 0.01560762
[150/200] Training loss: 0.01302805
[200/200] Training loss: 0.01153984
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4514.162602299567 ----------
[1/200] Training loss: 0.19045667
[2/200] Training loss: 0.06451003
[3/200] Training loss: 0.05485791
[4/200] Training loss: 0.05366061
[5/200] Training loss: 0.05318816
[6/200] Training loss: 0.04894562
[7/200] Training loss: 0.04830806
[8/200] Training loss: 0.04576084
[9/200] Training loss: 0.04420433
[10/200] Training loss: 0.04349327
[50/200] Training loss: 0.01914971
[100/200] Training loss: 0.01664916
[150/200] Training loss: 0.01429484
[200/200] Training loss: 0.01350801
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12310.687389419 ----------
[1/200] Training loss: 0.18349312
[2/200] Training loss: 0.05835961
[3/200] Training loss: 0.05121588
[4/200] Training loss: 0.05139195
[5/200] Training loss: 0.04824268
[6/200] Training loss: 0.04745412
[7/200] Training loss: 0.04483115
[8/200] Training loss: 0.04353057
[9/200] Training loss: 0.04157734
[10/200] Training loss: 0.03839682
[50/200] Training loss: 0.01779981
[100/200] Training loss: 0.01627284
[150/200] Training loss: 0.01513490
[200/200] Training loss: 0.01411635
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13686.162354728955 ----------
[1/200] Training loss: 0.14574672
[2/200] Training loss: 0.05673448
[3/200] Training loss: 0.05153548
[4/200] Training loss: 0.04801896
[5/200] Training loss: 0.04211786
[6/200] Training loss: 0.04160706
[7/200] Training loss: 0.03839803
[8/200] Training loss: 0.03749957
[9/200] Training loss: 0.03484147
[10/200] Training loss: 0.03380210
[50/200] Training loss: 0.01930388
[100/200] Training loss: 0.01450130
[150/200] Training loss: 0.01340235
[200/200] Training loss: 0.01130498
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4336.53640593504 ----------
[1/200] Training loss: 0.11219843
[2/200] Training loss: 0.05467219
[3/200] Training loss: 0.04882853
[4/200] Training loss: 0.04261103
[5/200] Training loss: 0.03779733
[6/200] Training loss: 0.03387233
[7/200] Training loss: 0.03014924
[8/200] Training loss: 0.02925252
[9/200] Training loss: 0.02623477
[10/200] Training loss: 0.02603698
[50/200] Training loss: 0.01481912
[100/200] Training loss: 0.01292711
[150/200] Training loss: 0.01199380
[200/200] Training loss: 0.01119031
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12466.39129820655 ----------
[1/200] Training loss: 0.14093601
[2/200] Training loss: 0.05252247
[3/200] Training loss: 0.04589642
[4/200] Training loss: 0.03982849
[5/200] Training loss: 0.03649697
[6/200] Training loss: 0.03386020
[7/200] Training loss: 0.03269999
[8/200] Training loss: 0.02958446
[9/200] Training loss: 0.02792262
[10/200] Training loss: 0.02867106
[50/200] Training loss: 0.01826894
[100/200] Training loss: 0.01609147
[150/200] Training loss: 0.01388585
[200/200] Training loss: 0.01240653
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11228.773040719987 ----------
[1/200] Training loss: 0.12016410
[2/200] Training loss: 0.05802133
[3/200] Training loss: 0.05283110
[4/200] Training loss: 0.05017325
[5/200] Training loss: 0.04533394
[6/200] Training loss: 0.04344916
[7/200] Training loss: 0.03897976
[8/200] Training loss: 0.03511102
[9/200] Training loss: 0.03150005
[10/200] Training loss: 0.02925095
[50/200] Training loss: 0.01628739
[100/200] Training loss: 0.01414207
[150/200] Training loss: 0.01284056
[200/200] Training loss: 0.01147064
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 21537.444973812468 ----------
[1/200] Training loss: 0.16247216
[2/200] Training loss: 0.05461579
[3/200] Training loss: 0.05424740
[4/200] Training loss: 0.05017043
[5/200] Training loss: 0.04582244
[6/200] Training loss: 0.04263509
[7/200] Training loss: 0.04024798
[8/200] Training loss: 0.03875052
[9/200] Training loss: 0.03703466
[10/200] Training loss: 0.03504068
[50/200] Training loss: 0.01720789
[100/200] Training loss: 0.01513626
[150/200] Training loss: 0.01382232
[200/200] Training loss: 0.01254647
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6457.997212758767 ----------
[1/200] Training loss: 0.16020011
[2/200] Training loss: 0.05291208
[3/200] Training loss: 0.04964555
[4/200] Training loss: 0.04792584
[5/200] Training loss: 0.04522339
[6/200] Training loss: 0.04192315
[7/200] Training loss: 0.03855351
[8/200] Training loss: 0.03895046
[9/200] Training loss: 0.03494216
[10/200] Training loss: 0.03533519
[50/200] Training loss: 0.01662235
[100/200] Training loss: 0.01414904
[150/200] Training loss: 0.01266209
[200/200] Training loss: 0.01163553
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7713.227080800876 ----------
[1/200] Training loss: 0.14256084
[2/200] Training loss: 0.05916563
[3/200] Training loss: 0.04909004
[4/200] Training loss: 0.04563076
[5/200] Training loss: 0.04308013
[6/200] Training loss: 0.04292417
[7/200] Training loss: 0.03891146
[8/200] Training loss: 0.03799161
[9/200] Training loss: 0.03508161
[10/200] Training loss: 0.03334939
[50/200] Training loss: 0.01938225
[100/200] Training loss: 0.01678381
[150/200] Training loss: 0.01504025
[200/200] Training loss: 0.01356915
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8040.2019875125025 ----------
[1/200] Training loss: 0.11235820
[2/200] Training loss: 0.04929357
[3/200] Training loss: 0.04411348
[4/200] Training loss: 0.03686891
[5/200] Training loss: 0.03570489
[6/200] Training loss: 0.03332104
[7/200] Training loss: 0.03168299
[8/200] Training loss: 0.03038975
[9/200] Training loss: 0.02872672
[10/200] Training loss: 0.02760782
[50/200] Training loss: 0.01778155
[100/200] Training loss: 0.01571151
[150/200] Training loss: 0.01346345
[200/200] Training loss: 0.01199464
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6469.646976458607 ----------
[1/200] Training loss: 0.14785267
[2/200] Training loss: 0.05591942
[3/200] Training loss: 0.04941591
[4/200] Training loss: 0.04710603
[5/200] Training loss: 0.04458373
[6/200] Training loss: 0.04425846
[7/200] Training loss: 0.04073568
[8/200] Training loss: 0.03615756
[9/200] Training loss: 0.03699966
[10/200] Training loss: 0.03496721
[50/200] Training loss: 0.01740906
[100/200] Training loss: 0.01503585
[150/200] Training loss: 0.01437921
[200/200] Training loss: 0.01307947
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9906.043407940428 ----------
[1/200] Training loss: 0.12068807
[2/200] Training loss: 0.05258819
[3/200] Training loss: 0.04699346
[4/200] Training loss: 0.04345552
[5/200] Training loss: 0.03926181
[6/200] Training loss: 0.03594957
[7/200] Training loss: 0.03319464
[8/200] Training loss: 0.03032030
[9/200] Training loss: 0.02884443
[10/200] Training loss: 0.02760947
[50/200] Training loss: 0.01890769
[100/200] Training loss: 0.01615666
[150/200] Training loss: 0.01467595
[200/200] Training loss: 0.01349039
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16446.387080450222 ----------
[1/200] Training loss: 0.13278437
[2/200] Training loss: 0.05398261
[3/200] Training loss: 0.04572953
[4/200] Training loss: 0.04509180
[5/200] Training loss: 0.03946782
[6/200] Training loss: 0.03736669
[7/200] Training loss: 0.03442587
[8/200] Training loss: 0.03424424
[9/200] Training loss: 0.03165641
[10/200] Training loss: 0.02940195
[50/200] Training loss: 0.01879430
[100/200] Training loss: 0.01638731
[150/200] Training loss: 0.01453135
[200/200] Training loss: 0.01333804
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6186.187517364794 ----------
[1/200] Training loss: 0.17480386
[2/200] Training loss: 0.05614342
[3/200] Training loss: 0.05072579
[4/200] Training loss: 0.04444762
[5/200] Training loss: 0.04163647
[6/200] Training loss: 0.03941481
[7/200] Training loss: 0.03745009
[8/200] Training loss: 0.03825282
[9/200] Training loss: 0.03621231
[10/200] Training loss: 0.03137723
[50/200] Training loss: 0.01917817
[100/200] Training loss: 0.01607110
[150/200] Training loss: 0.01329898
[200/200] Training loss: 0.01231513
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8709.000861178049 ----------
[1/200] Training loss: 0.15219619
[2/200] Training loss: 0.05812171
[3/200] Training loss: 0.05428216
[4/200] Training loss: 0.05226064
[5/200] Training loss: 0.04841331
[6/200] Training loss: 0.04606425
[7/200] Training loss: 0.04422380
[8/200] Training loss: 0.03984419
[9/200] Training loss: 0.03948489
[10/200] Training loss: 0.03401236
[50/200] Training loss: 0.01739195
[100/200] Training loss: 0.01453962
[150/200] Training loss: 0.01356565
[200/200] Training loss: 0.01177910
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6759.348489314632 ----------
[1/200] Training loss: 0.02662973
[2/200] Training loss: 0.00229991
[3/200] Training loss: 0.00200739
[4/200] Training loss: 0.00176293
[5/200] Training loss: 0.00149844
[6/200] Training loss: 0.00127779
[7/200] Training loss: 0.00099417
[8/200] Training loss: 0.00078726
[9/200] Training loss: 0.00068577
[10/200] Training loss: 0.00054697
[50/200] Training loss: 0.00019596
[100/200] Training loss: 0.00014077
[150/200] Training loss: 0.00011628
[200/200] Training loss: 0.00010417
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 3763.4223786335756 ----------
[1/200] Training loss: 0.18117215
[2/200] Training loss: 0.07021696
[3/200] Training loss: 0.05952282
[4/200] Training loss: 0.05546192
[5/200] Training loss: 0.05423517
[6/200] Training loss: 0.05325034
[7/200] Training loss: 0.05310110
[8/200] Training loss: 0.04961206
[9/200] Training loss: 0.04758917
[10/200] Training loss: 0.04728322
[50/200] Training loss: 0.02011484
[100/200] Training loss: 0.01644188
[150/200] Training loss: 0.01458673
[200/200] Training loss: 0.01370501
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 20373.954353536774 ----------
[1/200] Training loss: 0.15511020
[2/200] Training loss: 0.05862316
[3/200] Training loss: 0.04747835
[4/200] Training loss: 0.04258476
[5/200] Training loss: 0.04119916
[6/200] Training loss: 0.03969242
[7/200] Training loss: 0.03713384
[8/200] Training loss: 0.03330328
[9/200] Training loss: 0.03376576
[10/200] Training loss: 0.03029261
[50/200] Training loss: 0.01801730
[100/200] Training loss: 0.01610285
[150/200] Training loss: 0.01398773
[200/200] Training loss: 0.01241240
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8799.97136358977 ----------
[1/200] Training loss: 0.16960336
[2/200] Training loss: 0.06009888
[3/200] Training loss: 0.05315198
[4/200] Training loss: 0.05066682
[5/200] Training loss: 0.04788155
[6/200] Training loss: 0.04714305
[7/200] Training loss: 0.04421958
[8/200] Training loss: 0.04311836
[9/200] Training loss: 0.03989497
[10/200] Training loss: 0.03846580
[50/200] Training loss: 0.01893160
[100/200] Training loss: 0.01559682
[150/200] Training loss: 0.01451934
[200/200] Training loss: 0.01239935
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14108.400334552462 ----------
[1/200] Training loss: 0.11740028
[2/200] Training loss: 0.05831166
[3/200] Training loss: 0.05352898
[4/200] Training loss: 0.05053875
[5/200] Training loss: 0.04857706
[6/200] Training loss: 0.04530829
[7/200] Training loss: 0.04355330
[8/200] Training loss: 0.03874194
[9/200] Training loss: 0.03449561
[10/200] Training loss: 0.03418503
[50/200] Training loss: 0.01705574
[100/200] Training loss: 0.01353445
[150/200] Training loss: 0.01207791
[200/200] Training loss: 0.01128438
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7944.287255632188 ----------
[1/200] Training loss: 0.14312454
[2/200] Training loss: 0.05642451
[3/200] Training loss: 0.05092456
[4/200] Training loss: 0.04724701
[5/200] Training loss: 0.04423727
[6/200] Training loss: 0.04308066
[7/200] Training loss: 0.04026411
[8/200] Training loss: 0.03878212
[9/200] Training loss: 0.03720346
[10/200] Training loss: 0.03397912
[50/200] Training loss: 0.01798237
[100/200] Training loss: 0.01528211
[150/200] Training loss: 0.01395345
[200/200] Training loss: 0.01327023
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11917.815907287712 ----------
[1/200] Training loss: 0.15487786
[2/200] Training loss: 0.05203396
[3/200] Training loss: 0.05019118
[4/200] Training loss: 0.04479160
[5/200] Training loss: 0.04762854
[6/200] Training loss: 0.04209964
[7/200] Training loss: 0.04026017
[8/200] Training loss: 0.03701303
[9/200] Training loss: 0.03368579
[10/200] Training loss: 0.03549985
[50/200] Training loss: 0.01723743
[100/200] Training loss: 0.01464860
[150/200] Training loss: 0.01363739
[200/200] Training loss: 0.01154957
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12126.257130706077 ----------
[1/200] Training loss: 0.15094582
[2/200] Training loss: 0.05675898
[3/200] Training loss: 0.04956795
[4/200] Training loss: 0.04262146
[5/200] Training loss: 0.04009957
[6/200] Training loss: 0.03534246
[7/200] Training loss: 0.03287128
[8/200] Training loss: 0.02965218
[9/200] Training loss: 0.02945656
[10/200] Training loss: 0.02699167
[50/200] Training loss: 0.01780099
[100/200] Training loss: 0.01657349
[150/200] Training loss: 0.01573450
[200/200] Training loss: 0.01455522
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14193.274463632415 ----------
[1/200] Training loss: 0.11418096
[2/200] Training loss: 0.05050069
[3/200] Training loss: 0.04552798
[4/200] Training loss: 0.03876234
[5/200] Training loss: 0.03431446
[6/200] Training loss: 0.03228747
[7/200] Training loss: 0.03204550
[8/200] Training loss: 0.02801642
[9/200] Training loss: 0.02882609
[10/200] Training loss: 0.02751803
[50/200] Training loss: 0.01610403
[100/200] Training loss: 0.01398252
[150/200] Training loss: 0.01274047
[200/200] Training loss: 0.01151803
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8650.575472186807 ----------
[1/200] Training loss: 0.13783220
[2/200] Training loss: 0.05472958
[3/200] Training loss: 0.04740846
[4/200] Training loss: 0.04342101
[5/200] Training loss: 0.04069160
[6/200] Training loss: 0.03875468
[7/200] Training loss: 0.03567150
[8/200] Training loss: 0.03557211
[9/200] Training loss: 0.03383551
[10/200] Training loss: 0.03433035
[50/200] Training loss: 0.01778137
[100/200] Training loss: 0.01438064
[150/200] Training loss: 0.01273584
[200/200] Training loss: 0.01160494
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5943.563577518121 ----------
[1/200] Training loss: 0.16144922
[2/200] Training loss: 0.05690307
[3/200] Training loss: 0.05175709
[4/200] Training loss: 0.04844125
[5/200] Training loss: 0.04165551
[6/200] Training loss: 0.03788862
[7/200] Training loss: 0.03872321
[8/200] Training loss: 0.03565026
[9/200] Training loss: 0.03048454
[10/200] Training loss: 0.02998595
[50/200] Training loss: 0.01866023
[100/200] Training loss: 0.01513691
[150/200] Training loss: 0.01517050
[200/200] Training loss: 0.01349451
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9355.621197975044 ----------
[1/200] Training loss: 0.13820996
[2/200] Training loss: 0.05908821
[3/200] Training loss: 0.05296386
[4/200] Training loss: 0.05191378
[5/200] Training loss: 0.04960380
[6/200] Training loss: 0.04800676
[7/200] Training loss: 0.04459712
[8/200] Training loss: 0.04151619
[9/200] Training loss: 0.03892858
[10/200] Training loss: 0.03455286
[50/200] Training loss: 0.01810045
[100/200] Training loss: 0.01631426
[150/200] Training loss: 0.01494838
[200/200] Training loss: 0.01392851
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14363.591751369155 ----------
[1/200] Training loss: 0.15470898
[2/200] Training loss: 0.05670745
[3/200] Training loss: 0.05051551
[4/200] Training loss: 0.04668903
[5/200] Training loss: 0.04356464
[6/200] Training loss: 0.04408305
[7/200] Training loss: 0.04045334
[8/200] Training loss: 0.03782534
[9/200] Training loss: 0.03615746
[10/200] Training loss: 0.03595139
[50/200] Training loss: 0.01813087
[100/200] Training loss: 0.01647211
[150/200] Training loss: 0.01579677
[200/200] Training loss: 0.01439728
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7912.746425862515 ----------
[1/200] Training loss: 0.11408990
[2/200] Training loss: 0.05589428
[3/200] Training loss: 0.04994734
[4/200] Training loss: 0.04620055
[5/200] Training loss: 0.03673081
[6/200] Training loss: 0.03484666
[7/200] Training loss: 0.03227933
[8/200] Training loss: 0.03111211
[9/200] Training loss: 0.03076460
[10/200] Training loss: 0.02733271
[50/200] Training loss: 0.01600171
[100/200] Training loss: 0.01365208
[150/200] Training loss: 0.01350125
[200/200] Training loss: 0.01232993
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6990.094992201465 ----------
[1/200] Training loss: 0.14388438
[2/200] Training loss: 0.05781931
[3/200] Training loss: 0.05459388
[4/200] Training loss: 0.05047993
[5/200] Training loss: 0.04967973
[6/200] Training loss: 0.04624135
[7/200] Training loss: 0.04508495
[8/200] Training loss: 0.04319525
[9/200] Training loss: 0.03923671
[10/200] Training loss: 0.03758186
[50/200] Training loss: 0.01783644
[100/200] Training loss: 0.01576939
[150/200] Training loss: 0.01422365
[200/200] Training loss: 0.01275572
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8202.689802741537 ----------
[1/200] Training loss: 0.15864461
[2/200] Training loss: 0.05614876
[3/200] Training loss: 0.05057420
[4/200] Training loss: 0.04655197
[5/200] Training loss: 0.04230537
[6/200] Training loss: 0.03865675
[7/200] Training loss: 0.03677311
[8/200] Training loss: 0.03623535
[9/200] Training loss: 0.03448570
[10/200] Training loss: 0.03115169
[50/200] Training loss: 0.01836060
[100/200] Training loss: 0.01532088
[150/200] Training loss: 0.01385373
[200/200] Training loss: 0.01319807
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7688.492440004087 ----------
[1/200] Training loss: 0.15302638
[2/200] Training loss: 0.05464021
[3/200] Training loss: 0.04975969
[4/200] Training loss: 0.04587079
[5/200] Training loss: 0.04259676
[6/200] Training loss: 0.04044452
[7/200] Training loss: 0.03766490
[8/200] Training loss: 0.03639973
[9/200] Training loss: 0.03320434
[10/200] Training loss: 0.03246267
[50/200] Training loss: 0.01895133
[100/200] Training loss: 0.01586725
[150/200] Training loss: 0.01456809
[200/200] Training loss: 0.01375645
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6347.279732294773 ----------
[1/200] Training loss: 0.09713052
[2/200] Training loss: 0.05063220
[3/200] Training loss: 0.04621558
[4/200] Training loss: 0.04199166
[5/200] Training loss: 0.03835222
[6/200] Training loss: 0.03475895
[7/200] Training loss: 0.03262171
[8/200] Training loss: 0.03124316
[9/200] Training loss: 0.02886171
[10/200] Training loss: 0.02936656
[50/200] Training loss: 0.01818782
[100/200] Training loss: 0.01412804
[150/200] Training loss: 0.01230220
[200/200] Training loss: 0.01083946
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16940.057142760765 ----------
[1/200] Training loss: 0.14146511
[2/200] Training loss: 0.05673610
[3/200] Training loss: 0.04965802
[4/200] Training loss: 0.04829245
[5/200] Training loss: 0.04563016
[6/200] Training loss: 0.04359201
[7/200] Training loss: 0.04330712
[8/200] Training loss: 0.04084680
[9/200] Training loss: 0.03696835
[10/200] Training loss: 0.03738622
[50/200] Training loss: 0.01866647
[100/200] Training loss: 0.01427194
[150/200] Training loss: 0.01288803
[200/200] Training loss: 0.01228463
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6669.11418405773 ----------
[1/200] Training loss: 0.02208823
[2/200] Training loss: 0.00238381
[3/200] Training loss: 0.00209399
[4/200] Training loss: 0.00181510
[5/200] Training loss: 0.00146653
[6/200] Training loss: 0.00126903
[7/200] Training loss: 0.00106077
[8/200] Training loss: 0.00088352
[9/200] Training loss: 0.00069313
[10/200] Training loss: 0.00060961
[50/200] Training loss: 0.00025767
[100/200] Training loss: 0.00018523
[150/200] Training loss: 0.00015298
[200/200] Training loss: 0.00012758
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11533.496954523376 ----------
[1/200] Training loss: 0.16127693
[2/200] Training loss: 0.05806918
[3/200] Training loss: 0.05262685
[4/200] Training loss: 0.04793131
[5/200] Training loss: 0.04332857
[6/200] Training loss: 0.04043705
[7/200] Training loss: 0.03983420
[8/200] Training loss: 0.03845312
[9/200] Training loss: 0.03607485
[10/200] Training loss: 0.03327939
[50/200] Training loss: 0.01710577
[100/200] Training loss: 0.01533467
[150/200] Training loss: 0.01431685
[200/200] Training loss: 0.01257923
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12179.509349723412 ----------
[1/200] Training loss: 0.15066931
[2/200] Training loss: 0.05768704
[3/200] Training loss: 0.04991449
[4/200] Training loss: 0.04322483
[5/200] Training loss: 0.04563051
[6/200] Training loss: 0.03992624
[7/200] Training loss: 0.04219171
[8/200] Training loss: 0.03699389
[9/200] Training loss: 0.03497275
[10/200] Training loss: 0.03587495
[50/200] Training loss: 0.01740714
[100/200] Training loss: 0.01413642
[150/200] Training loss: 0.01275417
[200/200] Training loss: 0.01217380
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 22702.524264935826 ----------
[1/200] Training loss: 0.12217434
[2/200] Training loss: 0.05614979
[3/200] Training loss: 0.04933003
[4/200] Training loss: 0.04448014
[5/200] Training loss: 0.03797617
[6/200] Training loss: 0.03572847
[7/200] Training loss: 0.03393691
[8/200] Training loss: 0.02991466
[9/200] Training loss: 0.02914902
[10/200] Training loss: 0.02693505
[50/200] Training loss: 0.01695518
[100/200] Training loss: 0.01495931
[150/200] Training loss: 0.01337821
[200/200] Training loss: 0.01226054
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8995.233849100312 ----------
[1/200] Training loss: 0.13378820
[2/200] Training loss: 0.05339394
[3/200] Training loss: 0.04825995
[4/200] Training loss: 0.04537349
[5/200] Training loss: 0.04321232
[6/200] Training loss: 0.03831586
[7/200] Training loss: 0.03720080
[8/200] Training loss: 0.03669275
[9/200] Training loss: 0.03472163
[10/200] Training loss: 0.03268325
[50/200] Training loss: 0.01887293
[100/200] Training loss: 0.01643299
[150/200] Training loss: 0.01514836
[200/200] Training loss: 0.01416730
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11874.59001397522 ----------
[1/200] Training loss: 0.11494390
[2/200] Training loss: 0.05590722
[3/200] Training loss: 0.05365346
[4/200] Training loss: 0.05185811
[5/200] Training loss: 0.04728134
[6/200] Training loss: 0.04389292
[7/200] Training loss: 0.04243316
[8/200] Training loss: 0.03991845
[9/200] Training loss: 0.03741036
[10/200] Training loss: 0.03522639
[50/200] Training loss: 0.01745866
[100/200] Training loss: 0.01388780
[150/200] Training loss: 0.01270122
[200/200] Training loss: 0.01191282
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 18984.173619096513 ----------
[1/200] Training loss: 0.17734255
[2/200] Training loss: 0.06329754
[3/200] Training loss: 0.05596701
[4/200] Training loss: 0.05594600
[5/200] Training loss: 0.05074744
[6/200] Training loss: 0.05198504
[7/200] Training loss: 0.05078264
[8/200] Training loss: 0.04830996
[9/200] Training loss: 0.04572670
[10/200] Training loss: 0.04423948
[50/200] Training loss: 0.01922957
[100/200] Training loss: 0.01603230
[150/200] Training loss: 0.01415768
[200/200] Training loss: 0.01324884
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14487.336815301838 ----------
[1/200] Training loss: 0.12561319
[2/200] Training loss: 0.05155048
[3/200] Training loss: 0.04704425
[4/200] Training loss: 0.04497941
[5/200] Training loss: 0.04234487
[6/200] Training loss: 0.03791925
[7/200] Training loss: 0.03371726
[8/200] Training loss: 0.03340103
[9/200] Training loss: 0.03057140
[10/200] Training loss: 0.03011052
[50/200] Training loss: 0.01727014
[100/200] Training loss: 0.01445762
[150/200] Training loss: 0.01346319
[200/200] Training loss: 0.01207835
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4937.6214921761675 ----------
[1/200] Training loss: 0.14991811
[2/200] Training loss: 0.05405034
[3/200] Training loss: 0.05047111
[4/200] Training loss: 0.04824408
[5/200] Training loss: 0.04384881
[6/200] Training loss: 0.04345962
[7/200] Training loss: 0.04075064
[8/200] Training loss: 0.03996982
[9/200] Training loss: 0.03842863
[10/200] Training loss: 0.03512003
[50/200] Training loss: 0.01869440
[100/200] Training loss: 0.01489150
[150/200] Training loss: 0.01400232
[200/200] Training loss: 0.01236487
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4246.401299924443 ----------
[1/200] Training loss: 0.12220209
[2/200] Training loss: 0.05938344
[3/200] Training loss: 0.05295169
[4/200] Training loss: 0.04978585
[5/200] Training loss: 0.04534082
[6/200] Training loss: 0.04319449
[7/200] Training loss: 0.04027987
[8/200] Training loss: 0.03891192
[9/200] Training loss: 0.03778506
[10/200] Training loss: 0.03556351
[50/200] Training loss: 0.01848555
[100/200] Training loss: 0.01581244
[150/200] Training loss: 0.01409323
[200/200] Training loss: 0.01313937
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8297.536983948912 ----------
[1/200] Training loss: 0.16963201
[2/200] Training loss: 0.06202499
[3/200] Training loss: 0.05289038
[4/200] Training loss: 0.05070115
[5/200] Training loss: 0.04723653
[6/200] Training loss: 0.04334085
[7/200] Training loss: 0.04095813
[8/200] Training loss: 0.03938055
[9/200] Training loss: 0.03915894
[10/200] Training loss: 0.03604991
[50/200] Training loss: 0.01933997
[100/200] Training loss: 0.01498477
[150/200] Training loss: 0.01365730
[200/200] Training loss: 0.01234429
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6506.862223837232 ----------
[1/200] Training loss: 0.16906694
[2/200] Training loss: 0.06485499
[3/200] Training loss: 0.05520045
[4/200] Training loss: 0.05310622
[5/200] Training loss: 0.05092606
[6/200] Training loss: 0.05019505
[7/200] Training loss: 0.04528498
[8/200] Training loss: 0.04075741
[9/200] Training loss: 0.03974491
[10/200] Training loss: 0.03529789
[50/200] Training loss: 0.01943724
[100/200] Training loss: 0.01642272
[150/200] Training loss: 0.01516849
[200/200] Training loss: 0.01423999
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 18392.76727412164 ----------
[1/200] Training loss: 0.01797318
[2/200] Training loss: 0.00176237
[3/200] Training loss: 0.00154489
[4/200] Training loss: 0.00122394
[5/200] Training loss: 0.00110939
[6/200] Training loss: 0.00090592
[7/200] Training loss: 0.00069817
[8/200] Training loss: 0.00055319
[9/200] Training loss: 0.00050974
[10/200] Training loss: 0.00049158
[50/200] Training loss: 0.00025530
[100/200] Training loss: 0.00019931
[150/200] Training loss: 0.00014750
[200/200] Training loss: 0.00012517
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10631.251666666536 ----------
[1/200] Training loss: 0.15186282
[2/200] Training loss: 0.05198365
[3/200] Training loss: 0.04749351
[4/200] Training loss: 0.04543253
[5/200] Training loss: 0.03811472
[6/200] Training loss: 0.03654839
[7/200] Training loss: 0.03194907
[8/200] Training loss: 0.03061995
[9/200] Training loss: 0.02996116
[10/200] Training loss: 0.02661921
[50/200] Training loss: 0.01802828
[100/200] Training loss: 0.01540060
[150/200] Training loss: 0.01397742
[200/200] Training loss: 0.01367469
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6866.691488628276 ----------
[1/200] Training loss: 0.11461013
[2/200] Training loss: 0.05082086
[3/200] Training loss: 0.04690031
[4/200] Training loss: 0.04238489
[5/200] Training loss: 0.04029262
[6/200] Training loss: 0.03688273
[7/200] Training loss: 0.03241327
[8/200] Training loss: 0.03093406
[9/200] Training loss: 0.03033460
[10/200] Training loss: 0.02816243
[50/200] Training loss: 0.01694305
[100/200] Training loss: 0.01378378
[150/200] Training loss: 0.01264606
[200/200] Training loss: 0.01179653
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5885.401940394556 ----------
[1/200] Training loss: 0.03557576
[2/200] Training loss: 0.00157850
[3/200] Training loss: 0.00145458
[4/200] Training loss: 0.00125399
[5/200] Training loss: 0.00106530
[6/200] Training loss: 0.00093789
[7/200] Training loss: 0.00084961
[8/200] Training loss: 0.00072920
[9/200] Training loss: 0.00069948
[10/200] Training loss: 0.00061958
[50/200] Training loss: 0.00025439
[100/200] Training loss: 0.00017281
[150/200] Training loss: 0.00013984
[200/200] Training loss: 0.00013383
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11529.374657803432 ----------
[1/200] Training loss: 0.19249923
[2/200] Training loss: 0.06126848
[3/200] Training loss: 0.05673122
[4/200] Training loss: 0.05083809
[5/200] Training loss: 0.04784051
[6/200] Training loss: 0.04864446
[7/200] Training loss: 0.04481129
[8/200] Training loss: 0.04064000
[9/200] Training loss: 0.04079278
[10/200] Training loss: 0.03858304
[50/200] Training loss: 0.01828293
[100/200] Training loss: 0.01597220
[150/200] Training loss: 0.01460303
[200/200] Training loss: 0.01467983
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10568.77400647776 ----------
[1/200] Training loss: 0.23669833
[2/200] Training loss: 0.03864023
[3/200] Training loss: 0.03421901
[4/200] Training loss: 0.03192516
[5/200] Training loss: 0.02981816
[6/200] Training loss: 0.02910278
[7/200] Training loss: 0.02871477
[8/200] Training loss: 0.02792406
[9/200] Training loss: 0.02770660
[10/200] Training loss: 0.02757515
[50/200] Training loss: 0.02514077
[100/200] Training loss: 0.02438896
[150/200] Training loss: 0.02402303
[200/200] Training loss: 0.02366348
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Rprop ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 20369.834167218938 ----------
[1/200] Training loss: 0.11039995
[2/200] Training loss: 0.05722322
[3/200] Training loss: 0.05201402
[4/200] Training loss: 0.04869570
[5/200] Training loss: 0.04457087
[6/200] Training loss: 0.04185634
[7/200] Training loss: 0.03647988
[8/200] Training loss: 0.03397663
[9/200] Training loss: 0.03186892
[10/200] Training loss: 0.02877203
[50/200] Training loss: 0.01673684
[100/200] Training loss: 0.01433239
[150/200] Training loss: 0.01388251
[200/200] Training loss: 0.01270365
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8129.092200239827 ----------
[1/200] Training loss: 0.09228415
[2/200] Training loss: 0.05195087
[3/200] Training loss: 0.04845243
[4/200] Training loss: 0.04327219
[5/200] Training loss: 0.03961344
[6/200] Training loss: 0.03567832
[7/200] Training loss: 0.03271084
[8/200] Training loss: 0.02982622
[9/200] Training loss: 0.02626538
[10/200] Training loss: 0.02666105
[50/200] Training loss: 0.01484451
[100/200] Training loss: 0.01275609
[150/200] Training loss: 0.01135110
[200/200] Training loss: 0.01087020
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10719.153697937165 ----------
[1/200] Training loss: 0.14844065
[2/200] Training loss: 0.05717901
[3/200] Training loss: 0.04914844
[4/200] Training loss: 0.04395426
[5/200] Training loss: 0.04322357
[6/200] Training loss: 0.03643704
[7/200] Training loss: 0.03465034
[8/200] Training loss: 0.03141498
[9/200] Training loss: 0.03248392
[10/200] Training loss: 0.03001137
[50/200] Training loss: 0.01865133
[100/200] Training loss: 0.01581727
[150/200] Training loss: 0.01432649
[200/200] Training loss: 0.01330647
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17029.78848958495 ----------
[1/200] Training loss: 0.14921178
[2/200] Training loss: 0.05879969
[3/200] Training loss: 0.05025225
[4/200] Training loss: 0.04740832
[5/200] Training loss: 0.04436233
[6/200] Training loss: 0.04315280
[7/200] Training loss: 0.04281916
[8/200] Training loss: 0.03938445
[9/200] Training loss: 0.03837160
[10/200] Training loss: 0.03536905
[50/200] Training loss: 0.01895660
[100/200] Training loss: 0.01561408
[150/200] Training loss: 0.01316873
[200/200] Training loss: 0.01199426
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 18580.01851452253 ----------
[1/200] Training loss: 0.14615345
[2/200] Training loss: 0.05658045
[3/200] Training loss: 0.05162370
[4/200] Training loss: 0.04984810
[5/200] Training loss: 0.04273229
[6/200] Training loss: 0.04387648
[7/200] Training loss: 0.04203765
[8/200] Training loss: 0.04139339
[9/200] Training loss: 0.03978780
[10/200] Training loss: 0.03629467
[50/200] Training loss: 0.01944293
[100/200] Training loss: 0.01647540
[150/200] Training loss: 0.01469761
[200/200] Training loss: 0.01380501
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12815.435068697434 ----------
[1/200] Training loss: 0.10911727
[2/200] Training loss: 0.05084701
[3/200] Training loss: 0.04597765
[4/200] Training loss: 0.04286952
[5/200] Training loss: 0.04065944
[6/200] Training loss: 0.03672187
[7/200] Training loss: 0.03372436
[8/200] Training loss: 0.03268240
[9/200] Training loss: 0.03075002
[10/200] Training loss: 0.02848088
[50/200] Training loss: 0.01604102
[100/200] Training loss: 0.01275399
[150/200] Training loss: 0.01147525
[200/200] Training loss: 0.01099322
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7727.16921000181 ----------
[1/200] Training loss: 0.15979261
[2/200] Training loss: 0.05772864
[3/200] Training loss: 0.05503165
[4/200] Training loss: 0.04980180
[5/200] Training loss: 0.04602609
[6/200] Training loss: 0.04607717
[7/200] Training loss: 0.03876231
[8/200] Training loss: 0.03683761
[9/200] Training loss: 0.03417556
[10/200] Training loss: 0.03150226
[50/200] Training loss: 0.01894870
[100/200] Training loss: 0.01552966
[150/200] Training loss: 0.01382876
[200/200] Training loss: 0.01323586
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8278.981096729234 ----------
[1/200] Training loss: 0.11339499
[2/200] Training loss: 0.05615113
[3/200] Training loss: 0.04699658
[4/200] Training loss: 0.04475730
[5/200] Training loss: 0.03954976
[6/200] Training loss: 0.03575696
[7/200] Training loss: 0.03154063
[8/200] Training loss: 0.03123149
[9/200] Training loss: 0.02755808
[10/200] Training loss: 0.02725683
[50/200] Training loss: 0.01531059
[100/200] Training loss: 0.01282947
[150/200] Training loss: 0.01201006
[200/200] Training loss: 0.01126680
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9762.838111942654 ----------
[1/200] Training loss: 0.17474984
[2/200] Training loss: 0.05938768
[3/200] Training loss: 0.05552164
[4/200] Training loss: 0.04997451
[5/200] Training loss: 0.04721124
[6/200] Training loss: 0.04564063
[7/200] Training loss: 0.04050493
[8/200] Training loss: 0.04061087
[9/200] Training loss: 0.03675665
[10/200] Training loss: 0.03404983
[50/200] Training loss: 0.01933495
[100/200] Training loss: 0.01584084
[150/200] Training loss: 0.01410792
[200/200] Training loss: 0.01332268
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5073.545111655163 ----------
[1/200] Training loss: 0.17078133
[2/200] Training loss: 0.05720788
[3/200] Training loss: 0.05181898
[4/200] Training loss: 0.04881220
[5/200] Training loss: 0.04828833
[6/200] Training loss: 0.04600206
[7/200] Training loss: 0.04523587
[8/200] Training loss: 0.04122713
[9/200] Training loss: 0.03913487
[10/200] Training loss: 0.03913093
[50/200] Training loss: 0.01838662
[100/200] Training loss: 0.01736901
[150/200] Training loss: 0.01572637
[200/200] Training loss: 0.01462564
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14074.278382922515 ----------
[1/200] Training loss: 0.11170393
[2/200] Training loss: 0.04809368
[3/200] Training loss: 0.04315997
[4/200] Training loss: 0.03620095
[5/200] Training loss: 0.03368624
[6/200] Training loss: 0.03011424
[7/200] Training loss: 0.03083905
[8/200] Training loss: 0.02847683
[9/200] Training loss: 0.02664575
[10/200] Training loss: 0.02641788
[50/200] Training loss: 0.01664392
[100/200] Training loss: 0.01323264
[150/200] Training loss: 0.01199497
[200/200] Training loss: 0.01129261
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 3794.790771570944 ----------
[1/200] Training loss: 0.10491244
[2/200] Training loss: 0.05426319
[3/200] Training loss: 0.04725640
[4/200] Training loss: 0.04395143
[5/200] Training loss: 0.03779591
[6/200] Training loss: 0.03115796
[7/200] Training loss: 0.02788639
[8/200] Training loss: 0.02785395
[9/200] Training loss: 0.02562470
[10/200] Training loss: 0.02477311
[50/200] Training loss: 0.01529892
[100/200] Training loss: 0.01316715
[150/200] Training loss: 0.01223480
[200/200] Training loss: 0.01112449
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13873.080119425535 ----------
[1/200] Training loss: 0.12155704
[2/200] Training loss: 0.05746044
[3/200] Training loss: 0.04902064
[4/200] Training loss: 0.04439153
[5/200] Training loss: 0.04048828
[6/200] Training loss: 0.04040603
[7/200] Training loss: 0.03695802
[8/200] Training loss: 0.03464798
[9/200] Training loss: 0.03359745
[10/200] Training loss: 0.03176026
[50/200] Training loss: 0.01802402
[100/200] Training loss: 0.01602460
[150/200] Training loss: 0.01373599
[200/200] Training loss: 0.01246986
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11936.642744088473 ----------
[1/200] Training loss: 0.16089810
[2/200] Training loss: 0.06050454
[3/200] Training loss: 0.05362706
[4/200] Training loss: 0.05109801
[5/200] Training loss: 0.05093380
[6/200] Training loss: 0.04611121
[7/200] Training loss: 0.04506106
[8/200] Training loss: 0.04417502
[9/200] Training loss: 0.04265059
[10/200] Training loss: 0.04176532
[50/200] Training loss: 0.01765062
[100/200] Training loss: 0.01561419
[150/200] Training loss: 0.01390471
[200/200] Training loss: 0.01304519
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6340.8750184812825 ----------
[1/200] Training loss: 0.10439481
[2/200] Training loss: 0.05211988
[3/200] Training loss: 0.04709043
[4/200] Training loss: 0.04289712
[5/200] Training loss: 0.03795736
[6/200] Training loss: 0.03325586
[7/200] Training loss: 0.03175754
[8/200] Training loss: 0.02829575
[9/200] Training loss: 0.02626392
[10/200] Training loss: 0.02522434
[50/200] Training loss: 0.01559050
[100/200] Training loss: 0.01347908
[150/200] Training loss: 0.01242069
[200/200] Training loss: 0.01172328
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8080.481173791571 ----------
[1/200] Training loss: 0.15532211
[2/200] Training loss: 0.05873330
[3/200] Training loss: 0.05265149
[4/200] Training loss: 0.04883217
[5/200] Training loss: 0.04784729
[6/200] Training loss: 0.04090608
[7/200] Training loss: 0.03953329
[8/200] Training loss: 0.03678057
[9/200] Training loss: 0.03638305
[10/200] Training loss: 0.03535014
[50/200] Training loss: 0.01945384
[100/200] Training loss: 0.01527641
[150/200] Training loss: 0.01375354
[200/200] Training loss: 0.01307185
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11337.408169418617 ----------
[1/200] Training loss: 0.09910182
[2/200] Training loss: 0.04592749
[3/200] Training loss: 0.04065130
[4/200] Training loss: 0.03546989
[5/200] Training loss: 0.03077748
[6/200] Training loss: 0.02960808
[7/200] Training loss: 0.02749924
[8/200] Training loss: 0.02560057
[9/200] Training loss: 0.02450829
[10/200] Training loss: 0.02436543
[50/200] Training loss: 0.01758130
[100/200] Training loss: 0.01530448
[150/200] Training loss: 0.01363292
[200/200] Training loss: 0.01217658
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10240.903866358673 ----------
[1/200] Training loss: 0.15877586
[2/200] Training loss: 0.05989402
[3/200] Training loss: 0.05458334
[4/200] Training loss: 0.04827728
[5/200] Training loss: 0.04600713
[6/200] Training loss: 0.04412827
[7/200] Training loss: 0.04437583
[8/200] Training loss: 0.03959651
[9/200] Training loss: 0.03688575
[10/200] Training loss: 0.03497012
[50/200] Training loss: 0.01861607
[100/200] Training loss: 0.01615745
[150/200] Training loss: 0.01447937
[200/200] Training loss: 0.01397143
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14718.135751514184 ----------
[1/200] Training loss: 0.16988848
[2/200] Training loss: 0.05813798
[3/200] Training loss: 0.05233157
[4/200] Training loss: 0.05173860
[5/200] Training loss: 0.04567674
[6/200] Training loss: 0.04149361
[7/200] Training loss: 0.03894605
[8/200] Training loss: 0.03446622
[9/200] Training loss: 0.03365617
[10/200] Training loss: 0.03026781
[50/200] Training loss: 0.01843120
[100/200] Training loss: 0.01599069
[150/200] Training loss: 0.01443676
[200/200] Training loss: 0.01366955
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12187.962586092886 ----------
[1/200] Training loss: 0.17194173
[2/200] Training loss: 0.06592271
[3/200] Training loss: 0.05234817
[4/200] Training loss: 0.04859527
[5/200] Training loss: 0.05095292
[6/200] Training loss: 0.04790289
[7/200] Training loss: 0.04476747
[8/200] Training loss: 0.04279166
[9/200] Training loss: 0.04140007
[10/200] Training loss: 0.03816520
[50/200] Training loss: 0.01893679
[100/200] Training loss: 0.01674375
[150/200] Training loss: 0.01470063
[200/200] Training loss: 0.01398684
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5367.736580720034 ----------
[1/200] Training loss: 0.18606843
[2/200] Training loss: 0.05891283
[3/200] Training loss: 0.05313175
[4/200] Training loss: 0.04748541
[5/200] Training loss: 0.04425368
[6/200] Training loss: 0.04282101
[7/200] Training loss: 0.04110791
[8/200] Training loss: 0.03891828
[9/200] Training loss: 0.04029894
[10/200] Training loss: 0.03491737
[50/200] Training loss: 0.01955241
[100/200] Training loss: 0.01717763
[150/200] Training loss: 0.01532195
[200/200] Training loss: 0.01407726
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14151.290541855184 ----------
[1/200] Training loss: 0.16558766
[2/200] Training loss: 0.05853626
[3/200] Training loss: 0.05515777
[4/200] Training loss: 0.04882553
[5/200] Training loss: 0.04736251
[6/200] Training loss: 0.04241612
[7/200] Training loss: 0.04153079
[8/200] Training loss: 0.03830923
[9/200] Training loss: 0.03434632
[10/200] Training loss: 0.03187410
[50/200] Training loss: 0.01578991
[100/200] Training loss: 0.01450585
[150/200] Training loss: 0.01288875
[200/200] Training loss: 0.01258833
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16489.719463956928 ----------
[1/200] Training loss: 0.17004104
[2/200] Training loss: 0.05799006
[3/200] Training loss: 0.05333870
[4/200] Training loss: 0.05124521
[5/200] Training loss: 0.04903890
[6/200] Training loss: 0.04523513
[7/200] Training loss: 0.04333251
[8/200] Training loss: 0.04101669
[9/200] Training loss: 0.03929635
[10/200] Training loss: 0.03664595
[50/200] Training loss: 0.01947007
[100/200] Training loss: 0.01598117
[150/200] Training loss: 0.01468703
[200/200] Training loss: 0.01300832
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7371.080246476768 ----------
[1/200] Training loss: 0.14924914
[2/200] Training loss: 0.05906295
[3/200] Training loss: 0.05066846
[4/200] Training loss: 0.04781667
[5/200] Training loss: 0.04211083
[6/200] Training loss: 0.03598149
[7/200] Training loss: 0.03247725
[8/200] Training loss: 0.03051605
[9/200] Training loss: 0.03165842
[10/200] Training loss: 0.02922485
[50/200] Training loss: 0.01870971
[100/200] Training loss: 0.01555138
[150/200] Training loss: 0.01444960
[200/200] Training loss: 0.01336448
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10243.539232120898 ----------
[1/200] Training loss: 0.11952886
[2/200] Training loss: 0.05568362
[3/200] Training loss: 0.05012052
[4/200] Training loss: 0.04773764
[5/200] Training loss: 0.04088232
[6/200] Training loss: 0.03766385
[7/200] Training loss: 0.03304516
[8/200] Training loss: 0.02942417
[9/200] Training loss: 0.02741905
[10/200] Training loss: 0.02614010
[50/200] Training loss: 0.01722834
[100/200] Training loss: 0.01466563
[150/200] Training loss: 0.01332252
[200/200] Training loss: 0.01212978
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6258.652890199296 ----------
[1/200] Training loss: 0.14446093
[2/200] Training loss: 0.05341013
[3/200] Training loss: 0.04884543
[4/200] Training loss: 0.04528472
[5/200] Training loss: 0.04093506
[6/200] Training loss: 0.03837149
[7/200] Training loss: 0.03507266
[8/200] Training loss: 0.03433162
[9/200] Training loss: 0.03213603
[10/200] Training loss: 0.02896781
[50/200] Training loss: 0.01783697
[100/200] Training loss: 0.01560662
[150/200] Training loss: 0.01474055
[200/200] Training loss: 0.01337769
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14448.673849180761 ----------
[1/200] Training loss: 0.17177043
[2/200] Training loss: 0.05975845
[3/200] Training loss: 0.05483029
[4/200] Training loss: 0.05019029
[5/200] Training loss: 0.04468655
[6/200] Training loss: 0.04334154
[7/200] Training loss: 0.04091209
[8/200] Training loss: 0.03755139
[9/200] Training loss: 0.03695840
[10/200] Training loss: 0.03245180
[50/200] Training loss: 0.01856772
[100/200] Training loss: 0.01487783
[150/200] Training loss: 0.01434383
[200/200] Training loss: 0.01355425
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17639.333320735226 ----------
[1/200] Training loss: 0.11453423
[2/200] Training loss: 0.05470690
[3/200] Training loss: 0.05021366
[4/200] Training loss: 0.04835060
[5/200] Training loss: 0.04345618
[6/200] Training loss: 0.04045609
[7/200] Training loss: 0.03574723
[8/200] Training loss: 0.03455166
[9/200] Training loss: 0.03256387
[10/200] Training loss: 0.03068584
[50/200] Training loss: 0.01568261
[100/200] Training loss: 0.01319350
[150/200] Training loss: 0.01129940
[200/200] Training loss: 0.01071789
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16558.934265223714 ----------
[1/200] Training loss: 0.16951662
[2/200] Training loss: 0.06623049
[3/200] Training loss: 0.05423903
[4/200] Training loss: 0.05299387
[5/200] Training loss: 0.04956910
[6/200] Training loss: 0.05093746
[7/200] Training loss: 0.04813249
[8/200] Training loss: 0.04395386
[9/200] Training loss: 0.04257862
[10/200] Training loss: 0.04154147
[50/200] Training loss: 0.02213475
[100/200] Training loss: 0.01705800
[150/200] Training loss: 0.01468490
[200/200] Training loss: 0.01242274
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7251.82542536705 ----------
[1/200] Training loss: 0.16125690
[2/200] Training loss: 0.05611998
[3/200] Training loss: 0.04865469
[4/200] Training loss: 0.04792850
[5/200] Training loss: 0.04171060
[6/200] Training loss: 0.04094625
[7/200] Training loss: 0.03853681
[8/200] Training loss: 0.03825292
[9/200] Training loss: 0.03509129
[10/200] Training loss: 0.03566762
[50/200] Training loss: 0.02013514
[100/200] Training loss: 0.01587348
[150/200] Training loss: 0.01395805
[200/200] Training loss: 0.01251960
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10424.626228311497 ----------
[1/200] Training loss: 0.14007671
[2/200] Training loss: 0.05419459
[3/200] Training loss: 0.04665899
[4/200] Training loss: 0.04394446
[5/200] Training loss: 0.04009852
[6/200] Training loss: 0.03754923
[7/200] Training loss: 0.03669049
[8/200] Training loss: 0.03416323
[9/200] Training loss: 0.03307595
[10/200] Training loss: 0.03168992
[50/200] Training loss: 0.02020020
[100/200] Training loss: 0.01450439
[150/200] Training loss: 0.01234880
[200/200] Training loss: 0.01210404
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5194.628379393467 ----------
[1/200] Training loss: 0.12778931
[2/200] Training loss: 0.05267743
[3/200] Training loss: 0.05009214
[4/200] Training loss: 0.04393183
[5/200] Training loss: 0.04277000
[6/200] Training loss: 0.04099398
[7/200] Training loss: 0.03773560
[8/200] Training loss: 0.03989848
[9/200] Training loss: 0.03753719
[10/200] Training loss: 0.03365393
[50/200] Training loss: 0.01683616
[100/200] Training loss: 0.01337006
[150/200] Training loss: 0.01281354
[200/200] Training loss: 0.01177491
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7345.793354022423 ----------
[1/200] Training loss: 0.16976321
[2/200] Training loss: 0.06133725
[3/200] Training loss: 0.05208793
[4/200] Training loss: 0.04998107
[5/200] Training loss: 0.04725187
[6/200] Training loss: 0.04314020
[7/200] Training loss: 0.04087562
[8/200] Training loss: 0.04080674
[9/200] Training loss: 0.03544309
[10/200] Training loss: 0.03517253
[50/200] Training loss: 0.01815855
[100/200] Training loss: 0.01623025
[150/200] Training loss: 0.01443886
[200/200] Training loss: 0.01309520
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13009.59276841516 ----------
[1/200] Training loss: 0.12127293
[2/200] Training loss: 0.06424658
[3/200] Training loss: 0.05483244
[4/200] Training loss: 0.05466921
[5/200] Training loss: 0.05056298
[6/200] Training loss: 0.04846226
[7/200] Training loss: 0.04478330
[8/200] Training loss: 0.04709147
[9/200] Training loss: 0.04225094
[10/200] Training loss: 0.03827205
[50/200] Training loss: 0.01646411
[100/200] Training loss: 0.01411031
[150/200] Training loss: 0.01258897
[200/200] Training loss: 0.01173162
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 19413.1378195283 ----------
[1/200] Training loss: 0.14001368
[2/200] Training loss: 0.05223008
[3/200] Training loss: 0.04800850
[4/200] Training loss: 0.04454651
[5/200] Training loss: 0.04079039
[6/200] Training loss: 0.03757927
[7/200] Training loss: 0.03607663
[8/200] Training loss: 0.03284966
[9/200] Training loss: 0.03296729
[10/200] Training loss: 0.03072780
[50/200] Training loss: 0.01988949
[100/200] Training loss: 0.01643311
[150/200] Training loss: 0.01566292
[200/200] Training loss: 0.01429875
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10928.842573667167 ----------
[1/200] Training loss: 0.15054310
[2/200] Training loss: 0.05924667
[3/200] Training loss: 0.04983779
[4/200] Training loss: 0.04798808
[5/200] Training loss: 0.04646906
[6/200] Training loss: 0.04569398
[7/200] Training loss: 0.04143087
[8/200] Training loss: 0.03889106
[9/200] Training loss: 0.03856411
[10/200] Training loss: 0.03488858
[50/200] Training loss: 0.01779751
[100/200] Training loss: 0.01453088
[150/200] Training loss: 0.01285725
[200/200] Training loss: 0.01216545
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7165.965392045931 ----------
[1/200] Training loss: 0.15250046
[2/200] Training loss: 0.05869755
[3/200] Training loss: 0.05214490
[4/200] Training loss: 0.04769715
[5/200] Training loss: 0.04560603
[6/200] Training loss: 0.04068603
[7/200] Training loss: 0.04147725
[8/200] Training loss: 0.03723187
[9/200] Training loss: 0.03324811
[10/200] Training loss: 0.03226695
[50/200] Training loss: 0.01920232
[100/200] Training loss: 0.01559371
[150/200] Training loss: 0.01425065
[200/200] Training loss: 0.01282920
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.028374503508948885
----FITNESS-----------RMSE---- 19368.325482601744 ----------
[1/200] Training loss: 0.13959494
[2/200] Training loss: 0.05578104
[3/200] Training loss: 0.05328023
[4/200] Training loss: 0.04949776
[5/200] Training loss: 0.04535863
[6/200] Training loss: 0.04583962
[7/200] Training loss: 0.04036116
[8/200] Training loss: 0.03608675
[9/200] Training loss: 0.03699399
[10/200] Training loss: 0.03680709
[50/200] Training loss: 0.01947032
[100/200] Training loss: 0.01602439
[150/200] Training loss: 0.01403557
[200/200] Training loss: 0.01304726
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14494.306744373805 ----------
[1/200] Training loss: 0.10982351
[2/200] Training loss: 0.05619627
[3/200] Training loss: 0.04787100
[4/200] Training loss: 0.04379722
[5/200] Training loss: 0.03771630
[6/200] Training loss: 0.03510571
[7/200] Training loss: 0.03089434
[8/200] Training loss: 0.02923840
[9/200] Training loss: 0.02819765
[10/200] Training loss: 0.02693170
[50/200] Training loss: 0.01595893
[100/200] Training loss: 0.01367653
[150/200] Training loss: 0.01213791
[200/200] Training loss: 0.01142864
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14729.351920570029 ----------
[1/200] Training loss: 0.12868661
[2/200] Training loss: 0.05390902
[3/200] Training loss: 0.05116411
[4/200] Training loss: 0.04405896
[5/200] Training loss: 0.04107914
[6/200] Training loss: 0.03969463
[7/200] Training loss: 0.03786227
[8/200] Training loss: 0.03587286
[9/200] Training loss: 0.03107135
[10/200] Training loss: 0.02945479
[50/200] Training loss: 0.01928929
[100/200] Training loss: 0.01590518
[150/200] Training loss: 0.01431001
[200/200] Training loss: 0.01323653
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 22305.959741737184 ----------
[1/200] Training loss: 0.15577926
[2/200] Training loss: 0.05481432
[3/200] Training loss: 0.04774990
[4/200] Training loss: 0.04452121
[5/200] Training loss: 0.03956432
[6/200] Training loss: 0.03481426
[7/200] Training loss: 0.03392067
[8/200] Training loss: 0.03030062
[9/200] Training loss: 0.03048306
[10/200] Training loss: 0.02855741
[50/200] Training loss: 0.01841322
[100/200] Training loss: 0.01635744
[150/200] Training loss: 0.01416847
[200/200] Training loss: 0.01381463
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7173.027533754488 ----------
[1/200] Training loss: 0.16808718
[2/200] Training loss: 0.06176365
[3/200] Training loss: 0.05618889
[4/200] Training loss: 0.05349986
[5/200] Training loss: 0.05038300
[6/200] Training loss: 0.04912454
[7/200] Training loss: 0.04773402
[8/200] Training loss: 0.04621674
[9/200] Training loss: 0.04061997
[10/200] Training loss: 0.04109733
[50/200] Training loss: 0.01874356
[100/200] Training loss: 0.01641739
[150/200] Training loss: 0.01471974
[200/200] Training loss: 0.01448024
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14652.432426051315 ----------
[1/200] Training loss: 0.16540999
[2/200] Training loss: 0.06222319
[3/200] Training loss: 0.04973952
[4/200] Training loss: 0.04763335
[5/200] Training loss: 0.04304840
[6/200] Training loss: 0.04025620
[7/200] Training loss: 0.03548083
[8/200] Training loss: 0.03296270
[9/200] Training loss: 0.03093187
[10/200] Training loss: 0.02993942
[50/200] Training loss: 0.01871560
[100/200] Training loss: 0.01558474
[150/200] Training loss: 0.01492271
[200/200] Training loss: 0.01409223
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11462.360664365784 ----------
[1/200] Training loss: 0.14693099
[2/200] Training loss: 0.05347015
[3/200] Training loss: 0.04914600
[4/200] Training loss: 0.04540518
[5/200] Training loss: 0.04327734
[6/200] Training loss: 0.04030479
[7/200] Training loss: 0.03841968
[8/200] Training loss: 0.03634972
[9/200] Training loss: 0.03427906
[10/200] Training loss: 0.03242084
[50/200] Training loss: 0.02004819
[100/200] Training loss: 0.01487612
[150/200] Training loss: 0.01371212
[200/200] Training loss: 0.01295001
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6385.439060863396 ----------
[1/200] Training loss: 0.16886328
[2/200] Training loss: 0.06075076
[3/200] Training loss: 0.05125594
[4/200] Training loss: 0.05088350
[5/200] Training loss: 0.04643405
[6/200] Training loss: 0.04552249
[7/200] Training loss: 0.04197145
[8/200] Training loss: 0.03933338
[9/200] Training loss: 0.03579025
[10/200] Training loss: 0.03430541
[50/200] Training loss: 0.01656260
[100/200] Training loss: 0.01417213
[150/200] Training loss: 0.01278512
[200/200] Training loss: 0.01182527
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6683.718725380355 ----------
[1/200] Training loss: 0.15469808
[2/200] Training loss: 0.05569466
[3/200] Training loss: 0.05436063
[4/200] Training loss: 0.04814530
[5/200] Training loss: 0.04485096
[6/200] Training loss: 0.04353984
[7/200] Training loss: 0.04162261
[8/200] Training loss: 0.03759872
[9/200] Training loss: 0.03958139
[10/200] Training loss: 0.03568680
[50/200] Training loss: 0.01917383
[100/200] Training loss: 0.01515793
[150/200] Training loss: 0.01323631
[200/200] Training loss: 0.01242167
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15588.685127360806 ----------
[1/50] Training loss: 0.09621296
[2/50] Training loss: 0.04532736
[3/50] Training loss: 0.03916744
[4/50] Training loss: 0.03615138
[5/50] Training loss: 0.03223792
[6/50] Training loss: 0.03091922
[7/50] Training loss: 0.02743899
[8/50] Training loss: 0.02795201
[9/50] Training loss: 0.02623403
[10/50] Training loss: 0.02512214
[50/50] Training loss: 0.01562507
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 50
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10333.450924062106 ----------
[1/200] Training loss: 0.15523425
[2/200] Training loss: 0.06005567
[3/200] Training loss: 0.05285976
[4/200] Training loss: 0.04987467
[5/200] Training loss: 0.04698095
[6/200] Training loss: 0.04381265
[7/200] Training loss: 0.04160538
[8/200] Training loss: 0.04049236
[9/200] Training loss: 0.03726877
[10/200] Training loss: 0.03685985
[50/200] Training loss: 0.01754920
[100/200] Training loss: 0.01475433
[150/200] Training loss: 0.01295079
[200/200] Training loss: 0.01181775
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14584.58144754247 ----------
[1/200] Training loss: 0.17635394
[2/200] Training loss: 0.06262661
[3/200] Training loss: 0.05664193
[4/200] Training loss: 0.05573045
[5/200] Training loss: 0.05377038
[6/200] Training loss: 0.05172717
[7/200] Training loss: 0.05059581
[8/200] Training loss: 0.04973720
[9/200] Training loss: 0.04827114
[10/200] Training loss: 0.04514556
[50/200] Training loss: 0.02010500
[100/200] Training loss: 0.01709380
[150/200] Training loss: 0.01559175
[200/200] Training loss: 0.01387119
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5665.308288169321 ----------
[1/200] Training loss: 0.11110294
[2/200] Training loss: 0.04936637
[3/200] Training loss: 0.03913491
[4/200] Training loss: 0.03666386
[5/200] Training loss: 0.03416607
[6/200] Training loss: 0.03078041
[7/200] Training loss: 0.02996622
[8/200] Training loss: 0.02921826
[9/200] Training loss: 0.02877267
[10/200] Training loss: 0.02692930
[50/200] Training loss: 0.01536068
[100/200] Training loss: 0.01327343
[150/200] Training loss: 0.01240434
[200/200] Training loss: 0.01117031
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8857.402327996624 ----------
[1/200] Training loss: 0.14948750
[2/200] Training loss: 0.06034437
[3/200] Training loss: 0.05364453
[4/200] Training loss: 0.05201155
[5/200] Training loss: 0.04819348
[6/200] Training loss: 0.04383026
[7/200] Training loss: 0.04029257
[8/200] Training loss: 0.03994364
[9/200] Training loss: 0.03501716
[10/200] Training loss: 0.03560639
[50/200] Training loss: 0.01790538
[100/200] Training loss: 0.01450734
[150/200] Training loss: 0.01345865
[200/200] Training loss: 0.01255615
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16501.004090660666 ----------
[1/200] Training loss: 0.11061337
[2/200] Training loss: 0.05228601
[3/200] Training loss: 0.04422386
[4/200] Training loss: 0.04005404
[5/200] Training loss: 0.03378786
[6/200] Training loss: 0.03130194
[7/200] Training loss: 0.02778879
[8/200] Training loss: 0.02631073
[9/200] Training loss: 0.02451551
[10/200] Training loss: 0.02341533
[50/200] Training loss: 0.01579951
[100/200] Training loss: 0.01246327
[150/200] Training loss: 0.01108854
[200/200] Training loss: 0.01050468
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15186.858529663072 ----------
[1/200] Training loss: 0.11238901
[2/200] Training loss: 0.05299032
[3/200] Training loss: 0.04577634
[4/200] Training loss: 0.04141641
[5/200] Training loss: 0.03805651
[6/200] Training loss: 0.03441678
[7/200] Training loss: 0.03102328
[8/200] Training loss: 0.03018340
[9/200] Training loss: 0.02827904
[10/200] Training loss: 0.02642924
[50/200] Training loss: 0.01645217
[100/200] Training loss: 0.01450742
[150/200] Training loss: 0.01278374
[200/200] Training loss: 0.01182349
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8162.163928763009 ----------
[1/200] Training loss: 0.16058730
[2/200] Training loss: 0.06188165
[3/200] Training loss: 0.05447466
[4/200] Training loss: 0.05222475
[5/200] Training loss: 0.05056887
[6/200] Training loss: 0.04688236
[7/200] Training loss: 0.04686912
[8/200] Training loss: 0.04347279
[9/200] Training loss: 0.03819271
[10/200] Training loss: 0.03930360
[50/200] Training loss: 0.01849681
[100/200] Training loss: 0.01547056
[150/200] Training loss: 0.01403657
[200/200] Training loss: 0.01348679
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13051.200711045709 ----------
[1/200] Training loss: 0.17428738
[2/200] Training loss: 0.06048648
[3/200] Training loss: 0.05362118
[4/200] Training loss: 0.05017954
[5/200] Training loss: 0.04555931
[6/200] Training loss: 0.04397759
[7/200] Training loss: 0.03928711
[8/200] Training loss: 0.03807909
[9/200] Training loss: 0.03680260
[10/200] Training loss: 0.03485954
[50/200] Training loss: 0.01886394
[100/200] Training loss: 0.01587848
[150/200] Training loss: 0.01532182
[200/200] Training loss: 0.01420674
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12549.215114898621 ----------
[1/200] Training loss: 0.15375615
[2/200] Training loss: 0.05569418
[3/200] Training loss: 0.05250408
[4/200] Training loss: 0.04655846
[5/200] Training loss: 0.04549586
[6/200] Training loss: 0.04376403
[7/200] Training loss: 0.04115707
[8/200] Training loss: 0.03784283
[9/200] Training loss: 0.03622077
[10/200] Training loss: 0.03565132
[50/200] Training loss: 0.01922187
[100/200] Training loss: 0.01631124
[150/200] Training loss: 0.01385377
[200/200] Training loss: 0.01357884
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11090.444175054487 ----------
[1/200] Training loss: 0.16814508
[2/200] Training loss: 0.05686345
[3/200] Training loss: 0.05185020
[4/200] Training loss: 0.04837832
[5/200] Training loss: 0.04640280
[6/200] Training loss: 0.04378016
[7/200] Training loss: 0.04011303
[8/200] Training loss: 0.03711245
[9/200] Training loss: 0.03746792
[10/200] Training loss: 0.03503622
[50/200] Training loss: 0.01793508
[100/200] Training loss: 0.01500110
[150/200] Training loss: 0.01399892
[200/200] Training loss: 0.01301815
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 22029.01214308077 ----------
[1/200] Training loss: 0.13910891
[2/200] Training loss: 0.05492356
[3/200] Training loss: 0.05062535
[4/200] Training loss: 0.04719230
[5/200] Training loss: 0.04322851
[6/200] Training loss: 0.04253229
[7/200] Training loss: 0.03956694
[8/200] Training loss: 0.03852301
[9/200] Training loss: 0.03591632
[10/200] Training loss: 0.03409380
[50/200] Training loss: 0.01965771
[100/200] Training loss: 0.01752386
[150/200] Training loss: 0.01552196
[200/200] Training loss: 0.01430107
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11204.293819781771 ----------
[1/200] Training loss: 0.16287362
[2/200] Training loss: 0.05838092
[3/200] Training loss: 0.05344626
[4/200] Training loss: 0.04945617
[5/200] Training loss: 0.04624159
[6/200] Training loss: 0.04275457
[7/200] Training loss: 0.03771683
[8/200] Training loss: 0.03480020
[9/200] Training loss: 0.03274931
[10/200] Training loss: 0.03617816
[50/200] Training loss: 0.01822843
[100/200] Training loss: 0.01605672
[150/200] Training loss: 0.01438788
[200/200] Training loss: 0.01402277
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8737.438068450041 ----------
[1/200] Training loss: 0.03845358
[2/200] Training loss: 0.00233593
[3/200] Training loss: 0.00202996
[4/200] Training loss: 0.00174659
[5/200] Training loss: 0.00146088
[6/200] Training loss: 0.00115522
[7/200] Training loss: 0.00101103
[8/200] Training loss: 0.00082251
[9/200] Training loss: 0.00071056
[10/200] Training loss: 0.00062620
[50/200] Training loss: 0.00029648
[100/200] Training loss: 0.00024240
[150/200] Training loss: 0.00018957
[200/200] Training loss: 0.00015836
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15947.326797930742 ----------
[1/200] Training loss: 0.13623799
[2/200] Training loss: 0.05710862
[3/200] Training loss: 0.05143628
[4/200] Training loss: 0.04660457
[5/200] Training loss: 0.04408118
[6/200] Training loss: 0.03795519
[7/200] Training loss: 0.03731403
[8/200] Training loss: 0.03325578
[9/200] Training loss: 0.03338195
[10/200] Training loss: 0.03074372
[50/200] Training loss: 0.01695754
[100/200] Training loss: 0.01543826
[150/200] Training loss: 0.01423277
[200/200] Training loss: 0.01241573
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10479.14500329106 ----------
[1/200] Training loss: 0.18267692
[2/200] Training loss: 0.06277673
[3/200] Training loss: 0.05252377
[4/200] Training loss: 0.05079092
[5/200] Training loss: 0.04581005
[6/200] Training loss: 0.04313153
[7/200] Training loss: 0.04320603
[8/200] Training loss: 0.03842978
[9/200] Training loss: 0.03839560
[10/200] Training loss: 0.03676283
[50/200] Training loss: 0.01912672
[100/200] Training loss: 0.01579993
[150/200] Training loss: 0.01383344
[200/200] Training loss: 0.01257924
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7546.312476965157 ----------
[1/200] Training loss: 0.15321667
[2/200] Training loss: 0.05844402
[3/200] Training loss: 0.05298282
[4/200] Training loss: 0.04980162
[5/200] Training loss: 0.04778303
[6/200] Training loss: 0.04429853
[7/200] Training loss: 0.04286796
[8/200] Training loss: 0.03790275
[9/200] Training loss: 0.03718702
[10/200] Training loss: 0.03396566
[50/200] Training loss: 0.01763679
[100/200] Training loss: 0.01542948
[150/200] Training loss: 0.01360697
[200/200] Training loss: 0.01252574
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14481.137524379776 ----------
[1/200] Training loss: 0.15851570
[2/200] Training loss: 0.05807312
[3/200] Training loss: 0.05158876
[4/200] Training loss: 0.05010654
[5/200] Training loss: 0.04744928
[6/200] Training loss: 0.04771470
[7/200] Training loss: 0.04263425
[8/200] Training loss: 0.04184873
[9/200] Training loss: 0.03863875
[10/200] Training loss: 0.03792768
[50/200] Training loss: 0.01742967
[100/200] Training loss: 0.01420605
[150/200] Training loss: 0.01354588
[200/200] Training loss: 0.01204644
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5402.054424013146 ----------
[1/200] Training loss: 0.16140633
[2/200] Training loss: 0.05515319
[3/200] Training loss: 0.04669769
[4/200] Training loss: 0.04692210
[5/200] Training loss: 0.04270178
[6/200] Training loss: 0.03864491
[7/200] Training loss: 0.03680215
[8/200] Training loss: 0.03454238
[9/200] Training loss: 0.03308235
[10/200] Training loss: 0.03222247
[50/200] Training loss: 0.01893538
[100/200] Training loss: 0.01589671
[150/200] Training loss: 0.01404700
[200/200] Training loss: 0.01404408
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5782.006572116638 ----------
[1/200] Training loss: 0.18299844
[2/200] Training loss: 0.06029193
[3/200] Training loss: 0.05414258
[4/200] Training loss: 0.05236602
[5/200] Training loss: 0.04681473
[6/200] Training loss: 0.04632046
[7/200] Training loss: 0.04343134
[8/200] Training loss: 0.04235754
[9/200] Training loss: 0.04109925
[10/200] Training loss: 0.03907945
[50/200] Training loss: 0.02009392
[100/200] Training loss: 0.01766853
[150/200] Training loss: 0.01590300
[200/200] Training loss: 0.01490605
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 18834.823917414255 ----------
[1/200] Training loss: 0.15166239
[2/200] Training loss: 0.05551151
[3/200] Training loss: 0.05176125
[4/200] Training loss: 0.04896331
[5/200] Training loss: 0.04523665
[6/200] Training loss: 0.04558760
[7/200] Training loss: 0.04396944
[8/200] Training loss: 0.04153555
[9/200] Training loss: 0.04137053
[10/200] Training loss: 0.03857053
[50/200] Training loss: 0.01865481
[100/200] Training loss: 0.01622650
[150/200] Training loss: 0.01469120
[200/200] Training loss: 0.01341542
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13837.103743197129 ----------
[1/200] Training loss: 0.16712079
[2/200] Training loss: 0.05460767
[3/200] Training loss: 0.05655951
[4/200] Training loss: 0.05119147
[5/200] Training loss: 0.04915978
[6/200] Training loss: 0.04601749
[7/200] Training loss: 0.04356019
[8/200] Training loss: 0.04097019
[9/200] Training loss: 0.04149876
[10/200] Training loss: 0.03752605
[50/200] Training loss: 0.01886163
[100/200] Training loss: 0.01639295
[150/200] Training loss: 0.01282681
[200/200] Training loss: 0.01262877
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11924.925827861573 ----------
[1/200] Training loss: 0.16752483
[2/200] Training loss: 0.06064356
[3/200] Training loss: 0.05657263
[4/200] Training loss: 0.05411275
[5/200] Training loss: 0.05057476
[6/200] Training loss: 0.05148385
[7/200] Training loss: 0.04857869
[8/200] Training loss: 0.04820754
[9/200] Training loss: 0.04506599
[10/200] Training loss: 0.04454791
[50/200] Training loss: 0.01910377
[100/200] Training loss: 0.01694445
[150/200] Training loss: 0.01614342
[200/200] Training loss: 0.01515274
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12374.392914401902 ----------
[1/200] Training loss: 0.14898095
[2/200] Training loss: 0.06142738
[3/200] Training loss: 0.05426169
[4/200] Training loss: 0.05022092
[5/200] Training loss: 0.05133839
[6/200] Training loss: 0.04546963
[7/200] Training loss: 0.04453074
[8/200] Training loss: 0.04250331
[9/200] Training loss: 0.04217560
[10/200] Training loss: 0.03835850
[50/200] Training loss: 0.01975879
[100/200] Training loss: 0.01635835
[150/200] Training loss: 0.01528757
[200/200] Training loss: 0.01414081
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 22429.544088099516 ----------
[1/200] Training loss: 0.12200722
[2/200] Training loss: 0.05432586
[3/200] Training loss: 0.04544754
[4/200] Training loss: 0.03848566
[5/200] Training loss: 0.03619662
[6/200] Training loss: 0.03339746
[7/200] Training loss: 0.03036093
[8/200] Training loss: 0.02924138
[9/200] Training loss: 0.02657011
[10/200] Training loss: 0.02702184
[50/200] Training loss: 0.01740304
[100/200] Training loss: 0.01548713
[150/200] Training loss: 0.01421807
[200/200] Training loss: 0.01349201
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8049.412897845407 ----------
[1/200] Training loss: 0.13688731
[2/200] Training loss: 0.05719697
[3/200] Training loss: 0.05424463
[4/200] Training loss: 0.05153809
[5/200] Training loss: 0.04632778
[6/200] Training loss: 0.04665992
[7/200] Training loss: 0.04066300
[8/200] Training loss: 0.04117694
[9/200] Training loss: 0.03885281
[10/200] Training loss: 0.03683984
[50/200] Training loss: 0.01794595
[100/200] Training loss: 0.01438707
[150/200] Training loss: 0.01315557
[200/200] Training loss: 0.01216386
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4249.995294115041 ----------
[1/200] Training loss: 0.12425518
[2/200] Training loss: 0.05344740
[3/200] Training loss: 0.04737087
[4/200] Training loss: 0.04415524
[5/200] Training loss: 0.03983826
[6/200] Training loss: 0.04202781
[7/200] Training loss: 0.03909987
[8/200] Training loss: 0.03858494
[9/200] Training loss: 0.03580535
[10/200] Training loss: 0.03642407
[50/200] Training loss: 0.01700911
[100/200] Training loss: 0.01451331
[150/200] Training loss: 0.01318597
[200/200] Training loss: 0.01255075
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14814.906007126741 ----------
[1/200] Training loss: 0.15710512
[2/200] Training loss: 0.05563296
[3/200] Training loss: 0.05444692
[4/200] Training loss: 0.04792238
[5/200] Training loss: 0.04508493
[6/200] Training loss: 0.04111532
[7/200] Training loss: 0.04103056
[8/200] Training loss: 0.03686908
[9/200] Training loss: 0.03510336
[10/200] Training loss: 0.03379621
[50/200] Training loss: 0.01710443
[100/200] Training loss: 0.01505660
[150/200] Training loss: 0.01312330
[200/200] Training loss: 0.01277837
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11559.695843749523 ----------
[1/200] Training loss: 0.12581226
[2/200] Training loss: 0.05401576
[3/200] Training loss: 0.05190493
[4/200] Training loss: 0.04645764
[5/200] Training loss: 0.04966677
[6/200] Training loss: 0.04539483
[7/200] Training loss: 0.04276999
[8/200] Training loss: 0.04215753
[9/200] Training loss: 0.03702232
[10/200] Training loss: 0.03656095
[50/200] Training loss: 0.01747861
[100/200] Training loss: 0.01447796
[150/200] Training loss: 0.01269454
[200/200] Training loss: 0.01206050
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5677.621861307778 ----------
[1/200] Training loss: 0.15633505
[2/200] Training loss: 0.05792902
[3/200] Training loss: 0.05502132
[4/200] Training loss: 0.04924419
[5/200] Training loss: 0.04613295
[6/200] Training loss: 0.04141032
[7/200] Training loss: 0.03787945
[8/200] Training loss: 0.03546167
[9/200] Training loss: 0.03407074
[10/200] Training loss: 0.03346086
[50/200] Training loss: 0.01761555
[100/200] Training loss: 0.01554664
[150/200] Training loss: 0.01447641
[200/200] Training loss: 0.01320285
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12430.454858934165 ----------
[1/200] Training loss: 0.13401710
[2/200] Training loss: 0.05281536
[3/200] Training loss: 0.04632089
[4/200] Training loss: 0.04424248
[5/200] Training loss: 0.04357603
[6/200] Training loss: 0.03902610
[7/200] Training loss: 0.03780850
[8/200] Training loss: 0.03779211
[9/200] Training loss: 0.03575015
[10/200] Training loss: 0.03540965
[50/200] Training loss: 0.01854395
[100/200] Training loss: 0.01455718
[150/200] Training loss: 0.01293775
[200/200] Training loss: 0.01207723
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.010792105419727659
----FITNESS-----------RMSE---- 8223.493175044288 ----------
[1/200] Training loss: 0.14459803
[2/200] Training loss: 0.05683204
[3/200] Training loss: 0.05498917
[4/200] Training loss: 0.04887337
[5/200] Training loss: 0.04787832
[6/200] Training loss: 0.04541345
[7/200] Training loss: 0.04019746
[8/200] Training loss: 0.03988990
[9/200] Training loss: 0.03861570
[10/200] Training loss: 0.03543352
[50/200] Training loss: 0.01845249
[100/200] Training loss: 0.01555049
[150/200] Training loss: 0.01422199
[200/200] Training loss: 0.01257881
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8691.970547580106 ----------
[1/200] Training loss: 0.13015258
[2/200] Training loss: 0.05084607
[3/200] Training loss: 0.04604897
[4/200] Training loss: 0.04052998
[5/200] Training loss: 0.04042712
[6/200] Training loss: 0.03714551
[7/200] Training loss: 0.03593519
[8/200] Training loss: 0.03069291
[9/200] Training loss: 0.02997827
[10/200] Training loss: 0.02998534
[50/200] Training loss: 0.01688383
[100/200] Training loss: 0.01456419
[150/200] Training loss: 0.01250433
[200/200] Training loss: 0.01214146
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11861.68756965045 ----------
[1/200] Training loss: 0.16966973
[2/200] Training loss: 0.05920104
[3/200] Training loss: 0.05139167
[4/200] Training loss: 0.04984140
[5/200] Training loss: 0.04673200
[6/200] Training loss: 0.04388154
[7/200] Training loss: 0.04081695
[8/200] Training loss: 0.03982936
[9/200] Training loss: 0.03517440
[10/200] Training loss: 0.03676165
[50/200] Training loss: 0.01884593
[100/200] Training loss: 0.01544115
[150/200] Training loss: 0.01377650
[200/200] Training loss: 0.01287190
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10129.229783157256 ----------
[1/200] Training loss: 0.16084095
[2/200] Training loss: 0.05532759
[3/200] Training loss: 0.05498546
[4/200] Training loss: 0.05077194
[5/200] Training loss: 0.04543119
[6/200] Training loss: 0.04329799
[7/200] Training loss: 0.04128617
[8/200] Training loss: 0.03811954
[9/200] Training loss: 0.03861900
[10/200] Training loss: 0.03672203
[50/200] Training loss: 0.01967380
[100/200] Training loss: 0.01656157
[150/200] Training loss: 0.01458608
[200/200] Training loss: 0.01377820
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8737.059459566473 ----------
[1/200] Training loss: 0.13818888
[2/200] Training loss: 0.05390434
[3/200] Training loss: 0.04954239
[4/200] Training loss: 0.05154439
[5/200] Training loss: 0.04570997
[6/200] Training loss: 0.04182981
[7/200] Training loss: 0.04040444
[8/200] Training loss: 0.03940136
[9/200] Training loss: 0.03862560
[10/200] Training loss: 0.03436133
[50/200] Training loss: 0.01776664
[100/200] Training loss: 0.01431423
[150/200] Training loss: 0.01282893
[200/200] Training loss: 0.01187826
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11217.914244635676 ----------
[1/200] Training loss: 0.11083776
[2/200] Training loss: 0.05317829
[3/200] Training loss: 0.04783282
[4/200] Training loss: 0.04556866
[5/200] Training loss: 0.04011993
[6/200] Training loss: 0.03723921
[7/200] Training loss: 0.03248039
[8/200] Training loss: 0.03040748
[9/200] Training loss: 0.02811235
[10/200] Training loss: 0.02708962
[50/200] Training loss: 0.01682060
[100/200] Training loss: 0.01401798
[150/200] Training loss: 0.01244125
[200/200] Training loss: 0.01091791
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17182.939445857337 ----------
[1/200] Training loss: 0.15260877
[2/200] Training loss: 0.06206533
[3/200] Training loss: 0.05336978
[4/200] Training loss: 0.04964060
[5/200] Training loss: 0.04731168
[6/200] Training loss: 0.04413252
[7/200] Training loss: 0.04083501
[8/200] Training loss: 0.03995351
[9/200] Training loss: 0.03992864
[10/200] Training loss: 0.03757546
[50/200] Training loss: 0.01893462
[100/200] Training loss: 0.01677801
[150/200] Training loss: 0.01474534
[200/200] Training loss: 0.01388066
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13126.628203769618 ----------
[1/200] Training loss: 0.13302693
[2/200] Training loss: 0.05330584
[3/200] Training loss: 0.04858892
[4/200] Training loss: 0.04396552
[5/200] Training loss: 0.04363592
[6/200] Training loss: 0.03824830
[7/200] Training loss: 0.03747594
[8/200] Training loss: 0.03573952
[9/200] Training loss: 0.03263576
[10/200] Training loss: 0.03061352
[50/200] Training loss: 0.01751028
[100/200] Training loss: 0.01469331
[150/200] Training loss: 0.01300341
[200/200] Training loss: 0.01175359
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 3459.1045083952004 ----------
[1/200] Training loss: 0.11711413
[2/200] Training loss: 0.05295642
[3/200] Training loss: 0.04682621
[4/200] Training loss: 0.04444332
[5/200] Training loss: 0.04324627
[6/200] Training loss: 0.04032588
[7/200] Training loss: 0.04033575
[8/200] Training loss: 0.03790966
[9/200] Training loss: 0.03634072
[10/200] Training loss: 0.03720234
[50/200] Training loss: 0.02250441
[100/200] Training loss: 0.01641811
[150/200] Training loss: 0.01475891
[200/200] Training loss: 0.01295173
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.010792105419727659
----FITNESS-----------RMSE---- 12650.763771409218 ----------
[1/200] Training loss: 0.12404600
[2/200] Training loss: 0.05067342
[3/200] Training loss: 0.04673489
[4/200] Training loss: 0.04342781
[5/200] Training loss: 0.04109684
[6/200] Training loss: 0.03931821
[7/200] Training loss: 0.03609145
[8/200] Training loss: 0.03245422
[9/200] Training loss: 0.03331880
[10/200] Training loss: 0.02952704
[50/200] Training loss: 0.01569221
[100/200] Training loss: 0.01372053
[150/200] Training loss: 0.01328365
[200/200] Training loss: 0.01155232
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15337.66396815369 ----------
[1/200] Training loss: 0.16499596
[2/200] Training loss: 0.06026658
[3/200] Training loss: 0.05310335
[4/200] Training loss: 0.05070004
[5/200] Training loss: 0.04473956
[6/200] Training loss: 0.04257963
[7/200] Training loss: 0.03786143
[8/200] Training loss: 0.03689600
[9/200] Training loss: 0.03491782
[10/200] Training loss: 0.03368372
[50/200] Training loss: 0.01865234
[100/200] Training loss: 0.01632150
[150/200] Training loss: 0.01505452
[200/200] Training loss: 0.01403114
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12478.03510172976 ----------
[1/200] Training loss: 0.17429120
[2/200] Training loss: 0.05819242
[3/200] Training loss: 0.05172677
[4/200] Training loss: 0.04966135
[5/200] Training loss: 0.04621941
[6/200] Training loss: 0.04143774
[7/200] Training loss: 0.03972525
[8/200] Training loss: 0.03831032
[9/200] Training loss: 0.03615598
[10/200] Training loss: 0.03623938
[50/200] Training loss: 0.01860066
[100/200] Training loss: 0.01559029
[150/200] Training loss: 0.01493864
[200/200] Training loss: 0.01386767
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10784.68247098634 ----------
[1/200] Training loss: 0.17453984
[2/200] Training loss: 0.06130905
[3/200] Training loss: 0.05361176
[4/200] Training loss: 0.04680169
[5/200] Training loss: 0.04942542
[6/200] Training loss: 0.04819032
[7/200] Training loss: 0.04526813
[8/200] Training loss: 0.04282957
[9/200] Training loss: 0.04013577
[10/200] Training loss: 0.03777462
[50/200] Training loss: 0.02050064
[100/200] Training loss: 0.01649978
[150/200] Training loss: 0.01570393
[200/200] Training loss: 0.01461082
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5540.267683063698 ----------
[1/200] Training loss: 0.15981774
[2/200] Training loss: 0.06364317
[3/200] Training loss: 0.05655643
[4/200] Training loss: 0.05343433
[5/200] Training loss: 0.05079032
[6/200] Training loss: 0.04818869
[7/200] Training loss: 0.04647111
[8/200] Training loss: 0.04335789
[9/200] Training loss: 0.03973496
[10/200] Training loss: 0.03575168
[50/200] Training loss: 0.01829814
[100/200] Training loss: 0.01592813
[150/200] Training loss: 0.01502445
[200/200] Training loss: 0.01384404
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17477.527513924844 ----------
[1/200] Training loss: 0.16081349
[2/200] Training loss: 0.05994411
[3/200] Training loss: 0.05231449
[4/200] Training loss: 0.04663756
[5/200] Training loss: 0.04241919
[6/200] Training loss: 0.03836233
[7/200] Training loss: 0.03667375
[8/200] Training loss: 0.03565020
[9/200] Training loss: 0.03082288
[10/200] Training loss: 0.02978901
[50/200] Training loss: 0.01885898
[100/200] Training loss: 0.01652002
[150/200] Training loss: 0.01470481
[200/200] Training loss: 0.01383595
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10834.898799711975 ----------
[1/200] Training loss: 0.18177816
[2/200] Training loss: 0.06554220
[3/200] Training loss: 0.05794671
[4/200] Training loss: 0.05013831
[5/200] Training loss: 0.04864175
[6/200] Training loss: 0.04731906
[7/200] Training loss: 0.04604815
[8/200] Training loss: 0.04545067
[9/200] Training loss: 0.04319592
[10/200] Training loss: 0.03820115
[50/200] Training loss: 0.01851069
[100/200] Training loss: 0.01635141
[150/200] Training loss: 0.01440388
[200/200] Training loss: 0.01313024
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13518.909423470519 ----------
[1/200] Training loss: 0.14230137
[2/200] Training loss: 0.05731818
[3/200] Training loss: 0.05283506
[4/200] Training loss: 0.04864917
[5/200] Training loss: 0.04968229
[6/200] Training loss: 0.04734981
[7/200] Training loss: 0.04630102
[8/200] Training loss: 0.04016174
[9/200] Training loss: 0.03827092
[10/200] Training loss: 0.03968678
[50/200] Training loss: 0.01731628
[100/200] Training loss: 0.01494730
[150/200] Training loss: 0.01367693
[200/200] Training loss: 0.01331580
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 22452.55976498003 ----------
[1/200] Training loss: 0.13068517
[2/200] Training loss: 0.05880113
[3/200] Training loss: 0.05304850
[4/200] Training loss: 0.05038621
[5/200] Training loss: 0.04891604
[6/200] Training loss: 0.04706717
[7/200] Training loss: 0.04603463
[8/200] Training loss: 0.04278217
[9/200] Training loss: 0.04090270
[10/200] Training loss: 0.03704922
[50/200] Training loss: 0.01987355
[100/200] Training loss: 0.01622584
[150/200] Training loss: 0.01398440
[200/200] Training loss: 0.01293028
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14777.863445031559 ----------
[1/200] Training loss: 0.12730153
[2/200] Training loss: 0.05828636
[3/200] Training loss: 0.04969883
[4/200] Training loss: 0.04930516
[5/200] Training loss: 0.04739274
[6/200] Training loss: 0.04310372
[7/200] Training loss: 0.04286622
[8/200] Training loss: 0.04106084
[9/200] Training loss: 0.03852184
[10/200] Training loss: 0.03874332
[50/200] Training loss: 0.02063165
[100/200] Training loss: 0.01601100
[150/200] Training loss: 0.01441130
[200/200] Training loss: 0.01336782
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.010792105419727659
----FITNESS-----------RMSE---- 6121.525300119244 ----------
[1/200] Training loss: 0.15518198
[2/200] Training loss: 0.05796377
[3/200] Training loss: 0.05460995
[4/200] Training loss: 0.04756698
[5/200] Training loss: 0.04191767
[6/200] Training loss: 0.03550878
[7/200] Training loss: 0.03296443
[8/200] Training loss: 0.03203363
[9/200] Training loss: 0.02924127
[10/200] Training loss: 0.02700667
[50/200] Training loss: 0.01675214
[100/200] Training loss: 0.01381550
[150/200] Training loss: 0.01272937
[200/200] Training loss: 0.01171664
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12200.145572901989 ----------
[1/200] Training loss: 0.15068994
[2/200] Training loss: 0.05589267
[3/200] Training loss: 0.05153291
[4/200] Training loss: 0.04940360
[5/200] Training loss: 0.04640791
[6/200] Training loss: 0.04339629
[7/200] Training loss: 0.03999990
[8/200] Training loss: 0.03674839
[9/200] Training loss: 0.03561418
[10/200] Training loss: 0.03307078
[50/200] Training loss: 0.01835446
[100/200] Training loss: 0.01531054
[150/200] Training loss: 0.01353138
[200/200] Training loss: 0.01230838
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.28567504546964273 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10216.158182017347 ----------
[1/200] Training loss: 0.14208306
[2/200] Training loss: 0.05441382
[3/200] Training loss: 0.05144704
[4/200] Training loss: 0.04910086
[5/200] Training loss: 0.04663779
[6/200] Training loss: 0.04836326
[7/200] Training loss: 0.04458729
[8/200] Training loss: 0.04143332
[9/200] Training loss: 0.04447192
[10/200] Training loss: 0.04077059
[50/200] Training loss: 0.02039209
[100/200] Training loss: 0.01714845
[150/200] Training loss: 0.01501534
[200/200] Training loss: 0.01422953
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.010792105419727659
----FITNESS-----------RMSE---- 8028.006726454581 ----------
[1/200] Training loss: 0.15637782
[2/200] Training loss: 0.05666153
[3/200] Training loss: 0.05451001
[4/200] Training loss: 0.04844485
[5/200] Training loss: 0.04668013
[6/200] Training loss: 0.04401735
[7/200] Training loss: 0.04439566
[8/200] Training loss: 0.04125209
[9/200] Training loss: 0.03779142
[10/200] Training loss: 0.03662415
[50/200] Training loss: 0.01797228
[100/200] Training loss: 0.01486340
[150/200] Training loss: 0.01366479
[200/200] Training loss: 0.01294507
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15641.527802615703 ----------
[1/200] Training loss: 0.15498554
[2/200] Training loss: 0.05657721
[3/200] Training loss: 0.05302741
[4/200] Training loss: 0.04782361
[5/200] Training loss: 0.04736448
[6/200] Training loss: 0.04476935
[7/200] Training loss: 0.04163564
[8/200] Training loss: 0.03899947
[9/200] Training loss: 0.03820080
[10/200] Training loss: 0.03860388
[50/200] Training loss: 0.01937108
[100/200] Training loss: 0.01536285
[150/200] Training loss: 0.01311497
[200/200] Training loss: 0.01221084
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9281.330508068335 ----------
[1/200] Training loss: 0.16748932
[2/200] Training loss: 0.06341071
[3/200] Training loss: 0.05566131
[4/200] Training loss: 0.05395119
[5/200] Training loss: 0.04870105
[6/200] Training loss: 0.04790176
[7/200] Training loss: 0.04579534
[8/200] Training loss: 0.04218852
[9/200] Training loss: 0.04348985
[10/200] Training loss: 0.04155793
[50/200] Training loss: 0.01998600
[100/200] Training loss: 0.01747688
[150/200] Training loss: 0.01512032
[200/200] Training loss: 0.01412374
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14946.379896148766 ----------
[1/200] Training loss: 0.15590053
[2/200] Training loss: 0.05836915
[3/200] Training loss: 0.05394212
[4/200] Training loss: 0.04852731
[5/200] Training loss: 0.04780750
[6/200] Training loss: 0.04443906
[7/200] Training loss: 0.04112151
[8/200] Training loss: 0.03558072
[9/200] Training loss: 0.03750217
[10/200] Training loss: 0.03177048
[50/200] Training loss: 0.01676868
[100/200] Training loss: 0.01409553
[150/200] Training loss: 0.01288541
[200/200] Training loss: 0.01241080
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 19594.132999446545 ----------
[1/200] Training loss: 0.15384485
[2/200] Training loss: 0.05655636
[3/200] Training loss: 0.05253040
[4/200] Training loss: 0.04763918
[5/200] Training loss: 0.04812607
[6/200] Training loss: 0.04484665
[7/200] Training loss: 0.04326069
[8/200] Training loss: 0.04086741
[9/200] Training loss: 0.03966396
[10/200] Training loss: 0.03840704
[50/200] Training loss: 0.02019313
[100/200] Training loss: 0.01502413
[150/200] Training loss: 0.01268396
[200/200] Training loss: 0.01205272
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.010792105419727659
----FITNESS-----------RMSE---- 9296.77965749431 ----------
[1/200] Training loss: 0.15341444
[2/200] Training loss: 0.05797885
[3/200] Training loss: 0.05030558
[4/200] Training loss: 0.04920721
[5/200] Training loss: 0.04507567
[6/200] Training loss: 0.04417146
[7/200] Training loss: 0.04001142
[8/200] Training loss: 0.03886026
[9/200] Training loss: 0.03725263
[10/200] Training loss: 0.03528809
[50/200] Training loss: 0.02005637
[100/200] Training loss: 0.01482270
[150/200] Training loss: 0.01441213
[200/200] Training loss: 0.01266088
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9914.076457239978 ----------
[1/200] Training loss: 0.15257456
[2/200] Training loss: 0.05704688
[3/200] Training loss: 0.05328402
[4/200] Training loss: 0.05171735
[5/200] Training loss: 0.04803081
[6/200] Training loss: 0.04565165
[7/200] Training loss: 0.04407225
[8/200] Training loss: 0.04473322
[9/200] Training loss: 0.04281505
[10/200] Training loss: 0.04067445
[50/200] Training loss: 0.02008575
[100/200] Training loss: 0.01624742
[150/200] Training loss: 0.01492607
[200/200] Training loss: 0.01402192
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.010792105419727659
----FITNESS-----------RMSE---- 10362.235666109897 ----------
[1/200] Training loss: 0.18213557
[2/200] Training loss: 0.05867188
[3/200] Training loss: 0.05579552
[4/200] Training loss: 0.04899024
[5/200] Training loss: 0.04990478
[6/200] Training loss: 0.04293883
[7/200] Training loss: 0.04066081
[8/200] Training loss: 0.03949138
[9/200] Training loss: 0.03567376
[10/200] Training loss: 0.03490890
[50/200] Training loss: 0.01823324
[100/200] Training loss: 0.01427982
[150/200] Training loss: 0.01233914
[200/200] Training loss: 0.01153621
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7787.216190655041 ----------
[1/200] Training loss: 0.12839829
[2/200] Training loss: 0.05221045
[3/200] Training loss: 0.05062041
[4/200] Training loss: 0.04429798
[5/200] Training loss: 0.04299017
[6/200] Training loss: 0.03982616
[7/200] Training loss: 0.03918189
[8/200] Training loss: 0.03414676
[9/200] Training loss: 0.03415298
[10/200] Training loss: 0.03065670
[50/200] Training loss: 0.01656670
[100/200] Training loss: 0.01458111
[150/200] Training loss: 0.01364859
[200/200] Training loss: 0.01182215
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9523.616119940996 ----------
[1/200] Training loss: 0.13453330
[2/200] Training loss: 0.05677199
[3/200] Training loss: 0.05263072
[4/200] Training loss: 0.05025969
[5/200] Training loss: 0.04836583
[6/200] Training loss: 0.04704337
[7/200] Training loss: 0.04621869
[8/200] Training loss: 0.04292593
[9/200] Training loss: 0.04324248
[10/200] Training loss: 0.04182903
[50/200] Training loss: 0.01697111
[100/200] Training loss: 0.01465646
[150/200] Training loss: 0.01314307
[200/200] Training loss: 0.01163750
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.010792105419727659
----FITNESS-----------RMSE---- 9419.546485898352 ----------
[1/200] Training loss: 0.17781439
[2/200] Training loss: 0.06064852
[3/200] Training loss: 0.05378848
[4/200] Training loss: 0.05238506
[5/200] Training loss: 0.05081657
[6/200] Training loss: 0.04756552
[7/200] Training loss: 0.04725788
[8/200] Training loss: 0.04634988
[9/200] Training loss: 0.04506839
[10/200] Training loss: 0.04274363
[50/200] Training loss: 0.02181067
[100/200] Training loss: 0.01833436
[150/200] Training loss: 0.01716976
[200/200] Training loss: 0.01511851
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10172.779364559126 ----------
[1/50] Training loss: 0.16868109
[2/50] Training loss: 0.05966469
[3/50] Training loss: 0.05198981
[4/50] Training loss: 0.04601032
[5/50] Training loss: 0.04255113
[6/50] Training loss: 0.04010115
[7/50] Training loss: 0.03780483
[8/50] Training loss: 0.03378246
[9/50] Training loss: 0.03264642
[10/50] Training loss: 0.03082193
[50/50] Training loss: 0.01754205
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 50
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11463.610251574326 ----------
[1/200] Training loss: 0.17236562
[2/200] Training loss: 0.06652251
[3/200] Training loss: 0.05470999
[4/200] Training loss: 0.05517945
[5/200] Training loss: 0.04776599
[6/200] Training loss: 0.04311440
[7/200] Training loss: 0.04218754
[8/200] Training loss: 0.03744902
[9/200] Training loss: 0.03429186
[10/200] Training loss: 0.03764650
[50/200] Training loss: 0.01708627
[100/200] Training loss: 0.01543406
[150/200] Training loss: 0.01361934
[200/200] Training loss: 0.01293083
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8720.320636306902 ----------
[1/200] Training loss: 0.13326493
[2/200] Training loss: 0.06086142
[3/200] Training loss: 0.05602090
[4/200] Training loss: 0.05513906
[5/200] Training loss: 0.05173786
[6/200] Training loss: 0.04984593
[7/200] Training loss: 0.04782418
[8/200] Training loss: 0.04726876
[9/200] Training loss: 0.04652630
[10/200] Training loss: 0.04540924
[50/200] Training loss: 0.02022570
[100/200] Training loss: 0.01488258
[150/200] Training loss: 0.01337708
[200/200] Training loss: 0.01311576
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.27882202027444614 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.010792105419727659
----FITNESS-----------RMSE---- 4808.731641503818 ----------
[1/200] Training loss: 0.16879053
[2/200] Training loss: 0.05956273
[3/200] Training loss: 0.05279754
[4/200] Training loss: 0.04810830
[5/200] Training loss: 0.04464119
[6/200] Training loss: 0.04359911
[7/200] Training loss: 0.03935375
[8/200] Training loss: 0.03865233
[9/200] Training loss: 0.03443345
[10/200] Training loss: 0.03680125
[50/200] Training loss: 0.01900966
[100/200] Training loss: 0.01567314
[150/200] Training loss: 0.01449564
[200/200] Training loss: 0.01365751
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7457.342690261727 ----------
[1/200] Training loss: 0.14172089
[2/200] Training loss: 0.05489676
[3/200] Training loss: 0.04929115
[4/200] Training loss: 0.04649199
[5/200] Training loss: 0.04332848
[6/200] Training loss: 0.04117401
[7/200] Training loss: 0.04057419
[8/200] Training loss: 0.03785561
[9/200] Training loss: 0.03632983
[10/200] Training loss: 0.03489387
[50/200] Training loss: 0.01836930
[100/200] Training loss: 0.01432490
[150/200] Training loss: 0.01226710
[200/200] Training loss: 0.01147553
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6403.297275622927 ----------
[1/200] Training loss: 0.11961140
[2/200] Training loss: 0.05676986
[3/200] Training loss: 0.04808724
[4/200] Training loss: 0.04143001
[5/200] Training loss: 0.03707501
[6/200] Training loss: 0.03459374
[7/200] Training loss: 0.03117790
[8/200] Training loss: 0.02948488
[9/200] Training loss: 0.02760013
[10/200] Training loss: 0.02703509
[50/200] Training loss: 0.01621724
[100/200] Training loss: 0.01385731
[150/200] Training loss: 0.01237487
[200/200] Training loss: 0.01159413
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13280.91502871696 ----------
[1/200] Training loss: 0.16556510
[2/200] Training loss: 0.06242559
[3/200] Training loss: 0.05951619
[4/200] Training loss: 0.05497134
[5/200] Training loss: 0.05374760
[6/200] Training loss: 0.05133502
[7/200] Training loss: 0.05075304
[8/200] Training loss: 0.04918415
[9/200] Training loss: 0.04770306
[10/200] Training loss: 0.04597112
[50/200] Training loss: 0.02052414
[100/200] Training loss: 0.01691563
[150/200] Training loss: 0.01458326
[200/200] Training loss: 0.01270405
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13654.467400817946 ----------
[1/200] Training loss: 0.12433543
[2/200] Training loss: 0.05483832
[3/200] Training loss: 0.05115421
[4/200] Training loss: 0.04696395
[5/200] Training loss: 0.04550938
[6/200] Training loss: 0.04074159
[7/200] Training loss: 0.03734198
[8/200] Training loss: 0.03536591
[9/200] Training loss: 0.03113119
[10/200] Training loss: 0.03178737
[50/200] Training loss: 0.01759609
[100/200] Training loss: 0.01527467
[150/200] Training loss: 0.01415662
[200/200] Training loss: 0.01294850
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10773.201195559284 ----------
[1/200] Training loss: 0.15807425
[2/200] Training loss: 0.06070460
[3/200] Training loss: 0.05448494
[4/200] Training loss: 0.05394859
[5/200] Training loss: 0.05096121
[6/200] Training loss: 0.05100057
[7/200] Training loss: 0.04866691
[8/200] Training loss: 0.04836194
[9/200] Training loss: 0.04133414
[10/200] Training loss: 0.03740408
[50/200] Training loss: 0.01881928
[100/200] Training loss: 0.01467827
[150/200] Training loss: 0.01303209
[200/200] Training loss: 0.01185036
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 22537.75428031817 ----------
[1/200] Training loss: 0.16729533
[2/200] Training loss: 0.06218487
[3/200] Training loss: 0.05454923
[4/200] Training loss: 0.05330326
[5/200] Training loss: 0.05303718
[6/200] Training loss: 0.05063637
[7/200] Training loss: 0.04680490
[8/200] Training loss: 0.04390797
[9/200] Training loss: 0.04256095
[10/200] Training loss: 0.03908606
[50/200] Training loss: 0.01883023
[100/200] Training loss: 0.01614248
[150/200] Training loss: 0.01454605
[200/200] Training loss: 0.01344127
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9040.19645804227 ----------
[1/200] Training loss: 0.11166259
[2/200] Training loss: 0.05202435
[3/200] Training loss: 0.04580398
[4/200] Training loss: 0.04317484
[5/200] Training loss: 0.03950686
[6/200] Training loss: 0.03602181
[7/200] Training loss: 0.03185225
[8/200] Training loss: 0.02939307
[9/200] Training loss: 0.02725025
[10/200] Training loss: 0.02431000
[50/200] Training loss: 0.01547687
[100/200] Training loss: 0.01300252
[150/200] Training loss: 0.01127351
[200/200] Training loss: 0.01118295
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9491.376717842359 ----------
[1/200] Training loss: 0.16194730
[2/200] Training loss: 0.06602329
[3/200] Training loss: 0.05307012
[4/200] Training loss: 0.05153322
[5/200] Training loss: 0.05203970
[6/200] Training loss: 0.05023931
[7/200] Training loss: 0.05039228
[8/200] Training loss: 0.04633283
[9/200] Training loss: 0.04609033
[10/200] Training loss: 0.04326422
[50/200] Training loss: 0.01874963
[100/200] Training loss: 0.01574470
[150/200] Training loss: 0.01453611
[200/200] Training loss: 0.01318289
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6645.657529545139 ----------
[1/200] Training loss: 0.10696960
[2/200] Training loss: 0.05229996
[3/200] Training loss: 0.04755539
[4/200] Training loss: 0.03863557
[5/200] Training loss: 0.03339370
[6/200] Training loss: 0.02967005
[7/200] Training loss: 0.02858580
[8/200] Training loss: 0.02740464
[9/200] Training loss: 0.02708816
[10/200] Training loss: 0.02539122
[50/200] Training loss: 0.01587376
[100/200] Training loss: 0.01322514
[150/200] Training loss: 0.01194984
[200/200] Training loss: 0.01096652
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 19863.032598271595 ----------
[1/200] Training loss: 0.16029844
[2/200] Training loss: 0.05811178
[3/200] Training loss: 0.05160173
[4/200] Training loss: 0.05293710
[5/200] Training loss: 0.04358710
[6/200] Training loss: 0.04255749
[7/200] Training loss: 0.03923023
[8/200] Training loss: 0.03702182
[9/200] Training loss: 0.03101752
[10/200] Training loss: 0.03111668
[50/200] Training loss: 0.01618857
[100/200] Training loss: 0.01400824
[150/200] Training loss: 0.01301097
[200/200] Training loss: 0.01201515
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12661.906965382426 ----------
[1/200] Training loss: 0.11305749
[2/200] Training loss: 0.05662337
[3/200] Training loss: 0.05383591
[4/200] Training loss: 0.04978710
[5/200] Training loss: 0.04776808
[6/200] Training loss: 0.04320486
[7/200] Training loss: 0.03851150
[8/200] Training loss: 0.03418432
[9/200] Training loss: 0.03074845
[10/200] Training loss: 0.02808631
[50/200] Training loss: 0.01568343
[100/200] Training loss: 0.01322522
[150/200] Training loss: 0.01169413
[200/200] Training loss: 0.01077866
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16461.74522947066 ----------
[1/200] Training loss: 0.16070805
[2/200] Training loss: 0.06415084
[3/200] Training loss: 0.05782517
[4/200] Training loss: 0.05268997
[5/200] Training loss: 0.05362795
[6/200] Training loss: 0.04743801
[7/200] Training loss: 0.04780119
[8/200] Training loss: 0.04432210
[9/200] Training loss: 0.04088559
[10/200] Training loss: 0.04052913
[50/200] Training loss: 0.01752611
[100/200] Training loss: 0.01575430
[150/200] Training loss: 0.01388906
[200/200] Training loss: 0.01364973
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 18968.46815111858 ----------
[1/200] Training loss: 0.10707812
[2/200] Training loss: 0.05286389
[3/200] Training loss: 0.04766377
[4/200] Training loss: 0.04408733
[5/200] Training loss: 0.04142228
[6/200] Training loss: 0.03778701
[7/200] Training loss: 0.03463651
[8/200] Training loss: 0.03170153
[9/200] Training loss: 0.02910172
[10/200] Training loss: 0.02764433
[50/200] Training loss: 0.01785686
[100/200] Training loss: 0.01520250
[150/200] Training loss: 0.01333689
[200/200] Training loss: 0.01228160
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9235.267186172796 ----------
[1/200] Training loss: 0.16521775
[2/200] Training loss: 0.06127758
[3/200] Training loss: 0.05363056
[4/200] Training loss: 0.04913889
[5/200] Training loss: 0.04442121
[6/200] Training loss: 0.04528951
[7/200] Training loss: 0.04083446
[8/200] Training loss: 0.03889677
[9/200] Training loss: 0.03826856
[10/200] Training loss: 0.03404827
[50/200] Training loss: 0.01891693
[100/200] Training loss: 0.01632993
[150/200] Training loss: 0.01467882
[200/200] Training loss: 0.01391036
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14262.481130574723 ----------
[1/200] Training loss: 0.17196987
[2/200] Training loss: 0.06427635
[3/200] Training loss: 0.05162307
[4/200] Training loss: 0.04917345
[5/200] Training loss: 0.04705013
[6/200] Training loss: 0.04238563
[7/200] Training loss: 0.03901025
[8/200] Training loss: 0.03796182
[9/200] Training loss: 0.03365095
[10/200] Training loss: 0.03412183
[50/200] Training loss: 0.01812381
[100/200] Training loss: 0.01653999
[150/200] Training loss: 0.01494991
[200/200] Training loss: 0.01431393
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9743.776680527935 ----------
[1/200] Training loss: 0.10634233
[2/200] Training loss: 0.04704075
[3/200] Training loss: 0.04237268
[4/200] Training loss: 0.03810244
[5/200] Training loss: 0.03663331
[6/200] Training loss: 0.03526243
[7/200] Training loss: 0.03192801
[8/200] Training loss: 0.03065677
[9/200] Training loss: 0.02947104
[10/200] Training loss: 0.02879783
[50/200] Training loss: 0.01624559
[100/200] Training loss: 0.01368428
[150/200] Training loss: 0.01194419
[200/200] Training loss: 0.01113525
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5513.174221807252 ----------
[1/200] Training loss: 0.14216836
[2/200] Training loss: 0.05734894
[3/200] Training loss: 0.05696534
[4/200] Training loss: 0.05310603
[5/200] Training loss: 0.05126028
[6/200] Training loss: 0.04703675
[7/200] Training loss: 0.04312598
[8/200] Training loss: 0.04036377
[9/200] Training loss: 0.03926335
[10/200] Training loss: 0.03676587
[50/200] Training loss: 0.02006890
[100/200] Training loss: 0.01534215
[150/200] Training loss: 0.01329019
[200/200] Training loss: 0.01268915
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12978.388806011322 ----------
[1/200] Training loss: 0.17267130
[2/200] Training loss: 0.05842375
[3/200] Training loss: 0.05013532
[4/200] Training loss: 0.04749721
[5/200] Training loss: 0.04569150
[6/200] Training loss: 0.04367328
[7/200] Training loss: 0.04197032
[8/200] Training loss: 0.03707187
[9/200] Training loss: 0.03860375
[10/200] Training loss: 0.03428103
[50/200] Training loss: 0.02021587
[100/200] Training loss: 0.01548228
[150/200] Training loss: 0.01404399
[200/200] Training loss: 0.01215732
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4447.644994825913 ----------
[1/200] Training loss: 0.12149844
[2/200] Training loss: 0.05504581
[3/200] Training loss: 0.04732063
[4/200] Training loss: 0.03993493
[5/200] Training loss: 0.03587059
[6/200] Training loss: 0.03519452
[7/200] Training loss: 0.02973123
[8/200] Training loss: 0.02733769
[9/200] Training loss: 0.02624731
[10/200] Training loss: 0.02498718
[50/200] Training loss: 0.01645443
[100/200] Training loss: 0.01447534
[150/200] Training loss: 0.01307579
[200/200] Training loss: 0.01235447
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7200.322770542998 ----------
[1/200] Training loss: 0.16160784
[2/200] Training loss: 0.05830410
[3/200] Training loss: 0.05240378
[4/200] Training loss: 0.05094938
[5/200] Training loss: 0.04584997
[6/200] Training loss: 0.04302897
[7/200] Training loss: 0.04077403
[8/200] Training loss: 0.03795269
[9/200] Training loss: 0.03943729
[10/200] Training loss: 0.03494426
[50/200] Training loss: 0.01830393
[100/200] Training loss: 0.01548697
[150/200] Training loss: 0.01349879
[200/200] Training loss: 0.01289009
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 18785.510160759542 ----------
[1/200] Training loss: 0.14150075
[2/200] Training loss: 0.05542004
[3/200] Training loss: 0.05280057
[4/200] Training loss: 0.04579657
[5/200] Training loss: 0.04340830
[6/200] Training loss: 0.04100880
[7/200] Training loss: 0.03948159
[8/200] Training loss: 0.03842136
[9/200] Training loss: 0.03608106
[10/200] Training loss: 0.03312124
[50/200] Training loss: 0.01827601
[100/200] Training loss: 0.01393637
[150/200] Training loss: 0.01247697
[200/200] Training loss: 0.01151724
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7633.696352357749 ----------
[1/200] Training loss: 0.14309374
[2/200] Training loss: 0.05942766
[3/200] Training loss: 0.05050350
[4/200] Training loss: 0.04762797
[5/200] Training loss: 0.04691129
[6/200] Training loss: 0.04068083
[7/200] Training loss: 0.03868331
[8/200] Training loss: 0.03761418
[9/200] Training loss: 0.03641559
[10/200] Training loss: 0.03495490
[50/200] Training loss: 0.01923641
[100/200] Training loss: 0.01613678
[150/200] Training loss: 0.01548818
[200/200] Training loss: 0.01442611
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9079.065590687183 ----------
[1/200] Training loss: 0.16302245
[2/200] Training loss: 0.05917205
[3/200] Training loss: 0.05281884
[4/200] Training loss: 0.05125056
[5/200] Training loss: 0.05031476
[6/200] Training loss: 0.04558926
[7/200] Training loss: 0.04152495
[8/200] Training loss: 0.04054734
[9/200] Training loss: 0.03948779
[10/200] Training loss: 0.03721396
[50/200] Training loss: 0.01776277
[100/200] Training loss: 0.01503326
[150/200] Training loss: 0.01467491
[200/200] Training loss: 0.01318737
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.27882202027444614 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12105.566983830207 ----------
[1/200] Training loss: 0.13483992
[2/200] Training loss: 0.05541691
[3/200] Training loss: 0.04970995
[4/200] Training loss: 0.04724388
[5/200] Training loss: 0.04221028
[6/200] Training loss: 0.04162852
[7/200] Training loss: 0.03915007
[8/200] Training loss: 0.03880724
[9/200] Training loss: 0.03656002
[10/200] Training loss: 0.03487083
[50/200] Training loss: 0.02161948
[100/200] Training loss: 0.01669637
[150/200] Training loss: 0.01497985
[200/200] Training loss: 0.01363066
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.010792105419727659
----FITNESS-----------RMSE---- 6220.496764728682 ----------
[1/200] Training loss: 0.18807231
[2/200] Training loss: 0.05812433
[3/200] Training loss: 0.05099044
[4/200] Training loss: 0.04717283
[5/200] Training loss: 0.04723151
[6/200] Training loss: 0.04324055
[7/200] Training loss: 0.03901171
[8/200] Training loss: 0.04003064
[9/200] Training loss: 0.03600966
[10/200] Training loss: 0.03265111
[50/200] Training loss: 0.01862496
[100/200] Training loss: 0.01415139
[150/200] Training loss: 0.01247309
[200/200] Training loss: 0.01193258
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6670.704910277474 ----------
[1/200] Training loss: 0.10778885
[2/200] Training loss: 0.05484235
[3/200] Training loss: 0.04923585
[4/200] Training loss: 0.04455004
[5/200] Training loss: 0.04140251
[6/200] Training loss: 0.03814914
[7/200] Training loss: 0.03501418
[8/200] Training loss: 0.03125665
[9/200] Training loss: 0.03146266
[10/200] Training loss: 0.02943967
[50/200] Training loss: 0.01538602
[100/200] Training loss: 0.01291616
[150/200] Training loss: 0.01178185
[200/200] Training loss: 0.01076791
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15010.408655329808 ----------
[1/200] Training loss: 0.11216703
[2/200] Training loss: 0.04929099
[3/200] Training loss: 0.04611830
[4/200] Training loss: 0.04274193
[5/200] Training loss: 0.03979329
[6/200] Training loss: 0.03747864
[7/200] Training loss: 0.03493921
[8/200] Training loss: 0.03208585
[9/200] Training loss: 0.03224381
[10/200] Training loss: 0.03221690
[50/200] Training loss: 0.01759383
[100/200] Training loss: 0.01578918
[150/200] Training loss: 0.01447561
[200/200] Training loss: 0.01269486
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11405.838855603739 ----------
[1/200] Training loss: 0.10597161
[2/200] Training loss: 0.05229427
[3/200] Training loss: 0.04657684
[4/200] Training loss: 0.04382510
[5/200] Training loss: 0.03893976
[6/200] Training loss: 0.03730897
[7/200] Training loss: 0.03323068
[8/200] Training loss: 0.02945461
[9/200] Training loss: 0.02816196
[10/200] Training loss: 0.02636597
[50/200] Training loss: 0.01806343
[100/200] Training loss: 0.01605427
[150/200] Training loss: 0.01392143
[200/200] Training loss: 0.01297628
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7603.490251193855 ----------
[1/200] Training loss: 0.16117675
[2/200] Training loss: 0.05898431
[3/200] Training loss: 0.05342422
[4/200] Training loss: 0.04869128
[5/200] Training loss: 0.04624215
[6/200] Training loss: 0.04630324
[7/200] Training loss: 0.04099697
[8/200] Training loss: 0.03752175
[9/200] Training loss: 0.03937140
[10/200] Training loss: 0.03249063
[50/200] Training loss: 0.01772067
[100/200] Training loss: 0.01533177
[150/200] Training loss: 0.01390408
[200/200] Training loss: 0.01285979
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6782.88581652382 ----------
[1/200] Training loss: 0.15928602
[2/200] Training loss: 0.05933649
[3/200] Training loss: 0.05116986
[4/200] Training loss: 0.05083989
[5/200] Training loss: 0.04730430
[6/200] Training loss: 0.04494270
[7/200] Training loss: 0.04394496
[8/200] Training loss: 0.04136491
[9/200] Training loss: 0.03877884
[10/200] Training loss: 0.03865155
[50/200] Training loss: 0.01759225
[100/200] Training loss: 0.01431069
[150/200] Training loss: 0.01230339
[200/200] Training loss: 0.01137721
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 3684.785204051927 ----------
[1/200] Training loss: 0.18215946
[2/200] Training loss: 0.06072854
[3/200] Training loss: 0.05466474
[4/200] Training loss: 0.05070892
[5/200] Training loss: 0.05012411
[6/200] Training loss: 0.04749277
[7/200] Training loss: 0.04427056
[8/200] Training loss: 0.04451549
[9/200] Training loss: 0.04175917
[10/200] Training loss: 0.03829341
[50/200] Training loss: 0.02153455
[100/200] Training loss: 0.01598315
[150/200] Training loss: 0.01479756
[200/200] Training loss: 0.01381472
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 3249.03401028675 ----------
[1/200] Training loss: 0.18900314
[2/200] Training loss: 0.05846756
[3/200] Training loss: 0.05604564
[4/200] Training loss: 0.04982780
[5/200] Training loss: 0.04798266
[6/200] Training loss: 0.04601457
[7/200] Training loss: 0.04371442
[8/200] Training loss: 0.04253412
[9/200] Training loss: 0.04008983
[10/200] Training loss: 0.03843511
[50/200] Training loss: 0.01882427
[100/200] Training loss: 0.01569632
[150/200] Training loss: 0.01473470
[200/200] Training loss: 0.01374843
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10950.297530204374 ----------
[1/200] Training loss: 0.11206524
[2/200] Training loss: 0.05299622
[3/200] Training loss: 0.04791068
[4/200] Training loss: 0.04649143
[5/200] Training loss: 0.04413389
[6/200] Training loss: 0.04263727
[7/200] Training loss: 0.04115398
[8/200] Training loss: 0.03900444
[9/200] Training loss: 0.03898809
[10/200] Training loss: 0.03701480
[50/200] Training loss: 0.02280594
[100/200] Training loss: 0.01834489
[150/200] Training loss: 0.01574812
[200/200] Training loss: 0.01254483
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.010792105419727659
----FITNESS-----------RMSE---- 3049.5183226208037 ----------
[1/200] Training loss: 0.11696558
[2/200] Training loss: 0.05465181
[3/200] Training loss: 0.05267362
[4/200] Training loss: 0.04803687
[5/200] Training loss: 0.04691797
[6/200] Training loss: 0.04587708
[7/200] Training loss: 0.04540704
[8/200] Training loss: 0.04433914
[9/200] Training loss: 0.04232194
[10/200] Training loss: 0.04076413
[50/200] Training loss: 0.01994524
[100/200] Training loss: 0.01674619
[150/200] Training loss: 0.01427364
[200/200] Training loss: 0.01321410
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.004906005955192384
----FITNESS-----------RMSE---- 13358.151369107927 ----------
[1/200] Training loss: 0.01971561
[2/200] Training loss: 0.00184819
[3/200] Training loss: 0.00150597
[4/200] Training loss: 0.00122286
[5/200] Training loss: 0.00088998
[6/200] Training loss: 0.00076202
[7/200] Training loss: 0.00063398
[8/200] Training loss: 0.00055056
[9/200] Training loss: 0.00054397
[10/200] Training loss: 0.00051770
[50/200] Training loss: 0.00022888
[100/200] Training loss: 0.00017909
[150/200] Training loss: 0.00015228
[200/200] Training loss: 0.00013317
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13817.23735049811 ----------
[1/200] Training loss: 0.07577851
[2/200] Training loss: 0.04465633
[3/200] Training loss: 0.03546509
[4/200] Training loss: 0.03181526
[5/200] Training loss: 0.02847392
[6/200] Training loss: 0.02572736
[7/200] Training loss: 0.02514732
[8/200] Training loss: 0.02411749
[9/200] Training loss: 0.02338636
[10/200] Training loss: 0.02162317
[50/200] Training loss: 0.01546602
[100/200] Training loss: 0.01353638
[150/200] Training loss: 0.01266217
[200/200] Training loss: 0.01185264
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6994.826659753621 ----------
[1/200] Training loss: 0.14061426
[2/200] Training loss: 0.06071276
[3/200] Training loss: 0.05252708
[4/200] Training loss: 0.04750313
[5/200] Training loss: 0.04364650
[6/200] Training loss: 0.04142948
[7/200] Training loss: 0.03855408
[8/200] Training loss: 0.03555784
[9/200] Training loss: 0.03363272
[10/200] Training loss: 0.03107271
[50/200] Training loss: 0.01736006
[100/200] Training loss: 0.01514512
[150/200] Training loss: 0.01425455
[200/200] Training loss: 0.01387837
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12873.71212976273 ----------
[1/200] Training loss: 0.11474715
[2/200] Training loss: 0.05247872
[3/200] Training loss: 0.04896310
[4/200] Training loss: 0.04507270
[5/200] Training loss: 0.04178566
[6/200] Training loss: 0.03857107
[7/200] Training loss: 0.03447064
[8/200] Training loss: 0.03064540
[9/200] Training loss: 0.02857253
[10/200] Training loss: 0.02769097
[50/200] Training loss: 0.01801056
[100/200] Training loss: 0.01567724
[150/200] Training loss: 0.01445349
[200/200] Training loss: 0.01315169
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 21485.14873115846 ----------
[1/200] Training loss: 0.16172348
[2/200] Training loss: 0.06102194
[3/200] Training loss: 0.05466044
[4/200] Training loss: 0.05096675
[5/200] Training loss: 0.04822009
[6/200] Training loss: 0.04640821
[7/200] Training loss: 0.04685479
[8/200] Training loss: 0.04554839
[9/200] Training loss: 0.04090383
[10/200] Training loss: 0.04047582
[50/200] Training loss: 0.01798583
[100/200] Training loss: 0.01549041
[150/200] Training loss: 0.01351703
[200/200] Training loss: 0.01314201
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10556.572549838324 ----------
[1/200] Training loss: 0.09701758
[2/200] Training loss: 0.04794824
[3/200] Training loss: 0.04272063
[4/200] Training loss: 0.03900850
[5/200] Training loss: 0.03391707
[6/200] Training loss: 0.03242362
[7/200] Training loss: 0.03081101
[8/200] Training loss: 0.02904527
[9/200] Training loss: 0.02706388
[10/200] Training loss: 0.02643390
[50/200] Training loss: 0.01515148
[100/200] Training loss: 0.01237286
[150/200] Training loss: 0.01139156
[200/200] Training loss: 0.01039284
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15288.027734145435 ----------
[1/200] Training loss: 0.09718904
[2/200] Training loss: 0.04965131
[3/200] Training loss: 0.04763362
[4/200] Training loss: 0.04023566
[5/200] Training loss: 0.03950688
[6/200] Training loss: 0.03596720
[7/200] Training loss: 0.03424865
[8/200] Training loss: 0.03394192
[9/200] Training loss: 0.03118141
[10/200] Training loss: 0.02958822
[50/200] Training loss: 0.01637590
[100/200] Training loss: 0.01358637
[150/200] Training loss: 0.01180761
[200/200] Training loss: 0.01110226
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 20232.120996079477 ----------
[1/200] Training loss: 0.13309015
[2/200] Training loss: 0.05843362
[3/200] Training loss: 0.04981800
[4/200] Training loss: 0.04662560
[5/200] Training loss: 0.04383203
[6/200] Training loss: 0.04113929
[7/200] Training loss: 0.03780545
[8/200] Training loss: 0.03481421
[9/200] Training loss: 0.03172941
[10/200] Training loss: 0.03043152
[50/200] Training loss: 0.01711942
[100/200] Training loss: 0.01511610
[150/200] Training loss: 0.01381364
[200/200] Training loss: 0.01288544
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8558.406393716064 ----------
[1/200] Training loss: 0.15484873
[2/200] Training loss: 0.05430780
[3/200] Training loss: 0.04881744
[4/200] Training loss: 0.04844658
[5/200] Training loss: 0.04447951
[6/200] Training loss: 0.04251059
[7/200] Training loss: 0.03932370
[8/200] Training loss: 0.03508918
[9/200] Training loss: 0.03568666
[10/200] Training loss: 0.03467318
[50/200] Training loss: 0.01782525
[100/200] Training loss: 0.01547384
[150/200] Training loss: 0.01399246
[200/200] Training loss: 0.01258797
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 31094.56235421235 ----------
[1/200] Training loss: 0.11197960
[2/200] Training loss: 0.05491939
[3/200] Training loss: 0.05185297
[4/200] Training loss: 0.04811041
[5/200] Training loss: 0.04439136
[6/200] Training loss: 0.04100586
[7/200] Training loss: 0.03578328
[8/200] Training loss: 0.03273907
[9/200] Training loss: 0.03221595
[10/200] Training loss: 0.02978395
[50/200] Training loss: 0.01613601
[100/200] Training loss: 0.01432833
[150/200] Training loss: 0.01286239
[200/200] Training loss: 0.01215024
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6699.752234224785 ----------
[1/200] Training loss: 0.09564574
[2/200] Training loss: 0.05027107
[3/200] Training loss: 0.04309843
[4/200] Training loss: 0.03955711
[5/200] Training loss: 0.03460680
[6/200] Training loss: 0.03215526
[7/200] Training loss: 0.03066359
[8/200] Training loss: 0.02808844
[9/200] Training loss: 0.02810704
[10/200] Training loss: 0.02504615
[50/200] Training loss: 0.01579989
[100/200] Training loss: 0.01313051
[150/200] Training loss: 0.01167046
[200/200] Training loss: 0.01098914
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15447.450533987801 ----------
[1/200] Training loss: 0.16106210
[2/200] Training loss: 0.05724799
[3/200] Training loss: 0.05452296
[4/200] Training loss: 0.04893347
[5/200] Training loss: 0.04628266
[6/200] Training loss: 0.04520920
[7/200] Training loss: 0.04127872
[8/200] Training loss: 0.03888939
[9/200] Training loss: 0.03816392
[10/200] Training loss: 0.03639564
[50/200] Training loss: 0.01730357
[100/200] Training loss: 0.01565340
[150/200] Training loss: 0.01449164
[200/200] Training loss: 0.01377821
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15879.68060132193 ----------
[1/200] Training loss: 0.15336510
[2/200] Training loss: 0.05864528
[3/200] Training loss: 0.05168826
[4/200] Training loss: 0.05086773
[5/200] Training loss: 0.04603256
[6/200] Training loss: 0.04466687
[7/200] Training loss: 0.04123570
[8/200] Training loss: 0.03855276
[9/200] Training loss: 0.03682020
[10/200] Training loss: 0.03327834
[50/200] Training loss: 0.01779217
[100/200] Training loss: 0.01515498
[150/200] Training loss: 0.01365516
[200/200] Training loss: 0.01298542
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11703.900204632642 ----------
[1/200] Training loss: 0.14749525
[2/200] Training loss: 0.05386548
[3/200] Training loss: 0.04830210
[4/200] Training loss: 0.04502498
[5/200] Training loss: 0.03963043
[6/200] Training loss: 0.04058733
[7/200] Training loss: 0.03647181
[8/200] Training loss: 0.03280722
[9/200] Training loss: 0.03092732
[10/200] Training loss: 0.03015639
[50/200] Training loss: 0.01887006
[100/200] Training loss: 0.01659197
[150/200] Training loss: 0.01496603
[200/200] Training loss: 0.01370849
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10454.984265889643 ----------
[1/200] Training loss: 0.11401421
[2/200] Training loss: 0.05542015
[3/200] Training loss: 0.05294206
[4/200] Training loss: 0.04809021
[5/200] Training loss: 0.04648334
[6/200] Training loss: 0.04311879
[7/200] Training loss: 0.04103038
[8/200] Training loss: 0.03785831
[9/200] Training loss: 0.03674548
[10/200] Training loss: 0.03281807
[50/200] Training loss: 0.01848541
[100/200] Training loss: 0.01585680
[150/200] Training loss: 0.01325371
[200/200] Training loss: 0.01150858
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17562.07914798245 ----------
[1/200] Training loss: 0.11057619
[2/200] Training loss: 0.05420021
[3/200] Training loss: 0.05083601
[4/200] Training loss: 0.04603246
[5/200] Training loss: 0.04188358
[6/200] Training loss: 0.03818639
[7/200] Training loss: 0.03605309
[8/200] Training loss: 0.03208809
[9/200] Training loss: 0.02906821
[10/200] Training loss: 0.02900860
[50/200] Training loss: 0.01788055
[100/200] Training loss: 0.01482333
[150/200] Training loss: 0.01268244
[200/200] Training loss: 0.01192812
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7385.548862474609 ----------
[1/200] Training loss: 0.06990706
[2/200] Training loss: 0.04402564
[3/200] Training loss: 0.03897692
[4/200] Training loss: 0.03340668
[5/200] Training loss: 0.03035146
[6/200] Training loss: 0.02842896
[7/200] Training loss: 0.02711212
[8/200] Training loss: 0.02620476
[9/200] Training loss: 0.02502284
[10/200] Training loss: 0.02430429
[50/200] Training loss: 0.01142050
[100/200] Training loss: 0.00936373
[150/200] Training loss: 0.00877230
[200/200] Training loss: 0.00843482
---batch_size---: 2 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9503.429275793029 ----------
[1/200] Training loss: 0.14265333
[2/200] Training loss: 0.05472401
[3/200] Training loss: 0.05165034
[4/200] Training loss: 0.04714154
[5/200] Training loss: 0.04462894
[6/200] Training loss: 0.04184834
[7/200] Training loss: 0.04044621
[8/200] Training loss: 0.03795535
[9/200] Training loss: 0.03680577
[10/200] Training loss: 0.03633227
[50/200] Training loss: 0.01971054
[100/200] Training loss: 0.01408181
[150/200] Training loss: 0.01335730
[200/200] Training loss: 0.01183239
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5619.830780370527 ----------
[1/200] Training loss: 0.10710084
[2/200] Training loss: 0.05318436
[3/200] Training loss: 0.04693740
[4/200] Training loss: 0.04086782
[5/200] Training loss: 0.03736213
[6/200] Training loss: 0.03163132
[7/200] Training loss: 0.03115656
[8/200] Training loss: 0.02803633
[9/200] Training loss: 0.02748701
[10/200] Training loss: 0.02599499
[50/200] Training loss: 0.01623617
[100/200] Training loss: 0.01368603
[150/200] Training loss: 0.01180363
[200/200] Training loss: 0.01104576
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 22003.439003937543 ----------
[1/200] Training loss: 0.11201009
[2/200] Training loss: 0.05416826
[3/200] Training loss: 0.05042187
[4/200] Training loss: 0.04638628
[5/200] Training loss: 0.04414517
[6/200] Training loss: 0.04051599
[7/200] Training loss: 0.03861323
[8/200] Training loss: 0.03610280
[9/200] Training loss: 0.03282653
[10/200] Training loss: 0.03243614
[50/200] Training loss: 0.01771486
[100/200] Training loss: 0.01443723
[150/200] Training loss: 0.01299848
[200/200] Training loss: 0.01211427
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7173.613315477773 ----------
[1/200] Training loss: 0.16639347
[2/200] Training loss: 0.06129193
[3/200] Training loss: 0.05525154
[4/200] Training loss: 0.05200807
[5/200] Training loss: 0.04548501
[6/200] Training loss: 0.04415252
[7/200] Training loss: 0.04113366
[8/200] Training loss: 0.03892531
[9/200] Training loss: 0.03745808
[10/200] Training loss: 0.03797396
[50/200] Training loss: 0.01891550
[100/200] Training loss: 0.01665114
[150/200] Training loss: 0.01541111
[200/200] Training loss: 0.01381206
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6707.292747450345 ----------
[1/200] Training loss: 0.16141729
[2/200] Training loss: 0.06037014
[3/200] Training loss: 0.05255810
[4/200] Training loss: 0.04709593
[5/200] Training loss: 0.04605275
[6/200] Training loss: 0.04481497
[7/200] Training loss: 0.04152907
[8/200] Training loss: 0.04136164
[9/200] Training loss: 0.03630989
[10/200] Training loss: 0.03756574
[50/200] Training loss: 0.01807445
[100/200] Training loss: 0.01558830
[150/200] Training loss: 0.01367678
[200/200] Training loss: 0.01295130
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10318.408404400361 ----------
[1/200] Training loss: 0.10507040
[2/200] Training loss: 0.05553277
[3/200] Training loss: 0.04985942
[4/200] Training loss: 0.04792928
[5/200] Training loss: 0.04276431
[6/200] Training loss: 0.03868595
[7/200] Training loss: 0.03485121
[8/200] Training loss: 0.03327790
[9/200] Training loss: 0.03101427
[10/200] Training loss: 0.02781953
[50/200] Training loss: 0.01573792
[100/200] Training loss: 0.01337662
[150/200] Training loss: 0.01220309
[200/200] Training loss: 0.01117894
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 27806.362437399108 ----------
[1/200] Training loss: 0.16027429
[2/200] Training loss: 0.06057791
[3/200] Training loss: 0.05735613
[4/200] Training loss: 0.05305998
[5/200] Training loss: 0.05153824
[6/200] Training loss: 0.04868839
[7/200] Training loss: 0.04933782
[8/200] Training loss: 0.04668958
[9/200] Training loss: 0.04400427
[10/200] Training loss: 0.04301932
[50/200] Training loss: 0.01981611
[100/200] Training loss: 0.01676887
[150/200] Training loss: 0.01547407
[200/200] Training loss: 0.01432100
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 23498.88984611826 ----------
[1/200] Training loss: 0.11413643
[2/200] Training loss: 0.05564546
[3/200] Training loss: 0.05065081
[4/200] Training loss: 0.04652249
[5/200] Training loss: 0.04421768
[6/200] Training loss: 0.04019851
[7/200] Training loss: 0.03600362
[8/200] Training loss: 0.03090910
[9/200] Training loss: 0.02749083
[10/200] Training loss: 0.02625425
[50/200] Training loss: 0.01741883
[100/200] Training loss: 0.01554243
[150/200] Training loss: 0.01403368
[200/200] Training loss: 0.01325496
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13471.473267612566 ----------
[1/200] Training loss: 0.06974812
[2/200] Training loss: 0.04468164
[3/200] Training loss: 0.03560542
[4/200] Training loss: 0.02982494
[5/200] Training loss: 0.02724681
[6/200] Training loss: 0.02545091
[7/200] Training loss: 0.02422536
[8/200] Training loss: 0.02327805
[9/200] Training loss: 0.02277471
[10/200] Training loss: 0.02177386
[50/200] Training loss: 0.01723058
[100/200] Training loss: 0.01485535
[150/200] Training loss: 0.01286397
[200/200] Training loss: 0.01121543
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12810.714265801107 ----------
[1/200] Training loss: 0.10760207
[2/200] Training loss: 0.05359794
[3/200] Training loss: 0.05176464
[4/200] Training loss: 0.04456237
[5/200] Training loss: 0.04333245
[6/200] Training loss: 0.03898107
[7/200] Training loss: 0.03705748
[8/200] Training loss: 0.03343325
[9/200] Training loss: 0.03132078
[10/200] Training loss: 0.02881886
[50/200] Training loss: 0.01719795
[100/200] Training loss: 0.01449049
[150/200] Training loss: 0.01235334
[200/200] Training loss: 0.01112263
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9357.249168425515 ----------
[1/200] Training loss: 0.16189805
[2/200] Training loss: 0.05584107
[3/200] Training loss: 0.04838946
[4/200] Training loss: 0.04482036
[5/200] Training loss: 0.04193272
[6/200] Training loss: 0.04209879
[7/200] Training loss: 0.03881378
[8/200] Training loss: 0.03508710
[9/200] Training loss: 0.03432919
[10/200] Training loss: 0.03466476
[50/200] Training loss: 0.01943903
[100/200] Training loss: 0.01741542
[150/200] Training loss: 0.01564068
[200/200] Training loss: 0.01508481
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11882.227400618118 ----------
[1/200] Training loss: 0.17033057
[2/200] Training loss: 0.05605953
[3/200] Training loss: 0.05202459
[4/200] Training loss: 0.04581094
[5/200] Training loss: 0.04459718
[6/200] Training loss: 0.03802040
[7/200] Training loss: 0.03656430
[8/200] Training loss: 0.03668670
[9/200] Training loss: 0.03346700
[10/200] Training loss: 0.03518695
[50/200] Training loss: 0.01808865
[100/200] Training loss: 0.01561561
[150/200] Training loss: 0.01407243
[200/200] Training loss: 0.01292580
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4497.367229835696 ----------
[1/200] Training loss: 0.13088842
[2/200] Training loss: 0.05893080
[3/200] Training loss: 0.05240993
[4/200] Training loss: 0.05063469
[5/200] Training loss: 0.04688212
[6/200] Training loss: 0.04512885
[7/200] Training loss: 0.04432274
[8/200] Training loss: 0.03885660
[9/200] Training loss: 0.03606935
[10/200] Training loss: 0.03505600
[50/200] Training loss: 0.01868622
[100/200] Training loss: 0.01802448
[150/200] Training loss: 0.01474788
[200/200] Training loss: 0.01355852
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10006.449919926647 ----------
[1/200] Training loss: 0.10537046
[2/200] Training loss: 0.04944825
[3/200] Training loss: 0.04530018
[4/200] Training loss: 0.04064982
[5/200] Training loss: 0.03622632
[6/200] Training loss: 0.03232992
[7/200] Training loss: 0.03011888
[8/200] Training loss: 0.02657134
[9/200] Training loss: 0.02690665
[10/200] Training loss: 0.02484510
[50/200] Training loss: 0.01588787
[100/200] Training loss: 0.01321357
[150/200] Training loss: 0.01183876
[200/200] Training loss: 0.01121012
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5568.4031822417455 ----------
[1/200] Training loss: 0.10538215
[2/200] Training loss: 0.05226194
[3/200] Training loss: 0.04763433
[4/200] Training loss: 0.04315828
[5/200] Training loss: 0.04047616
[6/200] Training loss: 0.03864781
[7/200] Training loss: 0.03464137
[8/200] Training loss: 0.03450374
[9/200] Training loss: 0.03215571
[10/200] Training loss: 0.03141955
[50/200] Training loss: 0.01783805
[100/200] Training loss: 0.01435334
[150/200] Training loss: 0.01290286
[200/200] Training loss: 0.01214437
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8798.443953336295 ----------
[1/200] Training loss: 0.16019284
[2/200] Training loss: 0.05746278
[3/200] Training loss: 0.05185572
[4/200] Training loss: 0.04788256
[5/200] Training loss: 0.04289465
[6/200] Training loss: 0.03720248
[7/200] Training loss: 0.03645745
[8/200] Training loss: 0.03284288
[9/200] Training loss: 0.02977835
[10/200] Training loss: 0.03083889
[50/200] Training loss: 0.01732620
[100/200] Training loss: 0.01552522
[150/200] Training loss: 0.01463031
[200/200] Training loss: 0.01365594
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6566.919825915343 ----------
[1/200] Training loss: 0.16172665
[2/200] Training loss: 0.05713326
[3/200] Training loss: 0.04838696
[4/200] Training loss: 0.04746742
[5/200] Training loss: 0.04396997
[6/200] Training loss: 0.04321766
[7/200] Training loss: 0.04004181
[8/200] Training loss: 0.03871868
[9/200] Training loss: 0.03554843
[10/200] Training loss: 0.03713384
[50/200] Training loss: 0.02067802
[100/200] Training loss: 0.01600396
[150/200] Training loss: 0.01390809
[200/200] Training loss: 0.01288628
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6556.1519201434 ----------
[1/200] Training loss: 0.09663633
[2/200] Training loss: 0.05400531
[3/200] Training loss: 0.04992831
[4/200] Training loss: 0.04582521
[5/200] Training loss: 0.04180249
[6/200] Training loss: 0.03719140
[7/200] Training loss: 0.03275736
[8/200] Training loss: 0.03084928
[9/200] Training loss: 0.02843455
[10/200] Training loss: 0.02530450
[50/200] Training loss: 0.01565264
[100/200] Training loss: 0.01343171
[150/200] Training loss: 0.01181799
[200/200] Training loss: 0.01161598
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 20051.177721021777 ----------
[1/200] Training loss: 0.11072682
[2/200] Training loss: 0.05219629
[3/200] Training loss: 0.04824295
[4/200] Training loss: 0.04397187
[5/200] Training loss: 0.04003616
[6/200] Training loss: 0.03547616
[7/200] Training loss: 0.03188661
[8/200] Training loss: 0.02898701
[9/200] Training loss: 0.02902024
[10/200] Training loss: 0.02745331
[50/200] Training loss: 0.01787272
[100/200] Training loss: 0.01553960
[150/200] Training loss: 0.01350780
[200/200] Training loss: 0.01290863
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17108.566275407182 ----------
[1/200] Training loss: 0.16261512
[2/200] Training loss: 0.05278564
[3/200] Training loss: 0.05099803
[4/200] Training loss: 0.04527268
[5/200] Training loss: 0.04198550
[6/200] Training loss: 0.04243616
[7/200] Training loss: 0.03552037
[8/200] Training loss: 0.03633190
[9/200] Training loss: 0.03362352
[10/200] Training loss: 0.03213475
[50/200] Training loss: 0.01871006
[100/200] Training loss: 0.01672251
[150/200] Training loss: 0.01524852
[200/200] Training loss: 0.01431026
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5144.929542763438 ----------
[1/200] Training loss: 0.10739625
[2/200] Training loss: 0.05399087
[3/200] Training loss: 0.04797304
[4/200] Training loss: 0.04529702
[5/200] Training loss: 0.03831768
[6/200] Training loss: 0.03530091
[7/200] Training loss: 0.03246210
[8/200] Training loss: 0.03228487
[9/200] Training loss: 0.02549739
[10/200] Training loss: 0.02613781
[50/200] Training loss: 0.01571175
[100/200] Training loss: 0.01428152
[150/200] Training loss: 0.01294649
[200/200] Training loss: 0.01245129
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6855.570873384652 ----------
[1/200] Training loss: 0.11896278
[2/200] Training loss: 0.05982670
[3/200] Training loss: 0.05351666
[4/200] Training loss: 0.04643530
[5/200] Training loss: 0.04364182
[6/200] Training loss: 0.04058338
[7/200] Training loss: 0.03638613
[8/200] Training loss: 0.03335699
[9/200] Training loss: 0.03225852
[10/200] Training loss: 0.02856037
[50/200] Training loss: 0.01728770
[100/200] Training loss: 0.01467997
[150/200] Training loss: 0.01302969
[200/200] Training loss: 0.01167293
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15190.230018008286 ----------
[1/200] Training loss: 0.07782454
[2/200] Training loss: 0.04989691
[3/200] Training loss: 0.04171022
[4/200] Training loss: 0.03717403
[5/200] Training loss: 0.03173830
[6/200] Training loss: 0.02851952
[7/200] Training loss: 0.02628810
[8/200] Training loss: 0.02517111
[9/200] Training loss: 0.02380431
[10/200] Training loss: 0.02306999
[50/200] Training loss: 0.01666369
[100/200] Training loss: 0.01458989
[150/200] Training loss: 0.01323655
[200/200] Training loss: 0.01230348
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11193.673213025293 ----------
[1/200] Training loss: 0.11047497
[2/200] Training loss: 0.05050676
[3/200] Training loss: 0.04378497
[4/200] Training loss: 0.03810039
[5/200] Training loss: 0.03550827
[6/200] Training loss: 0.03170276
[7/200] Training loss: 0.02833849
[8/200] Training loss: 0.02785344
[9/200] Training loss: 0.02740059
[10/200] Training loss: 0.02649237
[50/200] Training loss: 0.01668554
[100/200] Training loss: 0.01336203
[150/200] Training loss: 0.01219224
[200/200] Training loss: 0.01171625
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12875.479952219257 ----------
[1/200] Training loss: 0.10136166
[2/200] Training loss: 0.05242279
[3/200] Training loss: 0.04691772
[4/200] Training loss: 0.04192294
[5/200] Training loss: 0.03889040
[6/200] Training loss: 0.03566289
[7/200] Training loss: 0.03407224
[8/200] Training loss: 0.03273736
[9/200] Training loss: 0.02994253
[10/200] Training loss: 0.02809780
[50/200] Training loss: 0.01700563
[100/200] Training loss: 0.01442033
[150/200] Training loss: 0.01280301
[200/200] Training loss: 0.01145753
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 23074.266185514978 ----------
[1/200] Training loss: 0.07079721
[2/200] Training loss: 0.04115355
[3/200] Training loss: 0.03149261
[4/200] Training loss: 0.02659221
[5/200] Training loss: 0.02386614
[6/200] Training loss: 0.02275302
[7/200] Training loss: 0.02203740
[8/200] Training loss: 0.02208046
[9/200] Training loss: 0.02152289
[10/200] Training loss: 0.02009539
[50/200] Training loss: 0.01492196
[100/200] Training loss: 0.01293501
[150/200] Training loss: 0.01134737
[200/200] Training loss: 0.01059197
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9703.92827673412 ----------
[1/200] Training loss: 0.11894728
[2/200] Training loss: 0.05298649
[3/200] Training loss: 0.04593926
[4/200] Training loss: 0.04205499
[5/200] Training loss: 0.03839967
[6/200] Training loss: 0.03450836
[7/200] Training loss: 0.03169931
[8/200] Training loss: 0.02929112
[9/200] Training loss: 0.02785418
[10/200] Training loss: 0.02664060
[50/200] Training loss: 0.01625559
[100/200] Training loss: 0.01450194
[150/200] Training loss: 0.01307330
[200/200] Training loss: 0.01190621
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4139.733324744481 ----------
[1/200] Training loss: 0.07241495
[2/200] Training loss: 0.04750978
[3/200] Training loss: 0.03797282
[4/200] Training loss: 0.03250545
[5/200] Training loss: 0.02937204
[6/200] Training loss: 0.02798546
[7/200] Training loss: 0.02677755
[8/200] Training loss: 0.02545870
[9/200] Training loss: 0.02317593
[10/200] Training loss: 0.02284141
[50/200] Training loss: 0.01358546
[100/200] Training loss: 0.01163247
[150/200] Training loss: 0.01027481
[200/200] Training loss: 0.00972616
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13172.754305763088 ----------
[1/200] Training loss: 0.16082694
[2/200] Training loss: 0.06021565
[3/200] Training loss: 0.05632875
[4/200] Training loss: 0.05389170
[5/200] Training loss: 0.05138907
[6/200] Training loss: 0.04837441
[7/200] Training loss: 0.04625553
[8/200] Training loss: 0.04269122
[9/200] Training loss: 0.04164688
[10/200] Training loss: 0.03771794
[50/200] Training loss: 0.01637410
[100/200] Training loss: 0.01441133
[150/200] Training loss: 0.01351974
[200/200] Training loss: 0.01306146
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15389.092240934811 ----------
[1/200] Training loss: 0.13711276
[2/200] Training loss: 0.05835493
[3/200] Training loss: 0.05220727
[4/200] Training loss: 0.04634051
[5/200] Training loss: 0.04403676
[6/200] Training loss: 0.04280376
[7/200] Training loss: 0.03963827
[8/200] Training loss: 0.03563596
[9/200] Training loss: 0.03451192
[10/200] Training loss: 0.03393045
[50/200] Training loss: 0.01764913
[100/200] Training loss: 0.01395726
[150/200] Training loss: 0.01254935
[200/200] Training loss: 0.01202215
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9881.425403250281 ----------
[1/200] Training loss: 0.06902959
[2/200] Training loss: 0.04451006
[3/200] Training loss: 0.03541676
[4/200] Training loss: 0.03054230
[5/200] Training loss: 0.02606329
[6/200] Training loss: 0.02455763
[7/200] Training loss: 0.02307284
[8/200] Training loss: 0.02166643
[9/200] Training loss: 0.02153656
[10/200] Training loss: 0.02098188
[50/200] Training loss: 0.01378885
[100/200] Training loss: 0.01168487
[150/200] Training loss: 0.01061815
[200/200] Training loss: 0.00968232
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13202.748804699724 ----------
[1/200] Training loss: 0.16557685
[2/200] Training loss: 0.06345455
[3/200] Training loss: 0.05369272
[4/200] Training loss: 0.04938014
[5/200] Training loss: 0.04646698
[6/200] Training loss: 0.04506601
[7/200] Training loss: 0.04392696
[8/200] Training loss: 0.03845097
[9/200] Training loss: 0.03626800
[10/200] Training loss: 0.03283239
[50/200] Training loss: 0.01836274
[100/200] Training loss: 0.01516884
[150/200] Training loss: 0.01431716
[200/200] Training loss: 0.01379189
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8282.826570682257 ----------
[1/200] Training loss: 0.15210439
[2/200] Training loss: 0.05679308
[3/200] Training loss: 0.05186787
[4/200] Training loss: 0.04882995
[5/200] Training loss: 0.04722000
[6/200] Training loss: 0.04459389
[7/200] Training loss: 0.04170553
[8/200] Training loss: 0.04114131
[9/200] Training loss: 0.03660559
[10/200] Training loss: 0.03506962
[50/200] Training loss: 0.01853054
[100/200] Training loss: 0.01532640
[150/200] Training loss: 0.01348640
[200/200] Training loss: 0.01284690
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9021.837950218347 ----------
[1/200] Training loss: 0.12378673
[2/200] Training loss: 0.05670872
[3/200] Training loss: 0.05314327
[4/200] Training loss: 0.05016504
[5/200] Training loss: 0.04840774
[6/200] Training loss: 0.04513866
[7/200] Training loss: 0.04285938
[8/200] Training loss: 0.04043045
[9/200] Training loss: 0.03889403
[10/200] Training loss: 0.03718991
[50/200] Training loss: 0.01877375
[100/200] Training loss: 0.01528980
[150/200] Training loss: 0.01317811
[200/200] Training loss: 0.01208865
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13134.780089518057 ----------
[1/200] Training loss: 0.12520403
[2/200] Training loss: 0.05663694
[3/200] Training loss: 0.05108886
[4/200] Training loss: 0.04572875
[5/200] Training loss: 0.04255034
[6/200] Training loss: 0.03713457
[7/200] Training loss: 0.03289663
[8/200] Training loss: 0.03264424
[9/200] Training loss: 0.03206001
[10/200] Training loss: 0.03113186
[50/200] Training loss: 0.01737315
[100/200] Training loss: 0.01518027
[150/200] Training loss: 0.01262556
[200/200] Training loss: 0.01235374
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10785.248814932365 ----------
[1/200] Training loss: 0.06881795
[2/200] Training loss: 0.03758424
[3/200] Training loss: 0.03319509
[4/200] Training loss: 0.02849084
[5/200] Training loss: 0.02674086
[6/200] Training loss: 0.02495671
[7/200] Training loss: 0.02412886
[8/200] Training loss: 0.02269465
[9/200] Training loss: 0.02225787
[10/200] Training loss: 0.02162197
[50/200] Training loss: 0.01427311
[100/200] Training loss: 0.01158861
[150/200] Training loss: 0.01067738
[200/200] Training loss: 0.01019168
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6380.444498622333 ----------
[1/200] Training loss: 0.10651607
[2/200] Training loss: 0.05372489
[3/200] Training loss: 0.04421197
[4/200] Training loss: 0.03955166
[5/200] Training loss: 0.03289592
[6/200] Training loss: 0.03079369
[7/200] Training loss: 0.02875551
[8/200] Training loss: 0.02782781
[9/200] Training loss: 0.02687364
[10/200] Training loss: 0.02560774
[50/200] Training loss: 0.01575597
[100/200] Training loss: 0.01361106
[150/200] Training loss: 0.01285840
[200/200] Training loss: 0.01226005
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15907.563986984305 ----------
[1/200] Training loss: 0.10108097
[2/200] Training loss: 0.05258573
[3/200] Training loss: 0.04476624
[4/200] Training loss: 0.04056493
[5/200] Training loss: 0.03639865
[6/200] Training loss: 0.03292315
[7/200] Training loss: 0.02864424
[8/200] Training loss: 0.02740124
[9/200] Training loss: 0.02731542
[10/200] Training loss: 0.02585017
[50/200] Training loss: 0.01700906
[100/200] Training loss: 0.01495200
[150/200] Training loss: 0.01359592
[200/200] Training loss: 0.01206615
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 18254.609061823263 ----------
[1/200] Training loss: 0.16445411
[2/200] Training loss: 0.06192635
[3/200] Training loss: 0.05360448
[4/200] Training loss: 0.05049454
[5/200] Training loss: 0.04823537
[6/200] Training loss: 0.04545795
[7/200] Training loss: 0.04231613
[8/200] Training loss: 0.04218139
[9/200] Training loss: 0.03875719
[10/200] Training loss: 0.03924509
[50/200] Training loss: 0.01817181
[100/200] Training loss: 0.01529663
[150/200] Training loss: 0.01393927
[200/200] Training loss: 0.01397521
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12785.179545082658 ----------
[1/200] Training loss: 0.09905318
[2/200] Training loss: 0.04928342
[3/200] Training loss: 0.04053130
[4/200] Training loss: 0.04025461
[5/200] Training loss: 0.03404924
[6/200] Training loss: 0.03246601
[7/200] Training loss: 0.02992625
[8/200] Training loss: 0.02697456
[9/200] Training loss: 0.02638988
[10/200] Training loss: 0.02621695
[50/200] Training loss: 0.01566489
[100/200] Training loss: 0.01329842
[150/200] Training loss: 0.01196235
[200/200] Training loss: 0.01127328
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12224.074607102168 ----------
[1/200] Training loss: 0.09677958
[2/200] Training loss: 0.05273065
[3/200] Training loss: 0.04898232
[4/200] Training loss: 0.04580558
[5/200] Training loss: 0.04197935
[6/200] Training loss: 0.03961542
[7/200] Training loss: 0.03748421
[8/200] Training loss: 0.03218855
[9/200] Training loss: 0.03054452
[10/200] Training loss: 0.02750264
[50/200] Training loss: 0.01587557
[100/200] Training loss: 0.01353204
[150/200] Training loss: 0.01172094
[200/200] Training loss: 0.01115031
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6452.205204424298 ----------
[1/200] Training loss: 0.06604656
[2/200] Training loss: 0.04676012
[3/200] Training loss: 0.04033583
[4/200] Training loss: 0.03460823
[5/200] Training loss: 0.02928497
[6/200] Training loss: 0.02753373
[7/200] Training loss: 0.02615882
[8/200] Training loss: 0.02542258
[9/200] Training loss: 0.02346514
[10/200] Training loss: 0.02260541
[50/200] Training loss: 0.01312905
[100/200] Training loss: 0.01129220
[150/200] Training loss: 0.01038592
[200/200] Training loss: 0.00959448
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.0096643637735228
----FITNESS-----------RMSE---- 13721.411006161137 ----------
[1/200] Training loss: 0.13967928
[2/200] Training loss: 0.05886179
[3/200] Training loss: 0.05406718
[4/200] Training loss: 0.04990676
[5/200] Training loss: 0.04846399
[6/200] Training loss: 0.04582219
[7/200] Training loss: 0.04333811
[8/200] Training loss: 0.04367923
[9/200] Training loss: 0.04030666
[10/200] Training loss: 0.03661515
[50/200] Training loss: 0.01820398
[100/200] Training loss: 0.01541224
[150/200] Training loss: 0.01388551
[200/200] Training loss: 0.01245259
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10694.077987372264 ----------
[1/200] Training loss: 0.10138285
[2/200] Training loss: 0.05142004
[3/200] Training loss: 0.04615857
[4/200] Training loss: 0.04002561
[5/200] Training loss: 0.03813385
[6/200] Training loss: 0.03384388
[7/200] Training loss: 0.03144841
[8/200] Training loss: 0.02831240
[9/200] Training loss: 0.02944423
[10/200] Training loss: 0.02472755
[50/200] Training loss: 0.01584905
[100/200] Training loss: 0.01348283
[150/200] Training loss: 0.01202096
[200/200] Training loss: 0.01155182
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9498.317745790568 ----------
[1/200] Training loss: 0.11558896
[2/200] Training loss: 0.05788032
[3/200] Training loss: 0.04937444
[4/200] Training loss: 0.04640915
[5/200] Training loss: 0.04191177
[6/200] Training loss: 0.03723349
[7/200] Training loss: 0.03368326
[8/200] Training loss: 0.02969820
[9/200] Training loss: 0.02792474
[10/200] Training loss: 0.02720160
[50/200] Training loss: 0.01627561
[100/200] Training loss: 0.01434997
[150/200] Training loss: 0.01243875
[200/200] Training loss: 0.01147151
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8521.39753796289 ----------
[1/200] Training loss: 0.11991122
[2/200] Training loss: 0.05553731
[3/200] Training loss: 0.04937846
[4/200] Training loss: 0.04656011
[5/200] Training loss: 0.04290998
[6/200] Training loss: 0.03968018
[7/200] Training loss: 0.03694024
[8/200] Training loss: 0.03450622
[9/200] Training loss: 0.03333533
[10/200] Training loss: 0.03160298
[50/200] Training loss: 0.01808403
[100/200] Training loss: 0.01582684
[150/200] Training loss: 0.01455442
[200/200] Training loss: 0.01367000
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9814.318519387885 ----------
[1/200] Training loss: 0.15871580
[2/200] Training loss: 0.05536062
[3/200] Training loss: 0.05154128
[4/200] Training loss: 0.04571704
[5/200] Training loss: 0.04308296
[6/200] Training loss: 0.04044235
[7/200] Training loss: 0.03759373
[8/200] Training loss: 0.03423746
[9/200] Training loss: 0.03358949
[10/200] Training loss: 0.03444963
[50/200] Training loss: 0.01855284
[100/200] Training loss: 0.01420533
[150/200] Training loss: 0.01363688
[200/200] Training loss: 0.01244050
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8095.471079560472 ----------
[1/200] Training loss: 0.16246032
[2/200] Training loss: 0.05958207
[3/200] Training loss: 0.05063741
[4/200] Training loss: 0.04921136
[5/200] Training loss: 0.04581703
[6/200] Training loss: 0.04153017
[7/200] Training loss: 0.04192876
[8/200] Training loss: 0.03787269
[9/200] Training loss: 0.03619158
[10/200] Training loss: 0.03498233
[50/200] Training loss: 0.01894516
[100/200] Training loss: 0.01578534
[150/200] Training loss: 0.01505737
[200/200] Training loss: 0.01397945
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10380.767601675707 ----------
[1/200] Training loss: 0.10518486
[2/200] Training loss: 0.04984595
[3/200] Training loss: 0.04478318
[4/200] Training loss: 0.04426106
[5/200] Training loss: 0.03722742
[6/200] Training loss: 0.03608506
[7/200] Training loss: 0.03097737
[8/200] Training loss: 0.02910998
[9/200] Training loss: 0.02685354
[10/200] Training loss: 0.02570887
[50/200] Training loss: 0.01470781
[100/200] Training loss: 0.01300486
[150/200] Training loss: 0.01127135
[200/200] Training loss: 0.01065263
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11466.872633809098 ----------
[1/200] Training loss: 0.15093698
[2/200] Training loss: 0.06165074
[3/200] Training loss: 0.05385481
[4/200] Training loss: 0.05100940
[5/200] Training loss: 0.05052802
[6/200] Training loss: 0.04717604
[7/200] Training loss: 0.04708077
[8/200] Training loss: 0.04363874
[9/200] Training loss: 0.04135626
[10/200] Training loss: 0.03953729
[50/200] Training loss: 0.01729510
[100/200] Training loss: 0.01415843
[150/200] Training loss: 0.01293308
[200/200] Training loss: 0.01237776
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10017.673582224568 ----------
[1/200] Training loss: 0.16568605
[2/200] Training loss: 0.05745808
[3/200] Training loss: 0.05404292
[4/200] Training loss: 0.05187202
[5/200] Training loss: 0.05074408
[6/200] Training loss: 0.04640499
[7/200] Training loss: 0.04438545
[8/200] Training loss: 0.04084380
[9/200] Training loss: 0.03958930
[10/200] Training loss: 0.03642713
[50/200] Training loss: 0.01910770
[100/200] Training loss: 0.01661632
[150/200] Training loss: 0.01583769
[200/200] Training loss: 0.01434822
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17321.92691359711 ----------
[1/200] Training loss: 0.15734787
[2/200] Training loss: 0.05591471
[3/200] Training loss: 0.05210616
[4/200] Training loss: 0.04787761
[5/200] Training loss: 0.04544267
[6/200] Training loss: 0.04300830
[7/200] Training loss: 0.04019507
[8/200] Training loss: 0.03917393
[9/200] Training loss: 0.03714051
[10/200] Training loss: 0.03682559
[50/200] Training loss: 0.01952188
[100/200] Training loss: 0.01563671
[150/200] Training loss: 0.01469940
[200/200] Training loss: 0.01349771
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6849.187397056676 ----------
[1/200] Training loss: 0.09999785
[2/200] Training loss: 0.05249072
[3/200] Training loss: 0.04522592
[4/200] Training loss: 0.04013430
[5/200] Training loss: 0.03541833
[6/200] Training loss: 0.03157443
[7/200] Training loss: 0.02770008
[8/200] Training loss: 0.02673237
[9/200] Training loss: 0.02523815
[10/200] Training loss: 0.02399033
[50/200] Training loss: 0.01603836
[100/200] Training loss: 0.01426090
[150/200] Training loss: 0.01295568
[200/200] Training loss: 0.01170436
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13626.019815045038 ----------
[1/200] Training loss: 0.16474585
[2/200] Training loss: 0.05827290
[3/200] Training loss: 0.04597168
[4/200] Training loss: 0.04264651
[5/200] Training loss: 0.03687870
[6/200] Training loss: 0.03476167
[7/200] Training loss: 0.03393416
[8/200] Training loss: 0.03277546
[9/200] Training loss: 0.02969498
[10/200] Training loss: 0.02920961
[50/200] Training loss: 0.01679919
[100/200] Training loss: 0.01439328
[150/200] Training loss: 0.01274587
[200/200] Training loss: 0.01138636
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 23783.87352808621 ----------
[1/200] Training loss: 0.10373514
[2/200] Training loss: 0.05374629
[3/200] Training loss: 0.04705298
[4/200] Training loss: 0.04381000
[5/200] Training loss: 0.04012646
[6/200] Training loss: 0.03668448
[7/200] Training loss: 0.03245931
[8/200] Training loss: 0.03005615
[9/200] Training loss: 0.03015866
[10/200] Training loss: 0.02746125
[50/200] Training loss: 0.01674643
[100/200] Training loss: 0.01427293
[150/200] Training loss: 0.01303926
[200/200] Training loss: 0.01185042
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12287.321595856438 ----------
[1/200] Training loss: 0.10543423
[2/200] Training loss: 0.04828216
[3/200] Training loss: 0.04266456
[4/200] Training loss: 0.03678229
[5/200] Training loss: 0.03359060
[6/200] Training loss: 0.03089399
[7/200] Training loss: 0.02690333
[8/200] Training loss: 0.02571184
[9/200] Training loss: 0.02549453
[10/200] Training loss: 0.02534598
[50/200] Training loss: 0.01718480
[100/200] Training loss: 0.01482513
[150/200] Training loss: 0.01403470
[200/200] Training loss: 0.01281173
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6632.9510777632 ----------
[1/200] Training loss: 0.15143743
[2/200] Training loss: 0.05717650
[3/200] Training loss: 0.05033414
[4/200] Training loss: 0.05034788
[5/200] Training loss: 0.04638576
[6/200] Training loss: 0.04416898
[7/200] Training loss: 0.04057113
[8/200] Training loss: 0.04094135
[9/200] Training loss: 0.03607268
[10/200] Training loss: 0.03474773
[50/200] Training loss: 0.01850648
[100/200] Training loss: 0.01657125
[150/200] Training loss: 0.01514198
[200/200] Training loss: 0.01410066
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12970.656729711105 ----------
[1/200] Training loss: 0.10697648
[2/200] Training loss: 0.05076492
[3/200] Training loss: 0.04231606
[4/200] Training loss: 0.03758136
[5/200] Training loss: 0.03533571
[6/200] Training loss: 0.03120827
[7/200] Training loss: 0.02893696
[8/200] Training loss: 0.02828087
[9/200] Training loss: 0.02604807
[10/200] Training loss: 0.02507231
[50/200] Training loss: 0.01833833
[100/200] Training loss: 0.01569272
[150/200] Training loss: 0.01420613
[200/200] Training loss: 0.01324491
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11329.966990243176 ----------
[1/200] Training loss: 0.07532837
[2/200] Training loss: 0.04722745
[3/200] Training loss: 0.04056458
[4/200] Training loss: 0.03422894
[5/200] Training loss: 0.03054264
[6/200] Training loss: 0.02833436
[7/200] Training loss: 0.02584042
[8/200] Training loss: 0.02428386
[9/200] Training loss: 0.02315206
[10/200] Training loss: 0.02160824
[50/200] Training loss: 0.01465112
[100/200] Training loss: 0.01223707
[150/200] Training loss: 0.01082608
[200/200] Training loss: 0.00973375
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6504.105780197613 ----------
[1/200] Training loss: 0.11315705
[2/200] Training loss: 0.05600493
[3/200] Training loss: 0.05260344
[4/200] Training loss: 0.05013659
[5/200] Training loss: 0.04425633
[6/200] Training loss: 0.04181453
[7/200] Training loss: 0.03777248
[8/200] Training loss: 0.03729415
[9/200] Training loss: 0.03480257
[10/200] Training loss: 0.03231659
[50/200] Training loss: 0.01793964
[100/200] Training loss: 0.01602883
[150/200] Training loss: 0.01474241
[200/200] Training loss: 0.01344965
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10911.262071822855 ----------
[1/200] Training loss: 0.10742057
[2/200] Training loss: 0.05245135
[3/200] Training loss: 0.04439529
[4/200] Training loss: 0.03888426
[5/200] Training loss: 0.03283362
[6/200] Training loss: 0.03105217
[7/200] Training loss: 0.02784833
[8/200] Training loss: 0.02507791
[9/200] Training loss: 0.02428304
[10/200] Training loss: 0.02474157
[50/200] Training loss: 0.01623608
[100/200] Training loss: 0.01452521
[150/200] Training loss: 0.01360981
[200/200] Training loss: 0.01245392
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15020.053262222475 ----------
[1/200] Training loss: 0.07817638
[2/200] Training loss: 0.04032335
[3/200] Training loss: 0.03054966
[4/200] Training loss: 0.02786787
[5/200] Training loss: 0.02573534
[6/200] Training loss: 0.02417236
[7/200] Training loss: 0.02310076
[8/200] Training loss: 0.02285493
[9/200] Training loss: 0.02202066
[10/200] Training loss: 0.02112702
[50/200] Training loss: 0.01537442
[100/200] Training loss: 0.01365683
[150/200] Training loss: 0.01231674
[200/200] Training loss: 0.01137702
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8427.268121995407 ----------
[1/200] Training loss: 0.15945032
[2/200] Training loss: 0.06404334
[3/200] Training loss: 0.05380948
[4/200] Training loss: 0.05339613
[5/200] Training loss: 0.04920984
[6/200] Training loss: 0.04784842
[7/200] Training loss: 0.04684243
[8/200] Training loss: 0.04400792
[9/200] Training loss: 0.04055940
[10/200] Training loss: 0.03940513
[50/200] Training loss: 0.01872993
[100/200] Training loss: 0.01630326
[150/200] Training loss: 0.01448038
[200/200] Training loss: 0.01330892
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12657.936008686409 ----------
[1/200] Training loss: 0.11386700
[2/200] Training loss: 0.05377556
[3/200] Training loss: 0.04670295
[4/200] Training loss: 0.03989051
[5/200] Training loss: 0.03648067
[6/200] Training loss: 0.03231138
[7/200] Training loss: 0.02885323
[8/200] Training loss: 0.02910054
[9/200] Training loss: 0.02677923
[10/200] Training loss: 0.02605397
[50/200] Training loss: 0.01798410
[100/200] Training loss: 0.01577590
[150/200] Training loss: 0.01465403
[200/200] Training loss: 0.01375866
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11223.431917198946 ----------
[1/200] Training loss: 0.15492590
[2/200] Training loss: 0.06220745
[3/200] Training loss: 0.05611251
[4/200] Training loss: 0.05366768
[5/200] Training loss: 0.04950349
[6/200] Training loss: 0.04964000
[7/200] Training loss: 0.04743568
[8/200] Training loss: 0.04610721
[9/200] Training loss: 0.04435844
[10/200] Training loss: 0.04194268
[50/200] Training loss: 0.01924573
[100/200] Training loss: 0.01486493
[150/200] Training loss: 0.01361137
[200/200] Training loss: 0.01234714
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17901.82560522809 ----------
[1/200] Training loss: 0.10860524
[2/200] Training loss: 0.05418749
[3/200] Training loss: 0.04783116
[4/200] Training loss: 0.04101145
[5/200] Training loss: 0.03753049
[6/200] Training loss: 0.03406381
[7/200] Training loss: 0.02930267
[8/200] Training loss: 0.02887093
[9/200] Training loss: 0.02813636
[10/200] Training loss: 0.02621710
[50/200] Training loss: 0.01636744
[100/200] Training loss: 0.01422748
[150/200] Training loss: 0.01270639
[200/200] Training loss: 0.01192375
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5669.145261853853 ----------
[1/200] Training loss: 0.11108833
[2/200] Training loss: 0.05877221
[3/200] Training loss: 0.05249133
[4/200] Training loss: 0.04902195
[5/200] Training loss: 0.04314816
[6/200] Training loss: 0.03866590
[7/200] Training loss: 0.03473214
[8/200] Training loss: 0.03119622
[9/200] Training loss: 0.02808321
[10/200] Training loss: 0.02644082
[50/200] Training loss: 0.01686915
[100/200] Training loss: 0.01448514
[150/200] Training loss: 0.01314725
[200/200] Training loss: 0.01213285
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13795.672364912121 ----------
[1/200] Training loss: 0.08134420
[2/200] Training loss: 0.04821632
[3/200] Training loss: 0.03906574
[4/200] Training loss: 0.03237137
[5/200] Training loss: 0.02789848
[6/200] Training loss: 0.02675481
[7/200] Training loss: 0.02557313
[8/200] Training loss: 0.02425502
[9/200] Training loss: 0.02290553
[10/200] Training loss: 0.02225051
[50/200] Training loss: 0.01497175
[100/200] Training loss: 0.01268432
[150/200] Training loss: 0.01155286
[200/200] Training loss: 0.01048430
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 3993.0533429945563 ----------
[1/200] Training loss: 0.16618195
[2/200] Training loss: 0.05591662
[3/200] Training loss: 0.05672494
[4/200] Training loss: 0.05082598
[5/200] Training loss: 0.04784375
[6/200] Training loss: 0.04720140
[7/200] Training loss: 0.04608684
[8/200] Training loss: 0.04478027
[9/200] Training loss: 0.04280876
[10/200] Training loss: 0.03911982
[50/200] Training loss: 0.02568390
[100/200] Training loss: 0.01784689
[150/200] Training loss: 0.01580696
[200/200] Training loss: 0.01385198
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10381.660753463291 ----------
[1/200] Training loss: 0.10606354
[2/200] Training loss: 0.05087444
[3/200] Training loss: 0.04778436
[4/200] Training loss: 0.04260484
[5/200] Training loss: 0.04210511
[6/200] Training loss: 0.03755881
[7/200] Training loss: 0.03644859
[8/200] Training loss: 0.03442629
[9/200] Training loss: 0.03097607
[10/200] Training loss: 0.02733677
[50/200] Training loss: 0.01667958
[100/200] Training loss: 0.01453925
[150/200] Training loss: 0.01308471
[200/200] Training loss: 0.01212390
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16388.0336831482 ----------
[1/200] Training loss: 0.15601123
[2/200] Training loss: 0.05621133
[3/200] Training loss: 0.04642980
[4/200] Training loss: 0.04346901
[5/200] Training loss: 0.04063094
[6/200] Training loss: 0.03948575
[7/200] Training loss: 0.03596090
[8/200] Training loss: 0.03501052
[9/200] Training loss: 0.03348102
[10/200] Training loss: 0.03291655
[50/200] Training loss: 0.01946682
[100/200] Training loss: 0.01684443
[150/200] Training loss: 0.01470061
[200/200] Training loss: 0.01360777
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8439.600464476976 ----------
[1/200] Training loss: 0.07286332
[2/200] Training loss: 0.04800273
[3/200] Training loss: 0.04075867
[4/200] Training loss: 0.03356771
[5/200] Training loss: 0.02927752
[6/200] Training loss: 0.02762972
[7/200] Training loss: 0.02518017
[8/200] Training loss: 0.02463830
[9/200] Training loss: 0.02369542
[10/200] Training loss: 0.02215564
[50/200] Training loss: 0.01422031
[100/200] Training loss: 0.01197912
[150/200] Training loss: 0.01141549
[200/200] Training loss: 0.01042035
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6162.528052674486 ----------
[1/200] Training loss: 0.14414609
[2/200] Training loss: 0.05642879
[3/200] Training loss: 0.05005551
[4/200] Training loss: 0.04665051
[5/200] Training loss: 0.04550197
[6/200] Training loss: 0.04148685
[7/200] Training loss: 0.04027635
[8/200] Training loss: 0.03599019
[9/200] Training loss: 0.03340068
[10/200] Training loss: 0.03277492
[50/200] Training loss: 0.01830614
[100/200] Training loss: 0.01710429
[150/200] Training loss: 0.01521620
[200/200] Training loss: 0.01427846
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15372.953392240543 ----------
[1/200] Training loss: 0.07338449
[2/200] Training loss: 0.04688987
[3/200] Training loss: 0.03971266
[4/200] Training loss: 0.03489685
[5/200] Training loss: 0.03051209
[6/200] Training loss: 0.02895964
[7/200] Training loss: 0.02670981
[8/200] Training loss: 0.02539929
[9/200] Training loss: 0.02437944
[10/200] Training loss: 0.02357570
[50/200] Training loss: 0.01702594
[100/200] Training loss: 0.01425811
[150/200] Training loss: 0.01242574
[200/200] Training loss: 0.01139485
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7419.566564159931 ----------
[1/200] Training loss: 0.10380492
[2/200] Training loss: 0.05563359
[3/200] Training loss: 0.05176732
[4/200] Training loss: 0.04928652
[5/200] Training loss: 0.04541812
[6/200] Training loss: 0.04118923
[7/200] Training loss: 0.03742699
[8/200] Training loss: 0.03369867
[9/200] Training loss: 0.02999333
[10/200] Training loss: 0.02690453
[50/200] Training loss: 0.01558446
[100/200] Training loss: 0.01325680
[150/200] Training loss: 0.01200741
[200/200] Training loss: 0.01107090
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10338.733384704337 ----------
[1/200] Training loss: 0.15737350
[2/200] Training loss: 0.05889333
[3/200] Training loss: 0.05617552
[4/200] Training loss: 0.05273933
[5/200] Training loss: 0.04937171
[6/200] Training loss: 0.04766113
[7/200] Training loss: 0.04644417
[8/200] Training loss: 0.04124183
[9/200] Training loss: 0.04089810
[10/200] Training loss: 0.04009162
[50/200] Training loss: 0.01852878
[100/200] Training loss: 0.01602536
[150/200] Training loss: 0.01440829
[200/200] Training loss: 0.01360117
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 18610.206232065244 ----------
[1/200] Training loss: 0.12330407
[2/200] Training loss: 0.05395838
[3/200] Training loss: 0.04799063
[4/200] Training loss: 0.04360094
[5/200] Training loss: 0.04038240
[6/200] Training loss: 0.03563098
[7/200] Training loss: 0.03284255
[8/200] Training loss: 0.03345986
[9/200] Training loss: 0.03154049
[10/200] Training loss: 0.02973153
[50/200] Training loss: 0.01729208
[100/200] Training loss: 0.01502652
[150/200] Training loss: 0.01335481
[200/200] Training loss: 0.01179746
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10376.956779326008 ----------
[1/200] Training loss: 0.06592170
[2/200] Training loss: 0.04431657
[3/200] Training loss: 0.03285223
[4/200] Training loss: 0.02930499
[5/200] Training loss: 0.02603739
[6/200] Training loss: 0.02426759
[7/200] Training loss: 0.02306410
[8/200] Training loss: 0.02218958
[9/200] Training loss: 0.02080108
[10/200] Training loss: 0.02040220
[50/200] Training loss: 0.01471767
[100/200] Training loss: 0.01236559
[150/200] Training loss: 0.01129996
[200/200] Training loss: 0.01028188
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9786.816029741236 ----------
[1/200] Training loss: 0.10217855
[2/200] Training loss: 0.05515672
[3/200] Training loss: 0.05055321
[4/200] Training loss: 0.04609102
[5/200] Training loss: 0.04335047
[6/200] Training loss: 0.03882689
[7/200] Training loss: 0.03497205
[8/200] Training loss: 0.03007708
[9/200] Training loss: 0.02721435
[10/200] Training loss: 0.02582907
[50/200] Training loss: 0.01725203
[100/200] Training loss: 0.01543300
[150/200] Training loss: 0.01431421
[200/200] Training loss: 0.01315326
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11207.093111061406 ----------
[1/200] Training loss: 0.11608393
[2/200] Training loss: 0.05217580
[3/200] Training loss: 0.04765413
[4/200] Training loss: 0.04315767
[5/200] Training loss: 0.03887695
[6/200] Training loss: 0.03706962
[7/200] Training loss: 0.03274815
[8/200] Training loss: 0.02929372
[9/200] Training loss: 0.02816480
[10/200] Training loss: 0.02602071
[50/200] Training loss: 0.01789128
[100/200] Training loss: 0.01581114
[150/200] Training loss: 0.01460761
[200/200] Training loss: 0.01343713
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12200.990779440825 ----------
[1/200] Training loss: 0.11965448
[2/200] Training loss: 0.05412358
[3/200] Training loss: 0.05143720
[4/200] Training loss: 0.04862291
[5/200] Training loss: 0.04518079
[6/200] Training loss: 0.04207460
[7/200] Training loss: 0.03790100
[8/200] Training loss: 0.03479810
[9/200] Training loss: 0.03286996
[10/200] Training loss: 0.03047995
[50/200] Training loss: 0.01640032
[100/200] Training loss: 0.01406292
[150/200] Training loss: 0.01292667
[200/200] Training loss: 0.01212607
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14022.307941276998 ----------
[1/200] Training loss: 0.06528422
[2/200] Training loss: 0.03659780
[3/200] Training loss: 0.02911600
[4/200] Training loss: 0.02675332
[5/200] Training loss: 0.02441186
[6/200] Training loss: 0.02424202
[7/200] Training loss: 0.02310058
[8/200] Training loss: 0.02170447
[9/200] Training loss: 0.02157955
[10/200] Training loss: 0.02127720
[50/200] Training loss: 0.01280040
[100/200] Training loss: 0.01103852
[150/200] Training loss: 0.00991944
[200/200] Training loss: 0.00930521
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10820.624750909717 ----------
[1/200] Training loss: 0.11967868
[2/200] Training loss: 0.05248478
[3/200] Training loss: 0.04577847
[4/200] Training loss: 0.04363427
[5/200] Training loss: 0.03888241
[6/200] Training loss: 0.03473697
[7/200] Training loss: 0.03234631
[8/200] Training loss: 0.02980598
[9/200] Training loss: 0.02808775
[10/200] Training loss: 0.02881974
[50/200] Training loss: 0.01735485
[100/200] Training loss: 0.01553493
[150/200] Training loss: 0.01380346
[200/200] Training loss: 0.01301167
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5716.054758310141 ----------
[1/200] Training loss: 0.11279050
[2/200] Training loss: 0.05144669
[3/200] Training loss: 0.04411568
[4/200] Training loss: 0.04119221
[5/200] Training loss: 0.03900236
[6/200] Training loss: 0.03478465
[7/200] Training loss: 0.03080954
[8/200] Training loss: 0.03117072
[9/200] Training loss: 0.02920073
[10/200] Training loss: 0.02706406
[50/200] Training loss: 0.01563280
[100/200] Training loss: 0.01306499
[150/200] Training loss: 0.01162313
[200/200] Training loss: 0.01085676
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8882.402377735429 ----------
[1/200] Training loss: 0.07060568
[2/200] Training loss: 0.04238149
[3/200] Training loss: 0.03259790
[4/200] Training loss: 0.02840439
[5/200] Training loss: 0.02531299
[6/200] Training loss: 0.02334692
[7/200] Training loss: 0.02204102
[8/200] Training loss: 0.02151840
[9/200] Training loss: 0.02015677
[10/200] Training loss: 0.01954032
[50/200] Training loss: 0.01362426
[100/200] Training loss: 0.01192529
[150/200] Training loss: 0.01084956
[200/200] Training loss: 0.00998331
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12749.415672884777 ----------
[1/200] Training loss: 0.14596762
[2/200] Training loss: 0.05989968
[3/200] Training loss: 0.04889148
[4/200] Training loss: 0.04586843
[5/200] Training loss: 0.04086862
[6/200] Training loss: 0.04330531
[7/200] Training loss: 0.03602303
[8/200] Training loss: 0.03256063
[9/200] Training loss: 0.03395486
[10/200] Training loss: 0.02937478
[50/200] Training loss: 0.01631539
[100/200] Training loss: 0.01392977
[150/200] Training loss: 0.01245972
[200/200] Training loss: 0.01238893
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5666.773861731205 ----------
[1/200] Training loss: 0.11835401
[2/200] Training loss: 0.05698018
[3/200] Training loss: 0.05096965
[4/200] Training loss: 0.04821717
[5/200] Training loss: 0.04511455
[6/200] Training loss: 0.04151895
[7/200] Training loss: 0.03606623
[8/200] Training loss: 0.03214598
[9/200] Training loss: 0.02779958
[10/200] Training loss: 0.02797507
[50/200] Training loss: 0.01855505
[100/200] Training loss: 0.01619970
[150/200] Training loss: 0.01360603
[200/200] Training loss: 0.01293575
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11821.149859467987 ----------
[1/200] Training loss: 0.10641122
[2/200] Training loss: 0.05016426
[3/200] Training loss: 0.04501181
[4/200] Training loss: 0.03924017
[5/200] Training loss: 0.03604199
[6/200] Training loss: 0.03375443
[7/200] Training loss: 0.03066186
[8/200] Training loss: 0.02800942
[9/200] Training loss: 0.02778352
[10/200] Training loss: 0.02581665
[50/200] Training loss: 0.01519479
[100/200] Training loss: 0.01282622
[150/200] Training loss: 0.01109244
[200/200] Training loss: 0.01103011
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8141.51017932177 ----------
[1/200] Training loss: 0.12514137
[2/200] Training loss: 0.05363309
[3/200] Training loss: 0.04630557
[4/200] Training loss: 0.04262669
[5/200] Training loss: 0.03875486
[6/200] Training loss: 0.03574553
[7/200] Training loss: 0.03393445
[8/200] Training loss: 0.03145672
[9/200] Training loss: 0.02969443
[10/200] Training loss: 0.02808329
[50/200] Training loss: 0.01829775
[100/200] Training loss: 0.01627373
[150/200] Training loss: 0.01474454
[200/200] Training loss: 0.01324938
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5329.800371496103 ----------
[1/200] Training loss: 0.10550876
[2/200] Training loss: 0.05607131
[3/200] Training loss: 0.04908184
[4/200] Training loss: 0.04560369
[5/200] Training loss: 0.04120291
[6/200] Training loss: 0.03648321
[7/200] Training loss: 0.03190390
[8/200] Training loss: 0.03065875
[9/200] Training loss: 0.02701428
[10/200] Training loss: 0.02589651
[50/200] Training loss: 0.01560898
[100/200] Training loss: 0.01290839
[150/200] Training loss: 0.01166862
[200/200] Training loss: 0.01056918
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13212.123523491597 ----------
[1/200] Training loss: 0.10956420
[2/200] Training loss: 0.04837660
[3/200] Training loss: 0.04008458
[4/200] Training loss: 0.03737214
[5/200] Training loss: 0.03587605
[6/200] Training loss: 0.03237942
[7/200] Training loss: 0.02969061
[8/200] Training loss: 0.02885643
[9/200] Training loss: 0.02736485
[10/200] Training loss: 0.02553681
[50/200] Training loss: 0.01509888
[100/200] Training loss: 0.01319260
[150/200] Training loss: 0.01170297
[200/200] Training loss: 0.01023944
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4173.29893489551 ----------
[1/200] Training loss: 0.11709546
[2/200] Training loss: 0.05685499
[3/200] Training loss: 0.05174248
[4/200] Training loss: 0.04297928
[5/200] Training loss: 0.03632538
[6/200] Training loss: 0.03347821
[7/200] Training loss: 0.03177941
[8/200] Training loss: 0.02905564
[9/200] Training loss: 0.02622670
[10/200] Training loss: 0.02596258
[50/200] Training loss: 0.01734097
[100/200] Training loss: 0.01504703
[150/200] Training loss: 0.01381938
[200/200] Training loss: 0.01310303
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17581.11714311693 ----------
[1/200] Training loss: 0.14966435
[2/200] Training loss: 0.05852361
[3/200] Training loss: 0.05195580
[4/200] Training loss: 0.04890639
[5/200] Training loss: 0.04658054
[6/200] Training loss: 0.04387245
[7/200] Training loss: 0.04213851
[8/200] Training loss: 0.04004285
[9/200] Training loss: 0.03885472
[10/200] Training loss: 0.03430737
[50/200] Training loss: 0.01725525
[100/200] Training loss: 0.01500372
[150/200] Training loss: 0.01239900
[200/200] Training loss: 0.01250856
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6878.800476827337 ----------
[1/200] Training loss: 0.07222412
[2/200] Training loss: 0.04726162
[3/200] Training loss: 0.03779753
[4/200] Training loss: 0.03021590
[5/200] Training loss: 0.02651669
[6/200] Training loss: 0.02436850
[7/200] Training loss: 0.02360185
[8/200] Training loss: 0.02272731
[9/200] Training loss: 0.02206446
[10/200] Training loss: 0.02141107
[50/200] Training loss: 0.01566491
[100/200] Training loss: 0.01426121
[150/200] Training loss: 0.01291674
[200/200] Training loss: 0.01211434
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 34663.98984537123 ----------
[1/200] Training loss: 0.11922817
[2/200] Training loss: 0.05540606
[3/200] Training loss: 0.05114441
[4/200] Training loss: 0.04832892
[5/200] Training loss: 0.04182694
[6/200] Training loss: 0.03591741
[7/200] Training loss: 0.03397152
[8/200] Training loss: 0.02873855
[9/200] Training loss: 0.02818425
[10/200] Training loss: 0.02709422
[50/200] Training loss: 0.01638488
[100/200] Training loss: 0.01392473
[150/200] Training loss: 0.01255730
[200/200] Training loss: 0.01137729
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13472.616375448386 ----------
[1/200] Training loss: 0.10365446
[2/200] Training loss: 0.05307656
[3/200] Training loss: 0.04734379
[4/200] Training loss: 0.04369805
[5/200] Training loss: 0.04044877
[6/200] Training loss: 0.03714671
[7/200] Training loss: 0.03402547
[8/200] Training loss: 0.03128569
[9/200] Training loss: 0.02894636
[10/200] Training loss: 0.02779308
[50/200] Training loss: 0.01639662
[100/200] Training loss: 0.01337812
[150/200] Training loss: 0.01251093
[200/200] Training loss: 0.01133608
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11063.634122656082 ----------
[1/200] Training loss: 0.07085722
[2/200] Training loss: 0.04141174
[3/200] Training loss: 0.03380345
[4/200] Training loss: 0.02882654
[5/200] Training loss: 0.02619146
[6/200] Training loss: 0.02451510
[7/200] Training loss: 0.02320221
[8/200] Training loss: 0.02257690
[9/200] Training loss: 0.02184211
[10/200] Training loss: 0.02131043
[50/200] Training loss: 0.01632413
[100/200] Training loss: 0.01423152
[150/200] Training loss: 0.01221134
[200/200] Training loss: 0.01119432
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9386.342418642098 ----------
[1/200] Training loss: 0.09332939
[2/200] Training loss: 0.04593173
[3/200] Training loss: 0.03946033
[4/200] Training loss: 0.03737836
[5/200] Training loss: 0.03573597
[6/200] Training loss: 0.03394460
[7/200] Training loss: 0.03118015
[8/200] Training loss: 0.03037142
[9/200] Training loss: 0.02969589
[10/200] Training loss: 0.02867764
[50/200] Training loss: 0.01638247
[100/200] Training loss: 0.01273629
[150/200] Training loss: 0.01144085
[200/200] Training loss: 0.01048346
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14939.302259476512 ----------
[1/200] Training loss: 0.07690802
[2/200] Training loss: 0.05159096
[3/200] Training loss: 0.04540137
[4/200] Training loss: 0.04148797
[5/200] Training loss: 0.03614344
[6/200] Training loss: 0.03224155
[7/200] Training loss: 0.02779911
[8/200] Training loss: 0.02491828
[9/200] Training loss: 0.02380073
[10/200] Training loss: 0.02280446
[50/200] Training loss: 0.01614525
[100/200] Training loss: 0.01375796
[150/200] Training loss: 0.01228701
[200/200] Training loss: 0.01141769
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 22057.136713544667 ----------
[1/200] Training loss: 0.07179314
[2/200] Training loss: 0.04593626
[3/200] Training loss: 0.03951033
[4/200] Training loss: 0.03516379
[5/200] Training loss: 0.03073517
[6/200] Training loss: 0.02825416
[7/200] Training loss: 0.02617219
[8/200] Training loss: 0.02528007
[9/200] Training loss: 0.02494398
[10/200] Training loss: 0.02394171
[50/200] Training loss: 0.01542851
[100/200] Training loss: 0.01279313
[150/200] Training loss: 0.01116879
[200/200] Training loss: 0.01061493
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7638.814305898527 ----------
[1/200] Training loss: 0.08417480
[2/200] Training loss: 0.04638243
[3/200] Training loss: 0.04113712
[4/200] Training loss: 0.03875466
[5/200] Training loss: 0.03689772
[6/200] Training loss: 0.03580506
[7/200] Training loss: 0.03245803
[8/200] Training loss: 0.03097441
[9/200] Training loss: 0.03186693
[10/200] Training loss: 0.02891229
[50/200] Training loss: 0.01695824
[100/200] Training loss: 0.01379944
[150/200] Training loss: 0.01166214
[200/200] Training loss: 0.01083263
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4452.695588067974 ----------
[1/200] Training loss: 0.06403805
[2/200] Training loss: 0.03843689
[3/200] Training loss: 0.03207920
[4/200] Training loss: 0.02854207
[5/200] Training loss: 0.02516947
[6/200] Training loss: 0.02398196
[7/200] Training loss: 0.02230975
[8/200] Training loss: 0.02148511
[9/200] Training loss: 0.02003178
[10/200] Training loss: 0.02020391
[50/200] Training loss: 0.01464833
[100/200] Training loss: 0.01150472
[150/200] Training loss: 0.01048962
[200/200] Training loss: 0.00984024
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12704.854505266874 ----------
[1/200] Training loss: 0.10962119
[2/200] Training loss: 0.05525244
[3/200] Training loss: 0.05119952
[4/200] Training loss: 0.04898797
[5/200] Training loss: 0.04524098
[6/200] Training loss: 0.04170327
[7/200] Training loss: 0.03777719
[8/200] Training loss: 0.03592372
[9/200] Training loss: 0.03093012
[10/200] Training loss: 0.02920273
[50/200] Training loss: 0.01717114
[100/200] Training loss: 0.01388660
[150/200] Training loss: 0.01207516
[200/200] Training loss: 0.01150366
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8786.260182808155 ----------
[1/200] Training loss: 0.10822248
[2/200] Training loss: 0.05115422
[3/200] Training loss: 0.04771904
[4/200] Training loss: 0.04507201
[5/200] Training loss: 0.04047271
[6/200] Training loss: 0.03794052
[7/200] Training loss: 0.03434468
[8/200] Training loss: 0.03397432
[9/200] Training loss: 0.03220988
[10/200] Training loss: 0.02934245
[50/200] Training loss: 0.01700536
[100/200] Training loss: 0.01414966
[150/200] Training loss: 0.01262698
[200/200] Training loss: 0.01213349
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6387.196881261763 ----------
[1/200] Training loss: 0.16113517
[2/200] Training loss: 0.05714174
[3/200] Training loss: 0.05422882
[4/200] Training loss: 0.05093575
[5/200] Training loss: 0.04938765
[6/200] Training loss: 0.04527570
[7/200] Training loss: 0.04529100
[8/200] Training loss: 0.04151824
[9/200] Training loss: 0.04240985
[10/200] Training loss: 0.03789189
[50/200] Training loss: 0.01906118
[100/200] Training loss: 0.01628889
[150/200] Training loss: 0.01451662
[200/200] Training loss: 0.01379585
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15903.867202664891 ----------
[1/200] Training loss: 0.11537888
[2/200] Training loss: 0.05717573
[3/200] Training loss: 0.04930048
[4/200] Training loss: 0.04333246
[5/200] Training loss: 0.04218092
[6/200] Training loss: 0.03731367
[7/200] Training loss: 0.03642533
[8/200] Training loss: 0.03157134
[9/200] Training loss: 0.02853184
[10/200] Training loss: 0.02749287
[50/200] Training loss: 0.01699087
[100/200] Training loss: 0.01426915
[150/200] Training loss: 0.01298650
[200/200] Training loss: 0.01166272
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16361.308016170346 ----------
[1/200] Training loss: 0.11166348
[2/200] Training loss: 0.05321900
[3/200] Training loss: 0.04690943
[4/200] Training loss: 0.04284824
[5/200] Training loss: 0.03669337
[6/200] Training loss: 0.03325635
[7/200] Training loss: 0.03059379
[8/200] Training loss: 0.02872537
[9/200] Training loss: 0.02727963
[10/200] Training loss: 0.02437626
[50/200] Training loss: 0.01544830
[100/200] Training loss: 0.01322248
[150/200] Training loss: 0.01177708
[200/200] Training loss: 0.01071421
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 18902.325782823656 ----------
[1/200] Training loss: 0.12197040
[2/200] Training loss: 0.06322260
[3/200] Training loss: 0.05492967
[4/200] Training loss: 0.05087672
[5/200] Training loss: 0.04798461
[6/200] Training loss: 0.04543040
[7/200] Training loss: 0.04523416
[8/200] Training loss: 0.04315853
[9/200] Training loss: 0.04141697
[10/200] Training loss: 0.04073523
[50/200] Training loss: 0.01800591
[100/200] Training loss: 0.01479256
[150/200] Training loss: 0.01348124
[200/200] Training loss: 0.01254771
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16184.427577149585 ----------
[1/200] Training loss: 0.12260702
[2/200] Training loss: 0.05656747
[3/200] Training loss: 0.05209041
[4/200] Training loss: 0.04938889
[5/200] Training loss: 0.04655130
[6/200] Training loss: 0.04306612
[7/200] Training loss: 0.04033647
[8/200] Training loss: 0.03792467
[9/200] Training loss: 0.03614969
[10/200] Training loss: 0.03414638
[50/200] Training loss: 0.01907394
[100/200] Training loss: 0.01496302
[150/200] Training loss: 0.01282989
[200/200] Training loss: 0.01208729
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8209.75760908932 ----------
[1/200] Training loss: 0.07809326
[2/200] Training loss: 0.04285626
[3/200] Training loss: 0.03893821
[4/200] Training loss: 0.03542095
[5/200] Training loss: 0.03333116
[6/200] Training loss: 0.03213824
[7/200] Training loss: 0.03053322
[8/200] Training loss: 0.02930284
[9/200] Training loss: 0.02801546
[10/200] Training loss: 0.02666374
[50/200] Training loss: 0.01480473
[100/200] Training loss: 0.01203168
[150/200] Training loss: 0.01093780
[200/200] Training loss: 0.01022880
---batch_size---: 4 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12327.937702632991 ----------
[1/200] Training loss: 0.10528154
[2/200] Training loss: 0.04747544
[3/200] Training loss: 0.04530240
[4/200] Training loss: 0.04178236
[5/200] Training loss: 0.03655316
[6/200] Training loss: 0.03237867
[7/200] Training loss: 0.03262992
[8/200] Training loss: 0.02858320
[9/200] Training loss: 0.02662992
[10/200] Training loss: 0.02686619
[50/200] Training loss: 0.01586026
[100/200] Training loss: 0.01328380
[150/200] Training loss: 0.01194741
[200/200] Training loss: 0.01060003
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9231.571913818361 ----------
[1/200] Training loss: 0.09703300
[2/200] Training loss: 0.05446566
[3/200] Training loss: 0.04964516
[4/200] Training loss: 0.04541040
[5/200] Training loss: 0.04013736
[6/200] Training loss: 0.03526424
[7/200] Training loss: 0.03226818
[8/200] Training loss: 0.03056933
[9/200] Training loss: 0.02922506
[10/200] Training loss: 0.02726003
[50/200] Training loss: 0.01696284
[100/200] Training loss: 0.01460409
[150/200] Training loss: 0.01335251
[200/200] Training loss: 0.01252747
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16173.53492592142 ----------
[1/200] Training loss: 0.12547608
[2/200] Training loss: 0.05369699
[3/200] Training loss: 0.04928831
[4/200] Training loss: 0.04611011
[5/200] Training loss: 0.04116538
[6/200] Training loss: 0.03849222
[7/200] Training loss: 0.03717835
[8/200] Training loss: 0.03335909
[9/200] Training loss: 0.03143085
[10/200] Training loss: 0.02971637
[50/200] Training loss: 0.01680554
[100/200] Training loss: 0.01343850
[150/200] Training loss: 0.01171994
[200/200] Training loss: 0.01123172
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 3625.232820109627 ----------
[1/200] Training loss: 0.07222836
[2/200] Training loss: 0.03807290
[3/200] Training loss: 0.03072464
[4/200] Training loss: 0.02647354
[5/200] Training loss: 0.02538045
[6/200] Training loss: 0.02438362
[7/200] Training loss: 0.02302735
[8/200] Training loss: 0.02274414
[9/200] Training loss: 0.02162977
[10/200] Training loss: 0.02103447
[50/200] Training loss: 0.01630180
[100/200] Training loss: 0.01359902
[150/200] Training loss: 0.01195743
[200/200] Training loss: 0.01113399
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7750.311219557573 ----------
[1/200] Training loss: 0.17032934
[2/200] Training loss: 0.06394535
[3/200] Training loss: 0.05513210
[4/200] Training loss: 0.04952189
[5/200] Training loss: 0.04670190
[6/200] Training loss: 0.04155706
[7/200] Training loss: 0.03884726
[8/200] Training loss: 0.03680351
[9/200] Training loss: 0.03424333
[10/200] Training loss: 0.03421365
[50/200] Training loss: 0.01716236
[100/200] Training loss: 0.01478610
[150/200] Training loss: 0.01355935
[200/200] Training loss: 0.01269580
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12023.355937507631 ----------
[1/200] Training loss: 0.13714542
[2/200] Training loss: 0.05631452
[3/200] Training loss: 0.04988960
[4/200] Training loss: 0.04525012
[5/200] Training loss: 0.04219057
[6/200] Training loss: 0.03920940
[7/200] Training loss: 0.03492390
[8/200] Training loss: 0.03095830
[9/200] Training loss: 0.03115151
[10/200] Training loss: 0.02944055
[50/200] Training loss: 0.01738821
[100/200] Training loss: 0.01284918
[150/200] Training loss: 0.01198479
[200/200] Training loss: 0.01119164
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6306.248647175276 ----------
[1/200] Training loss: 0.10535042
[2/200] Training loss: 0.04949904
[3/200] Training loss: 0.04209812
[4/200] Training loss: 0.03752232
[5/200] Training loss: 0.03241261
[6/200] Training loss: 0.03126149
[7/200] Training loss: 0.02760445
[8/200] Training loss: 0.02879490
[9/200] Training loss: 0.02540829
[10/200] Training loss: 0.02565676
[50/200] Training loss: 0.01646493
[100/200] Training loss: 0.01365706
[150/200] Training loss: 0.01208367
[200/200] Training loss: 0.01098366
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10600.268298491317 ----------
[1/200] Training loss: 0.11088688
[2/200] Training loss: 0.05025381
[3/200] Training loss: 0.04566154
[4/200] Training loss: 0.04346901
[5/200] Training loss: 0.03814663
[6/200] Training loss: 0.03729339
[7/200] Training loss: 0.03532705
[8/200] Training loss: 0.03103210
[9/200] Training loss: 0.03067622
[10/200] Training loss: 0.02796917
[50/200] Training loss: 0.01659391
[100/200] Training loss: 0.01422881
[150/200] Training loss: 0.01232954
[200/200] Training loss: 0.01144315
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 3590.49230607726 ----------
[1/200] Training loss: 0.06854670
[2/200] Training loss: 0.04822501
[3/200] Training loss: 0.04018025
[4/200] Training loss: 0.03270962
[5/200] Training loss: 0.02912476
[6/200] Training loss: 0.02746719
[7/200] Training loss: 0.02581113
[8/200] Training loss: 0.02469058
[9/200] Training loss: 0.02407193
[10/200] Training loss: 0.02318641
[50/200] Training loss: 0.01441224
[100/200] Training loss: 0.01218015
[150/200] Training loss: 0.01072000
[200/200] Training loss: 0.01001649
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9626.192601439056 ----------
[1/200] Training loss: 0.07450483
[2/200] Training loss: 0.04416652
[3/200] Training loss: 0.03672051
[4/200] Training loss: 0.03266259
[5/200] Training loss: 0.02867895
[6/200] Training loss: 0.02684930
[7/200] Training loss: 0.02518733
[8/200] Training loss: 0.02498777
[9/200] Training loss: 0.02354408
[10/200] Training loss: 0.02250175
[50/200] Training loss: 0.01759979
[100/200] Training loss: 0.01289142
[150/200] Training loss: 0.01098547
[200/200] Training loss: 0.01029672
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6942.887007578332 ----------
[1/200] Training loss: 0.07008842
[2/200] Training loss: 0.04237445
[3/200] Training loss: 0.03292864
[4/200] Training loss: 0.02947002
[5/200] Training loss: 0.02644071
[6/200] Training loss: 0.02406736
[7/200] Training loss: 0.02331851
[8/200] Training loss: 0.02243014
[9/200] Training loss: 0.02139351
[10/200] Training loss: 0.02101459
[50/200] Training loss: 0.01579998
[100/200] Training loss: 0.01360180
[150/200] Training loss: 0.01221060
[200/200] Training loss: 0.01119218
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7584.083596585681 ----------
[1/200] Training loss: 0.07312452
[2/200] Training loss: 0.04818742
[3/200] Training loss: 0.04114984
[4/200] Training loss: 0.03573545
[5/200] Training loss: 0.03132034
[6/200] Training loss: 0.02785696
[7/200] Training loss: 0.02644497
[8/200] Training loss: 0.02482852
[9/200] Training loss: 0.02379131
[10/200] Training loss: 0.02306370
[50/200] Training loss: 0.01412373
[100/200] Training loss: 0.01214498
[150/200] Training loss: 0.01100167
[200/200] Training loss: 0.01019500
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4972.495349419645 ----------
[1/200] Training loss: 0.09937384
[2/200] Training loss: 0.04937103
[3/200] Training loss: 0.04208021
[4/200] Training loss: 0.03938027
[5/200] Training loss: 0.03411114
[6/200] Training loss: 0.03138139
[7/200] Training loss: 0.02899634
[8/200] Training loss: 0.02769994
[9/200] Training loss: 0.02554377
[10/200] Training loss: 0.02576584
[50/200] Training loss: 0.01505421
[100/200] Training loss: 0.01273746
[150/200] Training loss: 0.01182800
[200/200] Training loss: 0.01109053
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 3970.816792550369 ----------
[1/200] Training loss: 0.15614630
[2/200] Training loss: 0.06203982
[3/200] Training loss: 0.05500633
[4/200] Training loss: 0.05118953
[5/200] Training loss: 0.04677850
[6/200] Training loss: 0.04454599
[7/200] Training loss: 0.04415544
[8/200] Training loss: 0.04172484
[9/200] Training loss: 0.03828378
[10/200] Training loss: 0.03649963
[50/200] Training loss: 0.02031482
[100/200] Training loss: 0.01678963
[150/200] Training loss: 0.01475865
[200/200] Training loss: 0.01388877
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17958.34558081562 ----------
[1/200] Training loss: 0.11210650
[2/200] Training loss: 0.05115811
[3/200] Training loss: 0.04462516
[4/200] Training loss: 0.04127398
[5/200] Training loss: 0.03745827
[6/200] Training loss: 0.03568106
[7/200] Training loss: 0.03196700
[8/200] Training loss: 0.03052655
[9/200] Training loss: 0.02781095
[10/200] Training loss: 0.02699737
[50/200] Training loss: 0.01670320
[100/200] Training loss: 0.01471108
[150/200] Training loss: 0.01375743
[200/200] Training loss: 0.01259060
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7754.3448466005175 ----------
[1/200] Training loss: 0.11113036
[2/200] Training loss: 0.05474137
[3/200] Training loss: 0.04830663
[4/200] Training loss: 0.04334843
[5/200] Training loss: 0.03872229
[6/200] Training loss: 0.03441712
[7/200] Training loss: 0.02932418
[8/200] Training loss: 0.02859782
[9/200] Training loss: 0.02552978
[10/200] Training loss: 0.02600979
[50/200] Training loss: 0.01687363
[100/200] Training loss: 0.01447090
[150/200] Training loss: 0.01301949
[200/200] Training loss: 0.01174810
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 21678.57486090818 ----------
[1/200] Training loss: 0.14031966
[2/200] Training loss: 0.06310342
[3/200] Training loss: 0.05705140
[4/200] Training loss: 0.05093421
[5/200] Training loss: 0.04891968
[6/200] Training loss: 0.04683530
[7/200] Training loss: 0.04446611
[8/200] Training loss: 0.04425723
[9/200] Training loss: 0.04022926
[10/200] Training loss: 0.03807958
[50/200] Training loss: 0.01877986
[100/200] Training loss: 0.01550688
[150/200] Training loss: 0.01384551
[200/200] Training loss: 0.01316333
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 18477.611966918237 ----------
[1/200] Training loss: 0.07796651
[2/200] Training loss: 0.05021686
[3/200] Training loss: 0.04444545
[4/200] Training loss: 0.03973158
[5/200] Training loss: 0.03376996
[6/200] Training loss: 0.03046106
[7/200] Training loss: 0.02870424
[8/200] Training loss: 0.02606495
[9/200] Training loss: 0.02498550
[10/200] Training loss: 0.02391502
[50/200] Training loss: 0.01533769
[100/200] Training loss: 0.01349306
[150/200] Training loss: 0.01191613
[200/200] Training loss: 0.01105049
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10534.453189416145 ----------
[1/200] Training loss: 0.12016054
[2/200] Training loss: 0.05234567
[3/200] Training loss: 0.04806123
[4/200] Training loss: 0.04358472
[5/200] Training loss: 0.04110058
[6/200] Training loss: 0.03622996
[7/200] Training loss: 0.03520954
[8/200] Training loss: 0.03324508
[9/200] Training loss: 0.03018675
[10/200] Training loss: 0.02865093
[50/200] Training loss: 0.01801641
[100/200] Training loss: 0.01505470
[150/200] Training loss: 0.01340423
[200/200] Training loss: 0.01206631
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7242.342714895505 ----------
[1/200] Training loss: 0.09511589
[2/200] Training loss: 0.04497366
[3/200] Training loss: 0.04091715
[4/200] Training loss: 0.03458288
[5/200] Training loss: 0.03258295
[6/200] Training loss: 0.02761501
[7/200] Training loss: 0.02764948
[8/200] Training loss: 0.02513627
[9/200] Training loss: 0.02318847
[10/200] Training loss: 0.02336149
[50/200] Training loss: 0.01514455
[100/200] Training loss: 0.01299365
[150/200] Training loss: 0.01210509
[200/200] Training loss: 0.01113226
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16764.5712143198 ----------
[1/200] Training loss: 0.11270931
[2/200] Training loss: 0.05827616
[3/200] Training loss: 0.05142506
[4/200] Training loss: 0.04463064
[5/200] Training loss: 0.04086318
[6/200] Training loss: 0.03854437
[7/200] Training loss: 0.03397880
[8/200] Training loss: 0.03120017
[9/200] Training loss: 0.02869159
[10/200] Training loss: 0.02787013
[50/200] Training loss: 0.01628989
[100/200] Training loss: 0.01439123
[150/200] Training loss: 0.01177313
[200/200] Training loss: 0.01117032
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 20396.201999391946 ----------
[1/200] Training loss: 0.07164041
[2/200] Training loss: 0.04568410
[3/200] Training loss: 0.03593321
[4/200] Training loss: 0.03063640
[5/200] Training loss: 0.02827730
[6/200] Training loss: 0.02531800
[7/200] Training loss: 0.02472839
[8/200] Training loss: 0.02439857
[9/200] Training loss: 0.02324871
[10/200] Training loss: 0.02294677
[50/200] Training loss: 0.01395232
[100/200] Training loss: 0.01231442
[150/200] Training loss: 0.01147766
[200/200] Training loss: 0.01047350
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9917.592046459664 ----------
[1/200] Training loss: 0.11320200
[2/200] Training loss: 0.05236987
[3/200] Training loss: 0.04998016
[4/200] Training loss: 0.04577668
[5/200] Training loss: 0.04195690
[6/200] Training loss: 0.04056884
[7/200] Training loss: 0.03600239
[8/200] Training loss: 0.03136659
[9/200] Training loss: 0.02994148
[10/200] Training loss: 0.02750166
[50/200] Training loss: 0.01828149
[100/200] Training loss: 0.01590984
[150/200] Training loss: 0.01469828
[200/200] Training loss: 0.01370787
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7395.365034939114 ----------
[1/200] Training loss: 0.16503395
[2/200] Training loss: 0.05722190
[3/200] Training loss: 0.05245669
[4/200] Training loss: 0.04951404
[5/200] Training loss: 0.04588560
[6/200] Training loss: 0.04381590
[7/200] Training loss: 0.04158358
[8/200] Training loss: 0.04184888
[9/200] Training loss: 0.03815052
[10/200] Training loss: 0.03789529
[50/200] Training loss: 0.01924619
[100/200] Training loss: 0.01544692
[150/200] Training loss: 0.01380314
[200/200] Training loss: 0.01297866
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.35836733830073597 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7059.113542081612 ----------
[1/200] Training loss: 0.11568142
[2/200] Training loss: 0.05551166
[3/200] Training loss: 0.05384321
[4/200] Training loss: 0.04717325
[5/200] Training loss: 0.04607704
[6/200] Training loss: 0.03926298
[7/200] Training loss: 0.03715767
[8/200] Training loss: 0.03452400
[9/200] Training loss: 0.03095530
[10/200] Training loss: 0.02977047
[50/200] Training loss: 0.01772691
[100/200] Training loss: 0.01556585
[150/200] Training loss: 0.01380804
[200/200] Training loss: 0.01284442
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10550.971898360833 ----------
[1/200] Training loss: 0.10429651
[2/200] Training loss: 0.05318438
[3/200] Training loss: 0.04658490
[4/200] Training loss: 0.04403778
[5/200] Training loss: 0.03985854
[6/200] Training loss: 0.03524334
[7/200] Training loss: 0.03154867
[8/200] Training loss: 0.03078143
[9/200] Training loss: 0.02803682
[10/200] Training loss: 0.02754672
[50/200] Training loss: 0.01632977
[100/200] Training loss: 0.01495781
[150/200] Training loss: 0.01353359
[200/200] Training loss: 0.01287252
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 3899.5771565645423 ----------
[1/200] Training loss: 0.10349617
[2/200] Training loss: 0.04855993
[3/200] Training loss: 0.03641212
[4/200] Training loss: 0.03576868
[5/200] Training loss: 0.03174324
[6/200] Training loss: 0.03114506
[7/200] Training loss: 0.02649559
[8/200] Training loss: 0.02626565
[9/200] Training loss: 0.02520150
[10/200] Training loss: 0.02323873
[50/200] Training loss: 0.01484713
[100/200] Training loss: 0.01283212
[150/200] Training loss: 0.01152764
[200/200] Training loss: 0.01098200
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12967.929672850636 ----------
[1/200] Training loss: 0.11601350
[2/200] Training loss: 0.05198908
[3/200] Training loss: 0.04581616
[4/200] Training loss: 0.04330437
[5/200] Training loss: 0.03693813
[6/200] Training loss: 0.03478812
[7/200] Training loss: 0.03127572
[8/200] Training loss: 0.02926077
[9/200] Training loss: 0.02757735
[10/200] Training loss: 0.02609329
[50/200] Training loss: 0.01544256
[100/200] Training loss: 0.01333598
[150/200] Training loss: 0.01192688
[200/200] Training loss: 0.01144246
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5146.529510262231 ----------
[1/200] Training loss: 0.11103572
[2/200] Training loss: 0.05224032
[3/200] Training loss: 0.04609850
[4/200] Training loss: 0.04055172
[5/200] Training loss: 0.03750829
[6/200] Training loss: 0.03252810
[7/200] Training loss: 0.02974814
[8/200] Training loss: 0.02936307
[9/200] Training loss: 0.02666831
[10/200] Training loss: 0.02645618
[50/200] Training loss: 0.01482105
[100/200] Training loss: 0.01247449
[150/200] Training loss: 0.01167264
[200/200] Training loss: 0.01114784
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 3927.528230325022 ----------
[1/200] Training loss: 0.10269506
[2/200] Training loss: 0.05365246
[3/200] Training loss: 0.04845850
[4/200] Training loss: 0.04550343
[5/200] Training loss: 0.04276110
[6/200] Training loss: 0.03779441
[7/200] Training loss: 0.03527781
[8/200] Training loss: 0.03044420
[9/200] Training loss: 0.03068662
[10/200] Training loss: 0.02806259
[50/200] Training loss: 0.01546876
[100/200] Training loss: 0.01385276
[150/200] Training loss: 0.01209401
[200/200] Training loss: 0.01108507
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13425.549374234188 ----------
[1/200] Training loss: 0.07468227
[2/200] Training loss: 0.04361876
[3/200] Training loss: 0.03493870
[4/200] Training loss: 0.03001854
[5/200] Training loss: 0.02801691
[6/200] Training loss: 0.02592677
[7/200] Training loss: 0.02553630
[8/200] Training loss: 0.02426712
[9/200] Training loss: 0.02382773
[10/200] Training loss: 0.02290573
[50/200] Training loss: 0.01491016
[100/200] Training loss: 0.01161954
[150/200] Training loss: 0.01032725
[200/200] Training loss: 0.00991773
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.40640587077363866 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7531.528928444742 ----------
[1/200] Training loss: 0.10306267
[2/200] Training loss: 0.05538829
[3/200] Training loss: 0.04930623
[4/200] Training loss: 0.04628300
[5/200] Training loss: 0.04291196
[6/200] Training loss: 0.03453812
[7/200] Training loss: 0.03146917
[8/200] Training loss: 0.02994532
[9/200] Training loss: 0.02678891
[10/200] Training loss: 0.02652505
[50/200] Training loss: 0.01619638
[100/200] Training loss: 0.01356033
[150/200] Training loss: 0.01248935
[200/200] Training loss: 0.01164008
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 22115.84915846552 ----------
[1/200] Training loss: 0.14402152
[2/200] Training loss: 0.04451136
[3/200] Training loss: 0.03710299
[4/200] Training loss: 0.03701553
[5/200] Training loss: 0.03577195
[6/200] Training loss: 0.03239418
[7/200] Training loss: 0.02838989
[8/200] Training loss: 0.02917000
[9/200] Training loss: 0.02727288
[10/200] Training loss: 0.02892382
[50/200] Training loss: 0.01809749
[100/200] Training loss: 0.01598852
[150/200] Training loss: 0.01377172
[200/200] Training loss: 0.01650362
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adam ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 45435.73923686067 ----------
[1/200] Training loss: 0.07851345
[2/200] Training loss: 0.04610866
[3/200] Training loss: 0.03753515
[4/200] Training loss: 0.03026852
[5/200] Training loss: 0.02809978
[6/200] Training loss: 0.02663225
[7/200] Training loss: 0.02541937
[8/200] Training loss: 0.02467715
[9/200] Training loss: 0.02439981
[10/200] Training loss: 0.02407770
[50/200] Training loss: 0.01508746
[100/200] Training loss: 0.01302161
[150/200] Training loss: 0.01183363
[200/200] Training loss: 0.01091087
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7630.149146641893 ----------
[1/200] Training loss: 0.11489783
[2/200] Training loss: 0.05419758
[3/200] Training loss: 0.04781343
[4/200] Training loss: 0.04422822
[5/200] Training loss: 0.04069573
[6/200] Training loss: 0.03670416
[7/200] Training loss: 0.03523959
[8/200] Training loss: 0.03192829
[9/200] Training loss: 0.03119124
[10/200] Training loss: 0.03073929
[50/200] Training loss: 0.01660598
[100/200] Training loss: 0.01418642
[150/200] Training loss: 0.01306273
[200/200] Training loss: 0.01198973
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6061.353644195329 ----------
[1/200] Training loss: 0.09867320
[2/200] Training loss: 0.05044728
[3/200] Training loss: 0.04196169
[4/200] Training loss: 0.03860249
[5/200] Training loss: 0.03562416
[6/200] Training loss: 0.03295480
[7/200] Training loss: 0.02975512
[8/200] Training loss: 0.02752042
[9/200] Training loss: 0.02678512
[10/200] Training loss: 0.02556525
[50/200] Training loss: 0.01679786
[100/200] Training loss: 0.01291273
[150/200] Training loss: 0.01196961
[200/200] Training loss: 0.01092387
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4000.295239104234 ----------
[1/200] Training loss: 0.07074835
[2/200] Training loss: 0.04584944
[3/200] Training loss: 0.03789865
[4/200] Training loss: 0.03338836
[5/200] Training loss: 0.02938861
[6/200] Training loss: 0.02793953
[7/200] Training loss: 0.02601121
[8/200] Training loss: 0.02608381
[9/200] Training loss: 0.02460548
[10/200] Training loss: 0.02440953
[50/200] Training loss: 0.01562276
[100/200] Training loss: 0.01363418
[150/200] Training loss: 0.01216354
[200/200] Training loss: 0.01115815
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11499.614602237763 ----------
[1/200] Training loss: 0.12056186
[2/200] Training loss: 0.05338727
[3/200] Training loss: 0.04939631
[4/200] Training loss: 0.04162204
[5/200] Training loss: 0.03779436
[6/200] Training loss: 0.03374485
[7/200] Training loss: 0.03034067
[8/200] Training loss: 0.02842787
[9/200] Training loss: 0.02718859
[10/200] Training loss: 0.02468976
[50/200] Training loss: 0.01690687
[100/200] Training loss: 0.01499618
[150/200] Training loss: 0.01287906
[200/200] Training loss: 0.01182715
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15045.962116129363 ----------
[1/200] Training loss: 0.16381623
[2/200] Training loss: 0.06293222
[3/200] Training loss: 0.05075317
[4/200] Training loss: 0.04757587
[5/200] Training loss: 0.04397102
[6/200] Training loss: 0.04075624
[7/200] Training loss: 0.03937307
[8/200] Training loss: 0.03466057
[9/200] Training loss: 0.03775840
[10/200] Training loss: 0.03718016
[50/200] Training loss: 0.01896889
[100/200] Training loss: 0.01678467
[150/200] Training loss: 0.01546124
[200/200] Training loss: 0.01486735
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7449.5513958895535 ----------
[1/200] Training loss: 0.08933297
[2/200] Training loss: 0.04478740
[3/200] Training loss: 0.03678040
[4/200] Training loss: 0.03366712
[5/200] Training loss: 0.03019439
[6/200] Training loss: 0.02944715
[7/200] Training loss: 0.02779917
[8/200] Training loss: 0.02570588
[9/200] Training loss: 0.02709575
[10/200] Training loss: 0.02568112
[50/200] Training loss: 0.01518141
[100/200] Training loss: 0.01318728
[150/200] Training loss: 0.01118092
[200/200] Training loss: 0.01009643
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9756.433774694522 ----------
[1/200] Training loss: 0.07661391
[2/200] Training loss: 0.05311766
[3/200] Training loss: 0.04908748
[4/200] Training loss: 0.04485984
[5/200] Training loss: 0.03662196
[6/200] Training loss: 0.03196737
[7/200] Training loss: 0.02901284
[8/200] Training loss: 0.02732534
[9/200] Training loss: 0.02552647
[10/200] Training loss: 0.02469152
[50/200] Training loss: 0.01183704
[100/200] Training loss: 0.00967664
[150/200] Training loss: 0.00910434
[200/200] Training loss: 0.00854583
---batch_size---: 2 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17556.554559480057 ----------
[1/200] Training loss: 0.06779692
[2/200] Training loss: 0.04488680
[3/200] Training loss: 0.03706879
[4/200] Training loss: 0.02980239
[5/200] Training loss: 0.02630023
[6/200] Training loss: 0.02367215
[7/200] Training loss: 0.02186058
[8/200] Training loss: 0.02030797
[9/200] Training loss: 0.02043946
[10/200] Training loss: 0.02009414
[50/200] Training loss: 0.01348668
[100/200] Training loss: 0.01133052
[150/200] Training loss: 0.01012696
[200/200] Training loss: 0.00926866
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9682.655007796156 ----------
[1/200] Training loss: 0.09958723
[2/200] Training loss: 0.05129269
[3/200] Training loss: 0.04745083
[4/200] Training loss: 0.04114153
[5/200] Training loss: 0.03823717
[6/200] Training loss: 0.03351263
[7/200] Training loss: 0.03208607
[8/200] Training loss: 0.02920500
[9/200] Training loss: 0.02667928
[10/200] Training loss: 0.02648594
[50/200] Training loss: 0.01696445
[100/200] Training loss: 0.01473098
[150/200] Training loss: 0.01289297
[200/200] Training loss: 0.01195825
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 16 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10759.993680295542 ----------
[1/200] Training loss: 0.14822450
[2/200] Training loss: 0.05568011
[3/200] Training loss: 0.05218693
[4/200] Training loss: 0.05023199
[5/200] Training loss: 0.04787887
[6/200] Training loss: 0.04427582
[7/200] Training loss: 0.04147689
[8/200] Training loss: 0.03957168
[9/200] Training loss: 0.03898736
[10/200] Training loss: 0.03377524
[50/200] Training loss: 0.01771357
[100/200] Training loss: 0.01598490
[150/200] Training loss: 0.01438029
[200/200] Training loss: 0.01341946
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18482.24401959892 ----------
[1/200] Training loss: 0.12311106
[2/200] Training loss: 0.05567243
[3/200] Training loss: 0.05232682
[4/200] Training loss: 0.04806320
[5/200] Training loss: 0.04423143
[6/200] Training loss: 0.03935107
[7/200] Training loss: 0.03446257
[8/200] Training loss: 0.03246622
[9/200] Training loss: 0.02915462
[10/200] Training loss: 0.02835807
[50/200] Training loss: 0.01653285
[100/200] Training loss: 0.01342180
[150/200] Training loss: 0.01189199
[200/200] Training loss: 0.01101981
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15488.676638112114 ----------
[1/200] Training loss: 0.11609729
[2/200] Training loss: 0.05479498
[3/200] Training loss: 0.04937095
[4/200] Training loss: 0.04491211
[5/200] Training loss: 0.04058648
[6/200] Training loss: 0.03468845
[7/200] Training loss: 0.03061548
[8/200] Training loss: 0.02834036
[9/200] Training loss: 0.02586574
[10/200] Training loss: 0.02494747
[50/200] Training loss: 0.01483980
[100/200] Training loss: 0.01348013
[150/200] Training loss: 0.01211089
[200/200] Training loss: 0.01133000
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12960.417277233013 ----------
[1/200] Training loss: 0.13113859
[2/200] Training loss: 0.05648264
[3/200] Training loss: 0.05025800
[4/200] Training loss: 0.04726512
[5/200] Training loss: 0.04378380
[6/200] Training loss: 0.04256839
[7/200] Training loss: 0.03730140
[8/200] Training loss: 0.03632512
[9/200] Training loss: 0.03386916
[10/200] Training loss: 0.03297827
[50/200] Training loss: 0.01477997
[100/200] Training loss: 0.01307281
[150/200] Training loss: 0.01185037
[200/200] Training loss: 0.01100243
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 3799.2496627623723 ----------
[1/200] Training loss: 0.11856593
[2/200] Training loss: 0.05626216
[3/200] Training loss: 0.04853159
[4/200] Training loss: 0.04572974
[5/200] Training loss: 0.04007297
[6/200] Training loss: 0.03545009
[7/200] Training loss: 0.03439615
[8/200] Training loss: 0.03163505
[9/200] Training loss: 0.02987506
[10/200] Training loss: 0.02853355
[50/200] Training loss: 0.01573927
[100/200] Training loss: 0.01363848
[150/200] Training loss: 0.01214570
[200/200] Training loss: 0.01163643
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14957.47946680857 ----------
[1/200] Training loss: 0.10908522
[2/200] Training loss: 0.05462385
[3/200] Training loss: 0.04816538
[4/200] Training loss: 0.04295265
[5/200] Training loss: 0.03943851
[6/200] Training loss: 0.03546353
[7/200] Training loss: 0.03071254
[8/200] Training loss: 0.03032493
[9/200] Training loss: 0.02756452
[10/200] Training loss: 0.02555231
[50/200] Training loss: 0.01605230
[100/200] Training loss: 0.01345865
[150/200] Training loss: 0.01233379
[200/200] Training loss: 0.01100382
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8123.453452811803 ----------
[1/200] Training loss: 0.11825230
[2/200] Training loss: 0.05329305
[3/200] Training loss: 0.04746579
[4/200] Training loss: 0.04204951
[5/200] Training loss: 0.03897449
[6/200] Training loss: 0.03692111
[7/200] Training loss: 0.03367667
[8/200] Training loss: 0.03132419
[9/200] Training loss: 0.02904733
[10/200] Training loss: 0.02736936
[50/200] Training loss: 0.01854519
[100/200] Training loss: 0.01580579
[150/200] Training loss: 0.01499933
[200/200] Training loss: 0.01341966
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5053.04898056609 ----------
[1/200] Training loss: 0.12267953
[2/200] Training loss: 0.05262294
[3/200] Training loss: 0.04931643
[4/200] Training loss: 0.04405111
[5/200] Training loss: 0.04207489
[6/200] Training loss: 0.03946821
[7/200] Training loss: 0.03554466
[8/200] Training loss: 0.03453807
[9/200] Training loss: 0.03250015
[10/200] Training loss: 0.03116183
[50/200] Training loss: 0.01652641
[100/200] Training loss: 0.01469967
[150/200] Training loss: 0.01296485
[200/200] Training loss: 0.01198360
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 18321.35540837522 ----------
[1/200] Training loss: 0.10412843
[2/200] Training loss: 0.04970213
[3/200] Training loss: 0.04612758
[4/200] Training loss: 0.04060292
[5/200] Training loss: 0.03688362
[6/200] Training loss: 0.03336413
[7/200] Training loss: 0.03043914
[8/200] Training loss: 0.02958481
[9/200] Training loss: 0.02777057
[10/200] Training loss: 0.02705388
[50/200] Training loss: 0.01796660
[100/200] Training loss: 0.01545175
[150/200] Training loss: 0.01386407
[200/200] Training loss: 0.01263282
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9750.000205128203 ----------
[1/200] Training loss: 0.11305743
[2/200] Training loss: 0.05060684
[3/200] Training loss: 0.04689818
[4/200] Training loss: 0.03954144
[5/200] Training loss: 0.03827225
[6/200] Training loss: 0.03514120
[7/200] Training loss: 0.03253320
[8/200] Training loss: 0.03126369
[9/200] Training loss: 0.02647697
[10/200] Training loss: 0.02554779
[50/200] Training loss: 0.01722860
[100/200] Training loss: 0.01440361
[150/200] Training loss: 0.01262991
[200/200] Training loss: 0.01163367
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8159.800977965087 ----------
[1/200] Training loss: 0.09311949
[2/200] Training loss: 0.04937295
[3/200] Training loss: 0.04459651
[4/200] Training loss: 0.03995017
[5/200] Training loss: 0.03784781
[6/200] Training loss: 0.03618754
[7/200] Training loss: 0.03186926
[8/200] Training loss: 0.03044174
[9/200] Training loss: 0.03005664
[10/200] Training loss: 0.02756401
[50/200] Training loss: 0.01521440
[100/200] Training loss: 0.01332492
[150/200] Training loss: 0.01167082
[200/200] Training loss: 0.01036536
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8500.248466956717 ----------
[1/200] Training loss: 0.14847041
[2/200] Training loss: 0.05824155
[3/200] Training loss: 0.04993146
[4/200] Training loss: 0.04979772
[5/200] Training loss: 0.04676230
[6/200] Training loss: 0.04481957
[7/200] Training loss: 0.04096821
[8/200] Training loss: 0.03980418
[9/200] Training loss: 0.03932439
[10/200] Training loss: 0.03719516
[50/200] Training loss: 0.01845476
[100/200] Training loss: 0.01452686
[150/200] Training loss: 0.01315496
[200/200] Training loss: 0.01192079
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6520.6100941553 ----------
[1/200] Training loss: 0.11259369
[2/200] Training loss: 0.05401442
[3/200] Training loss: 0.05038032
[4/200] Training loss: 0.04386044
[5/200] Training loss: 0.03980463
[6/200] Training loss: 0.03620399
[7/200] Training loss: 0.03497987
[8/200] Training loss: 0.03217951
[9/200] Training loss: 0.03077319
[10/200] Training loss: 0.02758618
[50/200] Training loss: 0.01818178
[100/200] Training loss: 0.01622619
[150/200] Training loss: 0.01443874
[200/200] Training loss: 0.01283970
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11377.104376773555 ----------
[1/200] Training loss: 0.06761998
[2/200] Training loss: 0.03800003
[3/200] Training loss: 0.03033962
[4/200] Training loss: 0.02726302
[5/200] Training loss: 0.02539607
[6/200] Training loss: 0.02452261
[7/200] Training loss: 0.02294373
[8/200] Training loss: 0.02251488
[9/200] Training loss: 0.02208555
[10/200] Training loss: 0.02142755
[50/200] Training loss: 0.01571839
[100/200] Training loss: 0.01310885
[150/200] Training loss: 0.01136381
[200/200] Training loss: 0.01020970
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8555.890602386171 ----------
[1/200] Training loss: 0.16792768
[2/200] Training loss: 0.05647945
[3/200] Training loss: 0.05001373
[4/200] Training loss: 0.04267100
[5/200] Training loss: 0.04235461
[6/200] Training loss: 0.03674138
[7/200] Training loss: 0.03438551
[8/200] Training loss: 0.03207910
[9/200] Training loss: 0.03251879
[10/200] Training loss: 0.03012427
[50/200] Training loss: 0.01860229
[100/200] Training loss: 0.01582721
[150/200] Training loss: 0.01438903
[200/200] Training loss: 0.01324921
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.35836733830073597 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7680.448424408564 ----------
[1/200] Training loss: 0.07138142
[2/200] Training loss: 0.04824551
[3/200] Training loss: 0.04193423
[4/200] Training loss: 0.03668643
[5/200] Training loss: 0.03188930
[6/200] Training loss: 0.02922335
[7/200] Training loss: 0.02786828
[8/200] Training loss: 0.02650022
[9/200] Training loss: 0.02398456
[10/200] Training loss: 0.02325108
[50/200] Training loss: 0.01420605
[100/200] Training loss: 0.01209173
[150/200] Training loss: 0.01056805
[200/200] Training loss: 0.00961913
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12676.210159191902 ----------
[1/200] Training loss: 0.11653190
[2/200] Training loss: 0.05706282
[3/200] Training loss: 0.05200924
[4/200] Training loss: 0.04693767
[5/200] Training loss: 0.04437326
[6/200] Training loss: 0.04116423
[7/200] Training loss: 0.03720601
[8/200] Training loss: 0.03605560
[9/200] Training loss: 0.02931365
[10/200] Training loss: 0.02922755
[50/200] Training loss: 0.01546660
[100/200] Training loss: 0.01301769
[150/200] Training loss: 0.01211068
[200/200] Training loss: 0.01159647
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16034.451908312925 ----------
[1/200] Training loss: 0.07331202
[2/200] Training loss: 0.05117205
[3/200] Training loss: 0.04553485
[4/200] Training loss: 0.03829716
[5/200] Training loss: 0.03274776
[6/200] Training loss: 0.02855201
[7/200] Training loss: 0.02763242
[8/200] Training loss: 0.02523509
[9/200] Training loss: 0.02497135
[10/200] Training loss: 0.02334748
[50/200] Training loss: 0.01443238
[100/200] Training loss: 0.01247275
[150/200] Training loss: 0.01140420
[200/200] Training loss: 0.01059313
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8582.758530915338 ----------
[1/200] Training loss: 0.11801988
[2/200] Training loss: 0.05486021
[3/200] Training loss: 0.04962760
[4/200] Training loss: 0.04539141
[5/200] Training loss: 0.03930392
[6/200] Training loss: 0.03916679
[7/200] Training loss: 0.03486816
[8/200] Training loss: 0.03196995
[9/200] Training loss: 0.03026164
[10/200] Training loss: 0.02763094
[50/200] Training loss: 0.01721283
[100/200] Training loss: 0.01473886
[150/200] Training loss: 0.01364740
[200/200] Training loss: 0.01278910
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4588.048822756794 ----------
[1/200] Training loss: 0.10218656
[2/200] Training loss: 0.05160397
[3/200] Training loss: 0.04358545
[4/200] Training loss: 0.04165044
[5/200] Training loss: 0.03854822
[6/200] Training loss: 0.03505664
[7/200] Training loss: 0.03284512
[8/200] Training loss: 0.02983868
[9/200] Training loss: 0.02869643
[10/200] Training loss: 0.02697907
[50/200] Training loss: 0.01641804
[100/200] Training loss: 0.01416539
[150/200] Training loss: 0.01267094
[200/200] Training loss: 0.01154281
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7681.968237372503 ----------
[1/200] Training loss: 0.17157624
[2/200] Training loss: 0.05437612
[3/200] Training loss: 0.05081482
[4/200] Training loss: 0.04489992
[5/200] Training loss: 0.04181737
[6/200] Training loss: 0.03976091
[7/200] Training loss: 0.03883411
[8/200] Training loss: 0.03679634
[9/200] Training loss: 0.03598265
[10/200] Training loss: 0.03277181
[50/200] Training loss: 0.01858074
[100/200] Training loss: 0.01653711
[150/200] Training loss: 0.01486064
[200/200] Training loss: 0.01382573
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8628.601277147995 ----------
[1/200] Training loss: 0.11271981
[2/200] Training loss: 0.05138090
[3/200] Training loss: 0.04681005
[4/200] Training loss: 0.04138041
[5/200] Training loss: 0.03535666
[6/200] Training loss: 0.03285949
[7/200] Training loss: 0.02961644
[8/200] Training loss: 0.02846577
[9/200] Training loss: 0.02610302
[10/200] Training loss: 0.02509158
[50/200] Training loss: 0.01696799
[100/200] Training loss: 0.01370324
[150/200] Training loss: 0.01200608
[200/200] Training loss: 0.01016826
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11243.463167547621 ----------
[1/200] Training loss: 0.10301008
[2/200] Training loss: 0.05233565
[3/200] Training loss: 0.04547247
[4/200] Training loss: 0.04347884
[5/200] Training loss: 0.04093581
[6/200] Training loss: 0.03815061
[7/200] Training loss: 0.03707127
[8/200] Training loss: 0.03548811
[9/200] Training loss: 0.03341978
[10/200] Training loss: 0.03295257
[50/200] Training loss: 0.01726075
[100/200] Training loss: 0.01445295
[150/200] Training loss: 0.01276599
[200/200] Training loss: 0.01109877
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6273.745292885264 ----------
[1/200] Training loss: 0.14677878
[2/200] Training loss: 0.05339285
[3/200] Training loss: 0.04951929
[4/200] Training loss: 0.04619942
[5/200] Training loss: 0.04389532
[6/200] Training loss: 0.04036590
[7/200] Training loss: 0.03617713
[8/200] Training loss: 0.03403532
[9/200] Training loss: 0.03294749
[10/200] Training loss: 0.03019261
[50/200] Training loss: 0.01834357
[100/200] Training loss: 0.01525671
[150/200] Training loss: 0.01436880
[200/200] Training loss: 0.01342941
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7003.137011368548 ----------
[1/200] Training loss: 0.16046500
[2/200] Training loss: 0.05618729
[3/200] Training loss: 0.05334005
[4/200] Training loss: 0.05290288
[5/200] Training loss: 0.04971196
[6/200] Training loss: 0.04584558
[7/200] Training loss: 0.04608871
[8/200] Training loss: 0.04386104
[9/200] Training loss: 0.04057294
[10/200] Training loss: 0.03566619
[50/200] Training loss: 0.01892798
[100/200] Training loss: 0.01550066
[150/200] Training loss: 0.01393299
[200/200] Training loss: 0.01276514
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8270.62416024329 ----------
[1/200] Training loss: 0.11720474
[2/200] Training loss: 0.05444858
[3/200] Training loss: 0.04884966
[4/200] Training loss: 0.04504052
[5/200] Training loss: 0.03903716
[6/200] Training loss: 0.03615678
[7/200] Training loss: 0.03300722
[8/200] Training loss: 0.03141226
[9/200] Training loss: 0.02988003
[10/200] Training loss: 0.02848261
[50/200] Training loss: 0.01661646
[100/200] Training loss: 0.01484950
[150/200] Training loss: 0.01384604
[200/200] Training loss: 0.01281138
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6396.60503704895 ----------
[1/200] Training loss: 0.11333990
[2/200] Training loss: 0.05160053
[3/200] Training loss: 0.04632976
[4/200] Training loss: 0.04472249
[5/200] Training loss: 0.04210542
[6/200] Training loss: 0.03942957
[7/200] Training loss: 0.03332471
[8/200] Training loss: 0.03283718
[9/200] Training loss: 0.02783906
[10/200] Training loss: 0.02886523
[50/200] Training loss: 0.01611924
[100/200] Training loss: 0.01364648
[150/200] Training loss: 0.01283845
[200/200] Training loss: 0.01232044
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 3916.883454993268 ----------
[1/200] Training loss: 0.09945166
[2/200] Training loss: 0.05279312
[3/200] Training loss: 0.04816759
[4/200] Training loss: 0.04348850
[5/200] Training loss: 0.03764178
[6/200] Training loss: 0.03372155
[7/200] Training loss: 0.03073647
[8/200] Training loss: 0.02888233
[9/200] Training loss: 0.02737143
[10/200] Training loss: 0.02540050
[50/200] Training loss: 0.01640348
[100/200] Training loss: 0.01445484
[150/200] Training loss: 0.01383333
[200/200] Training loss: 0.01263566
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9508.63733665345 ----------
[1/200] Training loss: 0.14885259
[2/200] Training loss: 0.05637991
[3/200] Training loss: 0.05076902
[4/200] Training loss: 0.04663104
[5/200] Training loss: 0.04577562
[6/200] Training loss: 0.04300031
[7/200] Training loss: 0.04005737
[8/200] Training loss: 0.03679877
[9/200] Training loss: 0.03699965
[10/200] Training loss: 0.03260610
[50/200] Training loss: 0.01864462
[100/200] Training loss: 0.01602315
[150/200] Training loss: 0.01489702
[200/200] Training loss: 0.01317377
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15820.26902426125 ----------
[1/200] Training loss: 0.10575888
[2/200] Training loss: 0.05119633
[3/200] Training loss: 0.04732989
[4/200] Training loss: 0.04276050
[5/200] Training loss: 0.03842718
[6/200] Training loss: 0.03467281
[7/200] Training loss: 0.03131215
[8/200] Training loss: 0.02840677
[9/200] Training loss: 0.02728975
[10/200] Training loss: 0.02693269
[50/200] Training loss: 0.01642849
[100/200] Training loss: 0.01424554
[150/200] Training loss: 0.01272947
[200/200] Training loss: 0.01127519
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14624.083150748289 ----------
[1/200] Training loss: 0.12602529
[2/200] Training loss: 0.05888865
[3/200] Training loss: 0.05278479
[4/200] Training loss: 0.04827108
[5/200] Training loss: 0.04535506
[6/200] Training loss: 0.03975740
[7/200] Training loss: 0.03709922
[8/200] Training loss: 0.03243020
[9/200] Training loss: 0.02978178
[10/200] Training loss: 0.02727912
[50/200] Training loss: 0.01579609
[100/200] Training loss: 0.01378400
[150/200] Training loss: 0.01237578
[200/200] Training loss: 0.01134991
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13709.280360398207 ----------
[1/200] Training loss: 0.11397655
[2/200] Training loss: 0.04765271
[3/200] Training loss: 0.04233170
[4/200] Training loss: 0.03960152
[5/200] Training loss: 0.03504853
[6/200] Training loss: 0.03104235
[7/200] Training loss: 0.02900626
[8/200] Training loss: 0.02632335
[9/200] Training loss: 0.02392824
[10/200] Training loss: 0.02473901
[50/200] Training loss: 0.01608263
[100/200] Training loss: 0.01398281
[150/200] Training loss: 0.01250759
[200/200] Training loss: 0.01133010
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11434.511970346613 ----------
[1/200] Training loss: 0.11102304
[2/200] Training loss: 0.05528902
[3/200] Training loss: 0.05061421
[4/200] Training loss: 0.04622842
[5/200] Training loss: 0.04326167
[6/200] Training loss: 0.03961117
[7/200] Training loss: 0.03677618
[8/200] Training loss: 0.03339391
[9/200] Training loss: 0.03106993
[10/200] Training loss: 0.02963142
[50/200] Training loss: 0.01743596
[100/200] Training loss: 0.01443493
[150/200] Training loss: 0.01335169
[200/200] Training loss: 0.01235704
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17764.532754902393 ----------
[1/200] Training loss: 0.14745257
[2/200] Training loss: 0.05712848
[3/200] Training loss: 0.05485544
[4/200] Training loss: 0.05260361
[5/200] Training loss: 0.04969459
[6/200] Training loss: 0.04618453
[7/200] Training loss: 0.04352340
[8/200] Training loss: 0.04241670
[9/200] Training loss: 0.04207556
[10/200] Training loss: 0.04007749
[50/200] Training loss: 0.01739781
[100/200] Training loss: 0.01537453
[150/200] Training loss: 0.01386176
[200/200] Training loss: 0.01295375
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9281.39429180767 ----------
[1/200] Training loss: 0.16398427
[2/200] Training loss: 0.05894467
[3/200] Training loss: 0.05551959
[4/200] Training loss: 0.05113928
[5/200] Training loss: 0.05055288
[6/200] Training loss: 0.04635931
[7/200] Training loss: 0.04418104
[8/200] Training loss: 0.04175582
[9/200] Training loss: 0.04232325
[10/200] Training loss: 0.03677113
[50/200] Training loss: 0.01837969
[100/200] Training loss: 0.01584997
[150/200] Training loss: 0.01397704
[200/200] Training loss: 0.01218156
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.35836733830073597 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 18097.353618692432 ----------
[1/200] Training loss: 0.11514255
[2/200] Training loss: 0.05431537
[3/200] Training loss: 0.04614414
[4/200] Training loss: 0.04072765
[5/200] Training loss: 0.03688420
[6/200] Training loss: 0.03301698
[7/200] Training loss: 0.03054339
[8/200] Training loss: 0.03002048
[9/200] Training loss: 0.02848725
[10/200] Training loss: 0.02785330
[50/200] Training loss: 0.01860479
[100/200] Training loss: 0.01615670
[150/200] Training loss: 0.01460978
[200/200] Training loss: 0.01305689
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6593.468586411859 ----------
[1/200] Training loss: 0.15724760
[2/200] Training loss: 0.05841635
[3/200] Training loss: 0.05324147
[4/200] Training loss: 0.04963675
[5/200] Training loss: 0.04761313
[6/200] Training loss: 0.04144894
[7/200] Training loss: 0.03818090
[8/200] Training loss: 0.03594729
[9/200] Training loss: 0.03186978
[10/200] Training loss: 0.03095397
[50/200] Training loss: 0.01787545
[100/200] Training loss: 0.01560936
[150/200] Training loss: 0.01444710
[200/200] Training loss: 0.01352283
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13492.209900531492 ----------
[1/200] Training loss: 0.11538168
[2/200] Training loss: 0.05310375
[3/200] Training loss: 0.04763901
[4/200] Training loss: 0.04395792
[5/200] Training loss: 0.03881799
[6/200] Training loss: 0.03545668
[7/200] Training loss: 0.03370105
[8/200] Training loss: 0.03327581
[9/200] Training loss: 0.02968713
[10/200] Training loss: 0.02940385
[50/200] Training loss: 0.01824879
[100/200] Training loss: 0.01438478
[150/200] Training loss: 0.01211558
[200/200] Training loss: 0.01119439
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11078.10453101071 ----------
[1/200] Training loss: 0.14368978
[2/200] Training loss: 0.05587414
[3/200] Training loss: 0.04940720
[4/200] Training loss: 0.04470052
[5/200] Training loss: 0.04152904
[6/200] Training loss: 0.04146924
[7/200] Training loss: 0.03716125
[8/200] Training loss: 0.03493019
[9/200] Training loss: 0.03325463
[10/200] Training loss: 0.03246110
[50/200] Training loss: 0.01666506
[100/200] Training loss: 0.01494880
[150/200] Training loss: 0.01407613
[200/200] Training loss: 0.01350458
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13698.8933859637 ----------
[1/200] Training loss: 0.15480901
[2/200] Training loss: 0.05997956
[3/200] Training loss: 0.05536373
[4/200] Training loss: 0.05223286
[5/200] Training loss: 0.05006071
[6/200] Training loss: 0.04872864
[7/200] Training loss: 0.04665053
[8/200] Training loss: 0.04590692
[9/200] Training loss: 0.04301776
[10/200] Training loss: 0.03908985
[50/200] Training loss: 0.01697017
[100/200] Training loss: 0.01527162
[150/200] Training loss: 0.01387321
[200/200] Training loss: 0.01335158
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9393.126423081934 ----------
[1/200] Training loss: 0.09836562
[2/200] Training loss: 0.04989142
[3/200] Training loss: 0.04079183
[4/200] Training loss: 0.03671157
[5/200] Training loss: 0.03315272
[6/200] Training loss: 0.02979820
[7/200] Training loss: 0.02797216
[8/200] Training loss: 0.02756576
[9/200] Training loss: 0.02506549
[10/200] Training loss: 0.02380003
[50/200] Training loss: 0.01442933
[100/200] Training loss: 0.01208752
[150/200] Training loss: 0.01116592
[200/200] Training loss: 0.01079225
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9344.82145361804 ----------
[1/200] Training loss: 0.10325645
[2/200] Training loss: 0.04944921
[3/200] Training loss: 0.04515057
[4/200] Training loss: 0.04035847
[5/200] Training loss: 0.03911246
[6/200] Training loss: 0.03480881
[7/200] Training loss: 0.03350259
[8/200] Training loss: 0.03156127
[9/200] Training loss: 0.02989147
[10/200] Training loss: 0.02847816
[50/200] Training loss: 0.01542967
[100/200] Training loss: 0.01242732
[150/200] Training loss: 0.01192750
[200/200] Training loss: 0.01059902
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11485.720525940025 ----------
[1/200] Training loss: 0.11073695
[2/200] Training loss: 0.05406834
[3/200] Training loss: 0.04884411
[4/200] Training loss: 0.04481378
[5/200] Training loss: 0.03878635
[6/200] Training loss: 0.03465410
[7/200] Training loss: 0.03183976
[8/200] Training loss: 0.03057277
[9/200] Training loss: 0.02640377
[10/200] Training loss: 0.02561424
[50/200] Training loss: 0.01607940
[100/200] Training loss: 0.01385546
[150/200] Training loss: 0.01259339
[200/200] Training loss: 0.01154680
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 18134.476336525408 ----------
[1/200] Training loss: 0.10871054
[2/200] Training loss: 0.05332296
[3/200] Training loss: 0.05035450
[4/200] Training loss: 0.04255237
[5/200] Training loss: 0.03599654
[6/200] Training loss: 0.03390227
[7/200] Training loss: 0.03068748
[8/200] Training loss: 0.02911915
[9/200] Training loss: 0.02875575
[10/200] Training loss: 0.02706506
[50/200] Training loss: 0.01619212
[100/200] Training loss: 0.01415529
[150/200] Training loss: 0.01234886
[200/200] Training loss: 0.01117991
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8360.100477865084 ----------
[1/200] Training loss: 0.11238451
[2/200] Training loss: 0.05293133
[3/200] Training loss: 0.04887700
[4/200] Training loss: 0.04148592
[5/200] Training loss: 0.03826533
[6/200] Training loss: 0.03557351
[7/200] Training loss: 0.03088388
[8/200] Training loss: 0.03001018
[9/200] Training loss: 0.02890638
[10/200] Training loss: 0.02756551
[50/200] Training loss: 0.01763143
[100/200] Training loss: 0.01537036
[150/200] Training loss: 0.01418312
[200/200] Training loss: 0.01294433
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.35836733830073597 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10643.041670500026 ----------
[1/200] Training loss: 0.17123051
[2/200] Training loss: 0.05950666
[3/200] Training loss: 0.05411816
[4/200] Training loss: 0.04668204
[5/200] Training loss: 0.04633969
[6/200] Training loss: 0.04772867
[7/200] Training loss: 0.04165477
[8/200] Training loss: 0.04272918
[9/200] Training loss: 0.04027353
[10/200] Training loss: 0.03974467
[50/200] Training loss: 0.01903285
[100/200] Training loss: 0.01628753
[150/200] Training loss: 0.01534889
[200/200] Training loss: 0.01379147
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9382.108505021672 ----------
[1/200] Training loss: 0.15654682
[2/200] Training loss: 0.05635685
[3/200] Training loss: 0.04990543
[4/200] Training loss: 0.04852730
[5/200] Training loss: 0.04719180
[6/200] Training loss: 0.04280247
[7/200] Training loss: 0.04042114
[8/200] Training loss: 0.03914143
[9/200] Training loss: 0.03875271
[10/200] Training loss: 0.03573122
[50/200] Training loss: 0.01876025
[100/200] Training loss: 0.01494169
[150/200] Training loss: 0.01352421
[200/200] Training loss: 0.01250489
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7622.898661270528 ----------
[1/200] Training loss: 0.17391723
[2/200] Training loss: 0.06173486
[3/200] Training loss: 0.05136885
[4/200] Training loss: 0.04736591
[5/200] Training loss: 0.04535825
[6/200] Training loss: 0.04227374
[7/200] Training loss: 0.04014983
[8/200] Training loss: 0.03455171
[9/200] Training loss: 0.03205601
[10/200] Training loss: 0.03321880
[50/200] Training loss: 0.01726026
[100/200] Training loss: 0.01527816
[150/200] Training loss: 0.01397873
[200/200] Training loss: 0.01343436
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12887.088732526056 ----------
[1/200] Training loss: 0.13396295
[2/200] Training loss: 0.05416980
[3/200] Training loss: 0.05157143
[4/200] Training loss: 0.04849806
[5/200] Training loss: 0.04758900
[6/200] Training loss: 0.04531062
[7/200] Training loss: 0.04185904
[8/200] Training loss: 0.04067301
[9/200] Training loss: 0.04016566
[10/200] Training loss: 0.04027628
[50/200] Training loss: 0.01780777
[100/200] Training loss: 0.01491385
[150/200] Training loss: 0.01371172
[200/200] Training loss: 0.01151847
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9126.71770134258 ----------
[1/200] Training loss: 0.07255384
[2/200] Training loss: 0.04840931
[3/200] Training loss: 0.04087249
[4/200] Training loss: 0.03332133
[5/200] Training loss: 0.02874062
[6/200] Training loss: 0.02624695
[7/200] Training loss: 0.02464029
[8/200] Training loss: 0.02264889
[9/200] Training loss: 0.02235030
[10/200] Training loss: 0.02168249
[50/200] Training loss: 0.01439736
[100/200] Training loss: 0.01175060
[150/200] Training loss: 0.00986172
[200/200] Training loss: 0.00912459
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10880.619100032865 ----------
[1/200] Training loss: 0.14643185
[2/200] Training loss: 0.05817187
[3/200] Training loss: 0.05308261
[4/200] Training loss: 0.05130723
[5/200] Training loss: 0.04941956
[6/200] Training loss: 0.04493707
[7/200] Training loss: 0.04175628
[8/200] Training loss: 0.03802037
[9/200] Training loss: 0.03836589
[10/200] Training loss: 0.03630790
[50/200] Training loss: 0.01926694
[100/200] Training loss: 0.01680947
[150/200] Training loss: 0.01562449
[200/200] Training loss: 0.01488566
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15100.278143133655 ----------
[1/200] Training loss: 0.16978615
[2/200] Training loss: 0.06256989
[3/200] Training loss: 0.05675144
[4/200] Training loss: 0.04916653
[5/200] Training loss: 0.04635755
[6/200] Training loss: 0.04452077
[7/200] Training loss: 0.04233653
[8/200] Training loss: 0.04015440
[9/200] Training loss: 0.03718971
[10/200] Training loss: 0.03649309
[50/200] Training loss: 0.02046694
[100/200] Training loss: 0.01764621
[150/200] Training loss: 0.01617476
[200/200] Training loss: 0.01467313
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.35836733830073597 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16873.464137514857 ----------
[1/200] Training loss: 0.14676657
[2/200] Training loss: 0.05572022
[3/200] Training loss: 0.05033888
[4/200] Training loss: 0.04819169
[5/200] Training loss: 0.04059324
[6/200] Training loss: 0.04129037
[7/200] Training loss: 0.03897828
[8/200] Training loss: 0.03456380
[9/200] Training loss: 0.03467494
[10/200] Training loss: 0.03107365
[50/200] Training loss: 0.01669107
[100/200] Training loss: 0.01416325
[150/200] Training loss: 0.01314318
[200/200] Training loss: 0.01221058
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.35836733830073597 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10978.583515189926 ----------
[1/200] Training loss: 0.11628842
[2/200] Training loss: 0.05660445
[3/200] Training loss: 0.05081277
[4/200] Training loss: 0.04673727
[5/200] Training loss: 0.04232598
[6/200] Training loss: 0.03811924
[7/200] Training loss: 0.03235859
[8/200] Training loss: 0.02993711
[9/200] Training loss: 0.02830674
[10/200] Training loss: 0.02754434
[50/200] Training loss: 0.01609215
[100/200] Training loss: 0.01435705
[150/200] Training loss: 0.01359578
[200/200] Training loss: 0.01261909
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15003.86670162062 ----------
[1/200] Training loss: 0.10758636
[2/200] Training loss: 0.05304041
[3/200] Training loss: 0.04790831
[4/200] Training loss: 0.04314623
[5/200] Training loss: 0.03932375
[6/200] Training loss: 0.03337971
[7/200] Training loss: 0.03078394
[8/200] Training loss: 0.02853869
[9/200] Training loss: 0.02812559
[10/200] Training loss: 0.02429682
[50/200] Training loss: 0.01671491
[100/200] Training loss: 0.01376609
[150/200] Training loss: 0.01218747
[200/200] Training loss: 0.01107290
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12475.740939920162 ----------
[1/200] Training loss: 0.15009601
[2/200] Training loss: 0.05725363
[3/200] Training loss: 0.04969407
[4/200] Training loss: 0.04796536
[5/200] Training loss: 0.04692209
[6/200] Training loss: 0.04201265
[7/200] Training loss: 0.04068164
[8/200] Training loss: 0.03769958
[9/200] Training loss: 0.03861219
[10/200] Training loss: 0.03352957
[50/200] Training loss: 0.01907992
[100/200] Training loss: 0.01414045
[150/200] Training loss: 0.01352391
[200/200] Training loss: 0.01250313
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8098.658901324342 ----------
[1/200] Training loss: 0.16505198
[2/200] Training loss: 0.05867543
[3/200] Training loss: 0.05473897
[4/200] Training loss: 0.05381097
[5/200] Training loss: 0.05126591
[6/200] Training loss: 0.04907146
[7/200] Training loss: 0.04881528
[8/200] Training loss: 0.04596491
[9/200] Training loss: 0.04325048
[10/200] Training loss: 0.04170506
[50/200] Training loss: 0.01810735
[100/200] Training loss: 0.01634444
[150/200] Training loss: 0.01557411
[200/200] Training loss: 0.01440887
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12934.774215269472 ----------
[1/200] Training loss: 0.14802456
[2/200] Training loss: 0.05916977
[3/200] Training loss: 0.05014241
[4/200] Training loss: 0.04539908
[5/200] Training loss: 0.04167424
[6/200] Training loss: 0.03966397
[7/200] Training loss: 0.03739437
[8/200] Training loss: 0.03678519
[9/200] Training loss: 0.03383041
[10/200] Training loss: 0.03221624
[50/200] Training loss: 0.01689567
[100/200] Training loss: 0.01490207
[150/200] Training loss: 0.01375148
[200/200] Training loss: 0.01231042
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.35836733830073597 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9797.616853092388 ----------
[1/200] Training loss: 0.10594604
[2/200] Training loss: 0.05120354
[3/200] Training loss: 0.04829823
[4/200] Training loss: 0.04423508
[5/200] Training loss: 0.04106183
[6/200] Training loss: 0.03929714
[7/200] Training loss: 0.03461914
[8/200] Training loss: 0.03316407
[9/200] Training loss: 0.02950507
[10/200] Training loss: 0.02839154
[50/200] Training loss: 0.01660307
[100/200] Training loss: 0.01359216
[150/200] Training loss: 0.01242425
[200/200] Training loss: 0.01157161
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9064.624868134368 ----------
[1/200] Training loss: 0.16969489
[2/200] Training loss: 0.05886410
[3/200] Training loss: 0.05294670
[4/200] Training loss: 0.05262370
[5/200] Training loss: 0.04807671
[6/200] Training loss: 0.04802432
[7/200] Training loss: 0.04621846
[8/200] Training loss: 0.04107344
[9/200] Training loss: 0.04023637
[10/200] Training loss: 0.03851908
[50/200] Training loss: 0.01724006
[100/200] Training loss: 0.01439695
[150/200] Training loss: 0.01325685
[200/200] Training loss: 0.01252462
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9960.233330600242 ----------
[1/200] Training loss: 0.16576212
[2/200] Training loss: 0.05980884
[3/200] Training loss: 0.05396968
[4/200] Training loss: 0.04821445
[5/200] Training loss: 0.04556182
[6/200] Training loss: 0.04243262
[7/200] Training loss: 0.04219251
[8/200] Training loss: 0.04204190
[9/200] Training loss: 0.03761324
[10/200] Training loss: 0.03701901
[50/200] Training loss: 0.01980114
[100/200] Training loss: 0.01679743
[150/200] Training loss: 0.01504806
[200/200] Training loss: 0.01376240
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15366.037875783073 ----------
[1/200] Training loss: 0.11764406
[2/200] Training loss: 0.05434725
[3/200] Training loss: 0.04669144
[4/200] Training loss: 0.04178094
[5/200] Training loss: 0.03883433
[6/200] Training loss: 0.03446125
[7/200] Training loss: 0.03255963
[8/200] Training loss: 0.03138324
[9/200] Training loss: 0.02965763
[10/200] Training loss: 0.02788364
[50/200] Training loss: 0.01806880
[100/200] Training loss: 0.01587627
[150/200] Training loss: 0.01454386
[200/200] Training loss: 0.01301213
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11077.441942975824 ----------
[1/200] Training loss: 0.16144406
[2/200] Training loss: 0.05577342
[3/200] Training loss: 0.04861134
[4/200] Training loss: 0.04518890
[5/200] Training loss: 0.04298769
[6/200] Training loss: 0.03947916
[7/200] Training loss: 0.03621964
[8/200] Training loss: 0.03419195
[9/200] Training loss: 0.03234013
[10/200] Training loss: 0.02977564
[50/200] Training loss: 0.01881506
[100/200] Training loss: 0.01686691
[150/200] Training loss: 0.01563579
[200/200] Training loss: 0.01431043
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9303.201599449514 ----------
[1/200] Training loss: 0.11119641
[2/200] Training loss: 0.05228570
[3/200] Training loss: 0.04837392
[4/200] Training loss: 0.04190914
[5/200] Training loss: 0.03922187
[6/200] Training loss: 0.03602301
[7/200] Training loss: 0.03256074
[8/200] Training loss: 0.02904029
[9/200] Training loss: 0.02728325
[10/200] Training loss: 0.02622741
[50/200] Training loss: 0.01712197
[100/200] Training loss: 0.01538589
[150/200] Training loss: 0.01386257
[200/200] Training loss: 0.01224640
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8054.5778287878 ----------
[1/200] Training loss: 0.08668868
[2/200] Training loss: 0.04989359
[3/200] Training loss: 0.04427653
[4/200] Training loss: 0.04233115
[5/200] Training loss: 0.03869582
[6/200] Training loss: 0.03457898
[7/200] Training loss: 0.03359915
[8/200] Training loss: 0.03120438
[9/200] Training loss: 0.03040047
[10/200] Training loss: 0.02908376
[50/200] Training loss: 0.01666227
[100/200] Training loss: 0.01330120
[150/200] Training loss: 0.01201824
[200/200] Training loss: 0.01065394
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10479.253408521048 ----------
[1/200] Training loss: 0.07361376
[2/200] Training loss: 0.04450870
[3/200] Training loss: 0.03594442
[4/200] Training loss: 0.03040183
[5/200] Training loss: 0.02652121
[6/200] Training loss: 0.02416480
[7/200] Training loss: 0.02241646
[8/200] Training loss: 0.02082215
[9/200] Training loss: 0.02037602
[10/200] Training loss: 0.01963656
[50/200] Training loss: 0.01425774
[100/200] Training loss: 0.01225072
[150/200] Training loss: 0.01126414
[200/200] Training loss: 0.01048544
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10146.583267287566 ----------
[1/200] Training loss: 0.15429987
[2/200] Training loss: 0.05968169
[3/200] Training loss: 0.05167954
[4/200] Training loss: 0.05207212
[5/200] Training loss: 0.04747544
[6/200] Training loss: 0.04680085
[7/200] Training loss: 0.04445130
[8/200] Training loss: 0.04290294
[9/200] Training loss: 0.04231698
[10/200] Training loss: 0.03886363
[50/200] Training loss: 0.01615914
[100/200] Training loss: 0.01392046
[150/200] Training loss: 0.01228574
[200/200] Training loss: 0.01082547
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9806.058535415747 ----------
[1/200] Training loss: 0.12074594
[2/200] Training loss: 0.04966130
[3/200] Training loss: 0.04196109
[4/200] Training loss: 0.03748464
[5/200] Training loss: 0.03231675
[6/200] Training loss: 0.03015748
[7/200] Training loss: 0.02877200
[8/200] Training loss: 0.02624840
[9/200] Training loss: 0.02464571
[10/200] Training loss: 0.02423156
[50/200] Training loss: 0.01597559
[100/200] Training loss: 0.01327971
[150/200] Training loss: 0.01232456
[200/200] Training loss: 0.01091092
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9478.91090790498 ----------
[1/200] Training loss: 0.11281007
[2/200] Training loss: 0.05300317
[3/200] Training loss: 0.05008182
[4/200] Training loss: 0.04523283
[5/200] Training loss: 0.04222396
[6/200] Training loss: 0.04091503
[7/200] Training loss: 0.03710647
[8/200] Training loss: 0.03523840
[9/200] Training loss: 0.03307985
[10/200] Training loss: 0.03138842
[50/200] Training loss: 0.01584648
[100/200] Training loss: 0.01362950
[150/200] Training loss: 0.01192325
[200/200] Training loss: 0.01091197
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7661.553889388236 ----------
[1/200] Training loss: 0.11288746
[2/200] Training loss: 0.05485269
[3/200] Training loss: 0.05044114
[4/200] Training loss: 0.04729367
[5/200] Training loss: 0.03980457
[6/200] Training loss: 0.03301943
[7/200] Training loss: 0.03068553
[8/200] Training loss: 0.02866464
[9/200] Training loss: 0.02722651
[10/200] Training loss: 0.02643789
[50/200] Training loss: 0.01491692
[100/200] Training loss: 0.01259749
[150/200] Training loss: 0.01170587
[200/200] Training loss: 0.01046128
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9468.570747478207 ----------
[1/200] Training loss: 0.10540720
[2/200] Training loss: 0.04435179
[3/200] Training loss: 0.03920102
[4/200] Training loss: 0.03703082
[5/200] Training loss: 0.03132586
[6/200] Training loss: 0.02976833
[7/200] Training loss: 0.02841973
[8/200] Training loss: 0.02641560
[9/200] Training loss: 0.02695158
[10/200] Training loss: 0.02389615
[50/200] Training loss: 0.01549827
[100/200] Training loss: 0.01304124
[150/200] Training loss: 0.01209292
[200/200] Training loss: 0.01149555
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6363.649581804454 ----------
[1/200] Training loss: 0.10745165
[2/200] Training loss: 0.05011126
[3/200] Training loss: 0.04459219
[4/200] Training loss: 0.03789581
[5/200] Training loss: 0.03463429
[6/200] Training loss: 0.03238840
[7/200] Training loss: 0.02885527
[8/200] Training loss: 0.02768522
[9/200] Training loss: 0.02624840
[10/200] Training loss: 0.02619346
[50/200] Training loss: 0.01757107
[100/200] Training loss: 0.01520815
[150/200] Training loss: 0.01377374
[200/200] Training loss: 0.01283186
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5471.007219881912 ----------
[1/200] Training loss: 0.15961520
[2/200] Training loss: 0.05489859
[3/200] Training loss: 0.05201058
[4/200] Training loss: 0.04523319
[5/200] Training loss: 0.04375342
[6/200] Training loss: 0.03849217
[7/200] Training loss: 0.04038875
[8/200] Training loss: 0.03654081
[9/200] Training loss: 0.03358075
[10/200] Training loss: 0.03138597
[50/200] Training loss: 0.01747336
[100/200] Training loss: 0.01501377
[150/200] Training loss: 0.01363704
[200/200] Training loss: 0.01199713
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8783.826956401179 ----------
[1/200] Training loss: 0.11483411
[2/200] Training loss: 0.05423156
[3/200] Training loss: 0.04965207
[4/200] Training loss: 0.04675997
[5/200] Training loss: 0.04177606
[6/200] Training loss: 0.03886644
[7/200] Training loss: 0.03612386
[8/200] Training loss: 0.03292751
[9/200] Training loss: 0.03089619
[10/200] Training loss: 0.02985906
[50/200] Training loss: 0.01656999
[100/200] Training loss: 0.01348026
[150/200] Training loss: 0.01179277
[200/200] Training loss: 0.01130284
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13372.542017133466 ----------
[1/200] Training loss: 0.11435308
[2/200] Training loss: 0.05457357
[3/200] Training loss: 0.05021774
[4/200] Training loss: 0.04620080
[5/200] Training loss: 0.03981586
[6/200] Training loss: 0.03568389
[7/200] Training loss: 0.03353363
[8/200] Training loss: 0.02739044
[9/200] Training loss: 0.02611879
[10/200] Training loss: 0.02568014
[50/200] Training loss: 0.01720512
[100/200] Training loss: 0.01482699
[150/200] Training loss: 0.01410947
[200/200] Training loss: 0.01308924
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14707.95431050831 ----------
[1/200] Training loss: 0.65724402
[2/200] Training loss: 0.22777825
[3/200] Training loss: 0.20792687
[4/200] Training loss: 0.19366888
[5/200] Training loss: 0.18951348
[6/200] Training loss: 0.18121299
[7/200] Training loss: 0.17586004
[8/200] Training loss: 0.17282362
[9/200] Training loss: 0.17370081
[10/200] Training loss: 0.18043666
[50/200] Training loss: 0.06705239
[100/200] Training loss: 0.04283131
[150/200] Training loss: 0.03790356
[200/200] Training loss: 0.03928674
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: RMSprop ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 22826.286601197313 ----------
[1/200] Training loss: 0.09848121
[2/200] Training loss: 0.05230091
[3/200] Training loss: 0.04448866
[4/200] Training loss: 0.04199775
[5/200] Training loss: 0.04068797
[6/200] Training loss: 0.03548539
[7/200] Training loss: 0.03420211
[8/200] Training loss: 0.03309974
[9/200] Training loss: 0.03144209
[10/200] Training loss: 0.02999908
[50/200] Training loss: 0.01583798
[100/200] Training loss: 0.01247976
[150/200] Training loss: 0.01160852
[200/200] Training loss: 0.01088472
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11298.914992157434 ----------
[1/200] Training loss: 0.12679734
[2/200] Training loss: 0.05623647
[3/200] Training loss: 0.05048881
[4/200] Training loss: 0.04434868
[5/200] Training loss: 0.04100867
[6/200] Training loss: 0.03800720
[7/200] Training loss: 0.03516947
[8/200] Training loss: 0.03098015
[9/200] Training loss: 0.02858442
[10/200] Training loss: 0.02806868
[50/200] Training loss: 0.01764379
[100/200] Training loss: 0.01489901
[150/200] Training loss: 0.01296867
[200/200] Training loss: 0.01159174
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7938.969706454358 ----------
[1/200] Training loss: 0.15888275
[2/200] Training loss: 0.05895662
[3/200] Training loss: 0.05308268
[4/200] Training loss: 0.05224366
[5/200] Training loss: 0.05002776
[6/200] Training loss: 0.04936899
[7/200] Training loss: 0.04555797
[8/200] Training loss: 0.04262365
[9/200] Training loss: 0.04133148
[10/200] Training loss: 0.03914455
[50/200] Training loss: 0.01772304
[100/200] Training loss: 0.01601656
[150/200] Training loss: 0.01550620
[200/200] Training loss: 0.01392447
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.35836733830073597 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 21104.617125169552 ----------
[1/200] Training loss: 0.10468944
[2/200] Training loss: 0.05522918
[3/200] Training loss: 0.05187049
[4/200] Training loss: 0.04640508
[5/200] Training loss: 0.04248511
[6/200] Training loss: 0.03599435
[7/200] Training loss: 0.03221052
[8/200] Training loss: 0.02896765
[9/200] Training loss: 0.02776421
[10/200] Training loss: 0.02688693
[50/200] Training loss: 0.01633730
[100/200] Training loss: 0.01393077
[150/200] Training loss: 0.01277782
[200/200] Training loss: 0.01174965
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15354.82621197648 ----------
[1/200] Training loss: 0.10672318
[2/200] Training loss: 0.05338296
[3/200] Training loss: 0.04733038
[4/200] Training loss: 0.03979591
[5/200] Training loss: 0.03606318
[6/200] Training loss: 0.03359345
[7/200] Training loss: 0.03080347
[8/200] Training loss: 0.02870415
[9/200] Training loss: 0.02887791
[10/200] Training loss: 0.02583196
[50/200] Training loss: 0.01524111
[100/200] Training loss: 0.01348514
[150/200] Training loss: 0.01220716
[200/200] Training loss: 0.01136715
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12094.21812272294 ----------
[1/200] Training loss: 0.17100117
[2/200] Training loss: 0.06322630
[3/200] Training loss: 0.05681145
[4/200] Training loss: 0.05434809
[5/200] Training loss: 0.05088309
[6/200] Training loss: 0.04823532
[7/200] Training loss: 0.04591565
[8/200] Training loss: 0.04176134
[9/200] Training loss: 0.03952651
[10/200] Training loss: 0.03733372
[50/200] Training loss: 0.01757291
[100/200] Training loss: 0.01595771
[150/200] Training loss: 0.01380281
[200/200] Training loss: 0.01317014
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13290.679440871336 ----------
[1/200] Training loss: 0.11065496
[2/200] Training loss: 0.05654858
[3/200] Training loss: 0.05113192
[4/200] Training loss: 0.04511588
[5/200] Training loss: 0.04064743
[6/200] Training loss: 0.03615194
[7/200] Training loss: 0.03448125
[8/200] Training loss: 0.02977869
[9/200] Training loss: 0.02854558
[10/200] Training loss: 0.02788369
[50/200] Training loss: 0.01728693
[100/200] Training loss: 0.01556453
[150/200] Training loss: 0.01375723
[200/200] Training loss: 0.01225874
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9033.247920875414 ----------
[1/200] Training loss: 0.09890696
[2/200] Training loss: 0.05074098
[3/200] Training loss: 0.04713561
[4/200] Training loss: 0.04292432
[5/200] Training loss: 0.03708250
[6/200] Training loss: 0.03493960
[7/200] Training loss: 0.03380757
[8/200] Training loss: 0.03283466
[9/200] Training loss: 0.02922663
[10/200] Training loss: 0.02722462
[50/200] Training loss: 0.01707264
[100/200] Training loss: 0.01402026
[150/200] Training loss: 0.01195911
[200/200] Training loss: 0.01113475
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13596.948481185034 ----------
[1/200] Training loss: 0.16127093
[2/200] Training loss: 0.06657312
[3/200] Training loss: 0.05492357
[4/200] Training loss: 0.04933075
[5/200] Training loss: 0.05041279
[6/200] Training loss: 0.04201756
[7/200] Training loss: 0.03687638
[8/200] Training loss: 0.03481098
[9/200] Training loss: 0.03125303
[10/200] Training loss: 0.03013538
[50/200] Training loss: 0.01711475
[100/200] Training loss: 0.01483113
[150/200] Training loss: 0.01384797
[200/200] Training loss: 0.01331351
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10152.790355365367 ----------
[1/200] Training loss: 0.15898105
[2/200] Training loss: 0.05959400
[3/200] Training loss: 0.05294231
[4/200] Training loss: 0.04988623
[5/200] Training loss: 0.04674916
[6/200] Training loss: 0.04404974
[7/200] Training loss: 0.04332962
[8/200] Training loss: 0.03995254
[9/200] Training loss: 0.04097069
[10/200] Training loss: 0.03932279
[50/200] Training loss: 0.02167083
[100/200] Training loss: 0.01642718
[150/200] Training loss: 0.01445729
[200/200] Training loss: 0.01314681
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5001.381809060372 ----------
[1/200] Training loss: 0.09741930
[2/200] Training loss: 0.05230183
[3/200] Training loss: 0.04360218
[4/200] Training loss: 0.04036975
[5/200] Training loss: 0.03711946
[6/200] Training loss: 0.03247812
[7/200] Training loss: 0.02986357
[8/200] Training loss: 0.02808971
[9/200] Training loss: 0.02816951
[10/200] Training loss: 0.02597623
[50/200] Training loss: 0.01589095
[100/200] Training loss: 0.01325214
[150/200] Training loss: 0.01206385
[200/200] Training loss: 0.01148120
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14934.367880831114 ----------
[1/200] Training loss: 0.12183513
[2/200] Training loss: 0.05354474
[3/200] Training loss: 0.04638183
[4/200] Training loss: 0.04150995
[5/200] Training loss: 0.03810595
[6/200] Training loss: 0.03492817
[7/200] Training loss: 0.03191212
[8/200] Training loss: 0.03089829
[9/200] Training loss: 0.02740595
[10/200] Training loss: 0.02694662
[50/200] Training loss: 0.01768941
[100/200] Training loss: 0.01531881
[150/200] Training loss: 0.01397981
[200/200] Training loss: 0.01305857
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4431.445588067171 ----------
[1/200] Training loss: 0.13424544
[2/200] Training loss: 0.05300120
[3/200] Training loss: 0.04855600
[4/200] Training loss: 0.04153491
[5/200] Training loss: 0.03951038
[6/200] Training loss: 0.03555231
[7/200] Training loss: 0.03386028
[8/200] Training loss: 0.03068604
[9/200] Training loss: 0.02947749
[10/200] Training loss: 0.02882211
[50/200] Training loss: 0.01728072
[100/200] Training loss: 0.01528044
[150/200] Training loss: 0.01394340
[200/200] Training loss: 0.01190821
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8707.586577232523 ----------
[1/200] Training loss: 0.15180861
[2/200] Training loss: 0.05742495
[3/200] Training loss: 0.05273289
[4/200] Training loss: 0.04875059
[5/200] Training loss: 0.04359504
[6/200] Training loss: 0.04324395
[7/200] Training loss: 0.04229848
[8/200] Training loss: 0.03824914
[9/200] Training loss: 0.03746216
[10/200] Training loss: 0.03469365
[50/200] Training loss: 0.01853134
[100/200] Training loss: 0.01613923
[150/200] Training loss: 0.01450786
[200/200] Training loss: 0.01353769
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6908.67888962861 ----------
[1/200] Training loss: 0.11989599
[2/200] Training loss: 0.05499826
[3/200] Training loss: 0.04843936
[4/200] Training loss: 0.04666109
[5/200] Training loss: 0.04097885
[6/200] Training loss: 0.03690434
[7/200] Training loss: 0.03485340
[8/200] Training loss: 0.03224230
[9/200] Training loss: 0.03092011
[10/200] Training loss: 0.02857917
[50/200] Training loss: 0.01505064
[100/200] Training loss: 0.01311317
[150/200] Training loss: 0.01143315
[200/200] Training loss: 0.01114177
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027806129394196858
----FITNESS-----------RMSE---- 10866.741922029803 ----------
[1/200] Training loss: 0.10346727
[2/200] Training loss: 0.05342244
[3/200] Training loss: 0.04678420
[4/200] Training loss: 0.04079360
[5/200] Training loss: 0.03757786
[6/200] Training loss: 0.03338327
[7/200] Training loss: 0.03140112
[8/200] Training loss: 0.02965849
[9/200] Training loss: 0.02812121
[10/200] Training loss: 0.02537686
[50/200] Training loss: 0.01698357
[100/200] Training loss: 0.01455348
[150/200] Training loss: 0.01293500
[200/200] Training loss: 0.01207469
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7331.141793745364 ----------
[1/200] Training loss: 0.16721168
[2/200] Training loss: 0.05899394
[3/200] Training loss: 0.05169041
[4/200] Training loss: 0.04689917
[5/200] Training loss: 0.04545466
[6/200] Training loss: 0.04288299
[7/200] Training loss: 0.03860368
[8/200] Training loss: 0.03804815
[9/200] Training loss: 0.03690375
[10/200] Training loss: 0.03116867
[50/200] Training loss: 0.01898059
[100/200] Training loss: 0.01591564
[150/200] Training loss: 0.01475884
[200/200] Training loss: 0.01384950
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13076.759231552747 ----------
[1/200] Training loss: 0.14296956
[2/200] Training loss: 0.05396820
[3/200] Training loss: 0.04910224
[4/200] Training loss: 0.04557779
[5/200] Training loss: 0.04102403
[6/200] Training loss: 0.04050962
[7/200] Training loss: 0.03926025
[8/200] Training loss: 0.03745551
[9/200] Training loss: 0.03467905
[10/200] Training loss: 0.03417513
[50/200] Training loss: 0.01622707
[100/200] Training loss: 0.01413278
[150/200] Training loss: 0.01293545
[200/200] Training loss: 0.01200966
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4355.603517309628 ----------
[1/200] Training loss: 0.12967839
[2/200] Training loss: 0.05625800
[3/200] Training loss: 0.04955978
[4/200] Training loss: 0.04364611
[5/200] Training loss: 0.04392910
[6/200] Training loss: 0.04065062
[7/200] Training loss: 0.03748255
[8/200] Training loss: 0.03442251
[9/200] Training loss: 0.03236497
[10/200] Training loss: 0.03055128
[50/200] Training loss: 0.01830544
[100/200] Training loss: 0.01497650
[150/200] Training loss: 0.01281395
[200/200] Training loss: 0.01202012
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 18102.010496074738 ----------
[1/200] Training loss: 0.15521264
[2/200] Training loss: 0.05868686
[3/200] Training loss: 0.04588806
[4/200] Training loss: 0.04555953
[5/200] Training loss: 0.04195041
[6/200] Training loss: 0.03909993
[7/200] Training loss: 0.03674972
[8/200] Training loss: 0.03475129
[9/200] Training loss: 0.03427273
[10/200] Training loss: 0.03330427
[50/200] Training loss: 0.01733963
[100/200] Training loss: 0.01472622
[150/200] Training loss: 0.01252044
[200/200] Training loss: 0.01242689
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22869.201647630816 ----------
[1/200] Training loss: 0.10063086
[2/200] Training loss: 0.05073394
[3/200] Training loss: 0.04710378
[4/200] Training loss: 0.04216231
[5/200] Training loss: 0.03771334
[6/200] Training loss: 0.03530132
[7/200] Training loss: 0.03184046
[8/200] Training loss: 0.02934615
[9/200] Training loss: 0.02864686
[10/200] Training loss: 0.02554619
[50/200] Training loss: 0.01694556
[100/200] Training loss: 0.01352245
[150/200] Training loss: 0.01199003
[200/200] Training loss: 0.01145348
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5780.570041094563 ----------
[1/200] Training loss: 0.11267062
[2/200] Training loss: 0.05139290
[3/200] Training loss: 0.04585638
[4/200] Training loss: 0.04198942
[5/200] Training loss: 0.03644211
[6/200] Training loss: 0.03298346
[7/200] Training loss: 0.02733489
[8/200] Training loss: 0.02771731
[9/200] Training loss: 0.02536514
[10/200] Training loss: 0.02416312
[50/200] Training loss: 0.01561540
[100/200] Training loss: 0.01335974
[150/200] Training loss: 0.01238861
[200/200] Training loss: 0.01197837
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6637.079779541602 ----------
[1/200] Training loss: 0.16432126
[2/200] Training loss: 0.06346494
[3/200] Training loss: 0.05302841
[4/200] Training loss: 0.05222170
[5/200] Training loss: 0.04707842
[6/200] Training loss: 0.04557068
[7/200] Training loss: 0.04365449
[8/200] Training loss: 0.04170197
[9/200] Training loss: 0.03940976
[10/200] Training loss: 0.03576698
[50/200] Training loss: 0.01928016
[100/200] Training loss: 0.01656114
[150/200] Training loss: 0.01447997
[200/200] Training loss: 0.01374562
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8282.628568274687 ----------
[1/200] Training loss: 0.11759249
[2/200] Training loss: 0.05857705
[3/200] Training loss: 0.05191779
[4/200] Training loss: 0.04825479
[5/200] Training loss: 0.04131825
[6/200] Training loss: 0.03410693
[7/200] Training loss: 0.03048116
[8/200] Training loss: 0.02732304
[9/200] Training loss: 0.02777722
[10/200] Training loss: 0.02630553
[50/200] Training loss: 0.01724849
[100/200] Training loss: 0.01357254
[150/200] Training loss: 0.01275352
[200/200] Training loss: 0.01161817
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 23099.52311196056 ----------
[1/200] Training loss: 0.11616100
[2/200] Training loss: 0.05507353
[3/200] Training loss: 0.05067624
[4/200] Training loss: 0.04869133
[5/200] Training loss: 0.04348335
[6/200] Training loss: 0.03832947
[7/200] Training loss: 0.03715140
[8/200] Training loss: 0.03255012
[9/200] Training loss: 0.03103937
[10/200] Training loss: 0.03007016
[50/200] Training loss: 0.01653477
[100/200] Training loss: 0.01481460
[150/200] Training loss: 0.01327322
[200/200] Training loss: 0.01211341
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10002.379316942544 ----------
[1/200] Training loss: 0.15474759
[2/200] Training loss: 0.05911632
[3/200] Training loss: 0.05182764
[4/200] Training loss: 0.04879611
[5/200] Training loss: 0.04345011
[6/200] Training loss: 0.04177234
[7/200] Training loss: 0.03915800
[8/200] Training loss: 0.03641771
[9/200] Training loss: 0.03372905
[10/200] Training loss: 0.03241617
[50/200] Training loss: 0.01712925
[100/200] Training loss: 0.01548844
[150/200] Training loss: 0.01513796
[200/200] Training loss: 0.01299592
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9012.311135330381 ----------
[1/200] Training loss: 0.12530490
[2/200] Training loss: 0.05050691
[3/200] Training loss: 0.04252589
[4/200] Training loss: 0.03489069
[5/200] Training loss: 0.03010414
[6/200] Training loss: 0.02855670
[7/200] Training loss: 0.02774344
[8/200] Training loss: 0.02512243
[9/200] Training loss: 0.02416683
[10/200] Training loss: 0.02250466
[50/200] Training loss: 0.01587030
[100/200] Training loss: 0.01450157
[150/200] Training loss: 0.01337465
[200/200] Training loss: 0.01248336
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11212.889725668401 ----------
[1/200] Training loss: 0.15890195
[2/200] Training loss: 0.05778251
[3/200] Training loss: 0.05545999
[4/200] Training loss: 0.05390354
[5/200] Training loss: 0.05100589
[6/200] Training loss: 0.04892770
[7/200] Training loss: 0.04773475
[8/200] Training loss: 0.04623189
[9/200] Training loss: 0.04410343
[10/200] Training loss: 0.04246745
[50/200] Training loss: 0.01916954
[100/200] Training loss: 0.01486833
[150/200] Training loss: 0.01354330
[200/200] Training loss: 0.01270781
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 3592.190278924545 ----------
[1/200] Training loss: 0.11190053
[2/200] Training loss: 0.05178870
[3/200] Training loss: 0.04405899
[4/200] Training loss: 0.04454111
[5/200] Training loss: 0.04013975
[6/200] Training loss: 0.03793584
[7/200] Training loss: 0.03614749
[8/200] Training loss: 0.03411197
[9/200] Training loss: 0.03316185
[10/200] Training loss: 0.03149268
[50/200] Training loss: 0.01613001
[100/200] Training loss: 0.01413692
[150/200] Training loss: 0.01188245
[200/200] Training loss: 0.01094923
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4833.847535866227 ----------
[1/200] Training loss: 0.11228723
[2/200] Training loss: 0.05149893
[3/200] Training loss: 0.04633501
[4/200] Training loss: 0.04160290
[5/200] Training loss: 0.03831718
[6/200] Training loss: 0.03544310
[7/200] Training loss: 0.03287754
[8/200] Training loss: 0.02968993
[9/200] Training loss: 0.02782459
[10/200] Training loss: 0.02508653
[50/200] Training loss: 0.01573099
[100/200] Training loss: 0.01377725
[150/200] Training loss: 0.01296014
[200/200] Training loss: 0.01224623
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12883.967711850259 ----------
[1/200] Training loss: 0.16411791
[2/200] Training loss: 0.05853989
[3/200] Training loss: 0.05360176
[4/200] Training loss: 0.05077004
[5/200] Training loss: 0.04829995
[6/200] Training loss: 0.04836009
[7/200] Training loss: 0.04522490
[8/200] Training loss: 0.04527451
[9/200] Training loss: 0.04299332
[10/200] Training loss: 0.03864290
[50/200] Training loss: 0.01818648
[100/200] Training loss: 0.01494167
[150/200] Training loss: 0.01356511
[200/200] Training loss: 0.01248207
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14437.337704715506 ----------
[1/200] Training loss: 0.11852603
[2/200] Training loss: 0.05701765
[3/200] Training loss: 0.05376339
[4/200] Training loss: 0.05148557
[5/200] Training loss: 0.04716571
[6/200] Training loss: 0.04480997
[7/200] Training loss: 0.04427699
[8/200] Training loss: 0.04160132
[9/200] Training loss: 0.03963596
[10/200] Training loss: 0.03704761
[50/200] Training loss: 0.01847177
[100/200] Training loss: 0.01578038
[150/200] Training loss: 0.01427220
[200/200] Training loss: 0.01317473
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11432.6700293501 ----------
[1/200] Training loss: 0.09691042
[2/200] Training loss: 0.04943089
[3/200] Training loss: 0.04242270
[4/200] Training loss: 0.03893324
[5/200] Training loss: 0.03444647
[6/200] Training loss: 0.03310844
[7/200] Training loss: 0.03042347
[8/200] Training loss: 0.02803469
[9/200] Training loss: 0.02824619
[10/200] Training loss: 0.02682841
[50/200] Training loss: 0.01599417
[100/200] Training loss: 0.01261859
[150/200] Training loss: 0.01142330
[200/200] Training loss: 0.01035833
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.32887531315588825 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 3334.734472188153 ----------
[1/200] Training loss: 0.13754231
[2/200] Training loss: 0.05664934
[3/200] Training loss: 0.05248451
[4/200] Training loss: 0.04760434
[5/200] Training loss: 0.04556537
[6/200] Training loss: 0.04379827
[7/200] Training loss: 0.04109219
[8/200] Training loss: 0.03895657
[9/200] Training loss: 0.03572454
[10/200] Training loss: 0.03735460
[50/200] Training loss: 0.01772235
[100/200] Training loss: 0.01462959
[150/200] Training loss: 0.01344178
[200/200] Training loss: 0.01296118
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13278.52642426862 ----------
[1/200] Training loss: 0.16721041
[2/200] Training loss: 0.05857420
[3/200] Training loss: 0.05314269
[4/200] Training loss: 0.05169076
[5/200] Training loss: 0.04724338
[6/200] Training loss: 0.04599110
[7/200] Training loss: 0.04569749
[8/200] Training loss: 0.04236189
[9/200] Training loss: 0.03949976
[10/200] Training loss: 0.03820386
[50/200] Training loss: 0.01847944
[100/200] Training loss: 0.01496777
[150/200] Training loss: 0.01408198
[200/200] Training loss: 0.01257618
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16545.331667875384 ----------
[1/200] Training loss: 0.13390291
[2/200] Training loss: 0.05599594
[3/200] Training loss: 0.05308872
[4/200] Training loss: 0.04806974
[5/200] Training loss: 0.04472236
[6/200] Training loss: 0.04287470
[7/200] Training loss: 0.03794835
[8/200] Training loss: 0.03711628
[9/200] Training loss: 0.03396314
[10/200] Training loss: 0.03198293
[50/200] Training loss: 0.01746543
[100/200] Training loss: 0.01510929
[150/200] Training loss: 0.01325642
[200/200] Training loss: 0.01202635
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18875.965670661728 ----------
[1/200] Training loss: 0.15833960
[2/200] Training loss: 0.05671746
[3/200] Training loss: 0.05139386
[4/200] Training loss: 0.04455357
[5/200] Training loss: 0.04316203
[6/200] Training loss: 0.04235308
[7/200] Training loss: 0.03843336
[8/200] Training loss: 0.03573612
[9/200] Training loss: 0.03292463
[10/200] Training loss: 0.03361448
[50/200] Training loss: 0.01914705
[100/200] Training loss: 0.01413947
[150/200] Training loss: 0.01284634
[200/200] Training loss: 0.01163553
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5202.215874029066 ----------
[1/200] Training loss: 0.11090733
[2/200] Training loss: 0.05596207
[3/200] Training loss: 0.05006744
[4/200] Training loss: 0.04700935
[5/200] Training loss: 0.04368449
[6/200] Training loss: 0.03683890
[7/200] Training loss: 0.03487314
[8/200] Training loss: 0.03077398
[9/200] Training loss: 0.02958917
[10/200] Training loss: 0.02779860
[50/200] Training loss: 0.01792178
[100/200] Training loss: 0.01549677
[150/200] Training loss: 0.01417975
[200/200] Training loss: 0.01253257
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11570.269141208428 ----------
[1/200] Training loss: 0.12240885
[2/200] Training loss: 0.05745370
[3/200] Training loss: 0.04874374
[4/200] Training loss: 0.04840900
[5/200] Training loss: 0.04413417
[6/200] Training loss: 0.04090757
[7/200] Training loss: 0.03650887
[8/200] Training loss: 0.03364294
[9/200] Training loss: 0.03034160
[10/200] Training loss: 0.02808634
[50/200] Training loss: 0.01696689
[100/200] Training loss: 0.01400151
[150/200] Training loss: 0.01195867
[200/200] Training loss: 0.01116725
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9461.83026691982 ----------
[1/200] Training loss: 0.15315166
[2/200] Training loss: 0.05625364
[3/200] Training loss: 0.05288864
[4/200] Training loss: 0.04964384
[5/200] Training loss: 0.04939509
[6/200] Training loss: 0.04546297
[7/200] Training loss: 0.04515557
[8/200] Training loss: 0.04075315
[9/200] Training loss: 0.03696235
[10/200] Training loss: 0.03460160
[50/200] Training loss: 0.01909159
[100/200] Training loss: 0.01420046
[150/200] Training loss: 0.01253063
[200/200] Training loss: 0.01135294
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16765.213508929734 ----------
[1/200] Training loss: 0.11381098
[2/200] Training loss: 0.05619828
[3/200] Training loss: 0.04784211
[4/200] Training loss: 0.04438614
[5/200] Training loss: 0.03805667
[6/200] Training loss: 0.03630161
[7/200] Training loss: 0.03381894
[8/200] Training loss: 0.03219626
[9/200] Training loss: 0.03095106
[10/200] Training loss: 0.03005798
[50/200] Training loss: 0.01582889
[100/200] Training loss: 0.01345492
[150/200] Training loss: 0.01257190
[200/200] Training loss: 0.01118682
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14524.116771769635 ----------
[1/200] Training loss: 0.10613871
[2/200] Training loss: 0.05023232
[3/200] Training loss: 0.04544898
[4/200] Training loss: 0.04127062
[5/200] Training loss: 0.03747340
[6/200] Training loss: 0.03360846
[7/200] Training loss: 0.03275807
[8/200] Training loss: 0.03020327
[9/200] Training loss: 0.02817228
[10/200] Training loss: 0.02692687
[50/200] Training loss: 0.01619705
[100/200] Training loss: 0.01311901
[150/200] Training loss: 0.01165646
[200/200] Training loss: 0.01066188
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11797.78148636429 ----------
[1/200] Training loss: 0.11434758
[2/200] Training loss: 0.05442750
[3/200] Training loss: 0.04787958
[4/200] Training loss: 0.04323686
[5/200] Training loss: 0.03694659
[6/200] Training loss: 0.03574075
[7/200] Training loss: 0.03169745
[8/200] Training loss: 0.02871073
[9/200] Training loss: 0.03014226
[10/200] Training loss: 0.02576929
[50/200] Training loss: 0.01581948
[100/200] Training loss: 0.01392777
[150/200] Training loss: 0.01287912
[200/200] Training loss: 0.01134197
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 25621.935602135916 ----------
[1/200] Training loss: 0.06074542
[2/200] Training loss: 0.03802078
[3/200] Training loss: 0.03284609
[4/200] Training loss: 0.02886814
[5/200] Training loss: 0.02800530
[6/200] Training loss: 0.02672077
[7/200] Training loss: 0.02488839
[8/200] Training loss: 0.02358481
[9/200] Training loss: 0.02110761
[10/200] Training loss: 0.02099314
[50/200] Training loss: 0.01390772
[100/200] Training loss: 0.01114687
[150/200] Training loss: 0.01014515
[200/200] Training loss: 0.00957875
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12787.313713208103 ----------
[1/200] Training loss: 0.01873350
[2/200] Training loss: 0.00351473
[3/200] Training loss: 0.00280743
[4/200] Training loss: 0.00217239
[5/200] Training loss: 0.00145028
[6/200] Training loss: 0.00117125
[7/200] Training loss: 0.00099789
[8/200] Training loss: 0.00084590
[9/200] Training loss: 0.00081167
[10/200] Training loss: 0.00076043
[50/200] Training loss: 0.00037200
[100/200] Training loss: 0.00028833
[150/200] Training loss: 0.00024227
[200/200] Training loss: 0.00022008
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.32887531315588825 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 18612.16462424508 ----------
[1/200] Training loss: 0.12352915
[2/200] Training loss: 0.05452989
[3/200] Training loss: 0.05002870
[4/200] Training loss: 0.04799930
[5/200] Training loss: 0.04440774
[6/200] Training loss: 0.03809133
[7/200] Training loss: 0.03423403
[8/200] Training loss: 0.03173581
[9/200] Training loss: 0.02752224
[10/200] Training loss: 0.02677669
[50/200] Training loss: 0.01801295
[100/200] Training loss: 0.01620499
[150/200] Training loss: 0.01521090
[200/200] Training loss: 0.01342269
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10469.351842401706 ----------
[1/200] Training loss: 0.12031558
[2/200] Training loss: 0.05359056
[3/200] Training loss: 0.04482740
[4/200] Training loss: 0.04192864
[5/200] Training loss: 0.03524033
[6/200] Training loss: 0.03141136
[7/200] Training loss: 0.02910811
[8/200] Training loss: 0.02579984
[9/200] Training loss: 0.02674953
[10/200] Training loss: 0.02477105
[50/200] Training loss: 0.01749717
[100/200] Training loss: 0.01463519
[150/200] Training loss: 0.01261474
[200/200] Training loss: 0.01129359
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7097.976613091931 ----------
[1/200] Training loss: 0.11219091
[2/200] Training loss: 0.05656463
[3/200] Training loss: 0.05263211
[4/200] Training loss: 0.04741992
[5/200] Training loss: 0.04565375
[6/200] Training loss: 0.04194838
[7/200] Training loss: 0.04000796
[8/200] Training loss: 0.03639944
[9/200] Training loss: 0.03294249
[10/200] Training loss: 0.02972862
[50/200] Training loss: 0.01784337
[100/200] Training loss: 0.01381621
[150/200] Training loss: 0.01285332
[200/200] Training loss: 0.01205677
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9668.466269269393 ----------
[1/200] Training loss: 0.10593018
[2/200] Training loss: 0.05405909
[3/200] Training loss: 0.04940631
[4/200] Training loss: 0.04568341
[5/200] Training loss: 0.04178635
[6/200] Training loss: 0.03749454
[7/200] Training loss: 0.03425871
[8/200] Training loss: 0.02959092
[9/200] Training loss: 0.03071450
[10/200] Training loss: 0.02775464
[50/200] Training loss: 0.01616715
[100/200] Training loss: 0.01409898
[150/200] Training loss: 0.01188226
[200/200] Training loss: 0.01059779
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11722.702077592861 ----------
[1/200] Training loss: 0.15745789
[2/200] Training loss: 0.05621438
[3/200] Training loss: 0.04986570
[4/200] Training loss: 0.04671389
[5/200] Training loss: 0.04220467
[6/200] Training loss: 0.03724998
[7/200] Training loss: 0.03743180
[8/200] Training loss: 0.03548711
[9/200] Training loss: 0.03450778
[10/200] Training loss: 0.03471238
[50/200] Training loss: 0.01878339
[100/200] Training loss: 0.01540920
[150/200] Training loss: 0.01382233
[200/200] Training loss: 0.01251749
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17287.103169704285 ----------
[1/200] Training loss: 0.11475280
[2/200] Training loss: 0.05166263
[3/200] Training loss: 0.04569834
[4/200] Training loss: 0.03908799
[5/200] Training loss: 0.03472455
[6/200] Training loss: 0.03126723
[7/200] Training loss: 0.02898015
[8/200] Training loss: 0.02808491
[9/200] Training loss: 0.02827347
[10/200] Training loss: 0.02664977
[50/200] Training loss: 0.01842215
[100/200] Training loss: 0.01601861
[150/200] Training loss: 0.01426458
[200/200] Training loss: 0.01369217
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10541.917472642252 ----------
[1/200] Training loss: 0.16446550
[2/200] Training loss: 0.05215164
[3/200] Training loss: 0.04431833
[4/200] Training loss: 0.04159078
[5/200] Training loss: 0.03792872
[6/200] Training loss: 0.03553626
[7/200] Training loss: 0.03090288
[8/200] Training loss: 0.02984397
[9/200] Training loss: 0.02829855
[10/200] Training loss: 0.02614710
[50/200] Training loss: 0.01720104
[100/200] Training loss: 0.01527302
[150/200] Training loss: 0.01424275
[200/200] Training loss: 0.01349361
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7645.166054442507 ----------
[1/200] Training loss: 0.16309154
[2/200] Training loss: 0.06014958
[3/200] Training loss: 0.05144255
[4/200] Training loss: 0.04776016
[5/200] Training loss: 0.03995157
[6/200] Training loss: 0.03749463
[7/200] Training loss: 0.03426247
[8/200] Training loss: 0.03295760
[9/200] Training loss: 0.03083648
[10/200] Training loss: 0.03156532
[50/200] Training loss: 0.01808064
[100/200] Training loss: 0.01609499
[150/200] Training loss: 0.01460969
[200/200] Training loss: 0.01402947
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11823.443829950731 ----------
[1/200] Training loss: 0.17696742
[2/200] Training loss: 0.05853611
[3/200] Training loss: 0.05127685
[4/200] Training loss: 0.05253144
[5/200] Training loss: 0.04937534
[6/200] Training loss: 0.04896716
[7/200] Training loss: 0.04803173
[8/200] Training loss: 0.04326326
[9/200] Training loss: 0.04218094
[10/200] Training loss: 0.03846642
[50/200] Training loss: 0.01732118
[100/200] Training loss: 0.01489695
[150/200] Training loss: 0.01297723
[200/200] Training loss: 0.01273747
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16699.04907472279 ----------
[1/200] Training loss: 0.15950374
[2/200] Training loss: 0.05824298
[3/200] Training loss: 0.05287587
[4/200] Training loss: 0.04959721
[5/200] Training loss: 0.04470618
[6/200] Training loss: 0.03933612
[7/200] Training loss: 0.04096174
[8/200] Training loss: 0.03578355
[9/200] Training loss: 0.03521993
[10/200] Training loss: 0.03429060
[50/200] Training loss: 0.01771004
[100/200] Training loss: 0.01482143
[150/200] Training loss: 0.01386507
[200/200] Training loss: 0.01287423
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19091.003116651573 ----------
[1/200] Training loss: 0.15908577
[2/200] Training loss: 0.06555601
[3/200] Training loss: 0.05542930
[4/200] Training loss: 0.05337688
[5/200] Training loss: 0.05112356
[6/200] Training loss: 0.04660736
[7/200] Training loss: 0.04306526
[8/200] Training loss: 0.04001387
[9/200] Training loss: 0.03649105
[10/200] Training loss: 0.03445264
[50/200] Training loss: 0.01864967
[100/200] Training loss: 0.01552430
[150/200] Training loss: 0.01437022
[200/200] Training loss: 0.01263935
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6553.803170678839 ----------
[1/200] Training loss: 0.10591972
[2/200] Training loss: 0.05120883
[3/200] Training loss: 0.04805431
[4/200] Training loss: 0.04399583
[5/200] Training loss: 0.04077202
[6/200] Training loss: 0.03817406
[7/200] Training loss: 0.03748311
[8/200] Training loss: 0.03445716
[9/200] Training loss: 0.03160833
[10/200] Training loss: 0.02980015
[50/200] Training loss: 0.01795446
[100/200] Training loss: 0.01444002
[150/200] Training loss: 0.01397501
[200/200] Training loss: 0.01163454
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5444.977318593715 ----------
[1/200] Training loss: 0.13493939
[2/200] Training loss: 0.05262417
[3/200] Training loss: 0.05186433
[4/200] Training loss: 0.05003399
[5/200] Training loss: 0.04776251
[6/200] Training loss: 0.04226306
[7/200] Training loss: 0.03902443
[8/200] Training loss: 0.03864528
[9/200] Training loss: 0.03269365
[10/200] Training loss: 0.03038876
[50/200] Training loss: 0.01785339
[100/200] Training loss: 0.01512238
[150/200] Training loss: 0.01383197
[200/200] Training loss: 0.01275515
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.32887531315588825 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13525.817683230836 ----------
[1/200] Training loss: 0.14662626
[2/200] Training loss: 0.06084385
[3/200] Training loss: 0.05464319
[4/200] Training loss: 0.04943819
[5/200] Training loss: 0.05005594
[6/200] Training loss: 0.04606279
[7/200] Training loss: 0.04572241
[8/200] Training loss: 0.04051507
[9/200] Training loss: 0.03659953
[10/200] Training loss: 0.03499822
[50/200] Training loss: 0.01723549
[100/200] Training loss: 0.01452699
[150/200] Training loss: 0.01291931
[200/200] Training loss: 0.01280021
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17646.090558534488 ----------
[1/200] Training loss: 0.17147377
[2/200] Training loss: 0.06316196
[3/200] Training loss: 0.05389878
[4/200] Training loss: 0.05142656
[5/200] Training loss: 0.04734549
[6/200] Training loss: 0.04462551
[7/200] Training loss: 0.04467407
[8/200] Training loss: 0.04138454
[9/200] Training loss: 0.04013452
[10/200] Training loss: 0.03960356
[50/200] Training loss: 0.01858570
[100/200] Training loss: 0.01556697
[150/200] Training loss: 0.01412265
[200/200] Training loss: 0.01214118
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11410.215072469055 ----------
[1/200] Training loss: 0.10475049
[2/200] Training loss: 0.05319576
[3/200] Training loss: 0.04933805
[4/200] Training loss: 0.04158788
[5/200] Training loss: 0.03692744
[6/200] Training loss: 0.03247707
[7/200] Training loss: 0.02975504
[8/200] Training loss: 0.02856254
[9/200] Training loss: 0.02498690
[10/200] Training loss: 0.02523042
[50/200] Training loss: 0.01472086
[100/200] Training loss: 0.01278741
[150/200] Training loss: 0.01152887
[200/200] Training loss: 0.01057366
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 21018.508415203967 ----------
[1/200] Training loss: 0.15477645
[2/200] Training loss: 0.05446037
[3/200] Training loss: 0.05200015
[4/200] Training loss: 0.04610865
[5/200] Training loss: 0.04434295
[6/200] Training loss: 0.04145431
[7/200] Training loss: 0.04044314
[8/200] Training loss: 0.03657958
[9/200] Training loss: 0.03398987
[10/200] Training loss: 0.03278405
[50/200] Training loss: 0.01674035
[100/200] Training loss: 0.01531975
[150/200] Training loss: 0.01455757
[200/200] Training loss: 0.01316569
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7861.899770411729 ----------
[1/200] Training loss: 0.10748279
[2/200] Training loss: 0.05200435
[3/200] Training loss: 0.04548081
[4/200] Training loss: 0.04126858
[5/200] Training loss: 0.03667422
[6/200] Training loss: 0.03235525
[7/200] Training loss: 0.02879861
[8/200] Training loss: 0.02613095
[9/200] Training loss: 0.02649424
[10/200] Training loss: 0.02484088
[50/200] Training loss: 0.01666072
[100/200] Training loss: 0.01385326
[150/200] Training loss: 0.01253064
[200/200] Training loss: 0.01143689
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 22450.815931720612 ----------
[1/200] Training loss: 0.17723843
[2/200] Training loss: 0.06913230
[3/200] Training loss: 0.05774564
[4/200] Training loss: 0.05260524
[5/200] Training loss: 0.04958538
[6/200] Training loss: 0.04847588
[7/200] Training loss: 0.04583289
[8/200] Training loss: 0.04215200
[9/200] Training loss: 0.04237390
[10/200] Training loss: 0.04196200
[50/200] Training loss: 0.01866500
[100/200] Training loss: 0.01604790
[150/200] Training loss: 0.01433093
[200/200] Training loss: 0.01397774
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16049.51936975061 ----------
[1/200] Training loss: 0.17077464
[2/200] Training loss: 0.05463715
[3/200] Training loss: 0.04972685
[4/200] Training loss: 0.04493310
[5/200] Training loss: 0.04366505
[6/200] Training loss: 0.03851414
[7/200] Training loss: 0.03873445
[8/200] Training loss: 0.03461673
[9/200] Training loss: 0.03410996
[10/200] Training loss: 0.03234716
[50/200] Training loss: 0.01758583
[100/200] Training loss: 0.01566287
[150/200] Training loss: 0.01365684
[200/200] Training loss: 0.01246591
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4476.919253236538 ----------
[1/200] Training loss: 0.12290429
[2/200] Training loss: 0.05347039
[3/200] Training loss: 0.04817037
[4/200] Training loss: 0.04390160
[5/200] Training loss: 0.04070965
[6/200] Training loss: 0.03683826
[7/200] Training loss: 0.03395056
[8/200] Training loss: 0.02970775
[9/200] Training loss: 0.02654961
[10/200] Training loss: 0.02551355
[50/200] Training loss: 0.01460715
[100/200] Training loss: 0.01280872
[150/200] Training loss: 0.01219404
[200/200] Training loss: 0.01119854
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5710.979775835316 ----------
[1/200] Training loss: 0.17121519
[2/200] Training loss: 0.06193695
[3/200] Training loss: 0.05296909
[4/200] Training loss: 0.04691533
[5/200] Training loss: 0.04473516
[6/200] Training loss: 0.04239786
[7/200] Training loss: 0.04108727
[8/200] Training loss: 0.03864068
[9/200] Training loss: 0.03799406
[10/200] Training loss: 0.03675440
[50/200] Training loss: 0.01989003
[100/200] Training loss: 0.01699726
[150/200] Training loss: 0.01501480
[200/200] Training loss: 0.01394139
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6756.367663175236 ----------
[1/200] Training loss: 0.09442830
[2/200] Training loss: 0.04922867
[3/200] Training loss: 0.04185154
[4/200] Training loss: 0.03734825
[5/200] Training loss: 0.03356390
[6/200] Training loss: 0.03046624
[7/200] Training loss: 0.02866738
[8/200] Training loss: 0.02782187
[9/200] Training loss: 0.02608290
[10/200] Training loss: 0.02471570
[50/200] Training loss: 0.01576592
[100/200] Training loss: 0.01328723
[150/200] Training loss: 0.01205573
[200/200] Training loss: 0.01119858
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 18915.873545781596 ----------
[1/200] Training loss: 0.11680376
[2/200] Training loss: 0.05535648
[3/200] Training loss: 0.04840002
[4/200] Training loss: 0.04520340
[5/200] Training loss: 0.04315177
[6/200] Training loss: 0.04042581
[7/200] Training loss: 0.03835392
[8/200] Training loss: 0.03597559
[9/200] Training loss: 0.03314730
[10/200] Training loss: 0.03082126
[50/200] Training loss: 0.01581219
[100/200] Training loss: 0.01374903
[150/200] Training loss: 0.01200447
[200/200] Training loss: 0.01080447
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7104.811609043551 ----------
[1/200] Training loss: 0.16782754
[2/200] Training loss: 0.05914588
[3/200] Training loss: 0.05373609
[4/200] Training loss: 0.04884520
[5/200] Training loss: 0.04271260
[6/200] Training loss: 0.04149118
[7/200] Training loss: 0.04064600
[8/200] Training loss: 0.03560870
[9/200] Training loss: 0.03540126
[10/200] Training loss: 0.03057862
[50/200] Training loss: 0.01829488
[100/200] Training loss: 0.01463796
[150/200] Training loss: 0.01356719
[200/200] Training loss: 0.01272631
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15250.678148856201 ----------
[1/200] Training loss: 0.16217050
[2/200] Training loss: 0.05665051
[3/200] Training loss: 0.05072411
[4/200] Training loss: 0.04841956
[5/200] Training loss: 0.04483257
[6/200] Training loss: 0.04072843
[7/200] Training loss: 0.03808598
[8/200] Training loss: 0.03866258
[9/200] Training loss: 0.03581229
[10/200] Training loss: 0.03447360
[50/200] Training loss: 0.01779772
[100/200] Training loss: 0.01576677
[150/200] Training loss: 0.01376775
[200/200] Training loss: 0.01284393
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6141.438919341297 ----------
[1/200] Training loss: 0.10557206
[2/200] Training loss: 0.05080226
[3/200] Training loss: 0.04288972
[4/200] Training loss: 0.03808342
[5/200] Training loss: 0.03500025
[6/200] Training loss: 0.03303587
[7/200] Training loss: 0.02831779
[8/200] Training loss: 0.02760460
[9/200] Training loss: 0.02800622
[10/200] Training loss: 0.02564674
[50/200] Training loss: 0.01688071
[100/200] Training loss: 0.01406732
[150/200] Training loss: 0.01229136
[200/200] Training loss: 0.01145271
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9937.68061471086 ----------
[1/200] Training loss: 0.15830782
[2/200] Training loss: 0.05438096
[3/200] Training loss: 0.04918158
[4/200] Training loss: 0.04836783
[5/200] Training loss: 0.04438523
[6/200] Training loss: 0.04189787
[7/200] Training loss: 0.04006284
[8/200] Training loss: 0.03705375
[9/200] Training loss: 0.03582600
[10/200] Training loss: 0.03115881
[50/200] Training loss: 0.01750264
[100/200] Training loss: 0.01500354
[150/200] Training loss: 0.01432762
[200/200] Training loss: 0.01334938
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8813.03262220219 ----------
[1/200] Training loss: 0.16061990
[2/200] Training loss: 0.06260803
[3/200] Training loss: 0.05201983
[4/200] Training loss: 0.04674244
[5/200] Training loss: 0.04551037
[6/200] Training loss: 0.04087887
[7/200] Training loss: 0.03948641
[8/200] Training loss: 0.03745577
[9/200] Training loss: 0.03467069
[10/200] Training loss: 0.03376171
[50/200] Training loss: 0.01886226
[100/200] Training loss: 0.01713683
[150/200] Training loss: 0.01581632
[200/200] Training loss: 0.01529897
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9269.72189442596 ----------
[1/200] Training loss: 0.16410366
[2/200] Training loss: 0.05645743
[3/200] Training loss: 0.05444575
[4/200] Training loss: 0.05187228
[5/200] Training loss: 0.04692448
[6/200] Training loss: 0.04648050
[7/200] Training loss: 0.04468249
[8/200] Training loss: 0.03725219
[9/200] Training loss: 0.03326970
[10/200] Training loss: 0.03462099
[50/200] Training loss: 0.01869371
[100/200] Training loss: 0.01656173
[150/200] Training loss: 0.01537712
[200/200] Training loss: 0.01481114
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11534.198194933188 ----------
[1/200] Training loss: 0.16050022
[2/200] Training loss: 0.06204701
[3/200] Training loss: 0.05768145
[4/200] Training loss: 0.05356865
[5/200] Training loss: 0.04861926
[6/200] Training loss: 0.04777718
[7/200] Training loss: 0.04570372
[8/200] Training loss: 0.04493431
[9/200] Training loss: 0.03950369
[10/200] Training loss: 0.03883444
[50/200] Training loss: 0.01685842
[100/200] Training loss: 0.01444093
[150/200] Training loss: 0.01272616
[200/200] Training loss: 0.01201205
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 29056.900867091797 ----------
[1/200] Training loss: 0.15030661
[2/200] Training loss: 0.05706918
[3/200] Training loss: 0.05251740
[4/200] Training loss: 0.04893593
[5/200] Training loss: 0.04783705
[6/200] Training loss: 0.04663953
[7/200] Training loss: 0.04103764
[8/200] Training loss: 0.03962759
[9/200] Training loss: 0.03808105
[10/200] Training loss: 0.03563867
[50/200] Training loss: 0.01879525
[100/200] Training loss: 0.01632828
[150/200] Training loss: 0.01477591
[200/200] Training loss: 0.01383055
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6982.506999638454 ----------
[1/200] Training loss: 0.15516476
[2/200] Training loss: 0.05884261
[3/200] Training loss: 0.05375119
[4/200] Training loss: 0.05177343
[5/200] Training loss: 0.04939400
[6/200] Training loss: 0.04443531
[7/200] Training loss: 0.04411714
[8/200] Training loss: 0.04207036
[9/200] Training loss: 0.03802320
[10/200] Training loss: 0.03744423
[50/200] Training loss: 0.01939524
[100/200] Training loss: 0.01532308
[150/200] Training loss: 0.01349910
[200/200] Training loss: 0.01273487
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8568.63116256033 ----------
[1/200] Training loss: 0.16693060
[2/200] Training loss: 0.06340714
[3/200] Training loss: 0.05215609
[4/200] Training loss: 0.04768602
[5/200] Training loss: 0.04373000
[6/200] Training loss: 0.04303073
[7/200] Training loss: 0.04086198
[8/200] Training loss: 0.04032392
[9/200] Training loss: 0.04004393
[10/200] Training loss: 0.03772121
[50/200] Training loss: 0.01891862
[100/200] Training loss: 0.01684040
[150/200] Training loss: 0.01616560
[200/200] Training loss: 0.01447289
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6673.659565785477 ----------
[1/200] Training loss: 0.10813755
[2/200] Training loss: 0.05183537
[3/200] Training loss: 0.04551696
[4/200] Training loss: 0.04208001
[5/200] Training loss: 0.03619920
[6/200] Training loss: 0.03411622
[7/200] Training loss: 0.03007994
[8/200] Training loss: 0.02943268
[9/200] Training loss: 0.02672267
[10/200] Training loss: 0.02584622
[50/200] Training loss: 0.01526977
[100/200] Training loss: 0.01292240
[150/200] Training loss: 0.01163608
[200/200] Training loss: 0.00995552
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7674.320816854089 ----------
[1/200] Training loss: 0.13205691
[2/200] Training loss: 0.05533254
[3/200] Training loss: 0.04858834
[4/200] Training loss: 0.04131370
[5/200] Training loss: 0.04026430
[6/200] Training loss: 0.03633705
[7/200] Training loss: 0.03307651
[8/200] Training loss: 0.03147665
[9/200] Training loss: 0.03044588
[10/200] Training loss: 0.02962127
[50/200] Training loss: 0.01765976
[100/200] Training loss: 0.01535129
[150/200] Training loss: 0.01398373
[200/200] Training loss: 0.01276301
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16733.336308100665 ----------
[1/200] Training loss: 0.11731625
[2/200] Training loss: 0.05662288
[3/200] Training loss: 0.05040226
[4/200] Training loss: 0.04589774
[5/200] Training loss: 0.04313919
[6/200] Training loss: 0.04202863
[7/200] Training loss: 0.03820288
[8/200] Training loss: 0.03583183
[9/200] Training loss: 0.03232721
[10/200] Training loss: 0.02853137
[50/200] Training loss: 0.01787821
[100/200] Training loss: 0.01578866
[150/200] Training loss: 0.01369878
[200/200] Training loss: 0.01258495
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16720.08803804573 ----------
[1/200] Training loss: 0.15656648
[2/200] Training loss: 0.05202199
[3/200] Training loss: 0.04643044
[4/200] Training loss: 0.04449237
[5/200] Training loss: 0.04369762
[6/200] Training loss: 0.03968416
[7/200] Training loss: 0.03597622
[8/200] Training loss: 0.03456973
[9/200] Training loss: 0.03090941
[10/200] Training loss: 0.02888755
[50/200] Training loss: 0.01798785
[100/200] Training loss: 0.01488317
[150/200] Training loss: 0.01275137
[200/200] Training loss: 0.01299545
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 3891.1089678907733 ----------
[1/200] Training loss: 0.14293814
[2/200] Training loss: 0.05880290
[3/200] Training loss: 0.05015449
[4/200] Training loss: 0.04380609
[5/200] Training loss: 0.04637880
[6/200] Training loss: 0.04281911
[7/200] Training loss: 0.04105373
[8/200] Training loss: 0.03691830
[9/200] Training loss: 0.03514422
[10/200] Training loss: 0.03264411
[50/200] Training loss: 0.01775344
[100/200] Training loss: 0.01371759
[150/200] Training loss: 0.01309065
[200/200] Training loss: 0.01065464
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14510.916166803529 ----------
[1/200] Training loss: 0.17037276
[2/200] Training loss: 0.06301494
[3/200] Training loss: 0.05373307
[4/200] Training loss: 0.05207843
[5/200] Training loss: 0.04795887
[6/200] Training loss: 0.04252745
[7/200] Training loss: 0.04319353
[8/200] Training loss: 0.03990608
[9/200] Training loss: 0.03560594
[10/200] Training loss: 0.03774801
[50/200] Training loss: 0.01843645
[100/200] Training loss: 0.01461925
[150/200] Training loss: 0.01335202
[200/200] Training loss: 0.01225575
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14798.635072195002 ----------
[1/200] Training loss: 0.11738797
[2/200] Training loss: 0.05582436
[3/200] Training loss: 0.05044589
[4/200] Training loss: 0.04560740
[5/200] Training loss: 0.04098680
[6/200] Training loss: 0.03676596
[7/200] Training loss: 0.03367723
[8/200] Training loss: 0.02962729
[9/200] Training loss: 0.02697506
[10/200] Training loss: 0.02561844
[50/200] Training loss: 0.01558616
[100/200] Training loss: 0.01303734
[150/200] Training loss: 0.01172870
[200/200] Training loss: 0.01077056
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9664.555861497205 ----------
[1/200] Training loss: 0.15703021
[2/200] Training loss: 0.05634493
[3/200] Training loss: 0.04987559
[4/200] Training loss: 0.04745078
[5/200] Training loss: 0.04477342
[6/200] Training loss: 0.04004998
[7/200] Training loss: 0.03759750
[8/200] Training loss: 0.03562940
[9/200] Training loss: 0.03465409
[10/200] Training loss: 0.03321012
[50/200] Training loss: 0.01968303
[100/200] Training loss: 0.01726685
[150/200] Training loss: 0.01525433
[200/200] Training loss: 0.01466041
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10715.625600029147 ----------
[1/200] Training loss: 0.16257139
[2/200] Training loss: 0.06541146
[3/200] Training loss: 0.05376708
[4/200] Training loss: 0.05011268
[5/200] Training loss: 0.04539766
[6/200] Training loss: 0.04288768
[7/200] Training loss: 0.04175751
[8/200] Training loss: 0.03819764
[9/200] Training loss: 0.03533632
[10/200] Training loss: 0.03436849
[50/200] Training loss: 0.01799139
[100/200] Training loss: 0.01417351
[150/200] Training loss: 0.01286875
[200/200] Training loss: 0.01151172
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12054.688050712884 ----------
[1/200] Training loss: 0.14254134
[2/200] Training loss: 0.06064049
[3/200] Training loss: 0.05355007
[4/200] Training loss: 0.05191774
[5/200] Training loss: 0.05001175
[6/200] Training loss: 0.04585794
[7/200] Training loss: 0.04496596
[8/200] Training loss: 0.04194579
[9/200] Training loss: 0.04044821
[10/200] Training loss: 0.03689289
[50/200] Training loss: 0.01886834
[100/200] Training loss: 0.01644843
[150/200] Training loss: 0.01520466
[200/200] Training loss: 0.01391474
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9286.535198877997 ----------
[1/200] Training loss: 0.15849950
[2/200] Training loss: 0.06241839
[3/200] Training loss: 0.05098415
[4/200] Training loss: 0.04721740
[5/200] Training loss: 0.04615263
[6/200] Training loss: 0.04127771
[7/200] Training loss: 0.04275362
[8/200] Training loss: 0.03893948
[9/200] Training loss: 0.03625187
[10/200] Training loss: 0.03298163
[50/200] Training loss: 0.01803776
[100/200] Training loss: 0.01490954
[150/200] Training loss: 0.01281809
[200/200] Training loss: 0.01214389
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13076.502896416916 ----------
[1/200] Training loss: 0.13720944
[2/200] Training loss: 0.05544410
[3/200] Training loss: 0.04879527
[4/200] Training loss: 0.04634524
[5/200] Training loss: 0.04222501
[6/200] Training loss: 0.04026627
[7/200] Training loss: 0.04013855
[8/200] Training loss: 0.03509361
[9/200] Training loss: 0.03637912
[10/200] Training loss: 0.03454257
[50/200] Training loss: 0.01702587
[100/200] Training loss: 0.01363691
[150/200] Training loss: 0.01312836
[200/200] Training loss: 0.01173835
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5851.14381980139 ----------
[1/200] Training loss: 0.17951906
[2/200] Training loss: 0.06101249
[3/200] Training loss: 0.05427725
[4/200] Training loss: 0.04936618
[5/200] Training loss: 0.04349039
[6/200] Training loss: 0.04084223
[7/200] Training loss: 0.03844607
[8/200] Training loss: 0.03704286
[9/200] Training loss: 0.03531077
[10/200] Training loss: 0.03265337
[50/200] Training loss: 0.01817066
[100/200] Training loss: 0.01586553
[150/200] Training loss: 0.01468035
[200/200] Training loss: 0.01350661
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8601.697506887813 ----------
[1/200] Training loss: 0.15485471
[2/200] Training loss: 0.06161437
[3/200] Training loss: 0.05378575
[4/200] Training loss: 0.05099033
[5/200] Training loss: 0.04909312
[6/200] Training loss: 0.04612094
[7/200] Training loss: 0.04584566
[8/200] Training loss: 0.04394027
[9/200] Training loss: 0.04189481
[10/200] Training loss: 0.03902779
[50/200] Training loss: 0.01808327
[100/200] Training loss: 0.01614223
[150/200] Training loss: 0.01348625
[200/200] Training loss: 0.01299645
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13764.440562551026 ----------
[1/200] Training loss: 0.17433860
[2/200] Training loss: 0.06305381
[3/200] Training loss: 0.05710529
[4/200] Training loss: 0.05152191
[5/200] Training loss: 0.04896321
[6/200] Training loss: 0.04513670
[7/200] Training loss: 0.04378378
[8/200] Training loss: 0.04191680
[9/200] Training loss: 0.03716652
[10/200] Training loss: 0.03451097
[50/200] Training loss: 0.01808566
[100/200] Training loss: 0.01640373
[150/200] Training loss: 0.01480080
[200/200] Training loss: 0.01344388
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15630.209211651647 ----------
[1/200] Training loss: 0.16381142
[2/200] Training loss: 0.06352857
[3/200] Training loss: 0.05448441
[4/200] Training loss: 0.05109827
[5/200] Training loss: 0.04791603
[6/200] Training loss: 0.04652727
[7/200] Training loss: 0.04371180
[8/200] Training loss: 0.03976704
[9/200] Training loss: 0.04087900
[10/200] Training loss: 0.03645693
[50/200] Training loss: 0.01830223
[100/200] Training loss: 0.01678336
[150/200] Training loss: 0.01509892
[200/200] Training loss: 0.01371759
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10913.524820148621 ----------
[1/200] Training loss: 0.15778421
[2/200] Training loss: 0.05578298
[3/200] Training loss: 0.05074659
[4/200] Training loss: 0.04324437
[5/200] Training loss: 0.04216111
[6/200] Training loss: 0.03738613
[7/200] Training loss: 0.03498650
[8/200] Training loss: 0.03433668
[9/200] Training loss: 0.03138211
[10/200] Training loss: 0.02878234
[50/200] Training loss: 0.01927771
[100/200] Training loss: 0.01704849
[150/200] Training loss: 0.01562656
[200/200] Training loss: 0.01465571
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15122.488684075779 ----------
[1/200] Training loss: 0.13334012
[2/200] Training loss: 0.03960057
[3/200] Training loss: 0.03684857
[4/200] Training loss: 0.03659602
[5/200] Training loss: 0.03597874
[6/200] Training loss: 0.03589115
[7/200] Training loss: 0.03569893
[8/200] Training loss: 0.03571969
[9/200] Training loss: 0.03513345
[10/200] Training loss: 0.03463402
[50/200] Training loss: 0.03259070
[100/200] Training loss: 0.03213788
[150/200] Training loss: 0.03160672
[200/200] Training loss: 0.03091323
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Rprop ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 27617.082249940886 ----------
[1/200] Training loss: 0.10758251
[2/200] Training loss: 0.05406044
[3/200] Training loss: 0.04506730
[4/200] Training loss: 0.03976098
[5/200] Training loss: 0.03362564
[6/200] Training loss: 0.03199753
[7/200] Training loss: 0.02959630
[8/200] Training loss: 0.02727424
[9/200] Training loss: 0.02555968
[10/200] Training loss: 0.02402046
[50/200] Training loss: 0.01849524
[100/200] Training loss: 0.01564964
[150/200] Training loss: 0.01299663
[200/200] Training loss: 0.01170190
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18430.4443787989 ----------
[1/200] Training loss: 0.14340333
[2/200] Training loss: 0.05683432
[3/200] Training loss: 0.05426953
[4/200] Training loss: 0.04949954
[5/200] Training loss: 0.04812036
[6/200] Training loss: 0.04850469
[7/200] Training loss: 0.04239356
[8/200] Training loss: 0.04198711
[9/200] Training loss: 0.03893970
[10/200] Training loss: 0.03973297
[50/200] Training loss: 0.01839624
[100/200] Training loss: 0.01534824
[150/200] Training loss: 0.01403982
[200/200] Training loss: 0.01272388
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10648.662639035947 ----------
[1/200] Training loss: 0.15830000
[2/200] Training loss: 0.05421095
[3/200] Training loss: 0.05413247
[4/200] Training loss: 0.04862465
[5/200] Training loss: 0.04455534
[6/200] Training loss: 0.04058147
[7/200] Training loss: 0.04245165
[8/200] Training loss: 0.04068587
[9/200] Training loss: 0.03650818
[10/200] Training loss: 0.03659372
[50/200] Training loss: 0.01810166
[100/200] Training loss: 0.01579998
[150/200] Training loss: 0.01380855
[200/200] Training loss: 0.01295202
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18464.118717122677 ----------
[1/200] Training loss: 0.18627935
[2/200] Training loss: 0.06137889
[3/200] Training loss: 0.05407701
[4/200] Training loss: 0.05064167
[5/200] Training loss: 0.04890207
[6/200] Training loss: 0.05009947
[7/200] Training loss: 0.04610268
[8/200] Training loss: 0.04481476
[9/200] Training loss: 0.04421977
[10/200] Training loss: 0.04159840
[50/200] Training loss: 0.02071779
[100/200] Training loss: 0.01682200
[150/200] Training loss: 0.01411479
[200/200] Training loss: 0.01258846
---batch_size---: 16 ---n_steps---: 2 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9238.774810547122 ----------
[1/200] Training loss: 0.11370813
[2/200] Training loss: 0.05671904
[3/200] Training loss: 0.05024364
[4/200] Training loss: 0.04551429
[5/200] Training loss: 0.04295412
[6/200] Training loss: 0.03778718
[7/200] Training loss: 0.03415094
[8/200] Training loss: 0.03307627
[9/200] Training loss: 0.03175060
[10/200] Training loss: 0.02878444
[50/200] Training loss: 0.01731801
[100/200] Training loss: 0.01548572
[150/200] Training loss: 0.01434929
[200/200] Training loss: 0.01266840
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19524.36098826284 ----------
[1/200] Training loss: 0.16677177
[2/200] Training loss: 0.06316544
[3/200] Training loss: 0.05607740
[4/200] Training loss: 0.05205782
[5/200] Training loss: 0.05035676
[6/200] Training loss: 0.04675256
[7/200] Training loss: 0.04519340
[8/200] Training loss: 0.04529839
[9/200] Training loss: 0.04252914
[10/200] Training loss: 0.03947328
[50/200] Training loss: 0.02069082
[100/200] Training loss: 0.01620792
[150/200] Training loss: 0.01417255
[200/200] Training loss: 0.01345278
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6995.857917367962 ----------
[1/200] Training loss: 0.16833351
[2/200] Training loss: 0.06156682
[3/200] Training loss: 0.05456993
[4/200] Training loss: 0.05017730
[5/200] Training loss: 0.04737020
[6/200] Training loss: 0.04271062
[7/200] Training loss: 0.04259034
[8/200] Training loss: 0.04018286
[9/200] Training loss: 0.03711746
[10/200] Training loss: 0.03711950
[50/200] Training loss: 0.01743308
[100/200] Training loss: 0.01477297
[150/200] Training loss: 0.01456280
[200/200] Training loss: 0.01324997
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5024.118032052989 ----------
[1/200] Training loss: 0.15024877
[2/200] Training loss: 0.05246915
[3/200] Training loss: 0.04697286
[4/200] Training loss: 0.04207422
[5/200] Training loss: 0.03681503
[6/200] Training loss: 0.03473001
[7/200] Training loss: 0.03447309
[8/200] Training loss: 0.03112156
[9/200] Training loss: 0.03003893
[10/200] Training loss: 0.02838014
[50/200] Training loss: 0.01780836
[100/200] Training loss: 0.01560355
[150/200] Training loss: 0.01463394
[200/200] Training loss: 0.01379239
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.2641941659869305 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9711.228140662744 ----------
[1/200] Training loss: 0.19003281
[2/200] Training loss: 0.06110398
[3/200] Training loss: 0.05653973
[4/200] Training loss: 0.05078206
[5/200] Training loss: 0.05134065
[6/200] Training loss: 0.04784338
[7/200] Training loss: 0.04352214
[8/200] Training loss: 0.04275474
[9/200] Training loss: 0.04157588
[10/200] Training loss: 0.03860461
[50/200] Training loss: 0.02284340
[100/200] Training loss: 0.01701345
[150/200] Training loss: 0.01444788
[200/200] Training loss: 0.01355367
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12326.297091989954 ----------
[1/200] Training loss: 0.14254647
[2/200] Training loss: 0.05713786
[3/200] Training loss: 0.05061290
[4/200] Training loss: 0.05059830
[5/200] Training loss: 0.04527626
[6/200] Training loss: 0.04569959
[7/200] Training loss: 0.04290486
[8/200] Training loss: 0.04152396
[9/200] Training loss: 0.04136651
[10/200] Training loss: 0.03940122
[50/200] Training loss: 0.01853905
[100/200] Training loss: 0.01476697
[150/200] Training loss: 0.01360174
[200/200] Training loss: 0.01329023
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9968.213882135555 ----------
[1/200] Training loss: 0.17010463
[2/200] Training loss: 0.05998166
[3/200] Training loss: 0.05184789
[4/200] Training loss: 0.04999959
[5/200] Training loss: 0.04820377
[6/200] Training loss: 0.04606531
[7/200] Training loss: 0.04598281
[8/200] Training loss: 0.04117490
[9/200] Training loss: 0.03793467
[10/200] Training loss: 0.03682232
[50/200] Training loss: 0.01923640
[100/200] Training loss: 0.01620786
[150/200] Training loss: 0.01416735
[200/200] Training loss: 0.01318256
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6785.744763841328 ----------
[1/200] Training loss: 0.10966049
[2/200] Training loss: 0.05604883
[3/200] Training loss: 0.05171954
[4/200] Training loss: 0.04818737
[5/200] Training loss: 0.04351634
[6/200] Training loss: 0.03860818
[7/200] Training loss: 0.03460988
[8/200] Training loss: 0.03235465
[9/200] Training loss: 0.02927790
[10/200] Training loss: 0.02762949
[50/200] Training loss: 0.01747446
[100/200] Training loss: 0.01556513
[150/200] Training loss: 0.01380395
[200/200] Training loss: 0.01324295
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 26833.488330815282 ----------
[1/200] Training loss: 0.07943730
[2/200] Training loss: 0.04527679
[3/200] Training loss: 0.03322747
[4/200] Training loss: 0.02978931
[5/200] Training loss: 0.02816943
[6/200] Training loss: 0.02444651
[7/200] Training loss: 0.02423741
[8/200] Training loss: 0.02342084
[9/200] Training loss: 0.02290615
[10/200] Training loss: 0.02249588
[50/200] Training loss: 0.01655627
[100/200] Training loss: 0.01384268
[150/200] Training loss: 0.01189589
[200/200] Training loss: 0.01046053
---batch_size---: 4 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15784.080080891632 ----------
[1/200] Training loss: 0.16100690
[2/200] Training loss: 0.06351895
[3/200] Training loss: 0.05640712
[4/200] Training loss: 0.05197624
[5/200] Training loss: 0.05159190
[6/200] Training loss: 0.04888714
[7/200] Training loss: 0.04719706
[8/200] Training loss: 0.04658398
[9/200] Training loss: 0.04488402
[10/200] Training loss: 0.04332148
[50/200] Training loss: 0.01870763
[100/200] Training loss: 0.01727367
[150/200] Training loss: 0.01569496
[200/200] Training loss: 0.01466784
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14745.277752555223 ----------
[1/200] Training loss: 0.13393779
[2/200] Training loss: 0.05250946
[3/200] Training loss: 0.04805715
[4/200] Training loss: 0.04290408
[5/200] Training loss: 0.04229228
[6/200] Training loss: 0.03940584
[7/200] Training loss: 0.03728630
[8/200] Training loss: 0.03570475
[9/200] Training loss: 0.03512789
[10/200] Training loss: 0.03507700
[50/200] Training loss: 0.01680844
[100/200] Training loss: 0.01381914
[150/200] Training loss: 0.01313031
[200/200] Training loss: 0.01146741
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8447.152419602715 ----------
[1/200] Training loss: 0.16877088
[2/200] Training loss: 0.06022708
[3/200] Training loss: 0.05399552
[4/200] Training loss: 0.05005994
[5/200] Training loss: 0.04846643
[6/200] Training loss: 0.04639312
[7/200] Training loss: 0.04457203
[8/200] Training loss: 0.04231475
[9/200] Training loss: 0.03783194
[10/200] Training loss: 0.03644419
[50/200] Training loss: 0.01926923
[100/200] Training loss: 0.01622444
[150/200] Training loss: 0.01441847
[200/200] Training loss: 0.01376548
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7629.962516290627 ----------
[1/200] Training loss: 0.11895484
[2/200] Training loss: 0.05496249
[3/200] Training loss: 0.04999858
[4/200] Training loss: 0.04608237
[5/200] Training loss: 0.04166684
[6/200] Training loss: 0.03832126
[7/200] Training loss: 0.03283405
[8/200] Training loss: 0.03273737
[9/200] Training loss: 0.02845429
[10/200] Training loss: 0.02683323
[50/200] Training loss: 0.01675432
[100/200] Training loss: 0.01477543
[150/200] Training loss: 0.01289057
[200/200] Training loss: 0.01211464
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9508.759750882342 ----------
[1/200] Training loss: 0.16559218
[2/200] Training loss: 0.05907225
[3/200] Training loss: 0.05421685
[4/200] Training loss: 0.05187706
[5/200] Training loss: 0.04786401
[6/200] Training loss: 0.04489768
[7/200] Training loss: 0.04275555
[8/200] Training loss: 0.04035107
[9/200] Training loss: 0.03773458
[10/200] Training loss: 0.03659609
[50/200] Training loss: 0.01767202
[100/200] Training loss: 0.01587647
[150/200] Training loss: 0.01396704
[200/200] Training loss: 0.01292390
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 26649.390537121108 ----------
[1/200] Training loss: 0.14981384
[2/200] Training loss: 0.05882240
[3/200] Training loss: 0.05248054
[4/200] Training loss: 0.05129912
[5/200] Training loss: 0.04778925
[6/200] Training loss: 0.04707023
[7/200] Training loss: 0.04484385
[8/200] Training loss: 0.03914813
[9/200] Training loss: 0.03630246
[10/200] Training loss: 0.03514446
[50/200] Training loss: 0.01793785
[100/200] Training loss: 0.01482771
[150/200] Training loss: 0.01289437
[200/200] Training loss: 0.01220962
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 21794.207670846856 ----------
[1/200] Training loss: 0.15711571
[2/200] Training loss: 0.05671707
[3/200] Training loss: 0.05106087
[4/200] Training loss: 0.05041325
[5/200] Training loss: 0.04490648
[6/200] Training loss: 0.04441445
[7/200] Training loss: 0.04483462
[8/200] Training loss: 0.04032950
[9/200] Training loss: 0.04023725
[10/200] Training loss: 0.03754822
[50/200] Training loss: 0.01779897
[100/200] Training loss: 0.01523124
[150/200] Training loss: 0.01372128
[200/200] Training loss: 0.01244462
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12503.369785781751 ----------
[1/200] Training loss: 0.16811717
[2/200] Training loss: 0.06064584
[3/200] Training loss: 0.05096233
[4/200] Training loss: 0.04809999
[5/200] Training loss: 0.04298091
[6/200] Training loss: 0.04026710
[7/200] Training loss: 0.03820643
[8/200] Training loss: 0.03657059
[9/200] Training loss: 0.03272683
[10/200] Training loss: 0.03173436
[50/200] Training loss: 0.01708115
[100/200] Training loss: 0.01368027
[150/200] Training loss: 0.01320688
[200/200] Training loss: 0.01182791
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14608.566801709194 ----------
[1/200] Training loss: 0.18929677
[2/200] Training loss: 0.05445853
[3/200] Training loss: 0.05603301
[4/200] Training loss: 0.04736172
[5/200] Training loss: 0.04532305
[6/200] Training loss: 0.04194686
[7/200] Training loss: 0.03905733
[8/200] Training loss: 0.03888479
[9/200] Training loss: 0.03415483
[10/200] Training loss: 0.03302608
[50/200] Training loss: 0.01934628
[100/200] Training loss: 0.01555549
[150/200] Training loss: 0.01451083
[200/200] Training loss: 0.01365737
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10208.029780520823 ----------
[1/200] Training loss: 0.14496506
[2/200] Training loss: 0.05171321
[3/200] Training loss: 0.04499535
[4/200] Training loss: 0.04451027
[5/200] Training loss: 0.04260138
[6/200] Training loss: 0.03886918
[7/200] Training loss: 0.03581635
[8/200] Training loss: 0.03517593
[9/200] Training loss: 0.03302866
[10/200] Training loss: 0.03306122
[50/200] Training loss: 0.01672516
[100/200] Training loss: 0.01440034
[150/200] Training loss: 0.01390138
[200/200] Training loss: 0.01259644
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6249.213710539911 ----------
[1/50] Training loss: 0.12193733
[2/50] Training loss: 0.05421469
[3/50] Training loss: 0.04974346
[4/50] Training loss: 0.04430495
[5/50] Training loss: 0.03907239
[6/50] Training loss: 0.03415468
[7/50] Training loss: 0.03430982
[8/50] Training loss: 0.03009774
[9/50] Training loss: 0.02860346
[10/50] Training loss: 0.02839051
[50/50] Training loss: 0.01584705
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 50
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20314.60715839713 ----------
[1/200] Training loss: 0.14519256
[2/200] Training loss: 0.05942097
[3/200] Training loss: 0.04905997
[4/200] Training loss: 0.04666691
[5/200] Training loss: 0.04079680
[6/200] Training loss: 0.04120664
[7/200] Training loss: 0.03945245
[8/200] Training loss: 0.03559589
[9/200] Training loss: 0.03355575
[10/200] Training loss: 0.03218845
[50/200] Training loss: 0.01757353
[100/200] Training loss: 0.01507987
[150/200] Training loss: 0.01385064
[200/200] Training loss: 0.01262742
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 23179.107834427105 ----------
[1/200] Training loss: 0.11811251
[2/200] Training loss: 0.04944076
[3/200] Training loss: 0.03903359
[4/200] Training loss: 0.03728895
[5/200] Training loss: 0.03319108
[6/200] Training loss: 0.03181696
[7/200] Training loss: 0.03017262
[8/200] Training loss: 0.02818293
[9/200] Training loss: 0.02740440
[10/200] Training loss: 0.02573217
[50/200] Training loss: 0.01629470
[100/200] Training loss: 0.01431544
[150/200] Training loss: 0.01320536
[200/200] Training loss: 0.01259948
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11715.855239802171 ----------
[1/200] Training loss: 0.15072289
[2/200] Training loss: 0.06205340
[3/200] Training loss: 0.05192130
[4/200] Training loss: 0.04855103
[5/200] Training loss: 0.04768903
[6/200] Training loss: 0.04327263
[7/200] Training loss: 0.03999786
[8/200] Training loss: 0.03988535
[9/200] Training loss: 0.03824114
[10/200] Training loss: 0.03550781
[50/200] Training loss: 0.01776474
[100/200] Training loss: 0.01552809
[150/200] Training loss: 0.01371804
[200/200] Training loss: 0.01281520
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6102.190754147235 ----------
[1/200] Training loss: 0.15460345
[2/200] Training loss: 0.05329933
[3/200] Training loss: 0.04880439
[4/200] Training loss: 0.04639066
[5/200] Training loss: 0.04106088
[6/200] Training loss: 0.04020041
[7/200] Training loss: 0.03758914
[8/200] Training loss: 0.03369442
[9/200] Training loss: 0.03714152
[10/200] Training loss: 0.03220056
[50/200] Training loss: 0.01937063
[100/200] Training loss: 0.01718991
[150/200] Training loss: 0.01544137
[200/200] Training loss: 0.01485344
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11785.627857691758 ----------
[1/200] Training loss: 0.17764497
[2/200] Training loss: 0.06289736
[3/200] Training loss: 0.05391314
[4/200] Training loss: 0.04887171
[5/200] Training loss: 0.04701714
[6/200] Training loss: 0.04744775
[7/200] Training loss: 0.04394972
[8/200] Training loss: 0.04496743
[9/200] Training loss: 0.04193225
[10/200] Training loss: 0.04138622
[50/200] Training loss: 0.01911419
[100/200] Training loss: 0.01556486
[150/200] Training loss: 0.01379990
[200/200] Training loss: 0.01278443
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7813.087225930605 ----------
[1/200] Training loss: 0.17692566
[2/200] Training loss: 0.06043589
[3/200] Training loss: 0.05742716
[4/200] Training loss: 0.05475822
[5/200] Training loss: 0.04870839
[6/200] Training loss: 0.04694846
[7/200] Training loss: 0.04572993
[8/200] Training loss: 0.04546414
[9/200] Training loss: 0.04421441
[10/200] Training loss: 0.03995048
[50/200] Training loss: 0.01887839
[100/200] Training loss: 0.01528558
[150/200] Training loss: 0.01284919
[200/200] Training loss: 0.01239341
---batch_size---: 16 ---n_steps---: 2 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6549.978625919324 ----------
[1/200] Training loss: 0.14517540
[2/200] Training loss: 0.05944684
[3/200] Training loss: 0.05373920
[4/200] Training loss: 0.05053530
[5/200] Training loss: 0.04798569
[6/200] Training loss: 0.04628187
[7/200] Training loss: 0.04530564
[8/200] Training loss: 0.04290969
[9/200] Training loss: 0.04151137
[10/200] Training loss: 0.04014316
[50/200] Training loss: 0.01967223
[100/200] Training loss: 0.01754707
[150/200] Training loss: 0.01589083
[200/200] Training loss: 0.01419007
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 28214.172892360322 ----------
[1/200] Training loss: 0.15868011
[2/200] Training loss: 0.05980915
[3/200] Training loss: 0.05455575
[4/200] Training loss: 0.05254236
[5/200] Training loss: 0.04858431
[6/200] Training loss: 0.04718174
[7/200] Training loss: 0.04372483
[8/200] Training loss: 0.04421077
[9/200] Training loss: 0.04165361
[10/200] Training loss: 0.03887942
[50/200] Training loss: 0.01682342
[100/200] Training loss: 0.01489923
[150/200] Training loss: 0.01398569
[200/200] Training loss: 0.01355344
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 18377.325159010492 ----------
[1/200] Training loss: 0.16307517
[2/200] Training loss: 0.06076430
[3/200] Training loss: 0.05451144
[4/200] Training loss: 0.04891416
[5/200] Training loss: 0.04270331
[6/200] Training loss: 0.04084624
[7/200] Training loss: 0.03894104
[8/200] Training loss: 0.03679211
[9/200] Training loss: 0.03464474
[10/200] Training loss: 0.03281235
[50/200] Training loss: 0.01922712
[100/200] Training loss: 0.01626621
[150/200] Training loss: 0.01508232
[200/200] Training loss: 0.01432070
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15855.845609742799 ----------
[1/200] Training loss: 0.15304725
[2/200] Training loss: 0.05854013
[3/200] Training loss: 0.05400266
[4/200] Training loss: 0.04871136
[5/200] Training loss: 0.04653296
[6/200] Training loss: 0.04466210
[7/200] Training loss: 0.04300609
[8/200] Training loss: 0.03810629
[9/200] Training loss: 0.03838594
[10/200] Training loss: 0.03616013
[50/200] Training loss: 0.01851240
[100/200] Training loss: 0.01572868
[150/200] Training loss: 0.01473538
[200/200] Training loss: 0.01300752
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4313.674072064323 ----------
[1/200] Training loss: 0.14569686
[2/200] Training loss: 0.05817227
[3/200] Training loss: 0.05459288
[4/200] Training loss: 0.05234366
[5/200] Training loss: 0.05228304
[6/200] Training loss: 0.05018511
[7/200] Training loss: 0.04716873
[8/200] Training loss: 0.04633686
[9/200] Training loss: 0.04449112
[10/200] Training loss: 0.04111394
[50/200] Training loss: 0.01792332
[100/200] Training loss: 0.01392225
[150/200] Training loss: 0.01288802
[200/200] Training loss: 0.01174280
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12575.069940163356 ----------
[1/200] Training loss: 0.17222886
[2/200] Training loss: 0.06041700
[3/200] Training loss: 0.05475482
[4/200] Training loss: 0.05225901
[5/200] Training loss: 0.04558618
[6/200] Training loss: 0.04526191
[7/200] Training loss: 0.04127556
[8/200] Training loss: 0.03989048
[9/200] Training loss: 0.03802290
[10/200] Training loss: 0.03391586
[50/200] Training loss: 0.01809849
[100/200] Training loss: 0.01525253
[150/200] Training loss: 0.01429500
[200/200] Training loss: 0.01249752
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8995.794573021329 ----------
[1/200] Training loss: 0.18452140
[2/200] Training loss: 0.06727918
[3/200] Training loss: 0.05749964
[4/200] Training loss: 0.05229193
[5/200] Training loss: 0.05225409
[6/200] Training loss: 0.04722930
[7/200] Training loss: 0.04784499
[8/200] Training loss: 0.04508530
[9/200] Training loss: 0.04286840
[10/200] Training loss: 0.04101579
[50/200] Training loss: 0.01937070
[100/200] Training loss: 0.01718030
[150/200] Training loss: 0.01533418
[200/200] Training loss: 0.01519874
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15677.79066067665 ----------
[1/200] Training loss: 0.16931260
[2/200] Training loss: 0.06137058
[3/200] Training loss: 0.05125787
[4/200] Training loss: 0.04698607
[5/200] Training loss: 0.04426552
[6/200] Training loss: 0.04110250
[7/200] Training loss: 0.03990563
[8/200] Training loss: 0.03472872
[9/200] Training loss: 0.03521155
[10/200] Training loss: 0.03240967
[50/200] Training loss: 0.01909554
[100/200] Training loss: 0.01568889
[150/200] Training loss: 0.01426812
[200/200] Training loss: 0.01353698
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6618.275304035033 ----------
[1/200] Training loss: 0.09671792
[2/200] Training loss: 0.05432759
[3/200] Training loss: 0.04843309
[4/200] Training loss: 0.04551354
[5/200] Training loss: 0.04117771
[6/200] Training loss: 0.03683837
[7/200] Training loss: 0.03497622
[8/200] Training loss: 0.03111366
[9/200] Training loss: 0.03012803
[10/200] Training loss: 0.02926799
[50/200] Training loss: 0.01634307
[100/200] Training loss: 0.01262759
[150/200] Training loss: 0.01186971
[200/200] Training loss: 0.01057693
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6306.501407278048 ----------
[1/200] Training loss: 0.15738542
[2/200] Training loss: 0.05845812
[3/200] Training loss: 0.05454884
[4/200] Training loss: 0.04951742
[5/200] Training loss: 0.04919370
[6/200] Training loss: 0.04796711
[7/200] Training loss: 0.04201848
[8/200] Training loss: 0.04103685
[9/200] Training loss: 0.03463217
[10/200] Training loss: 0.03435342
[50/200] Training loss: 0.01866302
[100/200] Training loss: 0.01467264
[150/200] Training loss: 0.01322732
[200/200] Training loss: 0.01191251
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23839.855033116288 ----------
[1/200] Training loss: 0.16384425
[2/200] Training loss: 0.05744855
[3/200] Training loss: 0.05360113
[4/200] Training loss: 0.04346815
[5/200] Training loss: 0.04046271
[6/200] Training loss: 0.03653122
[7/200] Training loss: 0.03458087
[8/200] Training loss: 0.03353173
[9/200] Training loss: 0.03047036
[10/200] Training loss: 0.03154991
[50/200] Training loss: 0.01694651
[100/200] Training loss: 0.01502372
[150/200] Training loss: 0.01373916
[200/200] Training loss: 0.01261836
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19564.461658834367 ----------
[1/200] Training loss: 0.10330290
[2/200] Training loss: 0.04937938
[3/200] Training loss: 0.04197801
[4/200] Training loss: 0.03779382
[5/200] Training loss: 0.03404840
[6/200] Training loss: 0.03142326
[7/200] Training loss: 0.02791467
[8/200] Training loss: 0.02820941
[9/200] Training loss: 0.02592798
[10/200] Training loss: 0.02541190
[50/200] Training loss: 0.01667556
[100/200] Training loss: 0.01354748
[150/200] Training loss: 0.01202560
[200/200] Training loss: 0.01132630
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7502.726970908644 ----------
[1/200] Training loss: 0.14660111
[2/200] Training loss: 0.05741278
[3/200] Training loss: 0.04952929
[4/200] Training loss: 0.04915895
[5/200] Training loss: 0.04581372
[6/200] Training loss: 0.04361060
[7/200] Training loss: 0.04120528
[8/200] Training loss: 0.03964432
[9/200] Training loss: 0.03955624
[10/200] Training loss: 0.03647418
[50/200] Training loss: 0.01896917
[100/200] Training loss: 0.01547646
[150/200] Training loss: 0.01408596
[200/200] Training loss: 0.01305525
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4224.314618964832 ----------
[1/200] Training loss: 0.09871727
[2/200] Training loss: 0.04966002
[3/200] Training loss: 0.04296991
[4/200] Training loss: 0.03925587
[5/200] Training loss: 0.03576192
[6/200] Training loss: 0.03251507
[7/200] Training loss: 0.03134194
[8/200] Training loss: 0.03006207
[9/200] Training loss: 0.02799493
[10/200] Training loss: 0.02584736
[50/200] Training loss: 0.01469334
[100/200] Training loss: 0.01234703
[150/200] Training loss: 0.01163750
[200/200] Training loss: 0.01108978
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 3806.951273657177 ----------
[1/200] Training loss: 0.15368681
[2/200] Training loss: 0.05894131
[3/200] Training loss: 0.05166897
[4/200] Training loss: 0.04871923
[5/200] Training loss: 0.04458971
[6/200] Training loss: 0.04270243
[7/200] Training loss: 0.04098796
[8/200] Training loss: 0.04012900
[9/200] Training loss: 0.03806058
[10/200] Training loss: 0.03666716
[50/200] Training loss: 0.01760540
[100/200] Training loss: 0.01423734
[150/200] Training loss: 0.01353953
[200/200] Training loss: 0.01236768
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15382.018073061805 ----------
[1/200] Training loss: 0.17925454
[2/200] Training loss: 0.06850742
[3/200] Training loss: 0.05906909
[4/200] Training loss: 0.05366285
[5/200] Training loss: 0.05198099
[6/200] Training loss: 0.05075333
[7/200] Training loss: 0.04856936
[8/200] Training loss: 0.04671433
[9/200] Training loss: 0.04527714
[10/200] Training loss: 0.04183787
[50/200] Training loss: 0.01758567
[100/200] Training loss: 0.01556194
[150/200] Training loss: 0.01477563
[200/200] Training loss: 0.01383007
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6484.191545597647 ----------
[1/200] Training loss: 0.17927740
[2/200] Training loss: 0.06183972
[3/200] Training loss: 0.05655871
[4/200] Training loss: 0.05048425
[5/200] Training loss: 0.05071100
[6/200] Training loss: 0.04628105
[7/200] Training loss: 0.04474205
[8/200] Training loss: 0.04361321
[9/200] Training loss: 0.04046364
[10/200] Training loss: 0.04000960
[50/200] Training loss: 0.01827220
[100/200] Training loss: 0.01444860
[150/200] Training loss: 0.01383606
[200/200] Training loss: 0.01334834
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13584.396931774336 ----------
[1/200] Training loss: 0.13938228
[2/200] Training loss: 0.06073060
[3/200] Training loss: 0.05564252
[4/200] Training loss: 0.05511500
[5/200] Training loss: 0.05271397
[6/200] Training loss: 0.05094989
[7/200] Training loss: 0.04854989
[8/200] Training loss: 0.04770146
[9/200] Training loss: 0.04635190
[10/200] Training loss: 0.04400606
[50/200] Training loss: 0.01778803
[100/200] Training loss: 0.01505712
[150/200] Training loss: 0.01383399
[200/200] Training loss: 0.01272498
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 19658.922045727737 ----------
[1/200] Training loss: 0.11894951
[2/200] Training loss: 0.05409917
[3/200] Training loss: 0.04612907
[4/200] Training loss: 0.04302114
[5/200] Training loss: 0.03830193
[6/200] Training loss: 0.03561926
[7/200] Training loss: 0.03192685
[8/200] Training loss: 0.02626130
[9/200] Training loss: 0.02526268
[10/200] Training loss: 0.02315504
[50/200] Training loss: 0.01717614
[100/200] Training loss: 0.01508417
[150/200] Training loss: 0.01313929
[200/200] Training loss: 0.01193262
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15856.033804202109 ----------
[1/200] Training loss: 0.17282887
[2/200] Training loss: 0.06463742
[3/200] Training loss: 0.06020473
[4/200] Training loss: 0.05674728
[5/200] Training loss: 0.05454933
[6/200] Training loss: 0.05219346
[7/200] Training loss: 0.04945416
[8/200] Training loss: 0.04931098
[9/200] Training loss: 0.04760072
[10/200] Training loss: 0.04474145
[50/200] Training loss: 0.01978456
[100/200] Training loss: 0.01732798
[150/200] Training loss: 0.01605322
[200/200] Training loss: 0.01481704
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15911.225219950851 ----------
[1/200] Training loss: 0.13000964
[2/200] Training loss: 0.06020974
[3/200] Training loss: 0.05196992
[4/200] Training loss: 0.04862854
[5/200] Training loss: 0.04609525
[6/200] Training loss: 0.04325420
[7/200] Training loss: 0.04060891
[8/200] Training loss: 0.03682946
[9/200] Training loss: 0.03176779
[10/200] Training loss: 0.03055527
[50/200] Training loss: 0.01794487
[100/200] Training loss: 0.01528814
[150/200] Training loss: 0.01389267
[200/200] Training loss: 0.01291062
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17849.24693089319 ----------
[1/200] Training loss: 0.10973882
[2/200] Training loss: 0.05311741
[3/200] Training loss: 0.04784598
[4/200] Training loss: 0.04179971
[5/200] Training loss: 0.03908454
[6/200] Training loss: 0.03432312
[7/200] Training loss: 0.03296997
[8/200] Training loss: 0.02920118
[9/200] Training loss: 0.02763352
[10/200] Training loss: 0.02589451
[50/200] Training loss: 0.01852109
[100/200] Training loss: 0.01652250
[150/200] Training loss: 0.01509517
[200/200] Training loss: 0.01385822
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15450.710274935584 ----------
[1/200] Training loss: 0.16699578
[2/200] Training loss: 0.05973228
[3/200] Training loss: 0.04882809
[4/200] Training loss: 0.04399243
[5/200] Training loss: 0.04187415
[6/200] Training loss: 0.03752134
[7/200] Training loss: 0.03775438
[8/200] Training loss: 0.03625544
[9/200] Training loss: 0.03311559
[10/200] Training loss: 0.03160615
[50/200] Training loss: 0.01746568
[100/200] Training loss: 0.01481372
[150/200] Training loss: 0.01345028
[200/200] Training loss: 0.01280264
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16075.308768418727 ----------
[1/200] Training loss: 0.11125558
[2/200] Training loss: 0.05415560
[3/200] Training loss: 0.04760974
[4/200] Training loss: 0.04564415
[5/200] Training loss: 0.04349469
[6/200] Training loss: 0.04087234
[7/200] Training loss: 0.03879149
[8/200] Training loss: 0.03667560
[9/200] Training loss: 0.03358613
[10/200] Training loss: 0.03085980
[50/200] Training loss: 0.01697702
[100/200] Training loss: 0.01451008
[150/200] Training loss: 0.01301646
[200/200] Training loss: 0.01208580
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15228.651417640369 ----------
[1/200] Training loss: 0.15008713
[2/200] Training loss: 0.05980951
[3/200] Training loss: 0.04917627
[4/200] Training loss: 0.04986519
[5/200] Training loss: 0.04320481
[6/200] Training loss: 0.04096855
[7/200] Training loss: 0.04238734
[8/200] Training loss: 0.04114857
[9/200] Training loss: 0.03833178
[10/200] Training loss: 0.03311833
[50/200] Training loss: 0.01889431
[100/200] Training loss: 0.01348628
[150/200] Training loss: 0.01234911
[200/200] Training loss: 0.01167822
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11993.210746084636 ----------
[1/200] Training loss: 0.14987557
[2/200] Training loss: 0.05643222
[3/200] Training loss: 0.04999068
[4/200] Training loss: 0.04913620
[5/200] Training loss: 0.04635226
[6/200] Training loss: 0.04117620
[7/200] Training loss: 0.04120928
[8/200] Training loss: 0.03696011
[9/200] Training loss: 0.03439661
[10/200] Training loss: 0.03325331
[50/200] Training loss: 0.01627806
[100/200] Training loss: 0.01420860
[150/200] Training loss: 0.01375115
[200/200] Training loss: 0.01193713
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10978.642903382914 ----------
[1/200] Training loss: 0.14037526
[2/200] Training loss: 0.05754805
[3/200] Training loss: 0.05045847
[4/200] Training loss: 0.04894803
[5/200] Training loss: 0.04505708
[6/200] Training loss: 0.04521939
[7/200] Training loss: 0.04152012
[8/200] Training loss: 0.03913200
[9/200] Training loss: 0.03932151
[10/200] Training loss: 0.03710036
[50/200] Training loss: 0.01833650
[100/200] Training loss: 0.01612846
[150/200] Training loss: 0.01471783
[200/200] Training loss: 0.01354975
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12337.785214535063 ----------
[1/200] Training loss: 0.15218758
[2/200] Training loss: 0.05625678
[3/200] Training loss: 0.05194012
[4/200] Training loss: 0.04887870
[5/200] Training loss: 0.04572262
[6/200] Training loss: 0.04184281
[7/200] Training loss: 0.04047541
[8/200] Training loss: 0.04176052
[9/200] Training loss: 0.03940969
[10/200] Training loss: 0.03481250
[50/200] Training loss: 0.01879960
[100/200] Training loss: 0.01541306
[150/200] Training loss: 0.01444577
[200/200] Training loss: 0.01241891
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20684.279634543716 ----------
[1/200] Training loss: 0.11881943
[2/200] Training loss: 0.05535400
[3/200] Training loss: 0.04944415
[4/200] Training loss: 0.04661002
[5/200] Training loss: 0.04328116
[6/200] Training loss: 0.04024964
[7/200] Training loss: 0.03417532
[8/200] Training loss: 0.03308151
[9/200] Training loss: 0.03044831
[10/200] Training loss: 0.02771973
[50/200] Training loss: 0.01542825
[100/200] Training loss: 0.01322971
[150/200] Training loss: 0.01233276
[200/200] Training loss: 0.01129179
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7392.15205471316 ----------
[1/200] Training loss: 0.02187313
[2/200] Training loss: 0.00164529
[3/200] Training loss: 0.00123773
[4/200] Training loss: 0.00102789
[5/200] Training loss: 0.00077467
[6/200] Training loss: 0.00064588
[7/200] Training loss: 0.00060090
[8/200] Training loss: 0.00049421
[9/200] Training loss: 0.00047588
[10/200] Training loss: 0.00047581
[50/200] Training loss: 0.00023309
[100/200] Training loss: 0.00018700
[150/200] Training loss: 0.00015076
[200/200] Training loss: 0.00012905
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9838.009148196601 ----------
[1/200] Training loss: 0.16421158
[2/200] Training loss: 0.05780192
[3/200] Training loss: 0.05231407
[4/200] Training loss: 0.04631547
[5/200] Training loss: 0.04248224
[6/200] Training loss: 0.04127047
[7/200] Training loss: 0.03877029
[8/200] Training loss: 0.03841967
[9/200] Training loss: 0.03418266
[10/200] Training loss: 0.03416737
[50/200] Training loss: 0.01772008
[100/200] Training loss: 0.01604525
[150/200] Training loss: 0.01350693
[200/200] Training loss: 0.01335130
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6581.662707857339 ----------
[1/200] Training loss: 0.13538563
[2/200] Training loss: 0.05602051
[3/200] Training loss: 0.04895916
[4/200] Training loss: 0.04489276
[5/200] Training loss: 0.04397348
[6/200] Training loss: 0.04121226
[7/200] Training loss: 0.03890985
[8/200] Training loss: 0.03595120
[9/200] Training loss: 0.03576918
[10/200] Training loss: 0.03357302
[50/200] Training loss: 0.01895952
[100/200] Training loss: 0.01582436
[150/200] Training loss: 0.01386100
[200/200] Training loss: 0.01277300
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6513.69940970567 ----------
[1/200] Training loss: 0.11704822
[2/200] Training loss: 0.05246302
[3/200] Training loss: 0.04526285
[4/200] Training loss: 0.04108560
[5/200] Training loss: 0.03651482
[6/200] Training loss: 0.03316037
[7/200] Training loss: 0.03021223
[8/200] Training loss: 0.02858684
[9/200] Training loss: 0.02662439
[10/200] Training loss: 0.02613727
[50/200] Training loss: 0.01639262
[100/200] Training loss: 0.01511233
[150/200] Training loss: 0.01369476
[200/200] Training loss: 0.01252568
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7114.206912931335 ----------
[1/200] Training loss: 0.16131158
[2/200] Training loss: 0.06060065
[3/200] Training loss: 0.05474307
[4/200] Training loss: 0.04690768
[5/200] Training loss: 0.04635759
[6/200] Training loss: 0.04054866
[7/200] Training loss: 0.03953071
[8/200] Training loss: 0.03796285
[9/200] Training loss: 0.03502268
[10/200] Training loss: 0.03454753
[50/200] Training loss: 0.01725036
[100/200] Training loss: 0.01540960
[150/200] Training loss: 0.01363452
[200/200] Training loss: 0.01220347
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 33618.54726189101 ----------
[1/100] Training loss: 0.16571006
[2/100] Training loss: 0.05729438
[3/100] Training loss: 0.05056884
[4/100] Training loss: 0.04278550
[5/100] Training loss: 0.04011313
[6/100] Training loss: 0.04127421
[7/100] Training loss: 0.03592905
[8/100] Training loss: 0.03378667
[9/100] Training loss: 0.03474557
[10/100] Training loss: 0.03049062
[50/100] Training loss: 0.01840956
[100/100] Training loss: 0.01583230
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10305.77469188998 ----------
[1/200] Training loss: 0.15255884
[2/200] Training loss: 0.05882251
[3/200] Training loss: 0.05227449
[4/200] Training loss: 0.05369440
[5/200] Training loss: 0.04801887
[6/200] Training loss: 0.04744093
[7/200] Training loss: 0.04566767
[8/200] Training loss: 0.04282928
[9/200] Training loss: 0.04285386
[10/200] Training loss: 0.03934018
[50/200] Training loss: 0.01689713
[100/200] Training loss: 0.01473057
[150/200] Training loss: 0.01353617
[200/200] Training loss: 0.01266329
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9335.844043256078 ----------
[1/200] Training loss: 0.14570310
[2/200] Training loss: 0.05993233
[3/200] Training loss: 0.05373212
[4/200] Training loss: 0.05057797
[5/200] Training loss: 0.04705533
[6/200] Training loss: 0.04462122
[7/200] Training loss: 0.04110986
[8/200] Training loss: 0.03872353
[9/200] Training loss: 0.03436800
[10/200] Training loss: 0.03440368
[50/200] Training loss: 0.01871009
[100/200] Training loss: 0.01609976
[150/200] Training loss: 0.01390308
[200/200] Training loss: 0.01321239
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 22465.196549329365 ----------
[1/50] Training loss: 0.17421554
[2/50] Training loss: 0.05644312
[3/50] Training loss: 0.04994061
[4/50] Training loss: 0.04066697
[5/50] Training loss: 0.04107823
[6/50] Training loss: 0.03202154
[7/50] Training loss: 0.03199739
[8/50] Training loss: 0.02799626
[9/50] Training loss: 0.02728963
[10/50] Training loss: 0.02680788
[50/50] Training loss: 0.01816819
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 50
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7761.084460305789 ----------
[1/200] Training loss: 0.10741497
[2/200] Training loss: 0.05056818
[3/200] Training loss: 0.04440328
[4/200] Training loss: 0.04045437
[5/200] Training loss: 0.03631006
[6/200] Training loss: 0.03551771
[7/200] Training loss: 0.03190927
[8/200] Training loss: 0.03244697
[9/200] Training loss: 0.03049304
[10/200] Training loss: 0.02884970
[50/200] Training loss: 0.01764658
[100/200] Training loss: 0.01493035
[150/200] Training loss: 0.01256780
[200/200] Training loss: 0.01147420
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7051.699086035932 ----------
[1/200] Training loss: 0.16926209
[2/200] Training loss: 0.05873291
[3/200] Training loss: 0.05349796
[4/200] Training loss: 0.04906654
[5/200] Training loss: 0.04417150
[6/200] Training loss: 0.04244062
[7/200] Training loss: 0.03760605
[8/200] Training loss: 0.03434786
[9/200] Training loss: 0.03341146
[10/200] Training loss: 0.03014814
[50/200] Training loss: 0.01780316
[100/200] Training loss: 0.01618809
[150/200] Training loss: 0.01469804
[200/200] Training loss: 0.01342517
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11242.489048249057 ----------
[1/200] Training loss: 0.14919264
[2/200] Training loss: 0.05524097
[3/200] Training loss: 0.04963290
[4/200] Training loss: 0.04857640
[5/200] Training loss: 0.04204372
[6/200] Training loss: 0.03805607
[7/200] Training loss: 0.03540623
[8/200] Training loss: 0.03559656
[9/200] Training loss: 0.03275670
[10/200] Training loss: 0.02956478
[50/200] Training loss: 0.01904795
[100/200] Training loss: 0.01691598
[150/200] Training loss: 0.01524026
[200/200] Training loss: 0.01417811
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10574.71134357813 ----------
[1/200] Training loss: 0.12914329
[2/200] Training loss: 0.05426384
[3/200] Training loss: 0.04990353
[4/200] Training loss: 0.04502995
[5/200] Training loss: 0.04272103
[6/200] Training loss: 0.03927232
[7/200] Training loss: 0.03652393
[8/200] Training loss: 0.03590144
[9/200] Training loss: 0.03262050
[10/200] Training loss: 0.03047967
[50/200] Training loss: 0.01504703
[100/200] Training loss: 0.01342774
[150/200] Training loss: 0.01231001
[200/200] Training loss: 0.01150032
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5139.398602949571 ----------
[1/200] Training loss: 0.15815226
[2/200] Training loss: 0.05475150
[3/200] Training loss: 0.04798075
[4/200] Training loss: 0.04322516
[5/200] Training loss: 0.03755694
[6/200] Training loss: 0.03374093
[7/200] Training loss: 0.03449733
[8/200] Training loss: 0.03332285
[9/200] Training loss: 0.03272587
[10/200] Training loss: 0.03075634
[50/200] Training loss: 0.01825760
[100/200] Training loss: 0.01542680
[150/200] Training loss: 0.01407515
[200/200] Training loss: 0.01323423
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9321.262146297571 ----------
[1/200] Training loss: 0.11722158
[2/200] Training loss: 0.05088374
[3/200] Training loss: 0.04356826
[4/200] Training loss: 0.03930070
[5/200] Training loss: 0.03448862
[6/200] Training loss: 0.03275613
[7/200] Training loss: 0.03139401
[8/200] Training loss: 0.02844060
[9/200] Training loss: 0.02679150
[10/200] Training loss: 0.02681474
[50/200] Training loss: 0.01587073
[100/200] Training loss: 0.01346943
[150/200] Training loss: 0.01189340
[200/200] Training loss: 0.01068031
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11948.885136279452 ----------
[1/200] Training loss: 0.17086330
[2/200] Training loss: 0.05895167
[3/200] Training loss: 0.05305844
[4/200] Training loss: 0.05382952
[5/200] Training loss: 0.04324530
[6/200] Training loss: 0.04412973
[7/200] Training loss: 0.04015567
[8/200] Training loss: 0.03735649
[9/200] Training loss: 0.03750649
[10/200] Training loss: 0.03403254
[50/200] Training loss: 0.01752705
[100/200] Training loss: 0.01477289
[150/200] Training loss: 0.01321571
[200/200] Training loss: 0.01225388
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12929.903943958749 ----------
[1/200] Training loss: 0.16494395
[2/200] Training loss: 0.06077957
[3/200] Training loss: 0.05639725
[4/200] Training loss: 0.05428017
[5/200] Training loss: 0.05289649
[6/200] Training loss: 0.05079689
[7/200] Training loss: 0.05076853
[8/200] Training loss: 0.04958688
[9/200] Training loss: 0.04748855
[10/200] Training loss: 0.04590368
[50/200] Training loss: 0.02019568
[100/200] Training loss: 0.01805733
[150/200] Training loss: 0.01512441
[200/200] Training loss: 0.01411781
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9423.390897124029 ----------
[1/200] Training loss: 0.18019720
[2/200] Training loss: 0.05672310
[3/200] Training loss: 0.05084820
[4/200] Training loss: 0.04783365
[5/200] Training loss: 0.04297253
[6/200] Training loss: 0.04360236
[7/200] Training loss: 0.04020551
[8/200] Training loss: 0.03881534
[9/200] Training loss: 0.03451066
[10/200] Training loss: 0.03532941
[50/200] Training loss: 0.01770340
[100/200] Training loss: 0.01437895
[150/200] Training loss: 0.01262876
[200/200] Training loss: 0.01233428
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8362.835404335063 ----------
[1/200] Training loss: 0.14046864
[2/200] Training loss: 0.05154067
[3/200] Training loss: 0.04908488
[4/200] Training loss: 0.04401433
[5/200] Training loss: 0.04257224
[6/200] Training loss: 0.03755676
[7/200] Training loss: 0.03581650
[8/200] Training loss: 0.03566740
[9/200] Training loss: 0.03204695
[10/200] Training loss: 0.03229102
[50/200] Training loss: 0.01830618
[100/200] Training loss: 0.01527579
[150/200] Training loss: 0.01403887
[200/200] Training loss: 0.01261886
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8013.4701596748955 ----------
[1/200] Training loss: 0.18471513
[2/200] Training loss: 0.06554512
[3/200] Training loss: 0.05305999
[4/200] Training loss: 0.04601689
[5/200] Training loss: 0.04362834
[6/200] Training loss: 0.03669318
[7/200] Training loss: 0.03393612
[8/200] Training loss: 0.03091524
[9/200] Training loss: 0.03006417
[10/200] Training loss: 0.03140934
[50/200] Training loss: 0.01854469
[100/200] Training loss: 0.01564491
[150/200] Training loss: 0.01458805
[200/200] Training loss: 0.01334040
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11328.783871184056 ----------
[1/200] Training loss: 0.16525471
[2/200] Training loss: 0.05836764
[3/200] Training loss: 0.05109146
[4/200] Training loss: 0.05054467
[5/200] Training loss: 0.04962832
[6/200] Training loss: 0.04675566
[7/200] Training loss: 0.04131624
[8/200] Training loss: 0.04177691
[9/200] Training loss: 0.03880321
[10/200] Training loss: 0.03728005
[50/200] Training loss: 0.01798112
[100/200] Training loss: 0.01591751
[150/200] Training loss: 0.01403352
[200/200] Training loss: 0.01314537
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14315.572499903732 ----------
[1/200] Training loss: 0.16130318
[2/200] Training loss: 0.05640209
[3/200] Training loss: 0.05437884
[4/200] Training loss: 0.05089000
[5/200] Training loss: 0.04944252
[6/200] Training loss: 0.04524509
[7/200] Training loss: 0.04615666
[8/200] Training loss: 0.04104063
[9/200] Training loss: 0.03849448
[10/200] Training loss: 0.03485130
[50/200] Training loss: 0.01722350
[100/200] Training loss: 0.01425095
[150/200] Training loss: 0.01265348
[200/200] Training loss: 0.01211088
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14482.326884862114 ----------
[1/50] Training loss: 0.20134016
[2/50] Training loss: 0.06004436
[3/50] Training loss: 0.05352141
[4/50] Training loss: 0.04891972
[5/50] Training loss: 0.04519118
[6/50] Training loss: 0.04759111
[7/50] Training loss: 0.04065705
[8/50] Training loss: 0.03847772
[9/50] Training loss: 0.03461890
[10/50] Training loss: 0.03370141
[50/50] Training loss: 0.01656107
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 50
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16221.1619805734 ----------
[1/200] Training loss: 0.15528223
[2/200] Training loss: 0.05723400
[3/200] Training loss: 0.04992790
[4/200] Training loss: 0.04520600
[5/200] Training loss: 0.04020026
[6/200] Training loss: 0.03653984
[7/200] Training loss: 0.03458466
[8/200] Training loss: 0.03295741
[9/200] Training loss: 0.03172769
[10/200] Training loss: 0.03013743
[50/200] Training loss: 0.01804448
[100/200] Training loss: 0.01592640
[150/200] Training loss: 0.01485040
[200/200] Training loss: 0.01457889
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6350.0803144527235 ----------
[1/200] Training loss: 0.15753089
[2/200] Training loss: 0.05825644
[3/200] Training loss: 0.05254855
[4/200] Training loss: 0.05068111
[5/200] Training loss: 0.04437366
[6/200] Training loss: 0.04188006
[7/200] Training loss: 0.04140209
[8/200] Training loss: 0.04061238
[9/200] Training loss: 0.03815962
[10/200] Training loss: 0.03421293
[50/200] Training loss: 0.01570798
[100/200] Training loss: 0.01474058
[150/200] Training loss: 0.01316972
[200/200] Training loss: 0.01205996
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8986.508554494343 ----------
[1/200] Training loss: 0.14714114
[2/200] Training loss: 0.05722019
[3/200] Training loss: 0.04814045
[4/200] Training loss: 0.04561610
[5/200] Training loss: 0.03942270
[6/200] Training loss: 0.03588774
[7/200] Training loss: 0.03668440
[8/200] Training loss: 0.03290659
[9/200] Training loss: 0.03437889
[10/200] Training loss: 0.03134081
[50/200] Training loss: 0.01834957
[100/200] Training loss: 0.01519275
[150/200] Training loss: 0.01346645
[200/200] Training loss: 0.01259753
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16307.515322697078 ----------
[1/200] Training loss: 0.16637954
[2/200] Training loss: 0.06194132
[3/200] Training loss: 0.05393926
[4/200] Training loss: 0.05195808
[5/200] Training loss: 0.05012444
[6/200] Training loss: 0.04886450
[7/200] Training loss: 0.04526231
[8/200] Training loss: 0.04230100
[9/200] Training loss: 0.03923196
[10/200] Training loss: 0.03759957
[50/200] Training loss: 0.01757888
[100/200] Training loss: 0.01516193
[150/200] Training loss: 0.01390167
[200/200] Training loss: 0.01259518
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14140.736048735229 ----------
[1/200] Training loss: 0.15114486
[2/200] Training loss: 0.05877042
[3/200] Training loss: 0.05071192
[4/200] Training loss: 0.04859965
[5/200] Training loss: 0.04516060
[6/200] Training loss: 0.04137615
[7/200] Training loss: 0.03953580
[8/200] Training loss: 0.03727148
[9/200] Training loss: 0.03695173
[10/200] Training loss: 0.03349982
[50/200] Training loss: 0.01902760
[100/200] Training loss: 0.01691442
[150/200] Training loss: 0.01468569
[200/200] Training loss: 0.01452508
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9364.397684848716 ----------
[1/200] Training loss: 0.16687271
[2/200] Training loss: 0.05608959
[3/200] Training loss: 0.05537014
[4/200] Training loss: 0.04936945
[5/200] Training loss: 0.04497554
[6/200] Training loss: 0.04410164
[7/200] Training loss: 0.04202426
[8/200] Training loss: 0.04182200
[9/200] Training loss: 0.03871841
[10/200] Training loss: 0.03361596
[50/200] Training loss: 0.01620633
[100/200] Training loss: 0.01383119
[150/200] Training loss: 0.01278486
[200/200] Training loss: 0.01178438
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8537.707889123403 ----------
[1/200] Training loss: 0.19225931
[2/200] Training loss: 0.06301117
[3/200] Training loss: 0.05158702
[4/200] Training loss: 0.05069279
[5/200] Training loss: 0.04568231
[6/200] Training loss: 0.04789818
[7/200] Training loss: 0.04200810
[8/200] Training loss: 0.04147202
[9/200] Training loss: 0.03754280
[10/200] Training loss: 0.03915320
[50/200] Training loss: 0.01806749
[100/200] Training loss: 0.01506745
[150/200] Training loss: 0.01383630
[200/200] Training loss: 0.01229418
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13515.46432794671 ----------
[1/200] Training loss: 0.13476427
[2/200] Training loss: 0.05253648
[3/200] Training loss: 0.04587657
[4/200] Training loss: 0.04561131
[5/200] Training loss: 0.04180219
[6/200] Training loss: 0.03901190
[7/200] Training loss: 0.03768845
[8/200] Training loss: 0.03632278
[9/200] Training loss: 0.03461715
[10/200] Training loss: 0.03154866
[50/200] Training loss: 0.01764340
[100/200] Training loss: 0.01482628
[150/200] Training loss: 0.01305832
[200/200] Training loss: 0.01266056
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7982.750904293582 ----------
[1/200] Training loss: 0.12219864
[2/200] Training loss: 0.05270414
[3/200] Training loss: 0.04623434
[4/200] Training loss: 0.04484826
[5/200] Training loss: 0.04371970
[6/200] Training loss: 0.03841119
[7/200] Training loss: 0.03729276
[8/200] Training loss: 0.03392505
[9/200] Training loss: 0.03193262
[10/200] Training loss: 0.02813850
[50/200] Training loss: 0.01712158
[100/200] Training loss: 0.01402328
[150/200] Training loss: 0.01309246
[200/200] Training loss: 0.01222470
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 21295.25242865179 ----------
[1/200] Training loss: 0.15376271
[2/200] Training loss: 0.05843770
[3/200] Training loss: 0.05073788
[4/200] Training loss: 0.04766895
[5/200] Training loss: 0.04510752
[6/200] Training loss: 0.04206592
[7/200] Training loss: 0.03931400
[8/200] Training loss: 0.03557310
[9/200] Training loss: 0.03556382
[10/200] Training loss: 0.02937200
[50/200] Training loss: 0.01749541
[100/200] Training loss: 0.01620363
[150/200] Training loss: 0.01429789
[200/200] Training loss: 0.01372534
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7030.421324501114 ----------
[1/200] Training loss: 0.14436570
[2/200] Training loss: 0.05812688
[3/200] Training loss: 0.05135545
[4/200] Training loss: 0.04758953
[5/200] Training loss: 0.04622372
[6/200] Training loss: 0.04108011
[7/200] Training loss: 0.03825638
[8/200] Training loss: 0.03695310
[9/200] Training loss: 0.03166805
[10/200] Training loss: 0.03031678
[50/200] Training loss: 0.01649693
[100/200] Training loss: 0.01432361
[150/200] Training loss: 0.01333944
[200/200] Training loss: 0.01260787
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20485.282912373947 ----------
[1/50] Training loss: 0.17588394
[2/50] Training loss: 0.05856752
[3/50] Training loss: 0.05169709
[4/50] Training loss: 0.04919483
[5/50] Training loss: 0.04655910
[6/50] Training loss: 0.04422616
[7/50] Training loss: 0.04189668
[8/50] Training loss: 0.03754449
[9/50] Training loss: 0.03404129
[10/50] Training loss: 0.03458733
[50/50] Training loss: 0.02012735
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 50
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11877.178789594775 ----------
[1/200] Training loss: 0.16669527
[2/200] Training loss: 0.05728384
[3/200] Training loss: 0.05036300
[4/200] Training loss: 0.04966196
[5/200] Training loss: 0.04424909
[6/200] Training loss: 0.04310667
[7/200] Training loss: 0.04200479
[8/200] Training loss: 0.03929966
[9/200] Training loss: 0.03845867
[10/200] Training loss: 0.03588172
[50/200] Training loss: 0.01885719
[100/200] Training loss: 0.01649950
[150/200] Training loss: 0.01553654
[200/200] Training loss: 0.01393951
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8408.678374156072 ----------
[1/200] Training loss: 0.15607474
[2/200] Training loss: 0.05632005
[3/200] Training loss: 0.05305247
[4/200] Training loss: 0.04490234
[5/200] Training loss: 0.04580790
[6/200] Training loss: 0.04176381
[7/200] Training loss: 0.03727005
[8/200] Training loss: 0.03738934
[9/200] Training loss: 0.03350838
[10/200] Training loss: 0.03209120
[50/200] Training loss: 0.01911873
[100/200] Training loss: 0.01738903
[150/200] Training loss: 0.01543348
[200/200] Training loss: 0.01448357
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10864.811825337796 ----------
[1/200] Training loss: 0.14231230
[2/200] Training loss: 0.05964494
[3/200] Training loss: 0.05274253
[4/200] Training loss: 0.04820154
[5/200] Training loss: 0.04645898
[6/200] Training loss: 0.04369924
[7/200] Training loss: 0.04319038
[8/200] Training loss: 0.03890983
[9/200] Training loss: 0.03672539
[10/200] Training loss: 0.03337308
[50/200] Training loss: 0.01907608
[100/200] Training loss: 0.01682469
[150/200] Training loss: 0.01502875
[200/200] Training loss: 0.01391602
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14947.081588055911 ----------
[1/200] Training loss: 0.09119309
[2/200] Training loss: 0.04999098
[3/200] Training loss: 0.04476545
[4/200] Training loss: 0.03796518
[5/200] Training loss: 0.03546698
[6/200] Training loss: 0.03115569
[7/200] Training loss: 0.02734504
[8/200] Training loss: 0.02724348
[9/200] Training loss: 0.02555636
[10/200] Training loss: 0.02445319
[50/200] Training loss: 0.01641562
[100/200] Training loss: 0.01437718
[150/200] Training loss: 0.01282642
[200/200] Training loss: 0.01221094
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.7125534181204007 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16010.253214737111 ----------
[1/200] Training loss: 0.19768782
[2/200] Training loss: 0.06207867
[3/200] Training loss: 0.05053598
[4/200] Training loss: 0.04555871
[5/200] Training loss: 0.04230624
[6/200] Training loss: 0.04309855
[7/200] Training loss: 0.03702758
[8/200] Training loss: 0.03639726
[9/200] Training loss: 0.03533967
[10/200] Training loss: 0.03339123
[50/200] Training loss: 0.01706524
[100/200] Training loss: 0.01439389
[150/200] Training loss: 0.01293536
[200/200] Training loss: 0.01211536
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11054.777790620668 ----------
[1/200] Training loss: 0.15856708
[2/200] Training loss: 0.05936221
[3/200] Training loss: 0.05208189
[4/200] Training loss: 0.05017159
[5/200] Training loss: 0.04874790
[6/200] Training loss: 0.04631459
[7/200] Training loss: 0.04296570
[8/200] Training loss: 0.04333389
[9/200] Training loss: 0.04003403
[10/200] Training loss: 0.03745570
[50/200] Training loss: 0.02158016
[100/200] Training loss: 0.01616418
[150/200] Training loss: 0.01249644
[200/200] Training loss: 0.01268337
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7798.508062443739 ----------
[1/200] Training loss: 0.15885161
[2/200] Training loss: 0.06158985
[3/200] Training loss: 0.05174863
[4/200] Training loss: 0.04920964
[5/200] Training loss: 0.04651994
[6/200] Training loss: 0.04274092
[7/200] Training loss: 0.04264026
[8/200] Training loss: 0.04028717
[9/200] Training loss: 0.03955117
[10/200] Training loss: 0.03772767
[50/200] Training loss: 0.01900707
[100/200] Training loss: 0.01479024
[150/200] Training loss: 0.01371881
[200/200] Training loss: 0.01235946
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10862.926309240986 ----------
[1/200] Training loss: 0.15156027
[2/200] Training loss: 0.05548815
[3/200] Training loss: 0.05249557
[4/200] Training loss: 0.04655682
[5/200] Training loss: 0.04647938
[6/200] Training loss: 0.04149962
[7/200] Training loss: 0.04030601
[8/200] Training loss: 0.03639144
[9/200] Training loss: 0.03610248
[10/200] Training loss: 0.03427366
[50/200] Training loss: 0.01964268
[100/200] Training loss: 0.01640561
[150/200] Training loss: 0.01460648
[200/200] Training loss: 0.01322157
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6952.458557949123 ----------
[1/200] Training loss: 0.11194502
[2/200] Training loss: 0.05470705
[3/200] Training loss: 0.04513129
[4/200] Training loss: 0.04022291
[5/200] Training loss: 0.03486498
[6/200] Training loss: 0.03359582
[7/200] Training loss: 0.02918099
[8/200] Training loss: 0.02899630
[9/200] Training loss: 0.02714368
[10/200] Training loss: 0.02725909
[50/200] Training loss: 0.01676682
[100/200] Training loss: 0.01374698
[150/200] Training loss: 0.01238498
[200/200] Training loss: 0.01177425
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15777.831536684627 ----------
[1/200] Training loss: 0.11450823
[2/200] Training loss: 0.05343576
[3/200] Training loss: 0.04770792
[4/200] Training loss: 0.04481900
[5/200] Training loss: 0.03851152
[6/200] Training loss: 0.03586065
[7/200] Training loss: 0.03305799
[8/200] Training loss: 0.03060103
[9/200] Training loss: 0.02890301
[10/200] Training loss: 0.02651915
[50/200] Training loss: 0.01635115
[100/200] Training loss: 0.01321273
[150/200] Training loss: 0.01179147
[200/200] Training loss: 0.01096011
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6563.055995494782 ----------
[1/200] Training loss: 0.15576474
[2/200] Training loss: 0.05739909
[3/200] Training loss: 0.05361843
[4/200] Training loss: 0.05314332
[5/200] Training loss: 0.04778443
[6/200] Training loss: 0.04739093
[7/200] Training loss: 0.04681674
[8/200] Training loss: 0.04211714
[9/200] Training loss: 0.04083102
[10/200] Training loss: 0.03954418
[50/200] Training loss: 0.01827040
[100/200] Training loss: 0.01495987
[150/200] Training loss: 0.01397777
[200/200] Training loss: 0.01247576
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20147.611272803533 ----------
[1/200] Training loss: 0.15431716
[2/200] Training loss: 0.05795606
[3/200] Training loss: 0.05383690
[4/200] Training loss: 0.04892401
[5/200] Training loss: 0.04738894
[6/200] Training loss: 0.04524636
[7/200] Training loss: 0.04422700
[8/200] Training loss: 0.04235736
[9/200] Training loss: 0.04055872
[10/200] Training loss: 0.03722602
[50/200] Training loss: 0.01800580
[100/200] Training loss: 0.01349168
[150/200] Training loss: 0.01254281
[200/200] Training loss: 0.01179228
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15423.374468643366 ----------
[1/200] Training loss: 0.16552571
[2/200] Training loss: 0.05694793
[3/200] Training loss: 0.05092774
[4/200] Training loss: 0.04482725
[5/200] Training loss: 0.04217613
[6/200] Training loss: 0.03698681
[7/200] Training loss: 0.03639445
[8/200] Training loss: 0.03034352
[9/200] Training loss: 0.03281861
[10/200] Training loss: 0.02877860
[50/200] Training loss: 0.01754664
[100/200] Training loss: 0.01534784
[150/200] Training loss: 0.01386455
[200/200] Training loss: 0.01328537
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13777.834953286383 ----------
[1/200] Training loss: 0.12819905
[2/200] Training loss: 0.05784554
[3/200] Training loss: 0.05231641
[4/200] Training loss: 0.05062299
[5/200] Training loss: 0.04604940
[6/200] Training loss: 0.04454590
[7/200] Training loss: 0.04121320
[8/200] Training loss: 0.03893902
[9/200] Training loss: 0.03340793
[10/200] Training loss: 0.03204180
[50/200] Training loss: 0.01714401
[100/200] Training loss: 0.01514823
[150/200] Training loss: 0.01436826
[200/200] Training loss: 0.01320505
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11917.614526405861 ----------
[1/200] Training loss: 0.15266493
[2/200] Training loss: 0.05790659
[3/200] Training loss: 0.05006852
[4/200] Training loss: 0.04705583
[5/200] Training loss: 0.04067735
[6/200] Training loss: 0.04203564
[7/200] Training loss: 0.03646846
[8/200] Training loss: 0.03595853
[9/200] Training loss: 0.03221011
[10/200] Training loss: 0.02968381
[50/200] Training loss: 0.01677834
[100/200] Training loss: 0.01483140
[150/200] Training loss: 0.01312197
[200/200] Training loss: 0.01248184
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10392.123940754364 ----------
[1/200] Training loss: 0.15190494
[2/200] Training loss: 0.05469137
[3/200] Training loss: 0.04870915
[4/200] Training loss: 0.04618806
[5/200] Training loss: 0.04415211
[6/200] Training loss: 0.04212175
[7/200] Training loss: 0.04108785
[8/200] Training loss: 0.03748940
[9/200] Training loss: 0.03638822
[10/200] Training loss: 0.03523722
[50/200] Training loss: 0.01960932
[100/200] Training loss: 0.01413200
[150/200] Training loss: 0.01327822
[200/200] Training loss: 0.01195146
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7517.247368551869 ----------
[1/200] Training loss: 0.15929023
[2/200] Training loss: 0.05695604
[3/200] Training loss: 0.05179363
[4/200] Training loss: 0.04808857
[5/200] Training loss: 0.04729074
[6/200] Training loss: 0.04201558
[7/200] Training loss: 0.04162890
[8/200] Training loss: 0.03856675
[9/200] Training loss: 0.03716873
[10/200] Training loss: 0.03409207
[50/200] Training loss: 0.01894127
[100/200] Training loss: 0.01596104
[150/200] Training loss: 0.01517715
[200/200] Training loss: 0.01330787
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11037.46130231042 ----------
[1/50] Training loss: 0.15036867
[2/50] Training loss: 0.05988880
[3/50] Training loss: 0.05123114
[4/50] Training loss: 0.05129468
[5/50] Training loss: 0.04696654
[6/50] Training loss: 0.04645576
[7/50] Training loss: 0.04299545
[8/50] Training loss: 0.04277114
[9/50] Training loss: 0.03913641
[10/50] Training loss: 0.03984691
[50/50] Training loss: 0.01742227
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 50
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13313.79374934132 ----------
[1/200] Training loss: 0.16601682
[2/200] Training loss: 0.05875983
[3/200] Training loss: 0.05065639
[4/200] Training loss: 0.05014475
[5/200] Training loss: 0.04445626
[6/200] Training loss: 0.04507750
[7/200] Training loss: 0.04067709
[8/200] Training loss: 0.04068728
[9/200] Training loss: 0.03880912
[10/200] Training loss: 0.03800001
[50/200] Training loss: 0.01962827
[100/200] Training loss: 0.01543871
[150/200] Training loss: 0.01359135
[200/200] Training loss: 0.01297299
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6280.70473752747 ----------
[1/200] Training loss: 0.15051557
[2/200] Training loss: 0.06006440
[3/200] Training loss: 0.05144783
[4/200] Training loss: 0.04656995
[5/200] Training loss: 0.04550132
[6/200] Training loss: 0.04311822
[7/200] Training loss: 0.03838790
[8/200] Training loss: 0.03580305
[9/200] Training loss: 0.03635831
[10/200] Training loss: 0.03081420
[50/200] Training loss: 0.01839345
[100/200] Training loss: 0.01597931
[150/200] Training loss: 0.01423525
[200/200] Training loss: 0.01278680
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12583.749520711226 ----------
[1/200] Training loss: 0.16075253
[2/200] Training loss: 0.05824231
[3/200] Training loss: 0.05283951
[4/200] Training loss: 0.05055621
[5/200] Training loss: 0.04861644
[6/200] Training loss: 0.04300795
[7/200] Training loss: 0.04035085
[8/200] Training loss: 0.03617456
[9/200] Training loss: 0.03650422
[10/200] Training loss: 0.03394499
[50/200] Training loss: 0.01648420
[100/200] Training loss: 0.01416031
[150/200] Training loss: 0.01342354
[200/200] Training loss: 0.01195577
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8288.896669641865 ----------
[1/200] Training loss: 0.14162602
[2/200] Training loss: 0.05298144
[3/200] Training loss: 0.05063093
[4/200] Training loss: 0.04804075
[5/200] Training loss: 0.04493784
[6/200] Training loss: 0.04322627
[7/200] Training loss: 0.04106889
[8/200] Training loss: 0.04048204
[9/200] Training loss: 0.03777043
[10/200] Training loss: 0.03555108
[50/200] Training loss: 0.01841888
[100/200] Training loss: 0.01443842
[150/200] Training loss: 0.01268118
[200/200] Training loss: 0.01217283
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4324.266874280541 ----------
[1/200] Training loss: 0.09883239
[2/200] Training loss: 0.04480124
[3/200] Training loss: 0.04284175
[4/200] Training loss: 0.04103487
[5/200] Training loss: 0.03829577
[6/200] Training loss: 0.03655508
[7/200] Training loss: 0.03580256
[8/200] Training loss: 0.03481900
[9/200] Training loss: 0.03381015
[10/200] Training loss: 0.03343876
[50/200] Training loss: 0.02787383
[100/200] Training loss: 0.02646381
[150/200] Training loss: 0.02634340
[200/200] Training loss: 0.02565035
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Rprop ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 24059.85336613671 ----------
[1/200] Training loss: 0.17864259
[2/200] Training loss: 0.05944796
[3/200] Training loss: 0.05265988
[4/200] Training loss: 0.04889754
[5/200] Training loss: 0.04414709
[6/200] Training loss: 0.04237167
[7/200] Training loss: 0.04026281
[8/200] Training loss: 0.04150610
[9/200] Training loss: 0.03630032
[10/200] Training loss: 0.03567609
[50/200] Training loss: 0.01879664
[100/200] Training loss: 0.01612691
[150/200] Training loss: 0.01470782
[200/200] Training loss: 0.01415357
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16470.497260252952 ----------
[1/50] Training loss: 0.14541968
[2/50] Training loss: 0.04757435
[3/50] Training loss: 0.03593557
[4/50] Training loss: 0.03131827
[5/50] Training loss: 0.03135256
[6/50] Training loss: 0.02755502
[7/50] Training loss: 0.02777124
[8/50] Training loss: 0.02206842
[9/50] Training loss: 0.02527202
[10/50] Training loss: 0.02377588
[50/50] Training loss: 0.01994776
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 50
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adam ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 28605.558061327873 ----------
[1/50] Training loss: 0.15046565
[2/50] Training loss: 0.05535592
[3/50] Training loss: 0.04786546
[4/50] Training loss: 0.04384731
[5/50] Training loss: 0.04390357
[6/50] Training loss: 0.04156023
[7/50] Training loss: 0.04123391
[8/50] Training loss: 0.03671564
[9/50] Training loss: 0.03592852
[10/50] Training loss: 0.03604463
[50/50] Training loss: 0.01720426
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 50
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14127.237805034641 ----------
[1/200] Training loss: 0.16388762
[2/200] Training loss: 0.06292081
[3/200] Training loss: 0.05503454
[4/200] Training loss: 0.05213675
[5/200] Training loss: 0.04674332
[6/200] Training loss: 0.04482029
[7/200] Training loss: 0.04134382
[8/200] Training loss: 0.03760889
[9/200] Training loss: 0.03340234
[10/200] Training loss: 0.03459119
[50/200] Training loss: 0.01979409
[100/200] Training loss: 0.01678851
[150/200] Training loss: 0.01539791
[200/200] Training loss: 0.01477284
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15209.179333547225 ----------
[1/200] Training loss: 0.10779356
[2/200] Training loss: 0.05534536
[3/200] Training loss: 0.04949010
[4/200] Training loss: 0.04562988
[5/200] Training loss: 0.04264929
[6/200] Training loss: 0.03977215
[7/200] Training loss: 0.03742772
[8/200] Training loss: 0.03425255
[9/200] Training loss: 0.03290815
[10/200] Training loss: 0.02835550
[50/200] Training loss: 0.01806697
[100/200] Training loss: 0.01574985
[150/200] Training loss: 0.01435117
[200/200] Training loss: 0.01309165
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14343.30282745226 ----------
[1/50] Training loss: 0.16752717
[2/50] Training loss: 0.05428291
[3/50] Training loss: 0.04996084
[4/50] Training loss: 0.04609787
[5/50] Training loss: 0.04067169
[6/50] Training loss: 0.03832567
[7/50] Training loss: 0.03687392
[8/50] Training loss: 0.03272733
[9/50] Training loss: 0.02964728
[10/50] Training loss: 0.03034352
[50/50] Training loss: 0.01767378
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 50
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13820.380892001494 ----------
[1/200] Training loss: 0.15870991
[2/200] Training loss: 0.05999733
[3/200] Training loss: 0.05610210
[4/200] Training loss: 0.05341928
[5/200] Training loss: 0.05257978
[6/200] Training loss: 0.04787564
[7/200] Training loss: 0.04612296
[8/200] Training loss: 0.04511209
[9/200] Training loss: 0.04381579
[10/200] Training loss: 0.03972436
[50/200] Training loss: 0.02022167
[100/200] Training loss: 0.01770492
[150/200] Training loss: 0.01613811
[200/200] Training loss: 0.01457572
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 24914.96706800954 ----------
[1/200] Training loss: 0.17390804
[2/200] Training loss: 0.06088134
[3/200] Training loss: 0.05480829
[4/200] Training loss: 0.05038142
[5/200] Training loss: 0.04510115
[6/200] Training loss: 0.04803043
[7/200] Training loss: 0.04132727
[8/200] Training loss: 0.03815552
[9/200] Training loss: 0.03719230
[10/200] Training loss: 0.03574035
[50/200] Training loss: 0.01761339
[100/200] Training loss: 0.01589260
[150/200] Training loss: 0.01419445
[200/200] Training loss: 0.01327248
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 18864.687858536116 ----------
[1/200] Training loss: 0.14627681
[2/200] Training loss: 0.05991992
[3/200] Training loss: 0.05086139
[4/200] Training loss: 0.05031213
[5/200] Training loss: 0.04613802
[6/200] Training loss: 0.04431492
[7/200] Training loss: 0.04410300
[8/200] Training loss: 0.04044860
[9/200] Training loss: 0.03922149
[10/200] Training loss: 0.03826311
[50/200] Training loss: 0.01888766
[100/200] Training loss: 0.01614976
[150/200] Training loss: 0.01514509
[200/200] Training loss: 0.01361881
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7232.355909383885 ----------
[1/200] Training loss: 0.15062266
[2/200] Training loss: 0.05791021
[3/200] Training loss: 0.05642970
[4/200] Training loss: 0.05112070
[5/200] Training loss: 0.05116879
[6/200] Training loss: 0.04714827
[7/200] Training loss: 0.04432725
[8/200] Training loss: 0.04393668
[9/200] Training loss: 0.04203508
[10/200] Training loss: 0.03991157
[50/200] Training loss: 0.01888181
[100/200] Training loss: 0.01590020
[150/200] Training loss: 0.01446696
[200/200] Training loss: 0.01312998
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9588.522305339859 ----------
[1/200] Training loss: 0.19328224
[2/200] Training loss: 0.06419379
[3/200] Training loss: 0.05428290
[4/200] Training loss: 0.05105316
[5/200] Training loss: 0.04805169
[6/200] Training loss: 0.04729859
[7/200] Training loss: 0.04370543
[8/200] Training loss: 0.04420238
[9/200] Training loss: 0.04135408
[10/200] Training loss: 0.03748969
[50/200] Training loss: 0.01863306
[100/200] Training loss: 0.01603602
[150/200] Training loss: 0.01381293
[200/200] Training loss: 0.01291843
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4947.408412492342 ----------
[1/200] Training loss: 0.14915748
[2/200] Training loss: 0.05991147
[3/200] Training loss: 0.05303999
[4/200] Training loss: 0.05055562
[5/200] Training loss: 0.04963301
[6/200] Training loss: 0.04651534
[7/200] Training loss: 0.04406699
[8/200] Training loss: 0.04133571
[9/200] Training loss: 0.04038784
[10/200] Training loss: 0.03978232
[50/200] Training loss: 0.02031346
[100/200] Training loss: 0.01611073
[150/200] Training loss: 0.01469054
[200/200] Training loss: 0.01346952
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10750.91698414605 ----------
[1/200] Training loss: 0.13172294
[2/200] Training loss: 0.05154181
[3/200] Training loss: 0.04642963
[4/200] Training loss: 0.04526474
[5/200] Training loss: 0.04203913
[6/200] Training loss: 0.03796414
[7/200] Training loss: 0.03643050
[8/200] Training loss: 0.03437360
[9/200] Training loss: 0.03624038
[10/200] Training loss: 0.03456633
[50/200] Training loss: 0.01752112
[100/200] Training loss: 0.01483000
[150/200] Training loss: 0.01299174
[200/200] Training loss: 0.01178700
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 3774.1163204119716 ----------
[1/200] Training loss: 0.17295049
[2/200] Training loss: 0.05997866
[3/200] Training loss: 0.05290947
[4/200] Training loss: 0.04999005
[5/200] Training loss: 0.04858281
[6/200] Training loss: 0.04547910
[7/200] Training loss: 0.04266920
[8/200] Training loss: 0.04161504
[9/200] Training loss: 0.04022122
[10/200] Training loss: 0.03915481
[50/200] Training loss: 0.01868162
[100/200] Training loss: 0.01499732
[150/200] Training loss: 0.01329471
[200/200] Training loss: 0.01231276
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13665.793793263529 ----------
[1/200] Training loss: 0.14103668
[2/200] Training loss: 0.05523085
[3/200] Training loss: 0.05254506
[4/200] Training loss: 0.05058446
[5/200] Training loss: 0.04746441
[6/200] Training loss: 0.04525210
[7/200] Training loss: 0.04500057
[8/200] Training loss: 0.04294158
[9/200] Training loss: 0.03952289
[10/200] Training loss: 0.03809726
[50/200] Training loss: 0.01761185
[100/200] Training loss: 0.01522103
[150/200] Training loss: 0.01384255
[200/200] Training loss: 0.01248356
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8990.58707760511 ----------
[1/200] Training loss: 0.14856875
[2/200] Training loss: 0.06047310
[3/200] Training loss: 0.05448831
[4/200] Training loss: 0.05119273
[5/200] Training loss: 0.04926244
[6/200] Training loss: 0.04469135
[7/200] Training loss: 0.04452137
[8/200] Training loss: 0.04260939
[9/200] Training loss: 0.03717451
[10/200] Training loss: 0.03471075
[50/200] Training loss: 0.01867074
[100/200] Training loss: 0.01652115
[150/200] Training loss: 0.01439428
[200/200] Training loss: 0.01388730
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 23918.12166538167 ----------
[1/200] Training loss: 0.15281295
[2/200] Training loss: 0.05948749
[3/200] Training loss: 0.05280240
[4/200] Training loss: 0.04711906
[5/200] Training loss: 0.04279997
[6/200] Training loss: 0.04314750
[7/200] Training loss: 0.03659669
[8/200] Training loss: 0.03557408
[9/200] Training loss: 0.03242006
[10/200] Training loss: 0.03043251
[50/200] Training loss: 0.01737564
[100/200] Training loss: 0.01548858
[150/200] Training loss: 0.01443083
[200/200] Training loss: 0.01363267
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7298.64425766868 ----------
[1/200] Training loss: 0.14981840
[2/200] Training loss: 0.06179215
[3/200] Training loss: 0.05177374
[4/200] Training loss: 0.05029058
[5/200] Training loss: 0.04619840
[6/200] Training loss: 0.04131177
[7/200] Training loss: 0.04131837
[8/200] Training loss: 0.03668856
[9/200] Training loss: 0.03383588
[10/200] Training loss: 0.03259921
[50/200] Training loss: 0.01833993
[100/200] Training loss: 0.01606043
[150/200] Training loss: 0.01396858
[200/200] Training loss: 0.01324465
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6877.633604663743 ----------
[1/200] Training loss: 0.15716445
[2/200] Training loss: 0.05794645
[3/200] Training loss: 0.05134826
[4/200] Training loss: 0.04910725
[5/200] Training loss: 0.04493999
[6/200] Training loss: 0.04362882
[7/200] Training loss: 0.04173080
[8/200] Training loss: 0.04018739
[9/200] Training loss: 0.03614594
[10/200] Training loss: 0.03563724
[50/200] Training loss: 0.01960730
[100/200] Training loss: 0.01530682
[150/200] Training loss: 0.01309334
[200/200] Training loss: 0.01334938
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8025.798901044058 ----------
[1/50] Training loss: 0.16120541
[2/50] Training loss: 0.05849521
[3/50] Training loss: 0.05124958
[4/50] Training loss: 0.04977257
[5/50] Training loss: 0.04716815
[6/50] Training loss: 0.04671551
[7/50] Training loss: 0.04431160
[8/50] Training loss: 0.04188833
[9/50] Training loss: 0.03932161
[10/50] Training loss: 0.03789275
[50/50] Training loss: 0.02022450
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 50
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7167.6615433487095 ----------
[1/200] Training loss: 0.16100085
[2/200] Training loss: 0.06086088
[3/200] Training loss: 0.05257395
[4/200] Training loss: 0.04763921
[5/200] Training loss: 0.04364163
[6/200] Training loss: 0.04159356
[7/200] Training loss: 0.03995377
[8/200] Training loss: 0.03878028
[9/200] Training loss: 0.03790472
[10/200] Training loss: 0.03631050
[50/200] Training loss: 0.02045027
[100/200] Training loss: 0.01813114
[150/200] Training loss: 0.01565040
[200/200] Training loss: 0.01449068
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10913.992853213713 ----------
[1/200] Training loss: 0.16429653
[2/200] Training loss: 0.06088327
[3/200] Training loss: 0.05579564
[4/200] Training loss: 0.05385182
[5/200] Training loss: 0.05219165
[6/200] Training loss: 0.04883993
[7/200] Training loss: 0.04498139
[8/200] Training loss: 0.04371507
[9/200] Training loss: 0.03514855
[10/200] Training loss: 0.03429934
[50/200] Training loss: 0.01819316
[100/200] Training loss: 0.01661002
[150/200] Training loss: 0.01562850
[200/200] Training loss: 0.01452089
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14616.07772283659 ----------
[1/200] Training loss: 0.14378572
[2/200] Training loss: 0.05825072
[3/200] Training loss: 0.05417118
[4/200] Training loss: 0.04815132
[5/200] Training loss: 0.04769650
[6/200] Training loss: 0.04339172
[7/200] Training loss: 0.04141378
[8/200] Training loss: 0.03557705
[9/200] Training loss: 0.03408942
[10/200] Training loss: 0.02991783
[50/200] Training loss: 0.01817023
[100/200] Training loss: 0.01503037
[150/200] Training loss: 0.01354502
[200/200] Training loss: 0.01281571
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 18164.883924759884 ----------
[1/200] Training loss: 0.16014606
[2/200] Training loss: 0.05429573
[3/200] Training loss: 0.05454843
[4/200] Training loss: 0.04800618
[5/200] Training loss: 0.04605711
[6/200] Training loss: 0.04583403
[7/200] Training loss: 0.04329841
[8/200] Training loss: 0.04174497
[9/200] Training loss: 0.04037558
[10/200] Training loss: 0.03886018
[50/200] Training loss: 0.01976344
[100/200] Training loss: 0.01706477
[150/200] Training loss: 0.01534881
[200/200] Training loss: 0.01364808
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16374.82262499353 ----------
[1/200] Training loss: 0.16236677
[2/200] Training loss: 0.06129139
[3/200] Training loss: 0.05501319
[4/200] Training loss: 0.05254093
[5/200] Training loss: 0.04950470
[6/200] Training loss: 0.04850491
[7/200] Training loss: 0.04535477
[8/200] Training loss: 0.04399127
[9/200] Training loss: 0.04138129
[10/200] Training loss: 0.04039186
[50/200] Training loss: 0.01863284
[100/200] Training loss: 0.01537916
[150/200] Training loss: 0.01335517
[200/200] Training loss: 0.01306983
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 19173.98570981005 ----------
[1/200] Training loss: 0.17673070
[2/200] Training loss: 0.06356139
[3/200] Training loss: 0.05578294
[4/200] Training loss: 0.05290813
[5/200] Training loss: 0.05132203
[6/200] Training loss: 0.04995121
[7/200] Training loss: 0.04765226
[8/200] Training loss: 0.04772498
[9/200] Training loss: 0.04529780
[10/200] Training loss: 0.04412570
[50/200] Training loss: 0.02106631
[100/200] Training loss: 0.01661087
[150/200] Training loss: 0.01500058
[200/200] Training loss: 0.01378127
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13262.77075124199 ----------
[1/50] Training loss: 0.15026813
[2/50] Training loss: 0.05617713
[3/50] Training loss: 0.05447134
[4/50] Training loss: 0.05255758
[5/50] Training loss: 0.04864747
[6/50] Training loss: 0.04685936
[7/50] Training loss: 0.04366363
[8/50] Training loss: 0.04187532
[9/50] Training loss: 0.04040214
[10/50] Training loss: 0.03868151
[50/50] Training loss: 0.01722541
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 50
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10945.958889014704 ----------
[1/200] Training loss: 0.16836909
[2/200] Training loss: 0.05861385
[3/200] Training loss: 0.05135822
[4/200] Training loss: 0.04672527
[5/200] Training loss: 0.04069006
[6/200] Training loss: 0.03697244
[7/200] Training loss: 0.03512546
[8/200] Training loss: 0.03240340
[9/200] Training loss: 0.03184182
[10/200] Training loss: 0.02874282
[50/200] Training loss: 0.01812414
[100/200] Training loss: 0.01532464
[150/200] Training loss: 0.01364826
[200/200] Training loss: 0.01330625
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7122.105025903507 ----------
[1/200] Training loss: 0.16585203
[2/200] Training loss: 0.06076930
[3/200] Training loss: 0.05465429
[4/200] Training loss: 0.05141372
[5/200] Training loss: 0.04863361
[6/200] Training loss: 0.04774148
[7/200] Training loss: 0.04726127
[8/200] Training loss: 0.04305761
[9/200] Training loss: 0.03970820
[10/200] Training loss: 0.03938200
[50/200] Training loss: 0.01750032
[100/200] Training loss: 0.01628380
[150/200] Training loss: 0.01454308
[200/200] Training loss: 0.01338455
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13593.401928877112 ----------
[1/200] Training loss: 0.16592777
[2/200] Training loss: 0.05937494
[3/200] Training loss: 0.05557322
[4/200] Training loss: 0.05369581
[5/200] Training loss: 0.05277890
[6/200] Training loss: 0.04825975
[7/200] Training loss: 0.04499321
[8/200] Training loss: 0.04196676
[9/200] Training loss: 0.03830904
[10/200] Training loss: 0.04012968
[50/200] Training loss: 0.02083214
[100/200] Training loss: 0.01793314
[150/200] Training loss: 0.01582424
[200/200] Training loss: 0.01357302
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7959.134626327162 ----------
[1/200] Training loss: 0.15443374
[2/200] Training loss: 0.05643192
[3/200] Training loss: 0.05010886
[4/200] Training loss: 0.04653322
[5/200] Training loss: 0.04533613
[6/200] Training loss: 0.04567821
[7/200] Training loss: 0.03984597
[8/200] Training loss: 0.03937796
[9/200] Training loss: 0.03828522
[10/200] Training loss: 0.03676155
[50/200] Training loss: 0.01753400
[100/200] Training loss: 0.01422251
[150/200] Training loss: 0.01310413
[200/200] Training loss: 0.01255360
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13325.626439308586 ----------
[1/200] Training loss: 0.15058050
[2/200] Training loss: 0.05810311
[3/200] Training loss: 0.05281240
[4/200] Training loss: 0.05139060
[5/200] Training loss: 0.04570427
[6/200] Training loss: 0.04081910
[7/200] Training loss: 0.04068452
[8/200] Training loss: 0.03348983
[9/200] Training loss: 0.03349704
[10/200] Training loss: 0.03337635
[50/200] Training loss: 0.01939615
[100/200] Training loss: 0.01604897
[150/200] Training loss: 0.01439875
[200/200] Training loss: 0.01252038
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9763.30435866874 ----------
[1/200] Training loss: 0.16387274
[2/200] Training loss: 0.05934790
[3/200] Training loss: 0.05201722
[4/200] Training loss: 0.04730944
[5/200] Training loss: 0.04754426
[6/200] Training loss: 0.04669818
[7/200] Training loss: 0.03981082
[8/200] Training loss: 0.03981963
[9/200] Training loss: 0.03542728
[10/200] Training loss: 0.03536726
[50/200] Training loss: 0.01845467
[100/200] Training loss: 0.01491590
[150/200] Training loss: 0.01415769
[200/200] Training loss: 0.01266917
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4942.915738711313 ----------
[1/200] Training loss: 0.13125503
[2/200] Training loss: 0.05493780
[3/200] Training loss: 0.04944233
[4/200] Training loss: 0.04420731
[5/200] Training loss: 0.04375895
[6/200] Training loss: 0.03846540
[7/200] Training loss: 0.03728297
[8/200] Training loss: 0.03775237
[9/200] Training loss: 0.03374999
[10/200] Training loss: 0.03330822
[50/200] Training loss: 0.01598399
[100/200] Training loss: 0.01401165
[150/200] Training loss: 0.01337413
[200/200] Training loss: 0.01157479
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 22329.41593503959 ----------
[1/200] Training loss: 0.16497245
[2/200] Training loss: 0.05984973
[3/200] Training loss: 0.05246002
[4/200] Training loss: 0.04947247
[5/200] Training loss: 0.04693411
[6/200] Training loss: 0.04390165
[7/200] Training loss: 0.04033369
[8/200] Training loss: 0.03948855
[9/200] Training loss: 0.03748319
[10/200] Training loss: 0.03522831
[50/200] Training loss: 0.01989428
[100/200] Training loss: 0.01676028
[150/200] Training loss: 0.01537236
[200/200] Training loss: 0.01469574
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8066.651597782068 ----------
[1/200] Training loss: 0.16761684
[2/200] Training loss: 0.06001007
[3/200] Training loss: 0.05223421
[4/200] Training loss: 0.04909651
[5/200] Training loss: 0.04547987
[6/200] Training loss: 0.04331580
[7/200] Training loss: 0.03921952
[8/200] Training loss: 0.03677478
[9/200] Training loss: 0.03723082
[10/200] Training loss: 0.03551994
[50/200] Training loss: 0.01934130
[100/200] Training loss: 0.01647099
[150/200] Training loss: 0.01546834
[200/200] Training loss: 0.01384294
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5894.716617446508 ----------
[1/200] Training loss: 0.13927087
[2/200] Training loss: 0.05362847
[3/200] Training loss: 0.05144478
[4/200] Training loss: 0.04554415
[5/200] Training loss: 0.04110659
[6/200] Training loss: 0.03944073
[7/200] Training loss: 0.03797840
[8/200] Training loss: 0.03705141
[9/200] Training loss: 0.03624522
[10/200] Training loss: 0.03447399
[50/200] Training loss: 0.02033145
[100/200] Training loss: 0.01787441
[150/200] Training loss: 0.01493463
[200/200] Training loss: 0.01360360
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5207.721382716245 ----------
[1/50] Training loss: 0.17027149
[2/50] Training loss: 0.05687051
[3/50] Training loss: 0.05278494
[4/50] Training loss: 0.05250224
[5/50] Training loss: 0.04917540
[6/50] Training loss: 0.04651292
[7/50] Training loss: 0.04390679
[8/50] Training loss: 0.03899414
[9/50] Training loss: 0.03836071
[10/50] Training loss: 0.03675167
[50/50] Training loss: 0.01710797
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 50
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 18508.533815513318 ----------
[1/200] Training loss: 0.17066079
[2/200] Training loss: 0.05966027
[3/200] Training loss: 0.05614019
[4/200] Training loss: 0.05279292
[5/200] Training loss: 0.05023165
[6/200] Training loss: 0.04633493
[7/200] Training loss: 0.04277469
[8/200] Training loss: 0.04042720
[9/200] Training loss: 0.03641674
[10/200] Training loss: 0.03322241
[50/200] Training loss: 0.01709041
[100/200] Training loss: 0.01466164
[150/200] Training loss: 0.01327550
[200/200] Training loss: 0.01243833
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 19266.578731056536 ----------
[1/200] Training loss: 0.16585211
[2/200] Training loss: 0.05982482
[3/200] Training loss: 0.05361372
[4/200] Training loss: 0.05178773
[5/200] Training loss: 0.04874245
[6/200] Training loss: 0.04719858
[7/200] Training loss: 0.04333426
[8/200] Training loss: 0.04387382
[9/200] Training loss: 0.04163576
[10/200] Training loss: 0.03639840
[50/200] Training loss: 0.01767827
[100/200] Training loss: 0.01581288
[150/200] Training loss: 0.01431556
[200/200] Training loss: 0.01361953
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 24860.625253601327 ----------
[1/50] Training loss: 0.17089616
[2/50] Training loss: 0.05925974
[3/50] Training loss: 0.05694675
[4/50] Training loss: 0.05048252
[5/50] Training loss: 0.05068699
[6/50] Training loss: 0.05081562
[7/50] Training loss: 0.04492438
[8/50] Training loss: 0.04399331
[9/50] Training loss: 0.04176959
[10/50] Training loss: 0.03935741
[50/50] Training loss: 0.01834022
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 50
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10030.728787082222 ----------
[1/200] Training loss: 0.15930742
[2/200] Training loss: 0.06268219
[3/200] Training loss: 0.05316381
[4/200] Training loss: 0.05206783
[5/200] Training loss: 0.04873462
[6/200] Training loss: 0.04747926
[7/200] Training loss: 0.04535835
[8/200] Training loss: 0.04254557
[9/200] Training loss: 0.04155045
[10/200] Training loss: 0.03730270
[50/200] Training loss: 0.01794687
[100/200] Training loss: 0.01547072
[150/200] Training loss: 0.01467688
[200/200] Training loss: 0.01339912
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8750.128227631867 ----------
[1/100] Training loss: 0.15744773
[2/100] Training loss: 0.06108471
[3/100] Training loss: 0.05270012
[4/100] Training loss: 0.05194605
[5/100] Training loss: 0.04798419
[6/100] Training loss: 0.04591777
[7/100] Training loss: 0.04560476
[8/100] Training loss: 0.04309031
[9/100] Training loss: 0.04302692
[10/100] Training loss: 0.03907022
[50/100] Training loss: 0.01896667
[100/100] Training loss: 0.01709633
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12656.34323175537 ----------
[1/200] Training loss: 0.15785575
[2/200] Training loss: 0.05933873
[3/200] Training loss: 0.05213284
[4/200] Training loss: 0.04658758
[5/200] Training loss: 0.04384539
[6/200] Training loss: 0.04473556
[7/200] Training loss: 0.03954990
[8/200] Training loss: 0.03939595
[9/200] Training loss: 0.03714865
[10/200] Training loss: 0.03646740
[50/200] Training loss: 0.01766774
[100/200] Training loss: 0.01486154
[150/200] Training loss: 0.01406349
[200/200] Training loss: 0.01321820
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6907.703525774684 ----------
[1/50] Training loss: 0.16186926
[2/50] Training loss: 0.05647842
[3/50] Training loss: 0.05225977
[4/50] Training loss: 0.04791716
[5/50] Training loss: 0.04637293
[6/50] Training loss: 0.04205982
[7/50] Training loss: 0.04318576
[8/50] Training loss: 0.03824866
[9/50] Training loss: 0.03752371
[10/50] Training loss: 0.03595417
[50/50] Training loss: 0.01785741
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 50
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12823.411714516538 ----------
[1/200] Training loss: 0.15568377
[2/200] Training loss: 0.05572469
[3/200] Training loss: 0.04902393
[4/200] Training loss: 0.04334687
[5/200] Training loss: 0.04182643
[6/200] Training loss: 0.04064378
[7/200] Training loss: 0.03985580
[8/200] Training loss: 0.03532110
[9/200] Training loss: 0.03382282
[10/200] Training loss: 0.03263166
[50/200] Training loss: 0.01864515
[100/200] Training loss: 0.01703636
[150/200] Training loss: 0.01558415
[200/200] Training loss: 0.01458920
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12472.814598157065 ----------
[1/200] Training loss: 0.16905202
[2/200] Training loss: 0.06903414
[3/200] Training loss: 0.05443276
[4/200] Training loss: 0.05240661
[5/200] Training loss: 0.05073624
[6/200] Training loss: 0.04692194
[7/200] Training loss: 0.04644690
[8/200] Training loss: 0.04460566
[9/200] Training loss: 0.04031086
[10/200] Training loss: 0.03839618
[50/200] Training loss: 0.01770078
[100/200] Training loss: 0.01607895
[150/200] Training loss: 0.01481341
[200/200] Training loss: 0.01352452
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12038.700262071483 ----------
[1/50] Training loss: 0.15511274
[2/50] Training loss: 0.05679338
[3/50] Training loss: 0.04954867
[4/50] Training loss: 0.04667471
[5/50] Training loss: 0.04130909
[6/50] Training loss: 0.03869134
[7/50] Training loss: 0.03602791
[8/50] Training loss: 0.03610955
[9/50] Training loss: 0.03300348
[10/50] Training loss: 0.03225931
[50/50] Training loss: 0.01696743
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 50
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13163.918565533593 ----------
[1/200] Training loss: 0.17187800
[2/200] Training loss: 0.06636553
[3/200] Training loss: 0.05532797
[4/200] Training loss: 0.05280069
[5/200] Training loss: 0.05225977
[6/200] Training loss: 0.04889855
[7/200] Training loss: 0.04806813
[8/200] Training loss: 0.04468112
[9/200] Training loss: 0.04374237
[10/200] Training loss: 0.04199924
[50/200] Training loss: 0.01799320
[100/200] Training loss: 0.01512727
[150/200] Training loss: 0.01378281
[200/200] Training loss: 0.01300483
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 24941.638117814153 ----------
[1/200] Training loss: 0.14176197
[2/200] Training loss: 0.05907294
[3/200] Training loss: 0.05409708
[4/200] Training loss: 0.05082720
[5/200] Training loss: 0.04412910
[6/200] Training loss: 0.04350670
[7/200] Training loss: 0.03921689
[8/200] Training loss: 0.03603160
[9/200] Training loss: 0.03361133
[10/200] Training loss: 0.03401225
[50/200] Training loss: 0.01769034
[100/200] Training loss: 0.01605543
[150/200] Training loss: 0.01429822
[200/200] Training loss: 0.01231404
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9680.811536229801 ----------
[1/200] Training loss: 0.14440191
[2/200] Training loss: 0.05590326
[3/200] Training loss: 0.05046700
[4/200] Training loss: 0.04476745
[5/200] Training loss: 0.03938628
[6/200] Training loss: 0.03768452
[7/200] Training loss: 0.03417452
[8/200] Training loss: 0.03235813
[9/200] Training loss: 0.03327487
[10/200] Training loss: 0.02857427
[50/200] Training loss: 0.01743893
[100/200] Training loss: 0.01410208
[150/200] Training loss: 0.01245893
[200/200] Training loss: 0.01170577
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8944.432011033456 ----------
[1/200] Training loss: 0.17014768
[2/200] Training loss: 0.06381858
[3/200] Training loss: 0.05479643
[4/200] Training loss: 0.05595982
[5/200] Training loss: 0.04912929
[6/200] Training loss: 0.04828010
[7/200] Training loss: 0.04536518
[8/200] Training loss: 0.04441454
[9/200] Training loss: 0.04279348
[10/200] Training loss: 0.04016548
[50/200] Training loss: 0.01825642
[100/200] Training loss: 0.01620068
[150/200] Training loss: 0.01480824
[200/200] Training loss: 0.01346464
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11482.947356841796 ----------
[1/50] Training loss: 0.16811238
[2/50] Training loss: 0.06425000
[3/50] Training loss: 0.05671923
[4/50] Training loss: 0.05291362
[5/50] Training loss: 0.05105267
[6/50] Training loss: 0.04753561
[7/50] Training loss: 0.04742827
[8/50] Training loss: 0.04590811
[9/50] Training loss: 0.04397030
[10/50] Training loss: 0.04264717
[50/50] Training loss: 0.01953155
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 50
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 23280.55944344981 ----------
[1/200] Training loss: 0.16282416
[2/200] Training loss: 0.05405950
[3/200] Training loss: 0.05050070
[4/200] Training loss: 0.04563577
[5/200] Training loss: 0.04406242
[6/200] Training loss: 0.03938624
[7/200] Training loss: 0.03897563
[8/200] Training loss: 0.03639526
[9/200] Training loss: 0.03451000
[10/200] Training loss: 0.03580511
[50/200] Training loss: 0.01877828
[100/200] Training loss: 0.01564700
[150/200] Training loss: 0.01484277
[200/200] Training loss: 0.01364935
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 28700.372123023073 ----------
[1/200] Training loss: 0.16899497
[2/200] Training loss: 0.06138239
[3/200] Training loss: 0.05398261
[4/200] Training loss: 0.05020244
[5/200] Training loss: 0.04278572
[6/200] Training loss: 0.03881710
[7/200] Training loss: 0.03713234
[8/200] Training loss: 0.03595012
[9/200] Training loss: 0.03598912
[10/200] Training loss: 0.03427855
[50/200] Training loss: 0.01832456
[100/200] Training loss: 0.01507508
[150/200] Training loss: 0.01436309
[200/200] Training loss: 0.01338972
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7960.958233780655 ----------
[1/200] Training loss: 0.15418767
[2/200] Training loss: 0.06067977
[3/200] Training loss: 0.05544066
[4/200] Training loss: 0.05191219
[5/200] Training loss: 0.05051812
[6/200] Training loss: 0.04753395
[7/200] Training loss: 0.04659662
[8/200] Training loss: 0.04625105
[9/200] Training loss: 0.03996558
[10/200] Training loss: 0.03967688
[50/200] Training loss: 0.01744322
[100/200] Training loss: 0.01432319
[150/200] Training loss: 0.01399595
[200/200] Training loss: 0.01224995
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13560.84717117629 ----------
[1/150] Training loss: 0.15330903
[2/150] Training loss: 0.06321297
[3/150] Training loss: 0.05011801
[4/150] Training loss: 0.04682224
[5/150] Training loss: 0.04190169
[6/150] Training loss: 0.04263320
[7/150] Training loss: 0.03792707
[8/150] Training loss: 0.03522670
[9/150] Training loss: 0.03287429
[10/150] Training loss: 0.03266887
[50/150] Training loss: 0.01823651
[100/150] Training loss: 0.01662954
[150/150] Training loss: 0.01447485
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 150
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10356.694453347554 ----------
[1/200] Training loss: 0.16284814
[2/200] Training loss: 0.05618954
[3/200] Training loss: 0.04888329
[4/200] Training loss: 0.04653982
[5/200] Training loss: 0.04333477
[6/200] Training loss: 0.03969447
[7/200] Training loss: 0.03660527
[8/200] Training loss: 0.03500411
[9/200] Training loss: 0.03644399
[10/200] Training loss: 0.03381473
[50/200] Training loss: 0.01871986
[100/200] Training loss: 0.01626342
[150/200] Training loss: 0.01449098
[200/200] Training loss: 0.01420343
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7892.261272917921 ----------
[1/200] Training loss: 0.15066033
[2/200] Training loss: 0.06357721
[3/200] Training loss: 0.05673702
[4/200] Training loss: 0.05344331
[5/200] Training loss: 0.05232450
[6/200] Training loss: 0.05361876
[7/200] Training loss: 0.05043930
[8/200] Training loss: 0.05052571
[9/200] Training loss: 0.04884481
[10/200] Training loss: 0.04772175
[50/200] Training loss: 0.01972561
[100/200] Training loss: 0.01561791
[150/200] Training loss: 0.01431030
[200/200] Training loss: 0.01329235
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9021.740851964216 ----------
[1/200] Training loss: 0.18855956
[2/200] Training loss: 0.05988497
[3/200] Training loss: 0.05493456
[4/200] Training loss: 0.05163640
[5/200] Training loss: 0.05040221
[6/200] Training loss: 0.04848987
[7/200] Training loss: 0.04629091
[8/200] Training loss: 0.04469204
[9/200] Training loss: 0.04248060
[10/200] Training loss: 0.03986232
[50/200] Training loss: 0.01854975
[100/200] Training loss: 0.01587965
[150/200] Training loss: 0.01530649
[200/200] Training loss: 0.01340738
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18328.834551056432 ----------
[1/200] Training loss: 0.15244891
[2/200] Training loss: 0.05849616
[3/200] Training loss: 0.05339676
[4/200] Training loss: 0.04688155
[5/200] Training loss: 0.04271960
[6/200] Training loss: 0.03995481
[7/200] Training loss: 0.03896985
[8/200] Training loss: 0.04043964
[9/200] Training loss: 0.03767997
[10/200] Training loss: 0.03540607
[50/200] Training loss: 0.01791102
[100/200] Training loss: 0.01636106
[150/200] Training loss: 0.01490837
[200/200] Training loss: 0.01404871
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6488.7416345544225 ----------
[1/200] Training loss: 0.16135013
[2/200] Training loss: 0.05762634
[3/200] Training loss: 0.04779008
[4/200] Training loss: 0.04779441
[5/200] Training loss: 0.04365969
[6/200] Training loss: 0.04085813
[7/200] Training loss: 0.04142052
[8/200] Training loss: 0.03768640
[9/200] Training loss: 0.03575231
[10/200] Training loss: 0.03459877
[50/200] Training loss: 0.01725098
[100/200] Training loss: 0.01575745
[150/200] Training loss: 0.01375495
[200/200] Training loss: 0.01289845
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9874.802681572934 ----------
[1/200] Training loss: 0.14421483
[2/200] Training loss: 0.05975374
[3/200] Training loss: 0.05440950
[4/200] Training loss: 0.05105159
[5/200] Training loss: 0.04787972
[6/200] Training loss: 0.04630241
[7/200] Training loss: 0.04312652
[8/200] Training loss: 0.04489510
[9/200] Training loss: 0.04258051
[10/200] Training loss: 0.03817949
[50/200] Training loss: 0.01852366
[100/200] Training loss: 0.01513701
[150/200] Training loss: 0.01377033
[200/200] Training loss: 0.01221188
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5455817458303548 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13238.434952818252 ----------
[1/200] Training loss: 0.15506421
[2/200] Training loss: 0.06648078
[3/200] Training loss: 0.05394287
[4/200] Training loss: 0.05317078
[5/200] Training loss: 0.05003182
[6/200] Training loss: 0.04890037
[7/200] Training loss: 0.04631051
[8/200] Training loss: 0.04400108
[9/200] Training loss: 0.04349406
[10/200] Training loss: 0.04010616
[50/200] Training loss: 0.01860330
[100/200] Training loss: 0.01536994
[150/200] Training loss: 0.01388070
[200/200] Training loss: 0.01321325
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9611.57052723435 ----------
[1/200] Training loss: 0.17103154
[2/200] Training loss: 0.06386250
[3/200] Training loss: 0.05723962
[4/200] Training loss: 0.05184536
[5/200] Training loss: 0.05071588
[6/200] Training loss: 0.04786834
[7/200] Training loss: 0.04733223
[8/200] Training loss: 0.04487256
[9/200] Training loss: 0.04395797
[10/200] Training loss: 0.04094166
[50/200] Training loss: 0.01872268
[100/200] Training loss: 0.01548646
[150/200] Training loss: 0.01402047
[200/200] Training loss: 0.01312323
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6624.390990876067 ----------
[1/200] Training loss: 0.13703843
[2/200] Training loss: 0.05928408
[3/200] Training loss: 0.05248360
[4/200] Training loss: 0.04770962
[5/200] Training loss: 0.04568140
[6/200] Training loss: 0.04303796
[7/200] Training loss: 0.04354723
[8/200] Training loss: 0.04135185
[9/200] Training loss: 0.03657300
[10/200] Training loss: 0.03661470
[50/200] Training loss: 0.01749538
[100/200] Training loss: 0.01456277
[150/200] Training loss: 0.01283858
[200/200] Training loss: 0.01175055
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8049.0078891749135 ----------
[1/200] Training loss: 0.17258611
[2/200] Training loss: 0.05684814
[3/200] Training loss: 0.05420661
[4/200] Training loss: 0.05062322
[5/200] Training loss: 0.04575665
[6/200] Training loss: 0.04412383
[7/200] Training loss: 0.04278694
[8/200] Training loss: 0.04010041
[9/200] Training loss: 0.03964898
[10/200] Training loss: 0.03735827
[50/200] Training loss: 0.01979870
[100/200] Training loss: 0.01655014
[150/200] Training loss: 0.01472700
[200/200] Training loss: 0.01368295
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5664.300486379585 ----------
[1/200] Training loss: 0.17561957
[2/200] Training loss: 0.06478104
[3/200] Training loss: 0.05445740
[4/200] Training loss: 0.05371889
[5/200] Training loss: 0.04834568
[6/200] Training loss: 0.04562835
[7/200] Training loss: 0.04489037
[8/200] Training loss: 0.04351618
[9/200] Training loss: 0.03864625
[10/200] Training loss: 0.03708199
[50/200] Training loss: 0.02047958
[100/200] Training loss: 0.01590874
[150/200] Training loss: 0.01393712
[200/200] Training loss: 0.01263518
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10070.198011955872 ----------
[1/200] Training loss: 0.13909894
[2/200] Training loss: 0.05762669
[3/200] Training loss: 0.05447051
[4/200] Training loss: 0.05307782
[5/200] Training loss: 0.04909098
[6/200] Training loss: 0.04813276
[7/200] Training loss: 0.04548802
[8/200] Training loss: 0.04487233
[9/200] Training loss: 0.04320100
[10/200] Training loss: 0.03986860
[50/200] Training loss: 0.01880563
[100/200] Training loss: 0.01536420
[150/200] Training loss: 0.01306100
[200/200] Training loss: 0.01201664
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9836.82997718269 ----------
[1/200] Training loss: 0.14757553
[2/200] Training loss: 0.05499370
[3/200] Training loss: 0.05420784
[4/200] Training loss: 0.04801646
[5/200] Training loss: 0.04479061
[6/200] Training loss: 0.04539808
[7/200] Training loss: 0.04272377
[8/200] Training loss: 0.03718715
[9/200] Training loss: 0.03804003
[10/200] Training loss: 0.03760726
[50/200] Training loss: 0.01848670
[100/200] Training loss: 0.01610032
[150/200] Training loss: 0.01429091
[200/200] Training loss: 0.01309433
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22884.034609307862 ----------
[1/200] Training loss: 0.16572953
[2/200] Training loss: 0.06280245
[3/200] Training loss: 0.05162241
[4/200] Training loss: 0.04787338
[5/200] Training loss: 0.04490577
[6/200] Training loss: 0.04216963
[7/200] Training loss: 0.03792284
[8/200] Training loss: 0.03492117
[9/200] Training loss: 0.03326678
[10/200] Training loss: 0.03033671
[50/200] Training loss: 0.01678282
[100/200] Training loss: 0.01379664
[150/200] Training loss: 0.01324092
[200/200] Training loss: 0.01227037
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8341.848236452159 ----------
[1/200] Training loss: 0.17766837
[2/200] Training loss: 0.05845306
[3/200] Training loss: 0.05341787
[4/200] Training loss: 0.05102645
[5/200] Training loss: 0.04561629
[6/200] Training loss: 0.04653910
[7/200] Training loss: 0.04081802
[8/200] Training loss: 0.03678003
[9/200] Training loss: 0.03465762
[10/200] Training loss: 0.03208441
[50/200] Training loss: 0.01638504
[100/200] Training loss: 0.01470444
[150/200] Training loss: 0.01299672
[200/200] Training loss: 0.01223224
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10625.709199860497 ----------
[1/200] Training loss: 0.17023469
[2/200] Training loss: 0.05534373
[3/200] Training loss: 0.04745257
[4/200] Training loss: 0.04485293
[5/200] Training loss: 0.03701912
[6/200] Training loss: 0.03343003
[7/200] Training loss: 0.03214994
[8/200] Training loss: 0.03254048
[9/200] Training loss: 0.02911634
[10/200] Training loss: 0.02678410
[50/200] Training loss: 0.01812215
[100/200] Training loss: 0.01611421
[150/200] Training loss: 0.01526233
[200/200] Training loss: 0.01403237
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7363.012426989378 ----------
[1/200] Training loss: 0.14479785
[2/200] Training loss: 0.05709583
[3/200] Training loss: 0.05377139
[4/200] Training loss: 0.05027984
[5/200] Training loss: 0.04755363
[6/200] Training loss: 0.04584071
[7/200] Training loss: 0.04257802
[8/200] Training loss: 0.03894091
[9/200] Training loss: 0.03533650
[10/200] Training loss: 0.03355785
[50/200] Training loss: 0.01704980
[100/200] Training loss: 0.01459655
[150/200] Training loss: 0.01318505
[200/200] Training loss: 0.01246851
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11223.318582308888 ----------
[1/200] Training loss: 0.18920369
[2/200] Training loss: 0.06352307
[3/200] Training loss: 0.05528487
[4/200] Training loss: 0.04944330
[5/200] Training loss: 0.04593984
[6/200] Training loss: 0.04131006
[7/200] Training loss: 0.03985490
[8/200] Training loss: 0.04087017
[9/200] Training loss: 0.03738045
[10/200] Training loss: 0.03187100
[50/200] Training loss: 0.01797049
[100/200] Training loss: 0.01630414
[150/200] Training loss: 0.01448551
[200/200] Training loss: 0.01431942
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8299.951807089003 ----------
[1/200] Training loss: 0.18073521
[2/200] Training loss: 0.05993394
[3/200] Training loss: 0.05723227
[4/200] Training loss: 0.05049292
[5/200] Training loss: 0.05046433
[6/200] Training loss: 0.04582167
[7/200] Training loss: 0.04554287
[8/200] Training loss: 0.04391513
[9/200] Training loss: 0.04055080
[10/200] Training loss: 0.03760413
[50/200] Training loss: 0.01819841
[100/200] Training loss: 0.01639031
[150/200] Training loss: 0.01449166
[200/200] Training loss: 0.01396019
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 22226.712577437087 ----------
[1/200] Training loss: 0.16248933
[2/200] Training loss: 0.05904338
[3/200] Training loss: 0.04842879
[4/200] Training loss: 0.04883328
[5/200] Training loss: 0.04234479
[6/200] Training loss: 0.04160910
[7/200] Training loss: 0.03763799
[8/200] Training loss: 0.03570793
[9/200] Training loss: 0.03620628
[10/200] Training loss: 0.03568634
[50/200] Training loss: 0.02084656
[100/200] Training loss: 0.01526098
[150/200] Training loss: 0.01396503
[200/200] Training loss: 0.01292060
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.48009581562814674 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4435.71775477205 ----------
[1/200] Training loss: 0.15524373
[2/200] Training loss: 0.05405608
[3/200] Training loss: 0.05061900
[4/200] Training loss: 0.04878890
[5/200] Training loss: 0.04566639
[6/200] Training loss: 0.04116551
[7/200] Training loss: 0.03910070
[8/200] Training loss: 0.03973119
[9/200] Training loss: 0.03743754
[10/200] Training loss: 0.03528357
[50/200] Training loss: 0.01790113
[100/200] Training loss: 0.01466982
[150/200] Training loss: 0.01360313
[200/200] Training loss: 0.01260689
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11958.118246613887 ----------
[1/200] Training loss: 0.16414556
[2/200] Training loss: 0.05716205
[3/200] Training loss: 0.04997250
[4/200] Training loss: 0.04589223
[5/200] Training loss: 0.04246634
[6/200] Training loss: 0.04212614
[7/200] Training loss: 0.03994616
[8/200] Training loss: 0.03828464
[9/200] Training loss: 0.03547586
[10/200] Training loss: 0.03313187
[50/200] Training loss: 0.01761356
[100/200] Training loss: 0.01480072
[150/200] Training loss: 0.01341920
[200/200] Training loss: 0.01277199
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4297.486009284963 ----------
[1/200] Training loss: 0.12576563
[2/200] Training loss: 0.05658531
[3/200] Training loss: 0.04899138
[4/200] Training loss: 0.04787702
[5/200] Training loss: 0.04223428
[6/200] Training loss: 0.04380341
[7/200] Training loss: 0.04087314
[8/200] Training loss: 0.03705116
[9/200] Training loss: 0.03542415
[10/200] Training loss: 0.03234926
[50/200] Training loss: 0.01586645
[100/200] Training loss: 0.01431098
[150/200] Training loss: 0.01333573
[200/200] Training loss: 0.01270105
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14786.769762189442 ----------
[1/200] Training loss: 0.18189643
[2/200] Training loss: 0.06203843
[3/200] Training loss: 0.05426072
[4/200] Training loss: 0.05307720
[5/200] Training loss: 0.04568455
[6/200] Training loss: 0.04598725
[7/200] Training loss: 0.04365108
[8/200] Training loss: 0.04048913
[9/200] Training loss: 0.03856772
[10/200] Training loss: 0.03489757
[50/200] Training loss: 0.01795795
[100/200] Training loss: 0.01407386
[150/200] Training loss: 0.01204320
[200/200] Training loss: 0.01141897
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13668.527353010639 ----------
[1/200] Training loss: 0.13515138
[2/200] Training loss: 0.05544141
[3/200] Training loss: 0.05035803
[4/200] Training loss: 0.04833569
[5/200] Training loss: 0.04548604
[6/200] Training loss: 0.04243384
[7/200] Training loss: 0.04060159
[8/200] Training loss: 0.03604659
[9/200] Training loss: 0.03561771
[10/200] Training loss: 0.03594219
[50/200] Training loss: 0.01714223
[100/200] Training loss: 0.01463131
[150/200] Training loss: 0.01263843
[200/200] Training loss: 0.01124612
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4650.047956741952 ----------
[1/200] Training loss: 0.16414855
[2/200] Training loss: 0.05754931
[3/200] Training loss: 0.05039791
[4/200] Training loss: 0.04970986
[5/200] Training loss: 0.04506396
[6/200] Training loss: 0.04238014
[7/200] Training loss: 0.03929550
[8/200] Training loss: 0.03742589
[9/200] Training loss: 0.03600200
[10/200] Training loss: 0.03276805
[50/200] Training loss: 0.01727710
[100/200] Training loss: 0.01507355
[150/200] Training loss: 0.01353684
[200/200] Training loss: 0.01264370
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14492.267179430553 ----------
[1/200] Training loss: 0.16746413
[2/200] Training loss: 0.05705184
[3/200] Training loss: 0.05421811
[4/200] Training loss: 0.04973886
[5/200] Training loss: 0.04866895
[6/200] Training loss: 0.04574825
[7/200] Training loss: 0.04129652
[8/200] Training loss: 0.04010180
[9/200] Training loss: 0.03829394
[10/200] Training loss: 0.03519680
[50/200] Training loss: 0.01799817
[100/200] Training loss: 0.01595608
[150/200] Training loss: 0.01457153
[200/200] Training loss: 0.01286993
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10974.024603580949 ----------
[1/200] Training loss: 0.16200367
[2/200] Training loss: 0.06114238
[3/200] Training loss: 0.05387142
[4/200] Training loss: 0.05004474
[5/200] Training loss: 0.04636898
[6/200] Training loss: 0.04637780
[7/200] Training loss: 0.04340947
[8/200] Training loss: 0.04247022
[9/200] Training loss: 0.03931935
[10/200] Training loss: 0.03839036
[50/200] Training loss: 0.01885956
[100/200] Training loss: 0.01658236
[150/200] Training loss: 0.01474543
[200/200] Training loss: 0.01291302
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9101.336605136632 ----------
[1/200] Training loss: 0.17987157
[2/200] Training loss: 0.06181801
[3/200] Training loss: 0.05598940
[4/200] Training loss: 0.04893135
[5/200] Training loss: 0.05073582
[6/200] Training loss: 0.04569266
[7/200] Training loss: 0.04378556
[8/200] Training loss: 0.04319060
[9/200] Training loss: 0.04223865
[10/200] Training loss: 0.03824295
[50/200] Training loss: 0.01897225
[100/200] Training loss: 0.01634841
[150/200] Training loss: 0.01509876
[200/200] Training loss: 0.01417342
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6153.083779699412 ----------
[1/200] Training loss: 0.17599623
[2/200] Training loss: 0.06318201
[3/200] Training loss: 0.05336266
[4/200] Training loss: 0.04833082
[5/200] Training loss: 0.04491769
[6/200] Training loss: 0.03952158
[7/200] Training loss: 0.03915662
[8/200] Training loss: 0.03639602
[9/200] Training loss: 0.03441780
[10/200] Training loss: 0.03414709
[50/200] Training loss: 0.01724380
[100/200] Training loss: 0.01472586
[150/200] Training loss: 0.01246314
[200/200] Training loss: 0.01246005
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12616.30056712347 ----------
[1/150] Training loss: 0.17072912
[2/150] Training loss: 0.06040024
[3/150] Training loss: 0.05876008
[4/150] Training loss: 0.05105885
[5/150] Training loss: 0.05024616
[6/150] Training loss: 0.04917652
[7/150] Training loss: 0.04741207
[8/150] Training loss: 0.04696283
[9/150] Training loss: 0.04126154
[10/150] Training loss: 0.03869626
[50/150] Training loss: 0.01888547
[100/150] Training loss: 0.01610106
[150/150] Training loss: 0.01446017
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 150
---hidden_dim---: 80 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8744.497698553074 ----------
[1/200] Training loss: 0.15825533
[2/200] Training loss: 0.05545327
[3/200] Training loss: 0.04776948
[4/200] Training loss: 0.04503308
[5/200] Training loss: 0.04267760
[6/200] Training loss: 0.04098454
[7/200] Training loss: 0.03602379
[8/200] Training loss: 0.03562970
[9/200] Training loss: 0.03573848
[10/200] Training loss: 0.03234088
[50/200] Training loss: 0.01732004
[100/200] Training loss: 0.01461386
[150/200] Training loss: 0.01325898
[200/200] Training loss: 0.01261236
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.5563728816518204 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8554.485139387409 ----------
[1/200] Training loss: 0.15373407
[2/200] Training loss: 0.05948802
[3/200] Training loss: 0.05066695
[4/200] Training loss: 0.05021407
[5/200] Training loss: 0.04647416
[6/200] Training loss: 0.04599932
[7/200] Training loss: 0.04045611
[8/200] Training loss: 0.04026951
[9/200] Training loss: 0.04011847
[10/200] Training loss: 0.03613189
[50/200] Training loss: 0.01822879
[100/200] Training loss: 0.01522388
[150/200] Training loss: 0.01417128
[200/200] Training loss: 0.01310687
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13232.903233984596 ----------
[1/200] Training loss: 0.17625953
[2/200] Training loss: 0.06321902
[3/200] Training loss: 0.05490175
[4/200] Training loss: 0.05045019
[5/200] Training loss: 0.04913597
[6/200] Training loss: 0.04633746
[7/200] Training loss: 0.04427477
[8/200] Training loss: 0.04228478
[9/200] Training loss: 0.03982415
[10/200] Training loss: 0.03622899
[50/200] Training loss: 0.01800497
[100/200] Training loss: 0.01486334
[150/200] Training loss: 0.01399523
[200/200] Training loss: 0.01321119
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5408.77842030897 ----------
[1/200] Training loss: 0.15982392
[2/200] Training loss: 0.06052408
[3/200] Training loss: 0.05525932
[4/200] Training loss: 0.05593895
[5/200] Training loss: 0.05000623
[6/200] Training loss: 0.04840717
[7/200] Training loss: 0.04874116
[8/200] Training loss: 0.04578063
[9/200] Training loss: 0.04584879
[10/200] Training loss: 0.04354567
[50/200] Training loss: 0.01708819
[100/200] Training loss: 0.01556979
[150/200] Training loss: 0.01344481
[200/200] Training loss: 0.01246023
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 20737.052442427783 ----------
[1/200] Training loss: 0.08719387
[2/200] Training loss: 0.00472030
[3/200] Training loss: 0.00405175
[4/200] Training loss: 0.00358829
[5/200] Training loss: 0.00340822
[6/200] Training loss: 0.00293797
[7/200] Training loss: 0.00261125
[8/200] Training loss: 0.00229831
[9/200] Training loss: 0.00211963
[10/200] Training loss: 0.00189256
[50/200] Training loss: 0.00049281
[100/200] Training loss: 0.00037506
[150/200] Training loss: 0.00032817
[200/200] Training loss: 0.00027228
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8463.17718117729 ----------
[1/200] Training loss: 0.16150158
[2/200] Training loss: 0.06236008
[3/200] Training loss: 0.05485228
[4/200] Training loss: 0.05102052
[5/200] Training loss: 0.05125842
[6/200] Training loss: 0.04856304
[7/200] Training loss: 0.04637351
[8/200] Training loss: 0.04537545
[9/200] Training loss: 0.04243979
[10/200] Training loss: 0.03948350
[50/200] Training loss: 0.01692716
[100/200] Training loss: 0.01416880
[150/200] Training loss: 0.01387191
[200/200] Training loss: 0.01210858
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7580.61976358134 ----------
[1/200] Training loss: 0.14449752
[2/200] Training loss: 0.05729422
[3/200] Training loss: 0.05175229
[4/200] Training loss: 0.04926154
[5/200] Training loss: 0.04724165
[6/200] Training loss: 0.04535958
[7/200] Training loss: 0.04463601
[8/200] Training loss: 0.04255750
[9/200] Training loss: 0.04124975
[10/200] Training loss: 0.03801487
[50/200] Training loss: 0.01811605
[100/200] Training loss: 0.01525800
[150/200] Training loss: 0.01419077
[200/200] Training loss: 0.01325990
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8533.007441693697 ----------
[1/200] Training loss: 0.15723162
[2/200] Training loss: 0.06201139
[3/200] Training loss: 0.05112779
[4/200] Training loss: 0.04965858
[5/200] Training loss: 0.04579011
[6/200] Training loss: 0.04739331
[7/200] Training loss: 0.04230951
[8/200] Training loss: 0.04208236
[9/200] Training loss: 0.03892133
[10/200] Training loss: 0.03739953
[50/200] Training loss: 0.01716974
[100/200] Training loss: 0.01546649
[150/200] Training loss: 0.01353477
[200/200] Training loss: 0.01293592
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5154918391882848 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14320.96086161819 ----------
[1/200] Training loss: 0.16142700
[2/200] Training loss: 0.06076036
[3/200] Training loss: 0.04909914
[4/200] Training loss: 0.04767859
[5/200] Training loss: 0.04441600
[6/200] Training loss: 0.04078168
[7/200] Training loss: 0.04069800
[8/200] Training loss: 0.04007590
[9/200] Training loss: 0.03854069
[10/200] Training loss: 0.03811993
[50/200] Training loss: 0.01783179
[100/200] Training loss: 0.01464084
[150/200] Training loss: 0.01350429
[200/200] Training loss: 0.01250385
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6556.050945500652 ----------
[1/200] Training loss: 0.12347245
[2/200] Training loss: 0.05417603
[3/200] Training loss: 0.05160843
[4/200] Training loss: 0.05121749
[5/200] Training loss: 0.04625127
[6/200] Training loss: 0.04526005
[7/200] Training loss: 0.03952301
[8/200] Training loss: 0.03911562
[9/200] Training loss: 0.03675425
[10/200] Training loss: 0.03369116
[50/200] Training loss: 0.01861850
[100/200] Training loss: 0.01628370
[150/200] Training loss: 0.01455165
[200/200] Training loss: 0.01339586
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11183.87374749912 ----------
[1/200] Training loss: 0.15593675
[2/200] Training loss: 0.06045700
[3/200] Training loss: 0.04930278
[4/200] Training loss: 0.04935927
[5/200] Training loss: 0.04555163
[6/200] Training loss: 0.03937560
[7/200] Training loss: 0.03803996
[8/200] Training loss: 0.03701634
[9/200] Training loss: 0.03300258
[10/200] Training loss: 0.03226249
[50/200] Training loss: 0.01632746
[100/200] Training loss: 0.01511572
[150/200] Training loss: 0.01345745
[200/200] Training loss: 0.01292569
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17089.814510403558 ----------
[1/200] Training loss: 0.15954110
[2/200] Training loss: 0.05444559
[3/200] Training loss: 0.05011017
[4/200] Training loss: 0.04864847
[5/200] Training loss: 0.04544566
[6/200] Training loss: 0.04537390
[7/200] Training loss: 0.04218825
[8/200] Training loss: 0.04147778
[9/200] Training loss: 0.03887073
[10/200] Training loss: 0.03932683
[50/200] Training loss: 0.02007188
[100/200] Training loss: 0.01628575
[150/200] Training loss: 0.01490970
[200/200] Training loss: 0.01363507
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12809.209187143444 ----------
[1/200] Training loss: 0.15666679
[2/200] Training loss: 0.05925780
[3/200] Training loss: 0.05116048
[4/200] Training loss: 0.04602289
[5/200] Training loss: 0.04190352
[6/200] Training loss: 0.03870959
[7/200] Training loss: 0.03670455
[8/200] Training loss: 0.03552757
[9/200] Training loss: 0.03205389
[10/200] Training loss: 0.03225493
[50/200] Training loss: 0.01951131
[100/200] Training loss: 0.01565703
[150/200] Training loss: 0.01448916
[200/200] Training loss: 0.01323832
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11145.182995357232 ----------
[1/200] Training loss: 0.13588686
[2/200] Training loss: 0.05788354
[3/200] Training loss: 0.05027728
[4/200] Training loss: 0.04861606
[5/200] Training loss: 0.04352931
[6/200] Training loss: 0.04095544
[7/200] Training loss: 0.03823108
[8/200] Training loss: 0.03535090
[9/200] Training loss: 0.03318971
[10/200] Training loss: 0.03035815
[50/200] Training loss: 0.01738221
[100/200] Training loss: 0.01507313
[150/200] Training loss: 0.01312686
[200/200] Training loss: 0.01251438
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5474.288629584669 ----------
[1/200] Training loss: 0.16375331
[2/200] Training loss: 0.05749620
[3/200] Training loss: 0.05377693
[4/200] Training loss: 0.04846545
[5/200] Training loss: 0.04425630
[6/200] Training loss: 0.04345907
[7/200] Training loss: 0.04068720
[8/200] Training loss: 0.04068949
[9/200] Training loss: 0.03629784
[10/200] Training loss: 0.03817428
[50/200] Training loss: 0.01818361
[100/200] Training loss: 0.01506981
[150/200] Training loss: 0.01365128
[200/200] Training loss: 0.01273666
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14628.531573606422 ----------
[1/200] Training loss: 0.16903688
[2/200] Training loss: 0.05539897
[3/200] Training loss: 0.04902222
[4/200] Training loss: 0.04428637
[5/200] Training loss: 0.04139149
[6/200] Training loss: 0.03988949
[7/200] Training loss: 0.03680220
[8/200] Training loss: 0.03625487
[9/200] Training loss: 0.03412945
[10/200] Training loss: 0.03263703
[50/200] Training loss: 0.01809627
[100/200] Training loss: 0.01545788
[150/200] Training loss: 0.01411761
[200/200] Training loss: 0.01273549
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9525.439622400638 ----------
[1/200] Training loss: 0.17880994
[2/200] Training loss: 0.06553288
[3/200] Training loss: 0.05876125
[4/200] Training loss: 0.05446325
[5/200] Training loss: 0.05054899
[6/200] Training loss: 0.04880050
[7/200] Training loss: 0.04747075
[8/200] Training loss: 0.04426856
[9/200] Training loss: 0.04218796
[10/200] Training loss: 0.04316441
[50/200] Training loss: 0.01796429
[100/200] Training loss: 0.01577410
[150/200] Training loss: 0.01355402
[200/200] Training loss: 0.01320487
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10081.891489199832 ----------
[1/200] Training loss: 0.17190614
[2/200] Training loss: 0.06220931
[3/200] Training loss: 0.05694907
[4/200] Training loss: 0.05183127
[5/200] Training loss: 0.04298240
[6/200] Training loss: 0.04320743
[7/200] Training loss: 0.03681789
[8/200] Training loss: 0.03591042
[9/200] Training loss: 0.03591997
[10/200] Training loss: 0.03459385
[50/200] Training loss: 0.01757409
[100/200] Training loss: 0.01543641
[150/200] Training loss: 0.01420391
[200/200] Training loss: 0.01285966
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10029.514046054275 ----------
[1/200] Training loss: 0.15770372
[2/200] Training loss: 0.05615639
[3/200] Training loss: 0.05211956
[4/200] Training loss: 0.04815117
[5/200] Training loss: 0.04516446
[6/200] Training loss: 0.04025254
[7/200] Training loss: 0.03895126
[8/200] Training loss: 0.03717316
[9/200] Training loss: 0.03540217
[10/200] Training loss: 0.03172826
[50/200] Training loss: 0.01867356
[100/200] Training loss: 0.01589509
[150/200] Training loss: 0.01553584
[200/200] Training loss: 0.01407811
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15774.190565604309 ----------
[1/200] Training loss: 0.15635239
[2/200] Training loss: 0.05797079
[3/200] Training loss: 0.05044219
[4/200] Training loss: 0.04669067
[5/200] Training loss: 0.04455636
[6/200] Training loss: 0.04187068
[7/200] Training loss: 0.03806432
[8/200] Training loss: 0.03529622
[9/200] Training loss: 0.03320492
[10/200] Training loss: 0.03300167
[50/200] Training loss: 0.01707696
[100/200] Training loss: 0.01464045
[150/200] Training loss: 0.01289653
[200/200] Training loss: 0.01173037
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 20508.79928225931 ----------
[1/200] Training loss: 0.12311427
[2/200] Training loss: 0.04742752
[3/200] Training loss: 0.04215807
[4/200] Training loss: 0.04225783
[5/200] Training loss: 0.03805099
[6/200] Training loss: 0.03575477
[7/200] Training loss: 0.03302994
[8/200] Training loss: 0.03407560
[9/200] Training loss: 0.03239585
[10/200] Training loss: 0.02989293
[50/200] Training loss: 0.01822742
[100/200] Training loss: 0.01359872
[150/200] Training loss: 0.01165868
[200/200] Training loss: 0.01092192
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4940.479733791041 ----------
[1/200] Training loss: 0.16044156
[2/200] Training loss: 0.05817951
[3/200] Training loss: 0.05056755
[4/200] Training loss: 0.04835278
[5/200] Training loss: 0.04549138
[6/200] Training loss: 0.04311487
[7/200] Training loss: 0.03788651
[8/200] Training loss: 0.03337101
[9/200] Training loss: 0.03310030
[10/200] Training loss: 0.03259691
[50/200] Training loss: 0.02104817
[100/200] Training loss: 0.01814474
[150/200] Training loss: 0.01700507
[200/200] Training loss: 0.01586556
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6346.001890954651 ----------
[1/200] Training loss: 0.14290078
[2/200] Training loss: 0.05963448
[3/200] Training loss: 0.05363839
[4/200] Training loss: 0.04735074
[5/200] Training loss: 0.04943178
[6/200] Training loss: 0.04445011
[7/200] Training loss: 0.04260560
[8/200] Training loss: 0.04312696
[9/200] Training loss: 0.04179475
[10/200] Training loss: 0.03907277
[50/200] Training loss: 0.01948932
[100/200] Training loss: 0.01612267
[150/200] Training loss: 0.01414557
[200/200] Training loss: 0.01361417
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12115.026042068585 ----------
[1/200] Training loss: 0.15914245
[2/200] Training loss: 0.05790032
[3/200] Training loss: 0.05395385
[4/200] Training loss: 0.05117306
[5/200] Training loss: 0.04586387
[6/200] Training loss: 0.04011530
[7/200] Training loss: 0.03877165
[8/200] Training loss: 0.03773436
[9/200] Training loss: 0.03321331
[10/200] Training loss: 0.03005984
[50/200] Training loss: 0.01827877
[100/200] Training loss: 0.01525464
[150/200] Training loss: 0.01436224
[200/200] Training loss: 0.01281384
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 3381.9831460254204 ----------
[1/200] Training loss: 0.17726350
[2/200] Training loss: 0.05841975
[3/200] Training loss: 0.05648915
[4/200] Training loss: 0.05086405
[5/200] Training loss: 0.05069535
[6/200] Training loss: 0.04503591
[7/200] Training loss: 0.04460406
[8/200] Training loss: 0.03754407
[9/200] Training loss: 0.03481953
[10/200] Training loss: 0.03072070
[50/200] Training loss: 0.01771134
[100/200] Training loss: 0.01552087
[150/200] Training loss: 0.01481382
[200/200] Training loss: 0.01344232
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17482.068527494106 ----------
[1/200] Training loss: 0.18715861
[2/200] Training loss: 0.06251246
[3/200] Training loss: 0.05640566
[4/200] Training loss: 0.04808574
[5/200] Training loss: 0.04566347
[6/200] Training loss: 0.04220377
[7/200] Training loss: 0.03955841
[8/200] Training loss: 0.03790388
[9/200] Training loss: 0.03547987
[10/200] Training loss: 0.03522413
[50/200] Training loss: 0.01869809
[100/200] Training loss: 0.01651097
[150/200] Training loss: 0.01477025
[200/200] Training loss: 0.01434050
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8766.883140546588 ----------
[1/200] Training loss: 0.20366286
[2/200] Training loss: 0.06259133
[3/200] Training loss: 0.05963166
[4/200] Training loss: 0.05798722
[5/200] Training loss: 0.05481108
[6/200] Training loss: 0.05369918
[7/200] Training loss: 0.05027699
[8/200] Training loss: 0.05005968
[9/200] Training loss: 0.05043085
[10/200] Training loss: 0.05010115
[50/200] Training loss: 0.02096821
[100/200] Training loss: 0.01723946
[150/200] Training loss: 0.01442182
[200/200] Training loss: 0.01320701
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11951.831324110963 ----------
[1/200] Training loss: 0.17087294
[2/200] Training loss: 0.06301412
[3/200] Training loss: 0.05484980
[4/200] Training loss: 0.05228405
[5/200] Training loss: 0.04955774
[6/200] Training loss: 0.04640055
[7/200] Training loss: 0.04358336
[8/200] Training loss: 0.03931854
[9/200] Training loss: 0.03726539
[10/200] Training loss: 0.03221406
[50/200] Training loss: 0.01826980
[100/200] Training loss: 0.01650581
[150/200] Training loss: 0.01538548
[200/200] Training loss: 0.01447479
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13399.429838616268 ----------
[1/200] Training loss: 0.14050261
[2/200] Training loss: 0.05085152
[3/200] Training loss: 0.05099520
[4/200] Training loss: 0.04496273
[5/200] Training loss: 0.04310729
[6/200] Training loss: 0.03908581
[7/200] Training loss: 0.03858044
[8/200] Training loss: 0.03485625
[9/200] Training loss: 0.03432863
[10/200] Training loss: 0.03004302
[50/200] Training loss: 0.01612928
[100/200] Training loss: 0.01390702
[150/200] Training loss: 0.01326551
[200/200] Training loss: 0.01281032
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7596.076618886885 ----------
[1/200] Training loss: 0.16809878
[2/200] Training loss: 0.05951866
[3/200] Training loss: 0.05497795
[4/200] Training loss: 0.05109358
[5/200] Training loss: 0.05220534
[6/200] Training loss: 0.04535733
[7/200] Training loss: 0.04362907
[8/200] Training loss: 0.04149098
[9/200] Training loss: 0.04134610
[10/200] Training loss: 0.04038583
[50/200] Training loss: 0.01934665
[100/200] Training loss: 0.01683671
[150/200] Training loss: 0.01554771
[200/200] Training loss: 0.01462620
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15007.573021644772 ----------
[1/200] Training loss: 0.17229360
[2/200] Training loss: 0.06558283
[3/200] Training loss: 0.05574236
[4/200] Training loss: 0.05277416
[5/200] Training loss: 0.05080174
[6/200] Training loss: 0.04480918
[7/200] Training loss: 0.04545756
[8/200] Training loss: 0.04173334
[9/200] Training loss: 0.04036325
[10/200] Training loss: 0.03807106
[50/200] Training loss: 0.01818928
[100/200] Training loss: 0.01533763
[150/200] Training loss: 0.01352804
[200/200] Training loss: 0.01256464
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10561.583972113274 ----------
[1/200] Training loss: 0.17423391
[2/200] Training loss: 0.06244325
[3/200] Training loss: 0.05438115
[4/200] Training loss: 0.05013009
[5/200] Training loss: 0.04666339
[6/200] Training loss: 0.04769929
[7/200] Training loss: 0.04583559
[8/200] Training loss: 0.04401994
[9/200] Training loss: 0.04305692
[10/200] Training loss: 0.03903920
[50/200] Training loss: 0.01872628
[100/200] Training loss: 0.01543742
[150/200] Training loss: 0.01324434
[200/200] Training loss: 0.01180462
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10873.96119176448 ----------
[1/200] Training loss: 0.14626693
[2/200] Training loss: 0.05948769
[3/200] Training loss: 0.05434390
[4/200] Training loss: 0.04823533
[5/200] Training loss: 0.04510052
[6/200] Training loss: 0.04099384
[7/200] Training loss: 0.03734756
[8/200] Training loss: 0.03490747
[9/200] Training loss: 0.03071545
[10/200] Training loss: 0.03084941
[50/200] Training loss: 0.01801820
[100/200] Training loss: 0.01520601
[150/200] Training loss: 0.01405195
[200/200] Training loss: 0.01256528
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9617.971511706613 ----------
[1/200] Training loss: 0.17269844
[2/200] Training loss: 0.05759790
[3/200] Training loss: 0.05136908
[4/200] Training loss: 0.04926539
[5/200] Training loss: 0.04600154
[6/200] Training loss: 0.04417945
[7/200] Training loss: 0.03936086
[8/200] Training loss: 0.03746760
[9/200] Training loss: 0.03704913
[10/200] Training loss: 0.03400309
[50/200] Training loss: 0.01764283
[100/200] Training loss: 0.01467850
[150/200] Training loss: 0.01344301
[200/200] Training loss: 0.01276318
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7650.826360596612 ----------
[1/200] Training loss: 0.14753042
[2/200] Training loss: 0.06324305
[3/200] Training loss: 0.05070496
[4/200] Training loss: 0.04980435
[5/200] Training loss: 0.04636978
[6/200] Training loss: 0.04313733
[7/200] Training loss: 0.04289579
[8/200] Training loss: 0.03964852
[9/200] Training loss: 0.03990755
[10/200] Training loss: 0.03752511
[50/200] Training loss: 0.01913864
[100/200] Training loss: 0.01666606
[150/200] Training loss: 0.01341341
[200/200] Training loss: 0.01232159
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8855.357700285178 ----------
[1/200] Training loss: 0.16816028
[2/200] Training loss: 0.06087471
[3/200] Training loss: 0.05688586
[4/200] Training loss: 0.05059350
[5/200] Training loss: 0.04566167
[6/200] Training loss: 0.04771515
[7/200] Training loss: 0.04435513
[8/200] Training loss: 0.04352028
[9/200] Training loss: 0.03911545
[10/200] Training loss: 0.03933928
[50/200] Training loss: 0.01907744
[100/200] Training loss: 0.01665138
[150/200] Training loss: 0.01480469
[200/200] Training loss: 0.01403291
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8033.668153465141 ----------
[1/200] Training loss: 0.17441650
[2/200] Training loss: 0.06036060
[3/200] Training loss: 0.05334256
[4/200] Training loss: 0.04979864
[5/200] Training loss: 0.05117722
[6/200] Training loss: 0.04690666
[7/200] Training loss: 0.04254490
[8/200] Training loss: 0.04243045
[9/200] Training loss: 0.03742902
[10/200] Training loss: 0.03657621
[50/200] Training loss: 0.01777717
[100/200] Training loss: 0.01557390
[150/200] Training loss: 0.01475933
[200/200] Training loss: 0.01357835
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12910.346238579352 ----------
[1/200] Training loss: 0.15270449
[2/200] Training loss: 0.06021574
[3/200] Training loss: 0.05346537
[4/200] Training loss: 0.05189599
[5/200] Training loss: 0.04865395
[6/200] Training loss: 0.04451712
[7/200] Training loss: 0.04181867
[8/200] Training loss: 0.03816563
[9/200] Training loss: 0.03688450
[10/200] Training loss: 0.03327629
[50/200] Training loss: 0.01670713
[100/200] Training loss: 0.01357682
[150/200] Training loss: 0.01199523
[200/200] Training loss: 0.01130820
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11930.656310530449 ----------
[1/200] Training loss: 0.17037321
[2/200] Training loss: 0.06090960
[3/200] Training loss: 0.05207082
[4/200] Training loss: 0.04888293
[5/200] Training loss: 0.04537418
[6/200] Training loss: 0.04513491
[7/200] Training loss: 0.04020834
[8/200] Training loss: 0.03938846
[9/200] Training loss: 0.03779511
[10/200] Training loss: 0.03540178
[50/200] Training loss: 0.01818729
[100/200] Training loss: 0.01612266
[150/200] Training loss: 0.01471761
[200/200] Training loss: 0.01393313
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7126.922758105352 ----------
[1/200] Training loss: 0.16601941
[2/200] Training loss: 0.05641500
[3/200] Training loss: 0.05295727
[4/200] Training loss: 0.04870767
[5/200] Training loss: 0.04351590
[6/200] Training loss: 0.04179331
[7/200] Training loss: 0.03880237
[8/200] Training loss: 0.03501731
[9/200] Training loss: 0.03530634
[10/200] Training loss: 0.03153856
[50/200] Training loss: 0.01799086
[100/200] Training loss: 0.01537048
[150/200] Training loss: 0.01369324
[200/200] Training loss: 0.01245737
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9082.04822713467 ----------
[1/200] Training loss: 0.15379945
[2/200] Training loss: 0.06036188
[3/200] Training loss: 0.05249704
[4/200] Training loss: 0.05062860
[5/200] Training loss: 0.04621732
[6/200] Training loss: 0.04234727
[7/200] Training loss: 0.04233291
[8/200] Training loss: 0.04005994
[9/200] Training loss: 0.03566718
[10/200] Training loss: 0.03647635
[50/200] Training loss: 0.01744008
[100/200] Training loss: 0.01429045
[150/200] Training loss: 0.01379905
[200/200] Training loss: 0.01257358
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5505.823644106302 ----------
[1/200] Training loss: 0.14747493
[2/200] Training loss: 0.05740002
[3/200] Training loss: 0.05255985
[4/200] Training loss: 0.04858128
[5/200] Training loss: 0.04323614
[6/200] Training loss: 0.04064138
[7/200] Training loss: 0.04085110
[8/200] Training loss: 0.03695806
[9/200] Training loss: 0.03450267
[10/200] Training loss: 0.03238278
[50/200] Training loss: 0.01726101
[100/200] Training loss: 0.01467314
[150/200] Training loss: 0.01366103
[200/200] Training loss: 0.01258906
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16983.82665950168 ----------
[1/200] Training loss: 0.15091205
[2/200] Training loss: 0.05732472
[3/200] Training loss: 0.05206776
[4/200] Training loss: 0.04523334
[5/200] Training loss: 0.04303285
[6/200] Training loss: 0.03832658
[7/200] Training loss: 0.03808088
[8/200] Training loss: 0.03658097
[9/200] Training loss: 0.03371132
[10/200] Training loss: 0.03187246
[50/200] Training loss: 0.01778644
[100/200] Training loss: 0.01452195
[150/200] Training loss: 0.01311494
[200/200] Training loss: 0.01239463
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6980.23036869128 ----------
[1/200] Training loss: 0.15919084
[2/200] Training loss: 0.06305262
[3/200] Training loss: 0.05891948
[4/200] Training loss: 0.04922624
[5/200] Training loss: 0.04997246
[6/200] Training loss: 0.04771681
[7/200] Training loss: 0.04147686
[8/200] Training loss: 0.04157288
[9/200] Training loss: 0.03838174
[10/200] Training loss: 0.03693061
[50/200] Training loss: 0.01783202
[100/200] Training loss: 0.01402443
[150/200] Training loss: 0.01303557
[200/200] Training loss: 0.01176856
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16071.344934385548 ----------
[1/200] Training loss: 0.16868299
[2/200] Training loss: 0.06646237
[3/200] Training loss: 0.05652796
[4/200] Training loss: 0.05138923
[5/200] Training loss: 0.05067282
[6/200] Training loss: 0.04783152
[7/200] Training loss: 0.04588910
[8/200] Training loss: 0.04510730
[9/200] Training loss: 0.04022541
[10/200] Training loss: 0.03920145
[50/200] Training loss: 0.02023859
[100/200] Training loss: 0.01633335
[150/200] Training loss: 0.01486392
[200/200] Training loss: 0.01292790
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11551.140896032737 ----------
[1/200] Training loss: 0.09627432
[2/200] Training loss: 0.00447896
[3/200] Training loss: 0.00421077
[4/200] Training loss: 0.00389410
[5/200] Training loss: 0.00354512
[6/200] Training loss: 0.00330094
[7/200] Training loss: 0.00293805
[8/200] Training loss: 0.00267057
[9/200] Training loss: 0.00245613
[10/200] Training loss: 0.00226175
[50/200] Training loss: 0.00057234
[100/200] Training loss: 0.00035689
[150/200] Training loss: 0.00028925
[200/200] Training loss: 0.00025752
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.42916145580615583 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10737.26594622672 ----------
[1/200] Training loss: 0.15214390
[2/200] Training loss: 0.05621202
[3/200] Training loss: 0.05189977
[4/200] Training loss: 0.04690569
[5/200] Training loss: 0.04587898
[6/200] Training loss: 0.04455067
[7/200] Training loss: 0.03975595
[8/200] Training loss: 0.03799766
[9/200] Training loss: 0.03744160
[10/200] Training loss: 0.03688986
[50/200] Training loss: 0.01861760
[100/200] Training loss: 0.01487428
[150/200] Training loss: 0.01403333
[200/200] Training loss: 0.01266419
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4193.614908405396 ----------
[1/200] Training loss: 0.16617987
[2/200] Training loss: 0.06436857
[3/200] Training loss: 0.05437587
[4/200] Training loss: 0.04946348
[5/200] Training loss: 0.04901897
[6/200] Training loss: 0.04782597
[7/200] Training loss: 0.04530308
[8/200] Training loss: 0.04588489
[9/200] Training loss: 0.04136625
[10/200] Training loss: 0.04020238
[50/200] Training loss: 0.01887530
[100/200] Training loss: 0.01605446
[150/200] Training loss: 0.01511735
[200/200] Training loss: 0.01411277
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 19386.947361562623 ----------
[1/200] Training loss: 0.16645365
[2/200] Training loss: 0.05945522
[3/200] Training loss: 0.05239448
[4/200] Training loss: 0.04788516
[5/200] Training loss: 0.04748961
[6/200] Training loss: 0.04236227
[7/200] Training loss: 0.04055503
[8/200] Training loss: 0.03985416
[9/200] Training loss: 0.03927285
[10/200] Training loss: 0.03600674
[50/200] Training loss: 0.01912273
[100/200] Training loss: 0.01608432
[150/200] Training loss: 0.01436436
[200/200] Training loss: 0.01333491
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13908.682755746499 ----------
[1/200] Training loss: 0.15678877
[2/200] Training loss: 0.06259190
[3/200] Training loss: 0.05452049
[4/200] Training loss: 0.04868132
[5/200] Training loss: 0.04519990
[6/200] Training loss: 0.04365701
[7/200] Training loss: 0.04256500
[8/200] Training loss: 0.04098794
[9/200] Training loss: 0.03742142
[10/200] Training loss: 0.03711421
[50/200] Training loss: 0.01824135
[100/200] Training loss: 0.01580536
[150/200] Training loss: 0.01415521
[200/200] Training loss: 0.01289441
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7647.858262284939 ----------
[1/200] Training loss: 0.16695019
[2/200] Training loss: 0.05534642
[3/200] Training loss: 0.04937507
[4/200] Training loss: 0.04754090
[5/200] Training loss: 0.04421895
[6/200] Training loss: 0.04094307
[7/200] Training loss: 0.03866992
[8/200] Training loss: 0.03807665
[9/200] Training loss: 0.03572789
[10/200] Training loss: 0.03437091
[50/200] Training loss: 0.01869418
[100/200] Training loss: 0.01511697
[150/200] Training loss: 0.01448896
[200/200] Training loss: 0.01358546
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7313.360103263069 ----------
[1/200] Training loss: 0.14483862
[2/200] Training loss: 0.05701529
[3/200] Training loss: 0.05283844
[4/200] Training loss: 0.04998014
[5/200] Training loss: 0.04757957
[6/200] Training loss: 0.04294700
[7/200] Training loss: 0.04087321
[8/200] Training loss: 0.04008324
[9/200] Training loss: 0.03831783
[10/200] Training loss: 0.03747030
[50/200] Training loss: 0.01862049
[100/200] Training loss: 0.01653584
[150/200] Training loss: 0.01548226
[200/200] Training loss: 0.01436105
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18455.580402685795 ----------
[1/200] Training loss: 0.18086546
[2/200] Training loss: 0.06438078
[3/200] Training loss: 0.05227987
[4/200] Training loss: 0.04931782
[5/200] Training loss: 0.04657957
[6/200] Training loss: 0.04535439
[7/200] Training loss: 0.04094232
[8/200] Training loss: 0.04161018
[9/200] Training loss: 0.03684078
[10/200] Training loss: 0.03574234
[50/200] Training loss: 0.01858864
[100/200] Training loss: 0.01597329
[150/200] Training loss: 0.01428443
[200/200] Training loss: 0.01301277
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10071.262482926359 ----------
[1/200] Training loss: 0.14520610
[2/200] Training loss: 0.05531209
[3/200] Training loss: 0.04913358
[4/200] Training loss: 0.04595644
[5/200] Training loss: 0.04319668
[6/200] Training loss: 0.04060687
[7/200] Training loss: 0.03896342
[8/200] Training loss: 0.03816697
[9/200] Training loss: 0.03669902
[10/200] Training loss: 0.03427676
[50/200] Training loss: 0.01736459
[100/200] Training loss: 0.01482862
[150/200] Training loss: 0.01271023
[200/200] Training loss: 0.01126325
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8786.710874952014 ----------
[1/200] Training loss: 0.14203239
[2/200] Training loss: 0.05770428
[3/200] Training loss: 0.05615288
[4/200] Training loss: 0.05123354
[5/200] Training loss: 0.04873355
[6/200] Training loss: 0.04669927
[7/200] Training loss: 0.04609790
[8/200] Training loss: 0.04166267
[9/200] Training loss: 0.04231593
[10/200] Training loss: 0.03959744
[50/200] Training loss: 0.01782627
[100/200] Training loss: 0.01472013
[150/200] Training loss: 0.01310336
[200/200] Training loss: 0.01226568
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17703.963849940497 ----------
[1/200] Training loss: 0.16621739
[2/200] Training loss: 0.05806946
[3/200] Training loss: 0.05210920
[4/200] Training loss: 0.04840514
[5/200] Training loss: 0.04523110
[6/200] Training loss: 0.04398794
[7/200] Training loss: 0.04050997
[8/200] Training loss: 0.03956563
[9/200] Training loss: 0.03489707
[10/200] Training loss: 0.03492850
[50/200] Training loss: 0.01858645
[100/200] Training loss: 0.01626849
[150/200] Training loss: 0.01585383
[200/200] Training loss: 0.01378273
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16442.948640678776 ----------
[1/200] Training loss: 0.12533342
[2/200] Training loss: 0.05796205
[3/200] Training loss: 0.05120660
[4/200] Training loss: 0.04843290
[5/200] Training loss: 0.04695734
[6/200] Training loss: 0.04364610
[7/200] Training loss: 0.04172100
[8/200] Training loss: 0.04030995
[9/200] Training loss: 0.03701571
[10/200] Training loss: 0.03796582
[50/200] Training loss: 0.01785451
[100/200] Training loss: 0.01378468
[150/200] Training loss: 0.01294473
[200/200] Training loss: 0.01218411
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4469.143765868357 ----------
[1/200] Training loss: 0.15333702
[2/200] Training loss: 0.05779102
[3/200] Training loss: 0.05038651
[4/200] Training loss: 0.04832131
[5/200] Training loss: 0.04792705
[6/200] Training loss: 0.04580113
[7/200] Training loss: 0.04253666
[8/200] Training loss: 0.04193933
[9/200] Training loss: 0.04006706
[10/200] Training loss: 0.03523256
[50/200] Training loss: 0.01804882
[100/200] Training loss: 0.01492725
[150/200] Training loss: 0.01338479
[200/200] Training loss: 0.01242344
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14786.625308027522 ----------
[1/200] Training loss: 0.13500240
[2/200] Training loss: 0.05658056
[3/200] Training loss: 0.05172372
[4/200] Training loss: 0.04774519
[5/200] Training loss: 0.04482955
[6/200] Training loss: 0.04253647
[7/200] Training loss: 0.04062268
[8/200] Training loss: 0.04027365
[9/200] Training loss: 0.03576463
[10/200] Training loss: 0.03570520
[50/200] Training loss: 0.01732714
[100/200] Training loss: 0.01455909
[150/200] Training loss: 0.01325678
[200/200] Training loss: 0.01249509
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10239.251925800048 ----------
[1/200] Training loss: 0.16226298
[2/200] Training loss: 0.05765173
[3/200] Training loss: 0.04901254
[4/200] Training loss: 0.04624057
[5/200] Training loss: 0.04135325
[6/200] Training loss: 0.03996431
[7/200] Training loss: 0.03841953
[8/200] Training loss: 0.03729749
[9/200] Training loss: 0.03590211
[10/200] Training loss: 0.03164993
[50/200] Training loss: 0.01961270
[100/200] Training loss: 0.01708288
[150/200] Training loss: 0.01548118
[200/200] Training loss: 0.01460802
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16857.66294597208 ----------
[1/200] Training loss: 0.16479343
[2/200] Training loss: 0.06136760
[3/200] Training loss: 0.05785416
[4/200] Training loss: 0.05190553
[5/200] Training loss: 0.04916568
[6/200] Training loss: 0.04873654
[7/200] Training loss: 0.04745129
[8/200] Training loss: 0.04403691
[9/200] Training loss: 0.04337679
[10/200] Training loss: 0.04039328
[50/200] Training loss: 0.01973699
[100/200] Training loss: 0.01618098
[150/200] Training loss: 0.01476601
[200/200] Training loss: 0.01357825
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11687.947296253522 ----------
[1/200] Training loss: 0.12959516
[2/200] Training loss: 0.05696365
[3/200] Training loss: 0.05088324
[4/200] Training loss: 0.04393420
[5/200] Training loss: 0.04059778
[6/200] Training loss: 0.03593669
[7/200] Training loss: 0.03275185
[8/200] Training loss: 0.02896928
[9/200] Training loss: 0.02907043
[10/200] Training loss: 0.02643585
[50/200] Training loss: 0.01840723
[100/200] Training loss: 0.01594099
[150/200] Training loss: 0.01499554
[200/200] Training loss: 0.01358321
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14028.968315596127 ----------
[1/200] Training loss: 0.16302204
[2/200] Training loss: 0.05788986
[3/200] Training loss: 0.05003256
[4/200] Training loss: 0.04930356
[5/200] Training loss: 0.04577489
[6/200] Training loss: 0.04195109
[7/200] Training loss: 0.03935178
[8/200] Training loss: 0.03789694
[9/200] Training loss: 0.03526012
[10/200] Training loss: 0.03301919
[50/200] Training loss: 0.01716723
[100/200] Training loss: 0.01448848
[150/200] Training loss: 0.01266834
[200/200] Training loss: 0.01204947
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9523.718601470751 ----------
[1/200] Training loss: 0.14005466
[2/200] Training loss: 0.05371109
[3/200] Training loss: 0.05118116
[4/200] Training loss: 0.04600132
[5/200] Training loss: 0.04510328
[6/200] Training loss: 0.04389416
[7/200] Training loss: 0.04477478
[8/200] Training loss: 0.04009644
[9/200] Training loss: 0.03840826
[10/200] Training loss: 0.03654549
[50/200] Training loss: 0.02067313
[100/200] Training loss: 0.01526369
[150/200] Training loss: 0.01366386
[200/200] Training loss: 0.01204387
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6009.159675029447 ----------
[1/200] Training loss: 0.15389437
[2/200] Training loss: 0.05936175
[3/200] Training loss: 0.05330135
[4/200] Training loss: 0.04983706
[5/200] Training loss: 0.04525992
[6/200] Training loss: 0.04201092
[7/200] Training loss: 0.04003378
[8/200] Training loss: 0.03812200
[9/200] Training loss: 0.03367734
[10/200] Training loss: 0.03405810
[50/200] Training loss: 0.01721527
[100/200] Training loss: 0.01463559
[150/200] Training loss: 0.01299793
[200/200] Training loss: 0.01200397
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8531.825127134287 ----------
[1/200] Training loss: 0.07957988
[2/200] Training loss: 0.00442763
[3/200] Training loss: 0.00390088
[4/200] Training loss: 0.00360383
[5/200] Training loss: 0.00319903
[6/200] Training loss: 0.00302308
[7/200] Training loss: 0.00267718
[8/200] Training loss: 0.00246311
[9/200] Training loss: 0.00214679
[10/200] Training loss: 0.00198285
[50/200] Training loss: 0.00058100
[100/200] Training loss: 0.00042576
[150/200] Training loss: 0.00033358
[200/200] Training loss: 0.00027691
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8515.551420783037 ----------
[1/200] Training loss: 0.15915927
[2/200] Training loss: 0.05586173
[3/200] Training loss: 0.05195775
[4/200] Training loss: 0.05250440
[5/200] Training loss: 0.04646622
[6/200] Training loss: 0.04630645
[7/200] Training loss: 0.04498291
[8/200] Training loss: 0.04206299
[9/200] Training loss: 0.04045576
[10/200] Training loss: 0.03973823
[50/200] Training loss: 0.01936789
[100/200] Training loss: 0.01460968
[150/200] Training loss: 0.01290814
[200/200] Training loss: 0.01220961
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7094.514218746764 ----------
[1/200] Training loss: 0.07196545
[2/200] Training loss: 0.00468046
[3/200] Training loss: 0.00396751
[4/200] Training loss: 0.00313742
[5/200] Training loss: 0.00274079
[6/200] Training loss: 0.00246580
[7/200] Training loss: 0.00172782
[8/200] Training loss: 0.00163626
[9/200] Training loss: 0.00133582
[10/200] Training loss: 0.00119468
[50/200] Training loss: 0.00049361
[100/200] Training loss: 0.00038023
[150/200] Training loss: 0.00032335
[200/200] Training loss: 0.00029224
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16237.181036128162 ----------
[1/200] Training loss: 0.10238602
[2/200] Training loss: 0.00465875
[3/200] Training loss: 0.00431715
[4/200] Training loss: 0.00380852
[5/200] Training loss: 0.00346187
[6/200] Training loss: 0.00315662
[7/200] Training loss: 0.00286740
[8/200] Training loss: 0.00262070
[9/200] Training loss: 0.00242969
[10/200] Training loss: 0.00234364
[50/200] Training loss: 0.00052918
[100/200] Training loss: 0.00037759
[150/200] Training loss: 0.00031659
[200/200] Training loss: 0.00031603
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10179.149276830554 ----------
[1/200] Training loss: 0.16652290
[2/200] Training loss: 0.05736896
[3/200] Training loss: 0.05259625
[4/200] Training loss: 0.04399296
[5/200] Training loss: 0.04531922
[6/200] Training loss: 0.04159909
[7/200] Training loss: 0.03993845
[8/200] Training loss: 0.03736186
[9/200] Training loss: 0.03431663
[10/200] Training loss: 0.03601387
[50/200] Training loss: 0.01946732
[100/200] Training loss: 0.01600665
[150/200] Training loss: 0.01356086
[200/200] Training loss: 0.01209899
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19694.672579151957 ----------
[1/200] Training loss: 0.12994634
[2/200] Training loss: 0.05476574
[3/200] Training loss: 0.04994686
[4/200] Training loss: 0.04943696
[5/200] Training loss: 0.04488421
[6/200] Training loss: 0.04304259
[7/200] Training loss: 0.04088985
[8/200] Training loss: 0.03878982
[9/200] Training loss: 0.03856767
[10/200] Training loss: 0.03573311
[50/200] Training loss: 0.01757617
[100/200] Training loss: 0.01452087
[150/200] Training loss: 0.01297097
[200/200] Training loss: 0.01081200
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14040.978313493686 ----------
[1/200] Training loss: 0.15879611
[2/200] Training loss: 0.06116960
[3/200] Training loss: 0.04962931
[4/200] Training loss: 0.04968131
[5/200] Training loss: 0.04306123
[6/200] Training loss: 0.03934909
[7/200] Training loss: 0.03401032
[8/200] Training loss: 0.03394901
[9/200] Training loss: 0.02987899
[10/200] Training loss: 0.02720743
[50/200] Training loss: 0.01785278
[100/200] Training loss: 0.01546015
[150/200] Training loss: 0.01375836
[200/200] Training loss: 0.01286161
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10274.547970592186 ----------
[1/200] Training loss: 0.16301815
[2/200] Training loss: 0.05201430
[3/200] Training loss: 0.04762572
[4/200] Training loss: 0.04448333
[5/200] Training loss: 0.04315531
[6/200] Training loss: 0.04078757
[7/200] Training loss: 0.03465252
[8/200] Training loss: 0.03745841
[9/200] Training loss: 0.03129296
[10/200] Training loss: 0.02962650
[50/200] Training loss: 0.01760332
[100/200] Training loss: 0.01471622
[150/200] Training loss: 0.01267533
[200/200] Training loss: 0.01275033
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12991.95443341763 ----------
[1/200] Training loss: 0.14851154
[2/200] Training loss: 0.05417961
[3/200] Training loss: 0.05046538
[4/200] Training loss: 0.04867503
[5/200] Training loss: 0.04540531
[6/200] Training loss: 0.04201529
[7/200] Training loss: 0.04114302
[8/200] Training loss: 0.03820023
[9/200] Training loss: 0.03856254
[10/200] Training loss: 0.03530667
[50/200] Training loss: 0.01924211
[100/200] Training loss: 0.01714249
[150/200] Training loss: 0.01532276
[200/200] Training loss: 0.01448624
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15432.959016339024 ----------
[1/200] Training loss: 0.16424943
[2/200] Training loss: 0.06529866
[3/200] Training loss: 0.05731841
[4/200] Training loss: 0.05210336
[5/200] Training loss: 0.04595384
[6/200] Training loss: 0.04451261
[7/200] Training loss: 0.04218590
[8/200] Training loss: 0.04087933
[9/200] Training loss: 0.03531565
[10/200] Training loss: 0.03539682
[50/200] Training loss: 0.01735267
[100/200] Training loss: 0.01428813
[150/200] Training loss: 0.01235707
[200/200] Training loss: 0.01156064
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5641.638591756831 ----------
[1/200] Training loss: 0.14699725
[2/200] Training loss: 0.06009616
[3/200] Training loss: 0.05370742
[4/200] Training loss: 0.05238595
[5/200] Training loss: 0.04548489
[6/200] Training loss: 0.04396135
[7/200] Training loss: 0.03777322
[8/200] Training loss: 0.03571741
[9/200] Training loss: 0.03331917
[10/200] Training loss: 0.03173528
[50/200] Training loss: 0.01534093
[100/200] Training loss: 0.01328462
[150/200] Training loss: 0.01200897
[200/200] Training loss: 0.01118543
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10195.203185812434 ----------
[1/200] Training loss: 0.15354640
[2/200] Training loss: 0.06282282
[3/200] Training loss: 0.05551494
[4/200] Training loss: 0.05302613
[5/200] Training loss: 0.05093083
[6/200] Training loss: 0.04940773
[7/200] Training loss: 0.04368820
[8/200] Training loss: 0.04250463
[9/200] Training loss: 0.03751770
[10/200] Training loss: 0.03681637
[50/200] Training loss: 0.01670225
[100/200] Training loss: 0.01358270
[150/200] Training loss: 0.01250588
[200/200] Training loss: 0.01206901
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12735.035767519461 ----------
[1/200] Training loss: 0.15171555
[2/200] Training loss: 0.05185539
[3/200] Training loss: 0.04900751
[4/200] Training loss: 0.04328269
[5/200] Training loss: 0.04129849
[6/200] Training loss: 0.03595686
[7/200] Training loss: 0.03541428
[8/200] Training loss: 0.03458319
[9/200] Training loss: 0.03036354
[10/200] Training loss: 0.02942968
[50/200] Training loss: 0.01736288
[100/200] Training loss: 0.01521645
[150/200] Training loss: 0.01351309
[200/200] Training loss: 0.01241576
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6590.7456330828 ----------
[1/200] Training loss: 0.14625260
[2/200] Training loss: 0.05179279
[3/200] Training loss: 0.04969403
[4/200] Training loss: 0.04245485
[5/200] Training loss: 0.03721059
[6/200] Training loss: 0.03719127
[7/200] Training loss: 0.03479018
[8/200] Training loss: 0.03236800
[9/200] Training loss: 0.03060221
[10/200] Training loss: 0.03471074
[50/200] Training loss: 0.01784781
[100/200] Training loss: 0.01457414
[150/200] Training loss: 0.01265008
[200/200] Training loss: 0.01187922
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6836.033060189221 ----------
[1/200] Training loss: 0.16693454
[2/200] Training loss: 0.06094016
[3/200] Training loss: 0.05788561
[4/200] Training loss: 0.05343194
[5/200] Training loss: 0.05087351
[6/200] Training loss: 0.05013186
[7/200] Training loss: 0.04938379
[8/200] Training loss: 0.04696889
[9/200] Training loss: 0.04522061
[10/200] Training loss: 0.04099006
[50/200] Training loss: 0.01819992
[100/200] Training loss: 0.01518745
[150/200] Training loss: 0.01397770
[200/200] Training loss: 0.01337018
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5850.476561785373 ----------
[1/200] Training loss: 0.15635222
[2/200] Training loss: 0.06137830
[3/200] Training loss: 0.05251810
[4/200] Training loss: 0.05427346
[5/200] Training loss: 0.05104337
[6/200] Training loss: 0.04675941
[7/200] Training loss: 0.04462892
[8/200] Training loss: 0.04359890
[9/200] Training loss: 0.03699732
[10/200] Training loss: 0.03771275
[50/200] Training loss: 0.01807638
[100/200] Training loss: 0.01485801
[150/200] Training loss: 0.01288495
[200/200] Training loss: 0.01195697
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12395.015772478871 ----------
[1/200] Training loss: 0.07183799
[2/200] Training loss: 0.00451255
[3/200] Training loss: 0.00403251
[4/200] Training loss: 0.00386583
[5/200] Training loss: 0.00310320
[6/200] Training loss: 0.00272703
[7/200] Training loss: 0.00231526
[8/200] Training loss: 0.00192970
[9/200] Training loss: 0.00149099
[10/200] Training loss: 0.00107812
[50/200] Training loss: 0.00051747
[100/200] Training loss: 0.00037131
[150/200] Training loss: 0.00029481
[200/200] Training loss: 0.00027021
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7554.596481613032 ----------
[1/200] Training loss: 0.03102116
[2/200] Training loss: 0.00498349
[3/200] Training loss: 0.00398864
[4/200] Training loss: 0.00397532
[5/200] Training loss: 0.00360397
[6/200] Training loss: 0.00303533
[7/200] Training loss: 0.00326152
[8/200] Training loss: 0.00290229
[9/200] Training loss: 0.00249768
[10/200] Training loss: 0.00192951
[50/200] Training loss: 0.00043578
[100/200] Training loss: 0.00029782
[150/200] Training loss: 0.00026045
[200/200] Training loss: 0.00023447
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13228.656772325752 ----------
[1/50] Training loss: 0.15856227
[2/50] Training loss: 0.05996067
[3/50] Training loss: 0.05329759
[4/50] Training loss: 0.04906430
[5/50] Training loss: 0.04547354
[6/50] Training loss: 0.04262746
[7/50] Training loss: 0.04069382
[8/50] Training loss: 0.03725243
[9/50] Training loss: 0.03582568
[10/50] Training loss: 0.03446098
[50/50] Training loss: 0.02033271
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 50
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11013.780822224491 ----------
[1/200] Training loss: 0.03796766
[2/200] Training loss: 0.00420388
[3/200] Training loss: 0.00403478
[4/200] Training loss: 0.00384061
[5/200] Training loss: 0.00350690
[6/200] Training loss: 0.00336187
[7/200] Training loss: 0.00311527
[8/200] Training loss: 0.00295344
[9/200] Training loss: 0.00269252
[10/200] Training loss: 0.00268061
[50/200] Training loss: 0.00069235
[100/200] Training loss: 0.00042600
[150/200] Training loss: 0.00035208
[200/200] Training loss: 0.00031262
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8164.962951538727 ----------
[1/200] Training loss: 0.09022515
[2/200] Training loss: 0.00396986
[3/200] Training loss: 0.00351410
[4/200] Training loss: 0.00287284
[5/200] Training loss: 0.00229304
[6/200] Training loss: 0.00213951
[7/200] Training loss: 0.00197458
[8/200] Training loss: 0.00175702
[9/200] Training loss: 0.00133223
[10/200] Training loss: 0.00127803
[50/200] Training loss: 0.00050999
[100/200] Training loss: 0.00040535
[150/200] Training loss: 0.00031656
[200/200] Training loss: 0.00027927
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11628.518048315529 ----------
[1/200] Training loss: 0.09407234
[2/200] Training loss: 0.00446957
[3/200] Training loss: 0.00333580
[4/200] Training loss: 0.00307750
[5/200] Training loss: 0.00273256
[6/200] Training loss: 0.00222194
[7/200] Training loss: 0.00217063
[8/200] Training loss: 0.00191024
[9/200] Training loss: 0.00174481
[10/200] Training loss: 0.00155265
[50/200] Training loss: 0.00050242
[100/200] Training loss: 0.00038682
[150/200] Training loss: 0.00034642
[200/200] Training loss: 0.00028320
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11232.505686622197 ----------
[1/200] Training loss: 0.16954126
[2/200] Training loss: 0.06074226
[3/200] Training loss: 0.05486057
[4/200] Training loss: 0.05189501
[5/200] Training loss: 0.04984147
[6/200] Training loss: 0.04614548
[7/200] Training loss: 0.04288802
[8/200] Training loss: 0.03999205
[9/200] Training loss: 0.03467370
[10/200] Training loss: 0.03168329
[50/200] Training loss: 0.01878102
[100/200] Training loss: 0.01660637
[150/200] Training loss: 0.01597812
[200/200] Training loss: 0.01476994
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14044.090572194413 ----------
[1/200] Training loss: 0.16972443
[2/200] Training loss: 0.06209510
[3/200] Training loss: 0.05333581
[4/200] Training loss: 0.04964080
[5/200] Training loss: 0.04413817
[6/200] Training loss: 0.04352848
[7/200] Training loss: 0.03942714
[8/200] Training loss: 0.03874889
[9/200] Training loss: 0.03590915
[10/200] Training loss: 0.03481729
[50/200] Training loss: 0.01715834
[100/200] Training loss: 0.01469011
[150/200] Training loss: 0.01420070
[200/200] Training loss: 0.01362582
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7512.394558328257 ----------
[1/200] Training loss: 0.23926285
[2/200] Training loss: 0.04540838
[3/200] Training loss: 0.04047544
[4/200] Training loss: 0.03619602
[5/200] Training loss: 0.03396414
[6/200] Training loss: 0.03255472
[7/200] Training loss: 0.03212154
[8/200] Training loss: 0.03180604
[9/200] Training loss: 0.03178843
[10/200] Training loss: 0.03175993
[50/200] Training loss: 0.02640775
[100/200] Training loss: 0.02259441
[150/200] Training loss: 0.02205290
[200/200] Training loss: 0.02149480
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Rprop ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 24540.712948078748 ----------
[1/200] Training loss: 0.04551168
[2/200] Training loss: 0.00461649
[3/200] Training loss: 0.00404094
[4/200] Training loss: 0.00381185
[5/200] Training loss: 0.00323753
[6/200] Training loss: 0.00298483
[7/200] Training loss: 0.00263111
[8/200] Training loss: 0.00215676
[9/200] Training loss: 0.00199713
[10/200] Training loss: 0.00178499
[50/200] Training loss: 0.00055629
[100/200] Training loss: 0.00038656
[150/200] Training loss: 0.00035136
[200/200] Training loss: 0.00027284
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7743.503858073553 ----------
[1/200] Training loss: 0.14062259
[2/200] Training loss: 0.06293280
[3/200] Training loss: 0.05821982
[4/200] Training loss: 0.05234202
[5/200] Training loss: 0.04756630
[6/200] Training loss: 0.04682114
[7/200] Training loss: 0.04695223
[8/200] Training loss: 0.04102903
[9/200] Training loss: 0.03823769
[10/200] Training loss: 0.03508224
[50/200] Training loss: 0.01775002
[100/200] Training loss: 0.01467125
[150/200] Training loss: 0.01421000
[200/200] Training loss: 0.01243832
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15918.047618976392 ----------
[1/200] Training loss: 0.16941467
[2/200] Training loss: 0.05814044
[3/200] Training loss: 0.05286144
[4/200] Training loss: 0.04898767
[5/200] Training loss: 0.04918896
[6/200] Training loss: 0.04374172
[7/200] Training loss: 0.04224108
[8/200] Training loss: 0.04125744
[9/200] Training loss: 0.03719839
[10/200] Training loss: 0.03550928
[50/200] Training loss: 0.01887209
[100/200] Training loss: 0.01529305
[150/200] Training loss: 0.01391951
[200/200] Training loss: 0.01268269
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15773.99378724361 ----------
[1/200] Training loss: 0.14698438
[2/200] Training loss: 0.05726558
[3/200] Training loss: 0.05107330
[4/200] Training loss: 0.04608712
[5/200] Training loss: 0.04284897
[6/200] Training loss: 0.03885527
[7/200] Training loss: 0.03587669
[8/200] Training loss: 0.03731557
[9/200] Training loss: 0.03437294
[10/200] Training loss: 0.03004165
[50/200] Training loss: 0.01711731
[100/200] Training loss: 0.01611388
[150/200] Training loss: 0.01406936
[200/200] Training loss: 0.01351446
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 36226.7331124406 ----------
[1/200] Training loss: 0.06881184
[2/200] Training loss: 0.00465035
[3/200] Training loss: 0.00429001
[4/200] Training loss: 0.00403435
[5/200] Training loss: 0.00388741
[6/200] Training loss: 0.00362143
[7/200] Training loss: 0.00342526
[8/200] Training loss: 0.00307297
[9/200] Training loss: 0.00289835
[10/200] Training loss: 0.00256985
[50/200] Training loss: 0.00061469
[100/200] Training loss: 0.00034469
[150/200] Training loss: 0.00027605
[200/200] Training loss: 0.00026775
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9085.333675765574 ----------
[1/200] Training loss: 0.09188417
[2/200] Training loss: 0.00465585
[3/200] Training loss: 0.00405085
[4/200] Training loss: 0.00374926
[5/200] Training loss: 0.00345894
[6/200] Training loss: 0.00320109
[7/200] Training loss: 0.00261901
[8/200] Training loss: 0.00237307
[9/200] Training loss: 0.00186792
[10/200] Training loss: 0.00147006
[50/200] Training loss: 0.00049816
[100/200] Training loss: 0.00040164
[150/200] Training loss: 0.00032133
[200/200] Training loss: 0.00028609
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12426.724749506606 ----------
[1/200] Training loss: 0.14982269
[2/200] Training loss: 0.05805912
[3/200] Training loss: 0.05180476
[4/200] Training loss: 0.04916139
[5/200] Training loss: 0.04724623
[6/200] Training loss: 0.04305531
[7/200] Training loss: 0.04291558
[8/200] Training loss: 0.03527658
[9/200] Training loss: 0.03480333
[10/200] Training loss: 0.03235989
[50/200] Training loss: 0.01699064
[100/200] Training loss: 0.01509977
[150/200] Training loss: 0.01393503
[200/200] Training loss: 0.01229362
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10842.289057205586 ----------
[1/200] Training loss: 0.15056517
[2/200] Training loss: 0.05966575
[3/200] Training loss: 0.05588582
[4/200] Training loss: 0.05223245
[5/200] Training loss: 0.04781168
[6/200] Training loss: 0.04535543
[7/200] Training loss: 0.04544598
[8/200] Training loss: 0.04220584
[9/200] Training loss: 0.03954269
[10/200] Training loss: 0.03821488
[50/200] Training loss: 0.01726166
[100/200] Training loss: 0.01481495
[150/200] Training loss: 0.01411291
[200/200] Training loss: 0.01367309
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7056.662667295355 ----------
[1/200] Training loss: 0.07184678
[2/200] Training loss: 0.00442350
[3/200] Training loss: 0.00389574
[4/200] Training loss: 0.00349361
[5/200] Training loss: 0.00318330
[6/200] Training loss: 0.00272955
[7/200] Training loss: 0.00248926
[8/200] Training loss: 0.00238544
[9/200] Training loss: 0.00198622
[10/200] Training loss: 0.00181738
[50/200] Training loss: 0.00061645
[100/200] Training loss: 0.00047275
[150/200] Training loss: 0.00039519
[200/200] Training loss: 0.00034423
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14657.26468342576 ----------
[1/200] Training loss: 0.14004767
[2/200] Training loss: 0.05765480
[3/200] Training loss: 0.04989534
[4/200] Training loss: 0.04542427
[5/200] Training loss: 0.04438801
[6/200] Training loss: 0.04108804
[7/200] Training loss: 0.03772883
[8/200] Training loss: 0.03366612
[9/200] Training loss: 0.03491246
[10/200] Training loss: 0.03002043
[50/200] Training loss: 0.01712100
[100/200] Training loss: 0.01434374
[150/200] Training loss: 0.01296139
[200/200] Training loss: 0.01171829
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11660.281986298616 ----------
[1/200] Training loss: 0.16044385
[2/200] Training loss: 0.06423428
[3/200] Training loss: 0.05580784
[4/200] Training loss: 0.05371668
[5/200] Training loss: 0.05100795
[6/200] Training loss: 0.04956876
[7/200] Training loss: 0.05015192
[8/200] Training loss: 0.04939059
[9/200] Training loss: 0.04714686
[10/200] Training loss: 0.04451714
[50/200] Training loss: 0.01721188
[100/200] Training loss: 0.01434162
[150/200] Training loss: 0.01240379
[200/200] Training loss: 0.01211312
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 18738.23641648274 ----------
[1/200] Training loss: 0.08165171
[2/200] Training loss: 0.00490625
[3/200] Training loss: 0.00365938
[4/200] Training loss: 0.00296531
[5/200] Training loss: 0.00246609
[6/200] Training loss: 0.00209755
[7/200] Training loss: 0.00143804
[8/200] Training loss: 0.00122418
[9/200] Training loss: 0.00117450
[10/200] Training loss: 0.00125604
[50/200] Training loss: 0.00059032
[100/200] Training loss: 0.00041800
[150/200] Training loss: 0.00034615
[200/200] Training loss: 0.00030426
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13081.47575772703 ----------
[1/200] Training loss: 0.14103078
[2/200] Training loss: 0.05085486
[3/200] Training loss: 0.04838098
[4/200] Training loss: 0.04381802
[5/200] Training loss: 0.04167223
[6/200] Training loss: 0.03929328
[7/200] Training loss: 0.03886944
[8/200] Training loss: 0.03547614
[9/200] Training loss: 0.03200354
[10/200] Training loss: 0.03163152
[50/200] Training loss: 0.01836159
[100/200] Training loss: 0.01714229
[150/200] Training loss: 0.01599426
[200/200] Training loss: 0.01381758
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12367.126747955646 ----------
[1/200] Training loss: 0.18014282
[2/200] Training loss: 0.06227096
[3/200] Training loss: 0.05736312
[4/200] Training loss: 0.04866011
[5/200] Training loss: 0.04354284
[6/200] Training loss: 0.04168219
[7/200] Training loss: 0.04072179
[8/200] Training loss: 0.03745793
[9/200] Training loss: 0.03719469
[10/200] Training loss: 0.03507893
[50/200] Training loss: 0.02127713
[100/200] Training loss: 0.01805285
[150/200] Training loss: 0.01637872
[200/200] Training loss: 0.01535508
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10181.781376556855 ----------
[1/200] Training loss: 0.14169027
[2/200] Training loss: 0.05729845
[3/200] Training loss: 0.05241600
[4/200] Training loss: 0.04886097
[5/200] Training loss: 0.04454443
[6/200] Training loss: 0.04379661
[7/200] Training loss: 0.04180670
[8/200] Training loss: 0.03763044
[9/200] Training loss: 0.03600684
[10/200] Training loss: 0.03629777
[50/200] Training loss: 0.01732620
[100/200] Training loss: 0.01472536
[150/200] Training loss: 0.01324243
[200/200] Training loss: 0.01276937
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 18539.283265541846 ----------
[1/200] Training loss: 0.01085587
[2/200] Training loss: 0.00306103
[3/200] Training loss: 0.00140953
[4/200] Training loss: 0.00088484
[5/200] Training loss: 0.00073023
[6/200] Training loss: 0.00065011
[7/200] Training loss: 0.00060013
[8/200] Training loss: 0.00055067
[9/200] Training loss: 0.00052320
[10/200] Training loss: 0.00050307
[50/200] Training loss: 0.00026997
[100/200] Training loss: 0.00021203
[150/200] Training loss: 0.00018342
[200/200] Training loss: 0.00016331
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14606.889881148554 ----------
[1/200] Training loss: 0.17313302
[2/200] Training loss: 0.06085962
[3/200] Training loss: 0.05476252
[4/200] Training loss: 0.05129793
[5/200] Training loss: 0.04902748
[6/200] Training loss: 0.04700425
[7/200] Training loss: 0.04532497
[8/200] Training loss: 0.04275796
[9/200] Training loss: 0.04063203
[10/200] Training loss: 0.04002803
[50/200] Training loss: 0.01832541
[100/200] Training loss: 0.01582272
[150/200] Training loss: 0.01465781
[200/200] Training loss: 0.01381458
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13863.361208595843 ----------
[1/200] Training loss: 0.16869514
[2/200] Training loss: 0.06195482
[3/200] Training loss: 0.05523779
[4/200] Training loss: 0.05413819
[5/200] Training loss: 0.04964660
[6/200] Training loss: 0.04921923
[7/200] Training loss: 0.04710426
[8/200] Training loss: 0.04546120
[9/200] Training loss: 0.04329521
[10/200] Training loss: 0.04100381
[50/200] Training loss: 0.01910541
[100/200] Training loss: 0.01628799
[150/200] Training loss: 0.01496834
[200/200] Training loss: 0.01381426
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9753.868975950005 ----------
[1/200] Training loss: 0.14861063
[2/200] Training loss: 0.05951932
[3/200] Training loss: 0.04731631
[4/200] Training loss: 0.04577097
[5/200] Training loss: 0.04774320
[6/200] Training loss: 0.04360923
[7/200] Training loss: 0.04337487
[8/200] Training loss: 0.03958488
[9/200] Training loss: 0.03620749
[10/200] Training loss: 0.03542074
[50/200] Training loss: 0.01740589
[100/200] Training loss: 0.01417327
[150/200] Training loss: 0.01238371
[200/200] Training loss: 0.01193212
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16819.098667883485 ----------
[1/200] Training loss: 0.12751286
[2/200] Training loss: 0.00620037
[3/200] Training loss: 0.00505816
[4/200] Training loss: 0.00504844
[5/200] Training loss: 0.00438511
[6/200] Training loss: 0.00385386
[7/200] Training loss: 0.00332141
[8/200] Training loss: 0.00314927
[9/200] Training loss: 0.00252701
[10/200] Training loss: 0.00202031
[50/200] Training loss: 0.00043689
[100/200] Training loss: 0.00035305
[150/200] Training loss: 0.00030246
[200/200] Training loss: 0.00028607
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16185.736931014293 ----------
[1/200] Training loss: 0.14378315
[2/200] Training loss: 0.05101545
[3/200] Training loss: 0.04692324
[4/200] Training loss: 0.04211449
[5/200] Training loss: 0.04073394
[6/200] Training loss: 0.03824304
[7/200] Training loss: 0.03755491
[8/200] Training loss: 0.03419122
[9/200] Training loss: 0.03477162
[10/200] Training loss: 0.03167324
[50/200] Training loss: 0.01846931
[100/200] Training loss: 0.01494569
[150/200] Training loss: 0.01427198
[200/200] Training loss: 0.01188376
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7198.7746179471405 ----------
[1/200] Training loss: 0.16312780
[2/200] Training loss: 0.06459653
[3/200] Training loss: 0.05190507
[4/200] Training loss: 0.04749943
[5/200] Training loss: 0.04534355
[6/200] Training loss: 0.04494139
[7/200] Training loss: 0.04158219
[8/200] Training loss: 0.03847348
[9/200] Training loss: 0.03613313
[10/200] Training loss: 0.03520432
[50/200] Training loss: 0.01718165
[100/200] Training loss: 0.01438438
[150/200] Training loss: 0.01302892
[200/200] Training loss: 0.01193700
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16525.083963478068 ----------
[1/200] Training loss: 0.15171474
[2/200] Training loss: 0.05267802
[3/200] Training loss: 0.04542584
[4/200] Training loss: 0.04415775
[5/200] Training loss: 0.03993410
[6/200] Training loss: 0.03827110
[7/200] Training loss: 0.03685241
[8/200] Training loss: 0.03492157
[9/200] Training loss: 0.03263313
[10/200] Training loss: 0.03376469
[50/200] Training loss: 0.01826276
[100/200] Training loss: 0.01489759
[150/200] Training loss: 0.01326610
[200/200] Training loss: 0.01229503
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5965.217179617185 ----------
[1/200] Training loss: 0.15023475
[2/200] Training loss: 0.06040766
[3/200] Training loss: 0.05276225
[4/200] Training loss: 0.05011677
[5/200] Training loss: 0.04410011
[6/200] Training loss: 0.03979820
[7/200] Training loss: 0.03530828
[8/200] Training loss: 0.03434819
[9/200] Training loss: 0.03522143
[10/200] Training loss: 0.03250339
[50/200] Training loss: 0.01782588
[100/200] Training loss: 0.01588516
[150/200] Training loss: 0.01430223
[200/200] Training loss: 0.01343628
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10154.841997786081 ----------
[1/200] Training loss: 0.15868530
[2/200] Training loss: 0.06440742
[3/200] Training loss: 0.05625872
[4/200] Training loss: 0.05745021
[5/200] Training loss: 0.05375602
[6/200] Training loss: 0.05143277
[7/200] Training loss: 0.05047604
[8/200] Training loss: 0.04952834
[9/200] Training loss: 0.04672583
[10/200] Training loss: 0.04790598
[50/200] Training loss: 0.01890150
[100/200] Training loss: 0.01643242
[150/200] Training loss: 0.01509294
[200/200] Training loss: 0.01393854
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 18715.509717878376 ----------
[1/200] Training loss: 0.16540026
[2/200] Training loss: 0.06399876
[3/200] Training loss: 0.05204250
[4/200] Training loss: 0.05057321
[5/200] Training loss: 0.04756678
[6/200] Training loss: 0.04469241
[7/200] Training loss: 0.04548893
[8/200] Training loss: 0.04497425
[9/200] Training loss: 0.04096371
[10/200] Training loss: 0.03945943
[50/200] Training loss: 0.01870733
[100/200] Training loss: 0.01671143
[150/200] Training loss: 0.01550712
[200/200] Training loss: 0.01446896
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 22779.682877511706 ----------
[1/200] Training loss: 0.15654889
[2/200] Training loss: 0.05697629
[3/200] Training loss: 0.05718734
[4/200] Training loss: 0.05251864
[5/200] Training loss: 0.04899150
[6/200] Training loss: 0.05143738
[7/200] Training loss: 0.04379445
[8/200] Training loss: 0.04408291
[9/200] Training loss: 0.03795118
[10/200] Training loss: 0.03552405
[50/200] Training loss: 0.01828669
[100/200] Training loss: 0.01499857
[150/200] Training loss: 0.01359515
[200/200] Training loss: 0.01315641
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10492.414021568155 ----------
[1/200] Training loss: 0.09094541
[2/200] Training loss: 0.00561061
[3/200] Training loss: 0.00514743
[4/200] Training loss: 0.00477470
[5/200] Training loss: 0.00461622
[6/200] Training loss: 0.00415041
[7/200] Training loss: 0.00381337
[8/200] Training loss: 0.00363966
[9/200] Training loss: 0.00300118
[10/200] Training loss: 0.00267027
[50/200] Training loss: 0.00058185
[100/200] Training loss: 0.00045181
[150/200] Training loss: 0.00041588
[200/200] Training loss: 0.00034949
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13754.059182655861 ----------
[1/200] Training loss: 0.04369456
[2/200] Training loss: 0.00422954
[3/200] Training loss: 0.00379207
[4/200] Training loss: 0.00342202
[5/200] Training loss: 0.00302604
[6/200] Training loss: 0.00263338
[7/200] Training loss: 0.00217809
[8/200] Training loss: 0.00193843
[9/200] Training loss: 0.00169068
[10/200] Training loss: 0.00146988
[50/200] Training loss: 0.00053621
[100/200] Training loss: 0.00045160
[150/200] Training loss: 0.00038921
[200/200] Training loss: 0.00037191
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 19298.292566960426 ----------
[1/200] Training loss: 0.16930814
[2/200] Training loss: 0.05671939
[3/200] Training loss: 0.05226269
[4/200] Training loss: 0.04785428
[5/200] Training loss: 0.04293685
[6/200] Training loss: 0.04246627
[7/200] Training loss: 0.03687501
[8/200] Training loss: 0.03810785
[9/200] Training loss: 0.03538298
[10/200] Training loss: 0.03175656
[50/200] Training loss: 0.01809728
[100/200] Training loss: 0.01505726
[150/200] Training loss: 0.01378186
[200/200] Training loss: 0.01327557
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13896.690829114677 ----------
[1/200] Training loss: 0.14149284
[2/200] Training loss: 0.05782993
[3/200] Training loss: 0.05275184
[4/200] Training loss: 0.04996397
[5/200] Training loss: 0.04177643
[6/200] Training loss: 0.04186619
[7/200] Training loss: 0.03618782
[8/200] Training loss: 0.03816551
[9/200] Training loss: 0.03286587
[10/200] Training loss: 0.03325085
[50/200] Training loss: 0.01786839
[100/200] Training loss: 0.01524363
[150/200] Training loss: 0.01475196
[200/200] Training loss: 0.01273912
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9636.023246132192 ----------
[1/200] Training loss: 0.15897575
[2/200] Training loss: 0.06408448
[3/200] Training loss: 0.05406548
[4/200] Training loss: 0.05237694
[5/200] Training loss: 0.04910893
[6/200] Training loss: 0.04608211
[7/200] Training loss: 0.04261629
[8/200] Training loss: 0.04348750
[9/200] Training loss: 0.04057354
[10/200] Training loss: 0.03864404
[50/200] Training loss: 0.01986943
[100/200] Training loss: 0.01519642
[150/200] Training loss: 0.01413185
[200/200] Training loss: 0.01241167
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13368.985002609585 ----------
[1/200] Training loss: 0.16546203
[2/200] Training loss: 0.05563330
[3/200] Training loss: 0.04842002
[4/200] Training loss: 0.04592800
[5/200] Training loss: 0.04462954
[6/200] Training loss: 0.04423720
[7/200] Training loss: 0.03856395
[8/200] Training loss: 0.03865624
[9/200] Training loss: 0.03651833
[10/200] Training loss: 0.03412437
[50/200] Training loss: 0.01806390
[100/200] Training loss: 0.01632408
[150/200] Training loss: 0.01506394
[200/200] Training loss: 0.01401106
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10371.477426095089 ----------
[1/200] Training loss: 0.06536591
[2/200] Training loss: 0.00472176
[3/200] Training loss: 0.00388118
[4/200] Training loss: 0.00339808
[5/200] Training loss: 0.00257547
[6/200] Training loss: 0.00216243
[7/200] Training loss: 0.00173410
[8/200] Training loss: 0.00132908
[9/200] Training loss: 0.00130348
[10/200] Training loss: 0.00121442
[50/200] Training loss: 0.00049291
[100/200] Training loss: 0.00036466
[150/200] Training loss: 0.00031600
[200/200] Training loss: 0.00028029
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13323.819272265742 ----------
[1/200] Training loss: 0.15101419
[2/200] Training loss: 0.05613670
[3/200] Training loss: 0.05115748
[4/200] Training loss: 0.05002121
[5/200] Training loss: 0.04802373
[6/200] Training loss: 0.04195486
[7/200] Training loss: 0.03955829
[8/200] Training loss: 0.03585619
[9/200] Training loss: 0.03260251
[10/200] Training loss: 0.03326350
[50/200] Training loss: 0.01761769
[100/200] Training loss: 0.01513984
[150/200] Training loss: 0.01412912
[200/200] Training loss: 0.01354051
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14224.908294959234 ----------
[1/200] Training loss: 0.12003061
[2/200] Training loss: 0.00499848
[3/200] Training loss: 0.00408764
[4/200] Training loss: 0.00318717
[5/200] Training loss: 0.00246733
[6/200] Training loss: 0.00207905
[7/200] Training loss: 0.00174979
[8/200] Training loss: 0.00135710
[9/200] Training loss: 0.00113373
[10/200] Training loss: 0.00102267
[50/200] Training loss: 0.00057675
[100/200] Training loss: 0.00044957
[150/200] Training loss: 0.00037998
[200/200] Training loss: 0.00034375
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15750.355932486098 ----------
[1/200] Training loss: 0.17030293
[2/200] Training loss: 0.06112095
[3/200] Training loss: 0.05722282
[4/200] Training loss: 0.05462602
[5/200] Training loss: 0.04749542
[6/200] Training loss: 0.04409653
[7/200] Training loss: 0.04174023
[8/200] Training loss: 0.03884744
[9/200] Training loss: 0.04140331
[10/200] Training loss: 0.03640642
[50/200] Training loss: 0.02071881
[100/200] Training loss: 0.01751446
[150/200] Training loss: 0.01605981
[200/200] Training loss: 0.01489414
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17289.172565510475 ----------
[1/200] Training loss: 0.15030873
[2/200] Training loss: 0.05795906
[3/200] Training loss: 0.05243045
[4/200] Training loss: 0.04674026
[5/200] Training loss: 0.04257833
[6/200] Training loss: 0.03986201
[7/200] Training loss: 0.03980628
[8/200] Training loss: 0.03402498
[9/200] Training loss: 0.03420455
[10/200] Training loss: 0.03139048
[50/200] Training loss: 0.01863745
[100/200] Training loss: 0.01569429
[150/200] Training loss: 0.01471902
[200/200] Training loss: 0.01340264
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15683.491448016286 ----------
[1/200] Training loss: 0.16797063
[2/200] Training loss: 0.05688239
[3/200] Training loss: 0.05562302
[4/200] Training loss: 0.05032359
[5/200] Training loss: 0.04748238
[6/200] Training loss: 0.04418085
[7/200] Training loss: 0.04337089
[8/200] Training loss: 0.04181230
[9/200] Training loss: 0.03881837
[10/200] Training loss: 0.03418223
[50/200] Training loss: 0.01908495
[100/200] Training loss: 0.01631274
[150/200] Training loss: 0.01477099
[200/200] Training loss: 0.01406392
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9036.92115711983 ----------
[1/200] Training loss: 0.11827532
[2/200] Training loss: 0.00424017
[3/200] Training loss: 0.00418464
[4/200] Training loss: 0.00363633
[5/200] Training loss: 0.00331900
[6/200] Training loss: 0.00322299
[7/200] Training loss: 0.00267472
[8/200] Training loss: 0.00250189
[9/200] Training loss: 0.00219382
[10/200] Training loss: 0.00197930
[50/200] Training loss: 0.00062920
[100/200] Training loss: 0.00042348
[150/200] Training loss: 0.00033863
[200/200] Training loss: 0.00029311
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11252.867723385003 ----------
[1/200] Training loss: 0.17944002
[2/200] Training loss: 0.06097195
[3/200] Training loss: 0.05090765
[4/200] Training loss: 0.04903209
[5/200] Training loss: 0.03980279
[6/200] Training loss: 0.04025194
[7/200] Training loss: 0.03815391
[8/200] Training loss: 0.03450313
[9/200] Training loss: 0.03206787
[10/200] Training loss: 0.03016993
[50/200] Training loss: 0.01801286
[100/200] Training loss: 0.01614872
[150/200] Training loss: 0.01523799
[200/200] Training loss: 0.01420307
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14440.980022145312 ----------
[1/200] Training loss: 0.19691294
[2/200] Training loss: 0.06298763
[3/200] Training loss: 0.05702784
[4/200] Training loss: 0.05383353
[5/200] Training loss: 0.05008672
[6/200] Training loss: 0.04504063
[7/200] Training loss: 0.04323096
[8/200] Training loss: 0.04221266
[9/200] Training loss: 0.03840487
[10/200] Training loss: 0.03461669
[50/200] Training loss: 0.01820542
[100/200] Training loss: 0.01560933
[150/200] Training loss: 0.01484468
[200/200] Training loss: 0.01484616
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 28726.657724141874 ----------
[1/50] Training loss: 0.15021835
[2/50] Training loss: 0.00644661
[3/50] Training loss: 0.00585515
[4/50] Training loss: 0.00585798
[5/50] Training loss: 0.00533334
[6/50] Training loss: 0.00530394
[7/50] Training loss: 0.00491139
[8/50] Training loss: 0.00471138
[9/50] Training loss: 0.00452049
[10/50] Training loss: 0.00411995
[50/50] Training loss: 0.00057413
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 50
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18484.363986894437 ----------
[1/200] Training loss: 0.13431701
[2/200] Training loss: 0.05575064
[3/200] Training loss: 0.04951555
[4/200] Training loss: 0.04707518
[5/200] Training loss: 0.04643485
[6/200] Training loss: 0.04121785
[7/200] Training loss: 0.03807687
[8/200] Training loss: 0.03821733
[9/200] Training loss: 0.03509933
[10/200] Training loss: 0.03469661
[50/200] Training loss: 0.01786009
[100/200] Training loss: 0.01410817
[150/200] Training loss: 0.01283367
[200/200] Training loss: 0.01167664
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8838.651933411566 ----------
[1/200] Training loss: 0.15875681
[2/200] Training loss: 0.06044027
[3/200] Training loss: 0.05531295
[4/200] Training loss: 0.05150892
[5/200] Training loss: 0.05182033
[6/200] Training loss: 0.04803660
[7/200] Training loss: 0.04406622
[8/200] Training loss: 0.04556894
[9/200] Training loss: 0.04259175
[10/200] Training loss: 0.03740816
[50/200] Training loss: 0.01973675
[100/200] Training loss: 0.01728366
[150/200] Training loss: 0.01644007
[200/200] Training loss: 0.01570383
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12470.2128289777 ----------
[1/200] Training loss: 0.09431744
[2/200] Training loss: 0.00496820
[3/200] Training loss: 0.00398525
[4/200] Training loss: 0.00351348
[5/200] Training loss: 0.00334407
[6/200] Training loss: 0.00308346
[7/200] Training loss: 0.00283898
[8/200] Training loss: 0.00261795
[9/200] Training loss: 0.00247424
[10/200] Training loss: 0.00228443
[50/200] Training loss: 0.00054463
[100/200] Training loss: 0.00044212
[150/200] Training loss: 0.00037486
[200/200] Training loss: 0.00033689
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10278.976602755743 ----------
[1/200] Training loss: 0.02609927
[2/200] Training loss: 0.00387071
[3/200] Training loss: 0.00340266
[4/200] Training loss: 0.00309966
[5/200] Training loss: 0.00269721
[6/200] Training loss: 0.00243170
[7/200] Training loss: 0.00207304
[8/200] Training loss: 0.00191156
[9/200] Training loss: 0.00171572
[10/200] Training loss: 0.00155111
[50/200] Training loss: 0.00050234
[100/200] Training loss: 0.00035329
[150/200] Training loss: 0.00030179
[200/200] Training loss: 0.00025354
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8197.795801311471 ----------
[1/200] Training loss: 0.15404896
[2/200] Training loss: 0.06185689
[3/200] Training loss: 0.05537488
[4/200] Training loss: 0.05198221
[5/200] Training loss: 0.04942457
[6/200] Training loss: 0.04687372
[7/200] Training loss: 0.04462261
[8/200] Training loss: 0.04386635
[9/200] Training loss: 0.04104207
[10/200] Training loss: 0.03825208
[50/200] Training loss: 0.01787427
[100/200] Training loss: 0.01534767
[150/200] Training loss: 0.01382598
[200/200] Training loss: 0.01266915
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7781.9149314291535 ----------
[1/200] Training loss: 0.13034370
[2/200] Training loss: 0.05435615
[3/200] Training loss: 0.05141041
[4/200] Training loss: 0.04767829
[5/200] Training loss: 0.04460176
[6/200] Training loss: 0.04486281
[7/200] Training loss: 0.03841330
[8/200] Training loss: 0.03618318
[9/200] Training loss: 0.03706557
[10/200] Training loss: 0.03214412
[50/200] Training loss: 0.01663978
[100/200] Training loss: 0.01358309
[150/200] Training loss: 0.01259572
[200/200] Training loss: 0.01169418
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9779.325948141824 ----------
[1/200] Training loss: 0.14244774
[2/200] Training loss: 0.05886652
[3/200] Training loss: 0.05424801
[4/200] Training loss: 0.04486408
[5/200] Training loss: 0.04417399
[6/200] Training loss: 0.04053059
[7/200] Training loss: 0.03890323
[8/200] Training loss: 0.03539938
[9/200] Training loss: 0.03249066
[10/200] Training loss: 0.03169683
[50/200] Training loss: 0.01746239
[100/200] Training loss: 0.01615608
[150/200] Training loss: 0.01495896
[200/200] Training loss: 0.01394554
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9680.362389910824 ----------
[1/200] Training loss: 0.19563681
[2/200] Training loss: 0.06711888
[3/200] Training loss: 0.05777007
[4/200] Training loss: 0.05236352
[5/200] Training loss: 0.04830753
[6/200] Training loss: 0.04828372
[7/200] Training loss: 0.04888987
[8/200] Training loss: 0.04641416
[9/200] Training loss: 0.04362192
[10/200] Training loss: 0.04086208
[50/200] Training loss: 0.02032285
[100/200] Training loss: 0.01590401
[150/200] Training loss: 0.01403801
[200/200] Training loss: 0.01367637
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18481.5116264877 ----------
[1/200] Training loss: 0.09658900
[2/200] Training loss: 0.00440996
[3/200] Training loss: 0.00392496
[4/200] Training loss: 0.00322142
[5/200] Training loss: 0.00270888
[6/200] Training loss: 0.00241780
[7/200] Training loss: 0.00186477
[8/200] Training loss: 0.00154023
[9/200] Training loss: 0.00144530
[10/200] Training loss: 0.00119179
[50/200] Training loss: 0.00056674
[100/200] Training loss: 0.00043521
[150/200] Training loss: 0.00036218
[200/200] Training loss: 0.00031212
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14287.072758266475 ----------
[1/200] Training loss: 0.17175339
[2/200] Training loss: 0.05907448
[3/200] Training loss: 0.05206226
[4/200] Training loss: 0.05145698
[5/200] Training loss: 0.04423170
[6/200] Training loss: 0.04086844
[7/200] Training loss: 0.03644789
[8/200] Training loss: 0.03460699
[9/200] Training loss: 0.03133369
[10/200] Training loss: 0.03027769
[50/200] Training loss: 0.01800430
[100/200] Training loss: 0.01602860
[150/200] Training loss: 0.01474291
[200/200] Training loss: 0.01336907
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18508.723132620467 ----------
[1/100] Training loss: 0.17320538
[2/100] Training loss: 0.05640525
[3/100] Training loss: 0.05674773
[4/100] Training loss: 0.04881650
[5/100] Training loss: 0.04713461
[6/100] Training loss: 0.04405672
[7/100] Training loss: 0.04420441
[8/100] Training loss: 0.03979483
[9/100] Training loss: 0.03850199
[10/100] Training loss: 0.03855252
[50/100] Training loss: 0.02032529
[100/100] Training loss: 0.01571966
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 64 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6213.846151941646 ----------
[1/200] Training loss: 0.17115081
[2/200] Training loss: 0.06020160
[3/200] Training loss: 0.05191743
[4/200] Training loss: 0.04894855
[5/200] Training loss: 0.04590470
[6/200] Training loss: 0.03876515
[7/200] Training loss: 0.03902000
[8/200] Training loss: 0.03721910
[9/200] Training loss: 0.03353597
[10/200] Training loss: 0.03412047
[50/200] Training loss: 0.01598795
[100/200] Training loss: 0.01465764
[150/200] Training loss: 0.01333711
[200/200] Training loss: 0.01255125
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8299.53058913575 ----------
[1/200] Training loss: 0.17492162
[2/200] Training loss: 0.05848213
[3/200] Training loss: 0.04987655
[4/200] Training loss: 0.04844425
[5/200] Training loss: 0.04733419
[6/200] Training loss: 0.04379454
[7/200] Training loss: 0.04072437
[8/200] Training loss: 0.03962029
[9/200] Training loss: 0.03656545
[10/200] Training loss: 0.03652634
[50/200] Training loss: 0.01687564
[100/200] Training loss: 0.01464865
[150/200] Training loss: 0.01353764
[200/200] Training loss: 0.01224137
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 3957.5764048215165 ----------
[1/200] Training loss: 0.14347402
[2/200] Training loss: 0.05671618
[3/200] Training loss: 0.05157935
[4/200] Training loss: 0.04381587
[5/200] Training loss: 0.04491832
[6/200] Training loss: 0.03969475
[7/200] Training loss: 0.03622886
[8/200] Training loss: 0.03492796
[9/200] Training loss: 0.03049014
[10/200] Training loss: 0.03221645
[50/200] Training loss: 0.01789171
[100/200] Training loss: 0.01370226
[150/200] Training loss: 0.01291690
[200/200] Training loss: 0.01222844
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17692.95272135208 ----------
[1/200] Training loss: 0.15665258
[2/200] Training loss: 0.05378369
[3/200] Training loss: 0.04814764
[4/200] Training loss: 0.04326409
[5/200] Training loss: 0.04188397
[6/200] Training loss: 0.03758822
[7/200] Training loss: 0.03427820
[8/200] Training loss: 0.03634417
[9/200] Training loss: 0.03312316
[10/200] Training loss: 0.03020555
[50/200] Training loss: 0.01905519
[100/200] Training loss: 0.01642315
[150/200] Training loss: 0.01527497
[200/200] Training loss: 0.01437024
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21623.943026192053 ----------
[1/200] Training loss: 0.12256375
[2/200] Training loss: 0.00500464
[3/200] Training loss: 0.00434541
[4/200] Training loss: 0.00423331
[5/200] Training loss: 0.00407390
[6/200] Training loss: 0.00376112
[7/200] Training loss: 0.00349549
[8/200] Training loss: 0.00324548
[9/200] Training loss: 0.00296664
[10/200] Training loss: 0.00285164
[50/200] Training loss: 0.00045276
[100/200] Training loss: 0.00032827
[150/200] Training loss: 0.00027872
[200/200] Training loss: 0.00025538
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 17478.850305440574 ----------
[1/200] Training loss: 0.16057387
[2/200] Training loss: 0.05892290
[3/200] Training loss: 0.05431989
[4/200] Training loss: 0.04983021
[5/200] Training loss: 0.04483129
[6/200] Training loss: 0.04391584
[7/200] Training loss: 0.04229150
[8/200] Training loss: 0.03867104
[9/200] Training loss: 0.03511933
[10/200] Training loss: 0.03488642
[50/200] Training loss: 0.01666091
[100/200] Training loss: 0.01326023
[150/200] Training loss: 0.01180661
[200/200] Training loss: 0.01109364
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13957.242707641077 ----------
[1/200] Training loss: 0.14578595
[2/200] Training loss: 0.06003316
[3/200] Training loss: 0.05028287
[4/200] Training loss: 0.04963002
[5/200] Training loss: 0.04635275
[6/200] Training loss: 0.04162775
[7/200] Training loss: 0.03676458
[8/200] Training loss: 0.03615022
[9/200] Training loss: 0.03404042
[10/200] Training loss: 0.03210847
[50/200] Training loss: 0.01872537
[100/200] Training loss: 0.01575499
[150/200] Training loss: 0.01411786
[200/200] Training loss: 0.01247398
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15706.826541348191 ----------
[1/200] Training loss: 0.15228915
[2/200] Training loss: 0.06043886
[3/200] Training loss: 0.05381850
[4/200] Training loss: 0.04879770
[5/200] Training loss: 0.04421453
[6/200] Training loss: 0.04223642
[7/200] Training loss: 0.04136649
[8/200] Training loss: 0.03961186
[9/200] Training loss: 0.03581995
[10/200] Training loss: 0.03421051
[50/200] Training loss: 0.01891795
[100/200] Training loss: 0.01584077
[150/200] Training loss: 0.01491482
[200/200] Training loss: 0.01381077
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19918.74855506741 ----------
[1/200] Training loss: 0.06699998
[2/200] Training loss: 0.00565930
[3/200] Training loss: 0.00502835
[4/200] Training loss: 0.00453906
[5/200] Training loss: 0.00393060
[6/200] Training loss: 0.00318976
[7/200] Training loss: 0.00252672
[8/200] Training loss: 0.00208810
[9/200] Training loss: 0.00182311
[10/200] Training loss: 0.00185041
[50/200] Training loss: 0.00057719
[100/200] Training loss: 0.00038839
[150/200] Training loss: 0.00032220
[200/200] Training loss: 0.00030548
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16483.747146811005 ----------
[1/200] Training loss: 0.12286763
[2/200] Training loss: 0.05422349
[3/200] Training loss: 0.04922004
[4/200] Training loss: 0.04652667
[5/200] Training loss: 0.04331960
[6/200] Training loss: 0.03964690
[7/200] Training loss: 0.03644798
[8/200] Training loss: 0.03613325
[9/200] Training loss: 0.03347421
[10/200] Training loss: 0.03221423
[50/200] Training loss: 0.01719530
[100/200] Training loss: 0.01466157
[150/200] Training loss: 0.01344554
[200/200] Training loss: 0.01214012
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12954.910111614052 ----------
[1/200] Training loss: 0.18430709
[2/200] Training loss: 0.06620611
[3/200] Training loss: 0.05552235
[4/200] Training loss: 0.05020981
[5/200] Training loss: 0.04808466
[6/200] Training loss: 0.04539596
[7/200] Training loss: 0.04653762
[8/200] Training loss: 0.04147004
[9/200] Training loss: 0.03969940
[10/200] Training loss: 0.04040318
[50/200] Training loss: 0.01855278
[100/200] Training loss: 0.01452496
[150/200] Training loss: 0.01425437
[200/200] Training loss: 0.01353508
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5598.1132535882125 ----------
[1/200] Training loss: 0.16411202
[2/200] Training loss: 0.05719406
[3/200] Training loss: 0.04953767
[4/200] Training loss: 0.04666108
[5/200] Training loss: 0.04325232
[6/200] Training loss: 0.03846323
[7/200] Training loss: 0.04095988
[8/200] Training loss: 0.03761696
[9/200] Training loss: 0.03816653
[10/200] Training loss: 0.03511943
[50/200] Training loss: 0.02193030
[100/200] Training loss: 0.01630129
[150/200] Training loss: 0.01426676
[200/200] Training loss: 0.01305127
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 6832.400749370605 ----------
[1/200] Training loss: 0.16372431
[2/200] Training loss: 0.06200591
[3/200] Training loss: 0.05550987
[4/200] Training loss: 0.05248867
[5/200] Training loss: 0.04701291
[6/200] Training loss: 0.04476096
[7/200] Training loss: 0.03988969
[8/200] Training loss: 0.03854076
[9/200] Training loss: 0.03478530
[10/200] Training loss: 0.03353438
[50/200] Training loss: 0.01908976
[100/200] Training loss: 0.01751220
[150/200] Training loss: 0.01578783
[200/200] Training loss: 0.01484950
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11450.547235831133 ----------
[1/200] Training loss: 0.15601910
[2/200] Training loss: 0.05924510
[3/200] Training loss: 0.05131770
[4/200] Training loss: 0.05159029
[5/200] Training loss: 0.04922464
[6/200] Training loss: 0.04594297
[7/200] Training loss: 0.04355192
[8/200] Training loss: 0.04009065
[9/200] Training loss: 0.03506395
[10/200] Training loss: 0.03469383
[50/200] Training loss: 0.01820801
[100/200] Training loss: 0.01693914
[150/200] Training loss: 0.01552285
[200/200] Training loss: 0.01466122
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11795.097286584796 ----------
[1/200] Training loss: 0.16821873
[2/200] Training loss: 0.06065002
[3/200] Training loss: 0.05391778
[4/200] Training loss: 0.04898482
[5/200] Training loss: 0.04623592
[6/200] Training loss: 0.04322045
[7/200] Training loss: 0.04001018
[8/200] Training loss: 0.03475025
[9/200] Training loss: 0.03480949
[10/200] Training loss: 0.03169923
[50/200] Training loss: 0.01887839
[100/200] Training loss: 0.01645003
[150/200] Training loss: 0.01388012
[200/200] Training loss: 0.01355945
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22286.886547923197 ----------
[1/200] Training loss: 0.17834404
[2/200] Training loss: 0.06336012
[3/200] Training loss: 0.05376322
[4/200] Training loss: 0.05067840
[5/200] Training loss: 0.04626321
[6/200] Training loss: 0.04626787
[7/200] Training loss: 0.04203567
[8/200] Training loss: 0.04204625
[9/200] Training loss: 0.03996444
[10/200] Training loss: 0.03889202
[50/200] Training loss: 0.01859279
[100/200] Training loss: 0.01603706
[150/200] Training loss: 0.01465113
[200/200] Training loss: 0.01351445
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16608.54430707279 ----------
[1/200] Training loss: 0.03072024
[2/200] Training loss: 0.00519835
[3/200] Training loss: 0.00459829
[4/200] Training loss: 0.00408650
[5/200] Training loss: 0.00375031
[6/200] Training loss: 0.00357808
[7/200] Training loss: 0.00309891
[8/200] Training loss: 0.00281857
[9/200] Training loss: 0.00269093
[10/200] Training loss: 0.00262054
[50/200] Training loss: 0.00068749
[100/200] Training loss: 0.00044061
[150/200] Training loss: 0.00035110
[200/200] Training loss: 0.00029197
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7333.540481922766 ----------
[1/200] Training loss: 0.16394627
[2/200] Training loss: 0.05863452
[3/200] Training loss: 0.05352500
[4/200] Training loss: 0.04668204
[5/200] Training loss: 0.04603900
[6/200] Training loss: 0.04139807
[7/200] Training loss: 0.04064541
[8/200] Training loss: 0.03819514
[9/200] Training loss: 0.03795416
[10/200] Training loss: 0.03501914
[50/200] Training loss: 0.01781655
[100/200] Training loss: 0.01570937
[150/200] Training loss: 0.01394882
[200/200] Training loss: 0.01335770
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 3533.7335779597192 ----------
[1/200] Training loss: 0.15221120
[2/200] Training loss: 0.05748913
[3/200] Training loss: 0.05312307
[4/200] Training loss: 0.05209006
[5/200] Training loss: 0.04647609
[6/200] Training loss: 0.04373197
[7/200] Training loss: 0.04115400
[8/200] Training loss: 0.03799556
[9/200] Training loss: 0.03893071
[10/200] Training loss: 0.03247797
[50/200] Training loss: 0.01689094
[100/200] Training loss: 0.01420999
[150/200] Training loss: 0.01319150
[200/200] Training loss: 0.01173364
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18555.029506848 ----------
[1/200] Training loss: 0.16182988
[2/200] Training loss: 0.05860860
[3/200] Training loss: 0.05363344
[4/200] Training loss: 0.05260763
[5/200] Training loss: 0.04836422
[6/200] Training loss: 0.04563487
[7/200] Training loss: 0.04324337
[8/200] Training loss: 0.04275560
[9/200] Training loss: 0.04063135
[10/200] Training loss: 0.04023308
[50/200] Training loss: 0.01681785
[100/200] Training loss: 0.01512343
[150/200] Training loss: 0.01392776
[200/200] Training loss: 0.01294005
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10340.042553103927 ----------
[1/200] Training loss: 0.15856798
[2/200] Training loss: 0.05956653
[3/200] Training loss: 0.05235348
[4/200] Training loss: 0.05241459
[5/200] Training loss: 0.04552499
[6/200] Training loss: 0.04372538
[7/200] Training loss: 0.04236430
[8/200] Training loss: 0.04126659
[9/200] Training loss: 0.03769051
[10/200] Training loss: 0.04036354
[50/200] Training loss: 0.01952625
[100/200] Training loss: 0.01578457
[150/200] Training loss: 0.01387685
[200/200] Training loss: 0.01289215
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5532.356279199668 ----------
[1/200] Training loss: 0.15589310
[2/200] Training loss: 0.06205779
[3/200] Training loss: 0.04562132
[4/200] Training loss: 0.04582151
[5/200] Training loss: 0.04223756
[6/200] Training loss: 0.03552046
[7/200] Training loss: 0.03473616
[8/200] Training loss: 0.03326284
[9/200] Training loss: 0.03133591
[10/200] Training loss: 0.03308837
[50/200] Training loss: 0.01772649
[100/200] Training loss: 0.01478291
[150/200] Training loss: 0.01337073
[200/200] Training loss: 0.01224210
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17966.41667111169 ----------
[1/200] Training loss: 0.17494717
[2/200] Training loss: 0.06132401
[3/200] Training loss: 0.05356666
[4/200] Training loss: 0.05083579
[5/200] Training loss: 0.04799587
[6/200] Training loss: 0.04829087
[7/200] Training loss: 0.04651839
[8/200] Training loss: 0.04271601
[9/200] Training loss: 0.03972878
[10/200] Training loss: 0.03946221
[50/200] Training loss: 0.01917389
[100/200] Training loss: 0.01689150
[150/200] Training loss: 0.01513739
[200/200] Training loss: 0.01411839
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16197.979627101646 ----------
[1/200] Training loss: 0.14541906
[2/200] Training loss: 0.05791474
[3/200] Training loss: 0.05159145
[4/200] Training loss: 0.04897565
[5/200] Training loss: 0.04550513
[6/200] Training loss: 0.04256869
[7/200] Training loss: 0.04046077
[8/200] Training loss: 0.04191804
[9/200] Training loss: 0.03738082
[10/200] Training loss: 0.03502681
[50/200] Training loss: 0.01726573
[100/200] Training loss: 0.01582141
[150/200] Training loss: 0.01406718
[200/200] Training loss: 0.01266308
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18575.783805804804 ----------
[1/100] Training loss: 0.16583587
[2/100] Training loss: 0.05896649
[3/100] Training loss: 0.05227456
[4/100] Training loss: 0.04569439
[5/100] Training loss: 0.04266906
[6/100] Training loss: 0.04082592
[7/100] Training loss: 0.03946634
[8/100] Training loss: 0.03638085
[9/100] Training loss: 0.03561585
[10/100] Training loss: 0.03312143
[50/100] Training loss: 0.02046102
[100/100] Training loss: 0.01817528
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7896.658787107367 ----------
[1/200] Training loss: 0.16178437
[2/200] Training loss: 0.05468484
[3/200] Training loss: 0.04882549
[4/200] Training loss: 0.04484951
[5/200] Training loss: 0.04306156
[6/200] Training loss: 0.03885967
[7/200] Training loss: 0.03933021
[8/200] Training loss: 0.03556308
[9/200] Training loss: 0.03357080
[10/200] Training loss: 0.03178383
[50/200] Training loss: 0.01801740
[100/200] Training loss: 0.01561574
[150/200] Training loss: 0.01478182
[200/200] Training loss: 0.01303068
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11790.222389760085 ----------
[1/200] Training loss: 0.15496568
[2/200] Training loss: 0.06418146
[3/200] Training loss: 0.05623009
[4/200] Training loss: 0.05498206
[5/200] Training loss: 0.04996982
[6/200] Training loss: 0.04903232
[7/200] Training loss: 0.04472343
[8/200] Training loss: 0.04249116
[9/200] Training loss: 0.03959818
[10/200] Training loss: 0.03766713
[50/200] Training loss: 0.01951422
[100/200] Training loss: 0.01612106
[150/200] Training loss: 0.01462896
[200/200] Training loss: 0.01381295
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14639.877595116703 ----------
[1/200] Training loss: 0.15709208
[2/200] Training loss: 0.05889147
[3/200] Training loss: 0.05620252
[4/200] Training loss: 0.04704118
[5/200] Training loss: 0.04274422
[6/200] Training loss: 0.04051147
[7/200] Training loss: 0.03777578
[8/200] Training loss: 0.03576901
[9/200] Training loss: 0.03228292
[10/200] Training loss: 0.03061299
[50/200] Training loss: 0.01729125
[100/200] Training loss: 0.01502228
[150/200] Training loss: 0.01438889
[200/200] Training loss: 0.01242418
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13835.303249296707 ----------
[1/200] Training loss: 0.15035898
[2/200] Training loss: 0.05926291
[3/200] Training loss: 0.05166030
[4/200] Training loss: 0.05075175
[5/200] Training loss: 0.04472007
[6/200] Training loss: 0.03863439
[7/200] Training loss: 0.03371713
[8/200] Training loss: 0.03087880
[9/200] Training loss: 0.02860477
[10/200] Training loss: 0.02554226
[50/200] Training loss: 0.01666787
[100/200] Training loss: 0.01318078
[150/200] Training loss: 0.01201044
[200/200] Training loss: 0.01125017
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18467.114988541118 ----------
[1/200] Training loss: 0.14176061
[2/200] Training loss: 0.00494265
[3/200] Training loss: 0.00420441
[4/200] Training loss: 0.00346241
[5/200] Training loss: 0.00311847
[6/200] Training loss: 0.00261193
[7/200] Training loss: 0.00236281
[8/200] Training loss: 0.00195074
[9/200] Training loss: 0.00175855
[10/200] Training loss: 0.00168599
[50/200] Training loss: 0.00060620
[100/200] Training loss: 0.00044344
[150/200] Training loss: 0.00038921
[200/200] Training loss: 0.00032933
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10011.19892919924 ----------
[1/200] Training loss: 0.18196175
[2/200] Training loss: 0.06304477
[3/200] Training loss: 0.05439420
[4/200] Training loss: 0.04982509
[5/200] Training loss: 0.04932217
[6/200] Training loss: 0.04662178
[7/200] Training loss: 0.04204318
[8/200] Training loss: 0.04000950
[9/200] Training loss: 0.04190684
[10/200] Training loss: 0.03835172
[50/200] Training loss: 0.01846530
[100/200] Training loss: 0.01585244
[150/200] Training loss: 0.01454074
[200/200] Training loss: 0.01265115
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21397.381522046104 ----------
[1/200] Training loss: 0.12893959
[2/200] Training loss: 0.05410842
[3/200] Training loss: 0.04811622
[4/200] Training loss: 0.04730071
[5/200] Training loss: 0.04146343
[6/200] Training loss: 0.04115088
[7/200] Training loss: 0.03871346
[8/200] Training loss: 0.03451392
[9/200] Training loss: 0.03520656
[10/200] Training loss: 0.03350935
[50/200] Training loss: 0.01793753
[100/200] Training loss: 0.01497764
[150/200] Training loss: 0.01305356
[200/200] Training loss: 0.01260465
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12033.47796773651 ----------
[1/100] Training loss: 0.09630261
[2/100] Training loss: 0.00534482
[3/100] Training loss: 0.00479555
[4/100] Training loss: 0.00460864
[5/100] Training loss: 0.00417065
[6/100] Training loss: 0.00370908
[7/100] Training loss: 0.00350189
[8/100] Training loss: 0.00292608
[9/100] Training loss: 0.00272837
[10/100] Training loss: 0.00230038
[50/100] Training loss: 0.00055760
[100/100] Training loss: 0.00038201
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 15708.56428831101 ----------
[1/200] Training loss: 0.14742343
[2/200] Training loss: 0.05646249
[3/200] Training loss: 0.04997585
[4/200] Training loss: 0.04359537
[5/200] Training loss: 0.04061661
[6/200] Training loss: 0.03746749
[7/200] Training loss: 0.03787809
[8/200] Training loss: 0.03338387
[9/200] Training loss: 0.03154456
[10/200] Training loss: 0.02922599
[50/200] Training loss: 0.01885077
[100/200] Training loss: 0.01633661
[150/200] Training loss: 0.01542043
[200/200] Training loss: 0.01387521
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13354.83972198843 ----------
[1/200] Training loss: 0.16947754
[2/200] Training loss: 0.06201005
[3/200] Training loss: 0.05714888
[4/200] Training loss: 0.05416585
[5/200] Training loss: 0.05315515
[6/200] Training loss: 0.04921215
[7/200] Training loss: 0.04891807
[8/200] Training loss: 0.04741016
[9/200] Training loss: 0.04191342
[10/200] Training loss: 0.04403285
[50/200] Training loss: 0.01955123
[100/200] Training loss: 0.01520397
[150/200] Training loss: 0.01456793
[200/200] Training loss: 0.01222388
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16920.859552635025 ----------
[1/200] Training loss: 0.17617032
[2/200] Training loss: 0.05782561
[3/200] Training loss: 0.05048930
[4/200] Training loss: 0.04930556
[5/200] Training loss: 0.04512075
[6/200] Training loss: 0.04169045
[7/200] Training loss: 0.04071616
[8/200] Training loss: 0.03826663
[9/200] Training loss: 0.03681705
[10/200] Training loss: 0.03398156
[50/200] Training loss: 0.01805853
[100/200] Training loss: 0.01640481
[150/200] Training loss: 0.01473578
[200/200] Training loss: 0.01305653
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 5364.024049163091 ----------
[1/200] Training loss: 0.15181773
[2/200] Training loss: 0.06014458
[3/200] Training loss: 0.05269078
[4/200] Training loss: 0.04699469
[5/200] Training loss: 0.04347623
[6/200] Training loss: 0.04081927
[7/200] Training loss: 0.03813689
[8/200] Training loss: 0.03635905
[9/200] Training loss: 0.03286994
[10/200] Training loss: 0.03026970
[50/200] Training loss: 0.01999543
[100/200] Training loss: 0.01830170
[150/200] Training loss: 0.01642695
[200/200] Training loss: 0.01559495
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9225.642525049407 ----------
[1/200] Training loss: 0.14826366
[2/200] Training loss: 0.05479003
[3/200] Training loss: 0.05116488
[4/200] Training loss: 0.04712049
[5/200] Training loss: 0.04318057
[6/200] Training loss: 0.04168241
[7/200] Training loss: 0.04006424
[8/200] Training loss: 0.03751823
[9/200] Training loss: 0.03502646
[10/200] Training loss: 0.03245677
[50/200] Training loss: 0.02070498
[100/200] Training loss: 0.01767928
[150/200] Training loss: 0.01653397
[200/200] Training loss: 0.01492647
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.015883233798699076
----FITNESS-----------RMSE---- 6031.974469441992 ----------
[1/200] Training loss: 0.16019016
[2/200] Training loss: 0.06203286
[3/200] Training loss: 0.05631981
[4/200] Training loss: 0.05496497
[5/200] Training loss: 0.05190039
[6/200] Training loss: 0.05049075
[7/200] Training loss: 0.04742126
[8/200] Training loss: 0.04458301
[9/200] Training loss: 0.04473672
[10/200] Training loss: 0.04219289
[50/200] Training loss: 0.01803775
[100/200] Training loss: 0.01472024
[150/200] Training loss: 0.01348715
[200/200] Training loss: 0.01231066
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.015883233798699076
----FITNESS-----------RMSE---- 16504.996576794554 ----------
[1/200] Training loss: 0.17616553
[2/200] Training loss: 0.06353694
[3/200] Training loss: 0.05099338
[4/200] Training loss: 0.04897409
[5/200] Training loss: 0.04759422
[6/200] Training loss: 0.04343415
[7/200] Training loss: 0.04326412
[8/200] Training loss: 0.03725044
[9/200] Training loss: 0.03936035
[10/200] Training loss: 0.03456868
[50/200] Training loss: 0.01799272
[100/200] Training loss: 0.01506811
[150/200] Training loss: 0.01321537
[200/200] Training loss: 0.01261800
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11394.41055956823 ----------
[1/200] Training loss: 0.05773603
[2/200] Training loss: 0.00335314
[3/200] Training loss: 0.00265700
[4/200] Training loss: 0.00223265
[5/200] Training loss: 0.00214744
[6/200] Training loss: 0.00191598
[7/200] Training loss: 0.00166521
[8/200] Training loss: 0.00166265
[9/200] Training loss: 0.00129829
[10/200] Training loss: 0.00131867
[50/200] Training loss: 0.00052751
[100/200] Training loss: 0.00035948
[150/200] Training loss: 0.00031955
[200/200] Training loss: 0.00025482
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15350.45432552405 ----------
[1/200] Training loss: 0.14309684
[2/200] Training loss: 0.05903767
[3/200] Training loss: 0.05345294
[4/200] Training loss: 0.05160130
[5/200] Training loss: 0.04925905
[6/200] Training loss: 0.04694173
[7/200] Training loss: 0.04303416
[8/200] Training loss: 0.04235607
[9/200] Training loss: 0.04016814
[10/200] Training loss: 0.03918194
[50/200] Training loss: 0.02038311
[100/200] Training loss: 0.01565623
[150/200] Training loss: 0.01471001
[200/200] Training loss: 0.01354556
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 14356.56226260312 ----------
[1/200] Training loss: 0.14382573
[2/200] Training loss: 0.05505644
[3/200] Training loss: 0.05120155
[4/200] Training loss: 0.05019199
[5/200] Training loss: 0.04773572
[6/200] Training loss: 0.04194649
[7/200] Training loss: 0.03905307
[8/200] Training loss: 0.03671947
[9/200] Training loss: 0.03278951
[10/200] Training loss: 0.03173401
[50/200] Training loss: 0.01868988
[100/200] Training loss: 0.01566056
[150/200] Training loss: 0.01462127
[200/200] Training loss: 0.01280329
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.015883233798699076
----FITNESS-----------RMSE---- 15939.652191939447 ----------
[1/200] Training loss: 0.17721786
[2/200] Training loss: 0.06251466
[3/200] Training loss: 0.05766550
[4/200] Training loss: 0.05375587
[5/200] Training loss: 0.05259052
[6/200] Training loss: 0.05014424
[7/200] Training loss: 0.04768324
[8/200] Training loss: 0.04613443
[9/200] Training loss: 0.04485604
[10/200] Training loss: 0.04340912
[50/200] Training loss: 0.01812087
[100/200] Training loss: 0.01539506
[150/200] Training loss: 0.01391268
[200/200] Training loss: 0.01288479
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16228.465361826422 ----------
[1/200] Training loss: 0.16137731
[2/200] Training loss: 0.05992854
[3/200] Training loss: 0.05106856
[4/200] Training loss: 0.04580278
[5/200] Training loss: 0.04164908
[6/200] Training loss: 0.03834651
[7/200] Training loss: 0.03489506
[8/200] Training loss: 0.03415021
[9/200] Training loss: 0.03094508
[10/200] Training loss: 0.02925584
[50/200] Training loss: 0.01854583
[100/200] Training loss: 0.01591856
[150/200] Training loss: 0.01413548
[200/200] Training loss: 0.01275367
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22759.66959338382 ----------
[1/200] Training loss: 0.02306084
[2/200] Training loss: 0.00392721
[3/200] Training loss: 0.00334843
[4/200] Training loss: 0.00304052
[5/200] Training loss: 0.00270616
[6/200] Training loss: 0.00239716
[7/200] Training loss: 0.00209502
[8/200] Training loss: 0.00196003
[9/200] Training loss: 0.00186299
[10/200] Training loss: 0.00167507
[50/200] Training loss: 0.00047928
[100/200] Training loss: 0.00037545
[150/200] Training loss: 0.00029838
[200/200] Training loss: 0.00026951
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9136.986811854333 ----------
[1/200] Training loss: 0.08128423
[2/200] Training loss: 0.00386830
[3/200] Training loss: 0.00353626
[4/200] Training loss: 0.00321179
[5/200] Training loss: 0.00274031
[6/200] Training loss: 0.00269654
[7/200] Training loss: 0.00222450
[8/200] Training loss: 0.00231257
[9/200] Training loss: 0.00181854
[10/200] Training loss: 0.00147548
[50/200] Training loss: 0.00055268
[100/200] Training loss: 0.00038193
[150/200] Training loss: 0.00030653
[200/200] Training loss: 0.00027523
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13999.507991354554 ----------
[1/200] Training loss: 0.15105383
[2/200] Training loss: 0.05650472
[3/200] Training loss: 0.05095143
[4/200] Training loss: 0.04911608
[5/200] Training loss: 0.04232545
[6/200] Training loss: 0.04276510
[7/200] Training loss: 0.03874714
[8/200] Training loss: 0.03841620
[9/200] Training loss: 0.03950267
[10/200] Training loss: 0.03474727
[50/200] Training loss: 0.01866645
[100/200] Training loss: 0.01529915
[150/200] Training loss: 0.01321720
[200/200] Training loss: 0.01242407
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4297.221660561624 ----------
[1/200] Training loss: 0.18869214
[2/200] Training loss: 0.06700222
[3/200] Training loss: 0.05479273
[4/200] Training loss: 0.05332131
[5/200] Training loss: 0.04753725
[6/200] Training loss: 0.04862918
[7/200] Training loss: 0.04473783
[8/200] Training loss: 0.04272247
[9/200] Training loss: 0.04149306
[10/200] Training loss: 0.03954718
[50/200] Training loss: 0.01942978
[100/200] Training loss: 0.01633784
[150/200] Training loss: 0.01457338
[200/200] Training loss: 0.01430950
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 11853.320209966489 ----------
[1/200] Training loss: 0.04924993
[2/200] Training loss: 0.00370852
[3/200] Training loss: 0.00371063
[4/200] Training loss: 0.00307253
[5/200] Training loss: 0.00264758
[6/200] Training loss: 0.00212661
[7/200] Training loss: 0.00154574
[8/200] Training loss: 0.00164876
[9/200] Training loss: 0.00131580
[10/200] Training loss: 0.00111250
[50/200] Training loss: 0.00044997
[100/200] Training loss: 0.00033479
[150/200] Training loss: 0.00029973
[200/200] Training loss: 0.00025054
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6890.395344245496 ----------
[1/100] Training loss: 0.14935036
[2/100] Training loss: 0.05725182
[3/100] Training loss: 0.05093895
[4/100] Training loss: 0.04536892
[5/100] Training loss: 0.04167701
[6/100] Training loss: 0.04055153
[7/100] Training loss: 0.03794386
[8/100] Training loss: 0.03768324
[9/100] Training loss: 0.03606587
[10/100] Training loss: 0.03260381
[50/100] Training loss: 0.01613361
[100/100] Training loss: 0.01326278
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10954.226946708744 ----------
[1/200] Training loss: 0.15211685
[2/200] Training loss: 0.05722461
[3/200] Training loss: 0.04627032
[4/200] Training loss: 0.04313421
[5/200] Training loss: 0.03743487
[6/200] Training loss: 0.03554197
[7/200] Training loss: 0.03188998
[8/200] Training loss: 0.02963014
[9/200] Training loss: 0.02757938
[10/200] Training loss: 0.02724429
[50/200] Training loss: 0.01716589
[100/200] Training loss: 0.01505283
[150/200] Training loss: 0.01311354
[200/200] Training loss: 0.01184856
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18281.497531657522 ----------
[1/200] Training loss: 0.13849053
[2/200] Training loss: 0.05217577
[3/200] Training loss: 0.04268195
[4/200] Training loss: 0.04725324
[5/200] Training loss: 0.03160773
[6/200] Training loss: 0.02530963
[7/200] Training loss: 0.02515867
[8/200] Training loss: 0.02368475
[9/200] Training loss: 0.03305346
[10/200] Training loss: 0.02376574
[50/200] Training loss: 0.01893932
[100/200] Training loss: 0.01942436
[150/200] Training loss: 0.01367008
[200/200] Training loss: 0.01350276
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adam ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 41123.38351838282 ----------
[1/200] Training loss: 0.15892639
[2/200] Training loss: 0.06380304
[3/200] Training loss: 0.05091043
[4/200] Training loss: 0.04913871
[5/200] Training loss: 0.04634501
[6/200] Training loss: 0.04375774
[7/200] Training loss: 0.04158621
[8/200] Training loss: 0.03851435
[9/200] Training loss: 0.03624893
[10/200] Training loss: 0.03331405
[50/200] Training loss: 0.01670736
[100/200] Training loss: 0.01395717
[150/200] Training loss: 0.01276642
[200/200] Training loss: 0.01203693
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14507.274313254024 ----------
[1/200] Training loss: 0.17356754
[2/200] Training loss: 0.05989769
[3/200] Training loss: 0.05672516
[4/200] Training loss: 0.05226687
[5/200] Training loss: 0.04974101
[6/200] Training loss: 0.04820586
[7/200] Training loss: 0.04589274
[8/200] Training loss: 0.04103182
[9/200] Training loss: 0.03960062
[10/200] Training loss: 0.03942013
[50/200] Training loss: 0.01838806
[100/200] Training loss: 0.01551252
[150/200] Training loss: 0.01437422
[200/200] Training loss: 0.01325505
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8471.671381728638 ----------
[1/200] Training loss: 0.17031039
[2/200] Training loss: 0.03929822
[3/200] Training loss: 0.03154973
[4/200] Training loss: 0.03139944
[5/200] Training loss: 0.03136424
[6/200] Training loss: 0.03134408
[7/200] Training loss: 0.03117337
[8/200] Training loss: 0.03112668
[9/200] Training loss: 0.03085409
[10/200] Training loss: 0.03097887
[50/200] Training loss: 0.02800337
[100/200] Training loss: 0.02703312
[150/200] Training loss: 0.02626196
[200/200] Training loss: 0.02595033
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Rprop ---learning_rate---: 0.015883233798699076
----FITNESS-----------RMSE---- 14602.736729805136 ----------
[1/200] Training loss: 0.17241140
[2/200] Training loss: 0.06092799
[3/200] Training loss: 0.05321356
[4/200] Training loss: 0.04936681
[5/200] Training loss: 0.04673924
[6/200] Training loss: 0.04410622
[7/200] Training loss: 0.04124975
[8/200] Training loss: 0.03932752
[9/200] Training loss: 0.03638153
[10/200] Training loss: 0.03582354
[50/200] Training loss: 0.01798422
[100/200] Training loss: 0.01508449
[150/200] Training loss: 0.01381680
[200/200] Training loss: 0.01303631
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 4879.076347014873 ----------
[1/200] Training loss: 0.19462416
[2/200] Training loss: 0.05675933
[3/200] Training loss: 0.05450643
[4/200] Training loss: 0.04735339
[5/200] Training loss: 0.04529259
[6/200] Training loss: 0.04307230
[7/200] Training loss: 0.04211240
[8/200] Training loss: 0.04000352
[9/200] Training loss: 0.03813571
[10/200] Training loss: 0.03716717
[50/200] Training loss: 0.01903038
[100/200] Training loss: 0.01637089
[150/200] Training loss: 0.01532068
[200/200] Training loss: 0.01441379
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15670.69724039106 ----------
[1/200] Training loss: 0.16381393
[2/200] Training loss: 0.05848375
[3/200] Training loss: 0.05441275
[4/200] Training loss: 0.04938619
[5/200] Training loss: 0.04670590
[6/200] Training loss: 0.04252353
[7/200] Training loss: 0.04028573
[8/200] Training loss: 0.03808484
[9/200] Training loss: 0.03444317
[10/200] Training loss: 0.03428304
[50/200] Training loss: 0.01834755
[100/200] Training loss: 0.01572908
[150/200] Training loss: 0.01421128
[200/200] Training loss: 0.01323006
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8134.617630841662 ----------
[1/200] Training loss: 0.15901043
[2/200] Training loss: 0.05902849
[3/200] Training loss: 0.05522900
[4/200] Training loss: 0.04959834
[5/200] Training loss: 0.04832510
[6/200] Training loss: 0.04547445
[7/200] Training loss: 0.04312199
[8/200] Training loss: 0.04163868
[9/200] Training loss: 0.03890312
[10/200] Training loss: 0.03770202
[50/200] Training loss: 0.01777128
[100/200] Training loss: 0.01620109
[150/200] Training loss: 0.01478020
[200/200] Training loss: 0.01426647
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12362.916160841665 ----------
[1/200] Training loss: 0.16896484
[2/200] Training loss: 0.06291550
[3/200] Training loss: 0.05352466
[4/200] Training loss: 0.04990712
[5/200] Training loss: 0.04746752
[6/200] Training loss: 0.04298552
[7/200] Training loss: 0.04189337
[8/200] Training loss: 0.03935595
[9/200] Training loss: 0.03786828
[10/200] Training loss: 0.03280882
[50/200] Training loss: 0.01699921
[100/200] Training loss: 0.01496369
[150/200] Training loss: 0.01290283
[200/200] Training loss: 0.01219592
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 28056.903321642607 ----------
[1/100] Training loss: 0.14137097
[2/100] Training loss: 0.05703363
[3/100] Training loss: 0.05366104
[4/100] Training loss: 0.05150679
[5/100] Training loss: 0.04823869
[6/100] Training loss: 0.04283870
[7/100] Training loss: 0.04281228
[8/100] Training loss: 0.03691294
[9/100] Training loss: 0.03628783
[10/100] Training loss: 0.03341127
[50/100] Training loss: 0.01812809
[100/100] Training loss: 0.01492725
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 26208.61293544548 ----------
[1/200] Training loss: 0.14585100
[2/200] Training loss: 0.05801486
[3/200] Training loss: 0.05505997
[4/200] Training loss: 0.04646896
[5/200] Training loss: 0.04631075
[6/200] Training loss: 0.04484129
[7/200] Training loss: 0.03864543
[8/200] Training loss: 0.03891358
[9/200] Training loss: 0.03662572
[10/200] Training loss: 0.03526445
[50/200] Training loss: 0.01750205
[100/200] Training loss: 0.01457378
[150/200] Training loss: 0.01284561
[200/200] Training loss: 0.01221114
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 26322.250359724185 ----------
[1/200] Training loss: 0.16985522
[2/200] Training loss: 0.05805572
[3/200] Training loss: 0.05198240
[4/200] Training loss: 0.05288688
[5/200] Training loss: 0.04801454
[6/200] Training loss: 0.04700568
[7/200] Training loss: 0.04297794
[8/200] Training loss: 0.04316571
[9/200] Training loss: 0.03994597
[10/200] Training loss: 0.03776182
[50/200] Training loss: 0.01893770
[100/200] Training loss: 0.01697463
[150/200] Training loss: 0.01423764
[200/200] Training loss: 0.01290002
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9290.413984317383 ----------
[1/200] Training loss: 0.14902253
[2/200] Training loss: 0.00427221
[3/200] Training loss: 0.00333544
[4/200] Training loss: 0.00261066
[5/200] Training loss: 0.00235129
[6/200] Training loss: 0.00195426
[7/200] Training loss: 0.00165786
[8/200] Training loss: 0.00154653
[9/200] Training loss: 0.00140150
[10/200] Training loss: 0.00127333
[50/200] Training loss: 0.00051565
[100/200] Training loss: 0.00037668
[150/200] Training loss: 0.00030999
[200/200] Training loss: 0.00027348
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9850.941477848704 ----------
[1/200] Training loss: 0.16007263
[2/200] Training loss: 0.05930867
[3/200] Training loss: 0.05395030
[4/200] Training loss: 0.05020961
[5/200] Training loss: 0.04823871
[6/200] Training loss: 0.04827790
[7/200] Training loss: 0.04438654
[8/200] Training loss: 0.04244633
[9/200] Training loss: 0.04033614
[10/200] Training loss: 0.04018016
[50/200] Training loss: 0.01762750
[100/200] Training loss: 0.01422047
[150/200] Training loss: 0.01288479
[200/200] Training loss: 0.01186613
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13463.32678055465 ----------
[1/200] Training loss: 0.17548889
[2/200] Training loss: 0.05952908
[3/200] Training loss: 0.05298100
[4/200] Training loss: 0.04842227
[5/200] Training loss: 0.04616430
[6/200] Training loss: 0.04405966
[7/200] Training loss: 0.03747631
[8/200] Training loss: 0.03534358
[9/200] Training loss: 0.07672682
[10/200] Training loss: 0.08902804
[50/200] Training loss: 0.02160273
[100/200] Training loss: 0.01819124
[150/200] Training loss: 0.01593122
[200/200] Training loss: 0.01475327
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22388.40842936362 ----------
[1/200] Training loss: 0.16620990
[2/200] Training loss: 0.06544986
[3/200] Training loss: 0.06026172
[4/200] Training loss: 0.05259862
[5/200] Training loss: 0.05289692
[6/200] Training loss: 0.04802706
[7/200] Training loss: 0.04966591
[8/200] Training loss: 0.04664280
[9/200] Training loss: 0.04366177
[10/200] Training loss: 0.04133546
[50/200] Training loss: 0.01900409
[100/200] Training loss: 0.01640975
[150/200] Training loss: 0.01558145
[200/200] Training loss: 0.01452605
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9835.103253143812 ----------
[1/200] Training loss: 0.16658630
[2/200] Training loss: 0.05826749
[3/200] Training loss: 0.05245662
[4/200] Training loss: 0.04916903
[5/200] Training loss: 0.04676172
[6/200] Training loss: 0.04500322
[7/200] Training loss: 0.04234333
[8/200] Training loss: 0.04115701
[9/200] Training loss: 0.03715292
[10/200] Training loss: 0.03570339
[50/200] Training loss: 0.01906469
[100/200] Training loss: 0.01622779
[150/200] Training loss: 0.01364092
[200/200] Training loss: 0.01347301
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12495.144016777078 ----------
[1/100] Training loss: 0.14279650
[2/100] Training loss: 0.05475336
[3/100] Training loss: 0.05192156
[4/100] Training loss: 0.04704379
[5/100] Training loss: 0.04894159
[6/100] Training loss: 0.04421980
[7/100] Training loss: 0.04150887
[8/100] Training loss: 0.04218941
[9/100] Training loss: 0.04018588
[10/100] Training loss: 0.03689441
[50/100] Training loss: 0.02106159
[100/100] Training loss: 0.01737454
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.0178172797419062
----FITNESS-----------RMSE---- 5764.67067576284 ----------
[1/200] Training loss: 0.18394126
[2/200] Training loss: 0.06746412
[3/200] Training loss: 0.05638815
[4/200] Training loss: 0.05069542
[5/200] Training loss: 0.05060917
[6/200] Training loss: 0.04941988
[7/200] Training loss: 0.04555845
[8/200] Training loss: 0.04213290
[9/200] Training loss: 0.04086404
[10/200] Training loss: 0.03987844
[50/200] Training loss: 0.02564339
[100/200] Training loss: 0.01678169
[150/200] Training loss: 0.01457579
[200/200] Training loss: 0.01418301
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17326.72202120182 ----------
[1/200] Training loss: 0.05267512
[2/200] Training loss: 0.00362497
[3/200] Training loss: 0.00312227
[4/200] Training loss: 0.00269448
[5/200] Training loss: 0.00235793
[6/200] Training loss: 0.00206932
[7/200] Training loss: 0.00182985
[8/200] Training loss: 0.00156105
[9/200] Training loss: 0.00138335
[10/200] Training loss: 0.00138910
[50/200] Training loss: 0.00050507
[100/200] Training loss: 0.00032207
[150/200] Training loss: 0.00026379
[200/200] Training loss: 0.00024888
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7743.345788481875 ----------
[1/200] Training loss: 0.15977692
[2/200] Training loss: 0.05535749
[3/200] Training loss: 0.04751686
[4/200] Training loss: 0.04417841
[5/200] Training loss: 0.04213494
[6/200] Training loss: 0.03710534
[7/200] Training loss: 0.03662200
[8/200] Training loss: 0.03425232
[9/200] Training loss: 0.03265909
[10/200] Training loss: 0.03088495
[50/200] Training loss: 0.01643193
[100/200] Training loss: 0.01438240
[150/200] Training loss: 0.01291902
[200/200] Training loss: 0.01169719
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15463.932229546273 ----------
[1/200] Training loss: 0.16653447
[2/200] Training loss: 0.06036147
[3/200] Training loss: 0.04966904
[4/200] Training loss: 0.04749280
[5/200] Training loss: 0.04345538
[6/200] Training loss: 0.04163400
[7/200] Training loss: 0.03985579
[8/200] Training loss: 0.03704544
[9/200] Training loss: 0.03616953
[10/200] Training loss: 0.03217784
[50/200] Training loss: 0.01701026
[100/200] Training loss: 0.01435449
[150/200] Training loss: 0.01310395
[200/200] Training loss: 0.01232351
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 8854.709029663256 ----------
[1/200] Training loss: 0.14498580
[2/200] Training loss: 0.05381227
[3/200] Training loss: 0.04967150
[4/200] Training loss: 0.04450781
[5/200] Training loss: 0.04424672
[6/200] Training loss: 0.04188372
[7/200] Training loss: 0.03748063
[8/200] Training loss: 0.03796161
[9/200] Training loss: 0.03567797
[10/200] Training loss: 0.03288350
[50/200] Training loss: 0.01748007
[100/200] Training loss: 0.01380775
[150/200] Training loss: 0.01273441
[200/200] Training loss: 0.01192034
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7062.645679913441 ----------
[1/200] Training loss: 0.07464225
[2/200] Training loss: 0.04452732
[3/200] Training loss: 0.03712058
[4/200] Training loss: 0.03366981
[5/200] Training loss: 0.02935276
[6/200] Training loss: 0.02695174
[7/200] Training loss: 0.02581134
[8/200] Training loss: 0.02410148
[9/200] Training loss: 0.02275296
[10/200] Training loss: 0.02242172
[50/200] Training loss: 0.01557681
[100/200] Training loss: 0.01240968
[150/200] Training loss: 0.01127420
[200/200] Training loss: 0.01024700
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10613.151464103394 ----------
[1/200] Training loss: 0.14164172
[2/200] Training loss: 0.05520045
[3/200] Training loss: 0.04734977
[4/200] Training loss: 0.04440394
[5/200] Training loss: 0.04076580
[6/200] Training loss: 0.03986541
[7/200] Training loss: 0.03872227
[8/200] Training loss: 0.03475447
[9/200] Training loss: 0.03371671
[10/200] Training loss: 0.03451511
[50/200] Training loss: 0.01625657
[100/200] Training loss: 0.01333869
[150/200] Training loss: 0.01256419
[200/200] Training loss: 0.01151469
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 23332.483794058444 ----------
[1/200] Training loss: 0.14467641
[2/200] Training loss: 0.05489219
[3/200] Training loss: 0.05092883
[4/200] Training loss: 0.04899857
[5/200] Training loss: 0.04645791
[6/200] Training loss: 0.04337294
[7/200] Training loss: 0.04113631
[8/200] Training loss: 0.03925664
[9/200] Training loss: 0.03472057
[10/200] Training loss: 0.03547714
[50/200] Training loss: 0.01896645
[100/200] Training loss: 0.01625159
[150/200] Training loss: 0.01532122
[200/200] Training loss: 0.01432212
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 9992.531210859439 ----------
[1/200] Training loss: 0.14842537
[2/200] Training loss: 0.05992220
[3/200] Training loss: 0.05571294
[4/200] Training loss: 0.05299184
[5/200] Training loss: 0.04998614
[6/200] Training loss: 0.04561106
[7/200] Training loss: 0.04340098
[8/200] Training loss: 0.04154308
[9/200] Training loss: 0.03769582
[10/200] Training loss: 0.03662378
[50/200] Training loss: 0.01730758
[100/200] Training loss: 0.01545691
[150/200] Training loss: 0.01379125
[200/200] Training loss: 0.01235948
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23629.798137097998 ----------
[1/200] Training loss: 0.18607911
[2/200] Training loss: 0.05968785
[3/200] Training loss: 0.05952312
[4/200] Training loss: 0.05335214
[5/200] Training loss: 0.04971828
[6/200] Training loss: 0.04739354
[7/200] Training loss: 0.04775109
[8/200] Training loss: 0.04314931
[9/200] Training loss: 0.04058550
[10/200] Training loss: 0.03796648
[50/200] Training loss: 0.02074113
[100/200] Training loss: 0.01730078
[150/200] Training loss: 0.01521093
[200/200] Training loss: 0.01401030
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14656.033296905407 ----------
[1/200] Training loss: 0.15071178
[2/200] Training loss: 0.06386338
[3/200] Training loss: 0.05276761
[4/200] Training loss: 0.04952040
[5/200] Training loss: 0.04574522
[6/200] Training loss: 0.04206735
[7/200] Training loss: 0.04206648
[8/200] Training loss: 0.03576058
[9/200] Training loss: 0.03397352
[10/200] Training loss: 0.03532639
[50/200] Training loss: 0.01867153
[100/200] Training loss: 0.01540448
[150/200] Training loss: 0.01349121
[200/200] Training loss: 0.01337826
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9824.591187423526 ----------
[1/200] Training loss: 0.15719550
[2/200] Training loss: 0.06038906
[3/200] Training loss: 0.05487105
[4/200] Training loss: 0.05352370
[5/200] Training loss: 0.05012083
[6/200] Training loss: 0.04763912
[7/200] Training loss: 0.04620150
[8/200] Training loss: 0.04615865
[9/200] Training loss: 0.04378738
[10/200] Training loss: 0.04127710
[50/200] Training loss: 0.01915776
[100/200] Training loss: 0.01717384
[150/200] Training loss: 0.01481950
[200/200] Training loss: 0.01371200
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12218.862140150366 ----------
[1/200] Training loss: 0.17027813
[2/200] Training loss: 0.05943230
[3/200] Training loss: 0.05538486
[4/200] Training loss: 0.05011055
[5/200] Training loss: 0.04644862
[6/200] Training loss: 0.04736201
[7/200] Training loss: 0.04212519
[8/200] Training loss: 0.04354207
[9/200] Training loss: 0.03883770
[10/200] Training loss: 0.03623469
[50/200] Training loss: 0.01814652
[100/200] Training loss: 0.01505159
[150/200] Training loss: 0.01378546
[200/200] Training loss: 0.01276578
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17997.23889934231 ----------
[1/200] Training loss: 0.14401206
[2/200] Training loss: 0.05747426
[3/200] Training loss: 0.05255446
[4/200] Training loss: 0.04763520
[5/200] Training loss: 0.04538175
[6/200] Training loss: 0.04133307
[7/200] Training loss: 0.03956636
[8/200] Training loss: 0.03972504
[9/200] Training loss: 0.03329662
[10/200] Training loss: 0.03191063
[50/200] Training loss: 0.01838448
[100/200] Training loss: 0.01467046
[150/200] Training loss: 0.01347664
[200/200] Training loss: 0.01209762
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20012.72954896458 ----------
[1/100] Training loss: 0.16999174
[2/100] Training loss: 0.06346749
[3/100] Training loss: 0.05588367
[4/100] Training loss: 0.05120416
[5/100] Training loss: 0.04867137
[6/100] Training loss: 0.04713456
[7/100] Training loss: 0.04421894
[8/100] Training loss: 0.03980186
[9/100] Training loss: 0.03925034
[10/100] Training loss: 0.03734116
[50/100] Training loss: 0.01957330
[100/100] Training loss: 0.01633112
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16640.823056567846 ----------
[1/200] Training loss: 0.13201297
[2/200] Training loss: 0.05439559
[3/200] Training loss: 0.05220218
[4/200] Training loss: 0.04601794
[5/200] Training loss: 0.04430784
[6/200] Training loss: 0.04178120
[7/200] Training loss: 0.03982401
[8/200] Training loss: 0.03854895
[9/200] Training loss: 0.03635322
[10/200] Training loss: 0.03624738
[50/200] Training loss: 0.01941794
[100/200] Training loss: 0.01582293
[150/200] Training loss: 0.01445322
[200/200] Training loss: 0.01297765
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.0178172797419062
----FITNESS-----------RMSE---- 13765.387898639108 ----------
[1/200] Training loss: 0.15839951
[2/200] Training loss: 0.05778485
[3/200] Training loss: 0.05100014
[4/200] Training loss: 0.04962412
[5/200] Training loss: 0.04696365
[6/200] Training loss: 0.04253347
[7/200] Training loss: 0.04049624
[8/200] Training loss: 0.03733132
[9/200] Training loss: 0.03712078
[10/200] Training loss: 0.03402581
[50/200] Training loss: 0.01985828
[100/200] Training loss: 0.01690209
[150/200] Training loss: 0.01422930
[200/200] Training loss: 0.01351704
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 13479.0777132562 ----------
[1/200] Training loss: 0.16693494
[2/200] Training loss: 0.06278093
[3/200] Training loss: 0.05267753
[4/200] Training loss: 0.04890288
[5/200] Training loss: 0.04631152
[6/200] Training loss: 0.04250756
[7/200] Training loss: 0.03951216
[8/200] Training loss: 0.03824056
[9/200] Training loss: 0.03547408
[10/200] Training loss: 0.03270025
[50/200] Training loss: 0.01888463
[100/200] Training loss: 0.01659071
[150/200] Training loss: 0.01504462
[200/200] Training loss: 0.01422736
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8954.617579774136 ----------
[1/200] Training loss: 0.13564381
[2/200] Training loss: 0.05855976
[3/200] Training loss: 0.05206124
[4/200] Training loss: 0.04791460
[5/200] Training loss: 0.04720762
[6/200] Training loss: 0.04552450
[7/200] Training loss: 0.04089630
[8/200] Training loss: 0.04067176
[9/200] Training loss: 0.03940407
[10/200] Training loss: 0.03529922
[50/200] Training loss: 0.01862035
[100/200] Training loss: 0.01418996
[150/200] Training loss: 0.01322734
[200/200] Training loss: 0.01249917
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.0178172797419062
----FITNESS-----------RMSE---- 15920.816059486398 ----------
[1/200] Training loss: 0.18998375
[2/200] Training loss: 0.06138806
[3/200] Training loss: 0.05424845
[4/200] Training loss: 0.05133168
[5/200] Training loss: 0.05224734
[6/200] Training loss: 0.04911162
[7/200] Training loss: 0.04434400
[8/200] Training loss: 0.04468529
[9/200] Training loss: 0.04216082
[10/200] Training loss: 0.04114801
[50/200] Training loss: 0.01812097
[100/200] Training loss: 0.01601961
[150/200] Training loss: 0.01416124
[200/200] Training loss: 0.01308240
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13106.145123567036 ----------
[1/200] Training loss: 0.17043124
[2/200] Training loss: 0.05791220
[3/200] Training loss: 0.05422761
[4/200] Training loss: 0.05030695
[5/200] Training loss: 0.04753382
[6/200] Training loss: 0.04607586
[7/200] Training loss: 0.04355862
[8/200] Training loss: 0.04032519
[9/200] Training loss: 0.04354534
[10/200] Training loss: 0.03677869
[50/200] Training loss: 0.01854461
[100/200] Training loss: 0.01633900
[150/200] Training loss: 0.01346485
[200/200] Training loss: 0.01243548
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11033.469807816578 ----------
[1/200] Training loss: 0.14089008
[2/200] Training loss: 0.05738261
[3/200] Training loss: 0.05242813
[4/200] Training loss: 0.04498115
[5/200] Training loss: 0.04099389
[6/200] Training loss: 0.03764191
[7/200] Training loss: 0.03650079
[8/200] Training loss: 0.03339239
[9/200] Training loss: 0.03195682
[10/200] Training loss: 0.03059649
[50/200] Training loss: 0.01886539
[100/200] Training loss: 0.01682594
[150/200] Training loss: 0.01550231
[200/200] Training loss: 0.01474161
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 16818.985462863093 ----------
[1/200] Training loss: 0.16184816
[2/200] Training loss: 0.06189718
[3/200] Training loss: 0.05235391
[4/200] Training loss: 0.04977760
[5/200] Training loss: 0.04371317
[6/200] Training loss: 0.04357647
[7/200] Training loss: 0.03840781
[8/200] Training loss: 0.03587994
[9/200] Training loss: 0.03436862
[10/200] Training loss: 0.03427163
[50/200] Training loss: 0.01945700
[100/200] Training loss: 0.01662307
[150/200] Training loss: 0.01609832
[200/200] Training loss: 0.01509049
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 10606.289454847063 ----------
[1/200] Training loss: 0.14909956
[2/200] Training loss: 0.05867012
[3/200] Training loss: 0.05291751
[4/200] Training loss: 0.04778982
[5/200] Training loss: 0.04453144
[6/200] Training loss: 0.04347764
[7/200] Training loss: 0.04047863
[8/200] Training loss: 0.03558970
[9/200] Training loss: 0.03396833
[10/200] Training loss: 0.03296027
[50/200] Training loss: 0.01856930
[100/200] Training loss: 0.01505464
[150/200] Training loss: 0.01351227
[200/200] Training loss: 0.01307024
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10318.276600285533 ----------
[1/200] Training loss: 0.17360356
[2/200] Training loss: 0.06073725
[3/200] Training loss: 0.05219403
[4/200] Training loss: 0.04656067
[5/200] Training loss: 0.04403649
[6/200] Training loss: 0.04111592
[7/200] Training loss: 0.03580097
[8/200] Training loss: 0.03747117
[9/200] Training loss: 0.03235717
[10/200] Training loss: 0.03314164
[50/200] Training loss: 0.01728764
[100/200] Training loss: 0.01554238
[150/200] Training loss: 0.01362837
[200/200] Training loss: 0.01272837
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19296.20895409251 ----------
[1/200] Training loss: 0.16156578
[2/200] Training loss: 0.05599850
[3/200] Training loss: 0.05413675
[4/200] Training loss: 0.04805322
[5/200] Training loss: 0.05124980
[6/200] Training loss: 0.04403673
[7/200] Training loss: 0.04219966
[8/200] Training loss: 0.03992946
[9/200] Training loss: 0.03742197
[10/200] Training loss: 0.03363588
[50/200] Training loss: 0.01985316
[100/200] Training loss: 0.01727322
[150/200] Training loss: 0.01609851
[200/200] Training loss: 0.01534178
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 7966.409479809584 ----------
[1/200] Training loss: 0.15392527
[2/200] Training loss: 0.05135799
[3/200] Training loss: 0.05093456
[4/200] Training loss: 0.04279797
[5/200] Training loss: 0.04228496
[6/200] Training loss: 0.03683826
[7/200] Training loss: 0.03756238
[8/200] Training loss: 0.03291807
[9/200] Training loss: 0.03346755
[10/200] Training loss: 0.03271949
[50/200] Training loss: 0.01947338
[100/200] Training loss: 0.01534612
[150/200] Training loss: 0.01419512
[200/200] Training loss: 0.01312708
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12205.91594268943 ----------
[1/200] Training loss: 0.03244904
[2/200] Training loss: 0.00444710
[3/200] Training loss: 0.00367058
[4/200] Training loss: 0.00327598
[5/200] Training loss: 0.00291614
[6/200] Training loss: 0.00230210
[7/200] Training loss: 0.00261772
[8/200] Training loss: 0.00189866
[9/200] Training loss: 0.00188113
[10/200] Training loss: 0.00188750
[50/200] Training loss: 0.00045788
[100/200] Training loss: 0.00031565
[150/200] Training loss: 0.00024077
[200/200] Training loss: 0.00021130
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14378.225203410886 ----------
[1/200] Training loss: 0.13475471
[2/200] Training loss: 0.05223458
[3/200] Training loss: 0.05114428
[4/200] Training loss: 0.04676709
[5/200] Training loss: 0.04344936
[6/200] Training loss: 0.03987961
[7/200] Training loss: 0.03841279
[8/200] Training loss: 0.03523760
[9/200] Training loss: 0.03392415
[10/200] Training loss: 0.03424142
[50/200] Training loss: 0.01622615
[100/200] Training loss: 0.01443308
[150/200] Training loss: 0.01315997
[200/200] Training loss: 0.01208859
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6681.907212764931 ----------
[1/200] Training loss: 0.17373579
[2/200] Training loss: 0.06320330
[3/200] Training loss: 0.05557092
[4/200] Training loss: 0.05065110
[5/200] Training loss: 0.04948325
[6/200] Training loss: 0.04980425
[7/200] Training loss: 0.04745687
[8/200] Training loss: 0.04688358
[9/200] Training loss: 0.04479564
[10/200] Training loss: 0.04191163
[50/200] Training loss: 0.02084128
[100/200] Training loss: 0.01408171
[150/200] Training loss: 0.01185546
[200/200] Training loss: 0.01067310
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7440.785173622472 ----------
[1/200] Training loss: 0.14323893
[2/200] Training loss: 0.05365853
[3/200] Training loss: 0.04955139
[4/200] Training loss: 0.04739244
[5/200] Training loss: 0.04451131
[6/200] Training loss: 0.04159076
[7/200] Training loss: 0.04042644
[8/200] Training loss: 0.03883175
[9/200] Training loss: 0.03767889
[10/200] Training loss: 0.03434097
[50/200] Training loss: 0.01687820
[100/200] Training loss: 0.01377089
[150/200] Training loss: 0.01283013
[200/200] Training loss: 0.01176062
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.022703457423850347
----FITNESS-----------RMSE---- 6309.399020509006 ----------
[1/200] Training loss: 0.17127677
[2/200] Training loss: 0.06073915
[3/200] Training loss: 0.05945381
[4/200] Training loss: 0.05452139
[5/200] Training loss: 0.05335568
[6/200] Training loss: 0.05340001
[7/200] Training loss: 0.04833000
[8/200] Training loss: 0.04863933
[9/200] Training loss: 0.05065627
[10/200] Training loss: 0.04607398
[50/200] Training loss: 0.02056570
[100/200] Training loss: 0.01642846
[150/200] Training loss: 0.01480141
[200/200] Training loss: 0.01365999
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11662.95811533249 ----------
[1/200] Training loss: 0.15599566
[2/200] Training loss: 0.06123314
[3/200] Training loss: 0.05544056
[4/200] Training loss: 0.05277857
[5/200] Training loss: 0.05278178
[6/200] Training loss: 0.04857502
[7/200] Training loss: 0.04804643
[8/200] Training loss: 0.04656874
[9/200] Training loss: 0.04472677
[10/200] Training loss: 0.04255725
[50/200] Training loss: 0.01885939
[100/200] Training loss: 0.01570372
[150/200] Training loss: 0.01386147
[200/200] Training loss: 0.01298491
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.014735619962945767
----FITNESS-----------RMSE---- 9795.4373051947 ----------
[1/200] Training loss: 0.15675422
[2/200] Training loss: 0.06180191
[3/200] Training loss: 0.04933866
[4/200] Training loss: 0.04588046
[5/200] Training loss: 0.04180332
[6/200] Training loss: 0.04119096
[7/200] Training loss: 0.03794384
[8/200] Training loss: 0.03424948
[9/200] Training loss: 0.03443955
[10/200] Training loss: 0.03495248
[50/200] Training loss: 0.02041554
[100/200] Training loss: 0.01789466
[150/200] Training loss: 0.01683777
[200/200] Training loss: 0.01620239
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12582.416302125757 ----------
[1/200] Training loss: 0.18751470
[2/200] Training loss: 0.06316685
[3/200] Training loss: 0.05206106
[4/200] Training loss: 0.05045352
[5/200] Training loss: 0.04978392
[6/200] Training loss: 0.04285612
[7/200] Training loss: 0.04105264
[8/200] Training loss: 0.04408778
[9/200] Training loss: 0.03815542
[10/200] Training loss: 0.03864313
[50/200] Training loss: 0.01955548
[100/200] Training loss: 0.01491113
[150/200] Training loss: 0.01414909
[200/200] Training loss: 0.01284954
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17488.85816741619 ----------
[1/200] Training loss: 0.17202802
[2/200] Training loss: 0.06011395
[3/200] Training loss: 0.05451852
[4/200] Training loss: 0.04316413
[5/200] Training loss: 0.04005194
[6/200] Training loss: 0.03592662
[7/200] Training loss: 0.03315563
[8/200] Training loss: 0.03225526
[9/200] Training loss: 0.03027330
[10/200] Training loss: 0.02799885
[50/200] Training loss: 0.01642043
[100/200] Training loss: 0.01461898
[150/200] Training loss: 0.01404598
[200/200] Training loss: 0.01284520
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14783.629865496498 ----------
[1/200] Training loss: 0.15576662
[2/200] Training loss: 0.05592614
[3/200] Training loss: 0.05178278
[4/200] Training loss: 0.04219621
[5/200] Training loss: 0.04144850
[6/200] Training loss: 0.03407688
[7/200] Training loss: 0.03274881
[8/200] Training loss: 0.03326930
[9/200] Training loss: 0.03305478
[10/200] Training loss: 0.02989528
[50/200] Training loss: 0.01681089
[100/200] Training loss: 0.01408469
[150/200] Training loss: 0.01284543
[200/200] Training loss: 0.01203311
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7608.618534267571 ----------
[1/200] Training loss: 0.13611230
[2/200] Training loss: 0.05712779
[3/200] Training loss: 0.05029095
[4/200] Training loss: 0.05005691
[5/200] Training loss: 0.04490135
[6/200] Training loss: 0.04300941
[7/200] Training loss: 0.04180886
[8/200] Training loss: 0.03919593
[9/200] Training loss: 0.03532729
[10/200] Training loss: 0.03425304
[50/200] Training loss: 0.01750213
[100/200] Training loss: 0.01371612
[150/200] Training loss: 0.01339823
[200/200] Training loss: 0.01223810
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7746.574985114389 ----------
[1/200] Training loss: 0.15051026
[2/200] Training loss: 0.06440964
[3/200] Training loss: 0.04694771
[4/200] Training loss: 0.04826087
[5/200] Training loss: 0.04442237
[6/200] Training loss: 0.04107710
[7/200] Training loss: 0.03774973
[8/200] Training loss: 0.03661576
[9/200] Training loss: 0.03587703
[10/200] Training loss: 0.03137846
[50/200] Training loss: 0.01788979
[100/200] Training loss: 0.01481870
[150/200] Training loss: 0.01396087
[200/200] Training loss: 0.01236327
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8372.205444206444 ----------
[1/200] Training loss: 0.13619154
[2/200] Training loss: 0.05510555
[3/200] Training loss: 0.04917022
[4/200] Training loss: 0.04540286
[5/200] Training loss: 0.04093980
[6/200] Training loss: 0.03553105
[7/200] Training loss: 0.03870384
[8/200] Training loss: 0.03344770
[9/200] Training loss: 0.03552917
[10/200] Training loss: 0.03354558
[50/200] Training loss: 0.01695033
[100/200] Training loss: 0.01506269
[150/200] Training loss: 0.01295315
[200/200] Training loss: 0.01161265
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18291.012875179986 ----------
[1/200] Training loss: 0.17857138
[2/200] Training loss: 0.06103630
[3/200] Training loss: 0.05919109
[4/200] Training loss: 0.05551511
[5/200] Training loss: 0.05035275
[6/200] Training loss: 0.05115254
[7/200] Training loss: 0.04842183
[8/200] Training loss: 0.04547522
[9/200] Training loss: 0.04647806
[10/200] Training loss: 0.04295800
[50/200] Training loss: 0.01964517
[100/200] Training loss: 0.01682299
[150/200] Training loss: 0.01446282
[200/200] Training loss: 0.01380815
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23430.41715377684 ----------
[1/200] Training loss: 0.17612802
[2/200] Training loss: 0.06119478
[3/200] Training loss: 0.05363463
[4/200] Training loss: 0.04928757
[5/200] Training loss: 0.04691434
[6/200] Training loss: 0.04174772
[7/200] Training loss: 0.04266245
[8/200] Training loss: 0.04134089
[9/200] Training loss: 0.03923243
[10/200] Training loss: 0.03958591
[50/200] Training loss: 0.01785011
[100/200] Training loss: 0.01445322
[150/200] Training loss: 0.01315420
[200/200] Training loss: 0.01226713
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11828.167060030899 ----------
[1/200] Training loss: 0.13067657
[2/200] Training loss: 0.06136632
[3/200] Training loss: 0.05038658
[4/200] Training loss: 0.04537521
[5/200] Training loss: 0.04411352
[6/200] Training loss: 0.04244500
[7/200] Training loss: 0.03840404
[8/200] Training loss: 0.03570894
[9/200] Training loss: 0.03718700
[10/200] Training loss: 0.03117700
[50/200] Training loss: 0.01630738
[100/200] Training loss: 0.01473364
[150/200] Training loss: 0.01258325
[200/200] Training loss: 0.01183267
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12813.04772487795 ----------
[1/200] Training loss: 0.14173965
[2/200] Training loss: 0.05574311
[3/200] Training loss: 0.05591667
[4/200] Training loss: 0.04532287
[5/200] Training loss: 0.04415449
[6/200] Training loss: 0.03778235
[7/200] Training loss: 0.03499066
[8/200] Training loss: 0.03247836
[9/200] Training loss: 0.03211518
[10/200] Training loss: 0.02787006
[50/200] Training loss: 0.01688094
[100/200] Training loss: 0.01500904
[150/200] Training loss: 0.01386973
[200/200] Training loss: 0.01326761
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.020241551304133618
----FITNESS-----------RMSE---- 12969.639624908627 ----------
[1/200] Training loss: 0.15322687
[2/200] Training loss: 0.05521653
[3/200] Training loss: 0.05280661
[4/200] Training loss: 0.04700154
[5/200] Training loss: 0.04216990
[6/200] Training loss: 0.04159748
[7/200] Training loss: 0.03881438
[8/200] Training loss: 0.03620512
[9/200] Training loss: 0.03668895
[10/200] Training loss: 0.03348755
[50/200] Training loss: 0.01699490
[100/200] Training loss: 0.01403711
[150/200] Training loss: 0.01288129
[200/200] Training loss: 0.01138532
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5830.355049222989 ----------
[1/200] Training loss: 0.16836758
[2/200] Training loss: 0.06159007
[3/200] Training loss: 0.05522700
[4/200] Training loss: 0.05014470
[5/200] Training loss: 0.04949477
[6/200] Training loss: 0.04360003
[7/200] Training loss: 0.04328083
[8/200] Training loss: 0.03970109
[9/200] Training loss: 0.03798674
[10/200] Training loss: 0.03594275
[50/200] Training loss: 0.01719571
[100/200] Training loss: 0.01603267
[150/200] Training loss: 0.01360766
[200/200] Training loss: 0.01272201
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17378.559664137876 ----------
[1/200] Training loss: 0.08937440
[2/200] Training loss: 0.00544301
[3/200] Training loss: 0.00475717
[4/200] Training loss: 0.00414622
[5/200] Training loss: 0.00346992
[6/200] Training loss: 0.00305631
[7/200] Training loss: 0.00290922
[8/200] Training loss: 0.00228303
[9/200] Training loss: 0.00220092
[10/200] Training loss: 0.00199379
[50/200] Training loss: 0.00062664
[100/200] Training loss: 0.00043953
[150/200] Training loss: 0.00034682
[200/200] Training loss: 0.00031487
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13792.333522649458 ----------
[1/200] Training loss: 0.16651697
[2/200] Training loss: 0.06364973
[3/200] Training loss: 0.05452935
[4/200] Training loss: 0.05340714
[5/200] Training loss: 0.04944541
[6/200] Training loss: 0.04607035
[7/200] Training loss: 0.04154405
[8/200] Training loss: 0.04046549
[9/200] Training loss: 0.03836485
[10/200] Training loss: 0.03675289
[50/200] Training loss: 0.01833731
[100/200] Training loss: 0.01562716
[150/200] Training loss: 0.01442187
[200/200] Training loss: 0.01314745
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12056.851993783452 ----------
[1/200] Training loss: 0.14033581
[2/200] Training loss: 0.06161525
[3/200] Training loss: 0.05292004
[4/200] Training loss: 0.05052994
[5/200] Training loss: 0.04779113
[6/200] Training loss: 0.04256997
[7/200] Training loss: 0.03915444
[8/200] Training loss: 0.03989443
[9/200] Training loss: 0.03711965
[10/200] Training loss: 0.03319062
[50/200] Training loss: 0.01870218
[100/200] Training loss: 0.01497334
[150/200] Training loss: 0.01323494
[200/200] Training loss: 0.01255589
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16971.168021088 ----------
[1/200] Training loss: 0.17446648
[2/200] Training loss: 0.06020932
[3/200] Training loss: 0.05415255
[4/200] Training loss: 0.05394019
[5/200] Training loss: 0.04741729
[6/200] Training loss: 0.04913527
[7/200] Training loss: 0.04560189
[8/200] Training loss: 0.04308226
[9/200] Training loss: 0.04264293
[10/200] Training loss: 0.04177731
[50/200] Training loss: 0.01761806
[100/200] Training loss: 0.01521562
[150/200] Training loss: 0.01357793
[200/200] Training loss: 0.01303265
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14207.030372319192 ----------
[1/200] Training loss: 0.17606389
[2/200] Training loss: 0.05675973
[3/200] Training loss: 0.05137897
[4/200] Training loss: 0.05079723
[5/200] Training loss: 0.04720289
[6/200] Training loss: 0.04277894
[7/200] Training loss: 0.03955638
[8/200] Training loss: 0.03739816
[9/200] Training loss: 0.03442011
[10/200] Training loss: 0.03049854
[50/200] Training loss: 0.01907112
[100/200] Training loss: 0.01707043
[150/200] Training loss: 0.01539885
[200/200] Training loss: 0.01419502
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13982.050207319384 ----------
[1/200] Training loss: 0.16082537
[2/200] Training loss: 0.05860981
[3/200] Training loss: 0.04940668
[4/200] Training loss: 0.05078237
[5/200] Training loss: 0.04492674
[6/200] Training loss: 0.04097021
[7/200] Training loss: 0.03681633
[8/200] Training loss: 0.03901560
[9/200] Training loss: 0.03533590
[10/200] Training loss: 0.03624263
[50/200] Training loss: 0.01842135
[100/200] Training loss: 0.01518409
[150/200] Training loss: 0.01355171
[200/200] Training loss: 0.01273941
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5902.565882732695 ----------
[1/200] Training loss: 0.16228150
[2/200] Training loss: 0.05743124
[3/200] Training loss: 0.04933131
[4/200] Training loss: 0.04264645
[5/200] Training loss: 0.04241413
[6/200] Training loss: 0.03892331
[7/200] Training loss: 0.03781738
[8/200] Training loss: 0.03505347
[9/200] Training loss: 0.03089655
[10/200] Training loss: 0.02987816
[50/200] Training loss: 0.01619537
[100/200] Training loss: 0.01475680
[150/200] Training loss: 0.01363359
[200/200] Training loss: 0.01261780
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15608.517674654438 ----------
[1/200] Training loss: 0.15165743
[2/200] Training loss: 0.05881682
[3/200] Training loss: 0.05007692
[4/200] Training loss: 0.04845636
[5/200] Training loss: 0.04278827
[6/200] Training loss: 0.03678998
[7/200] Training loss: 0.03478917
[8/200] Training loss: 0.03300414
[9/200] Training loss: 0.03238468
[10/200] Training loss: 0.03002772
[50/200] Training loss: 0.01714604
[100/200] Training loss: 0.01511148
[150/200] Training loss: 0.01323756
[200/200] Training loss: 0.01199171
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19451.82191980998 ----------
[1/200] Training loss: 0.14495096
[2/200] Training loss: 0.06036728
[3/200] Training loss: 0.04997217
[4/200] Training loss: 0.04648720
[5/200] Training loss: 0.04466599
[6/200] Training loss: 0.04292633
[7/200] Training loss: 0.03870693
[8/200] Training loss: 0.03971549
[9/200] Training loss: 0.03347916
[10/200] Training loss: 0.03448250
[50/200] Training loss: 0.01844725
[100/200] Training loss: 0.01572171
[150/200] Training loss: 0.01329009
[200/200] Training loss: 0.01274222
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8040.849457613293 ----------
[1/200] Training loss: 0.18068662
[2/200] Training loss: 0.05950531
[3/200] Training loss: 0.05558969
[4/200] Training loss: 0.04824911
[5/200] Training loss: 0.04985610
[6/200] Training loss: 0.04511961
[7/200] Training loss: 0.04249679
[8/200] Training loss: 0.04131892
[9/200] Training loss: 0.04026617
[10/200] Training loss: 0.03854747
[50/200] Training loss: 0.01974730
[100/200] Training loss: 0.01691101
[150/200] Training loss: 0.01509287
[200/200] Training loss: 0.01345744
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8629.08894379934 ----------
[1/200] Training loss: 0.15084067
[2/200] Training loss: 0.05598723
[3/200] Training loss: 0.04790856
[4/200] Training loss: 0.04294673
[5/200] Training loss: 0.04454346
[6/200] Training loss: 0.03986337
[7/200] Training loss: 0.03732870
[8/200] Training loss: 0.03853087
[9/200] Training loss: 0.03471099
[10/200] Training loss: 0.03166578
[50/200] Training loss: 0.01838979
[100/200] Training loss: 0.01379875
[150/200] Training loss: 0.01314422
[200/200] Training loss: 0.01165847
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7503.590074091201 ----------
[1/200] Training loss: 0.15356173
[2/200] Training loss: 0.06176432
[3/200] Training loss: 0.05124960
[4/200] Training loss: 0.04893431
[5/200] Training loss: 0.04644374
[6/200] Training loss: 0.04384476
[7/200] Training loss: 0.04160728
[8/200] Training loss: 0.03891127
[9/200] Training loss: 0.03917697
[10/200] Training loss: 0.03447200
[50/200] Training loss: 0.01784114
[100/200] Training loss: 0.01519962
[150/200] Training loss: 0.01340502
[200/200] Training loss: 0.01246346
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5057.61445742951 ----------
[1/200] Training loss: 0.16995586
[2/200] Training loss: 0.05828747
[3/200] Training loss: 0.05615044
[4/200] Training loss: 0.05110989
[5/200] Training loss: 0.04764517
[6/200] Training loss: 0.04317347
[7/200] Training loss: 0.04135288
[8/200] Training loss: 0.03664159
[9/200] Training loss: 0.03508336
[10/200] Training loss: 0.03286875
[50/200] Training loss: 0.01764137
[100/200] Training loss: 0.01413272
[150/200] Training loss: 0.01267094
[200/200] Training loss: 0.01186497
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22626.20852020948 ----------
[1/200] Training loss: 0.15732820
[2/200] Training loss: 0.05902873
[3/200] Training loss: 0.04958485
[4/200] Training loss: 0.04823210
[5/200] Training loss: 0.04451041
[6/200] Training loss: 0.04331357
[7/200] Training loss: 0.04023854
[8/200] Training loss: 0.03855843
[9/200] Training loss: 0.03544309
[10/200] Training loss: 0.03262323
[50/200] Training loss: 0.01891229
[100/200] Training loss: 0.01576281
[150/200] Training loss: 0.01475175
[200/200] Training loss: 0.01404835
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.022703457423850347
----FITNESS-----------RMSE---- 12470.724758409193 ----------
[1/200] Training loss: 0.15108387
[2/200] Training loss: 0.05739184
[3/200] Training loss: 0.05028120
[4/200] Training loss: 0.04686687
[5/200] Training loss: 0.04240264
[6/200] Training loss: 0.03990019
[7/200] Training loss: 0.03696633
[8/200] Training loss: 0.03653698
[9/200] Training loss: 0.03329881
[10/200] Training loss: 0.03210917
[50/200] Training loss: 0.01875478
[100/200] Training loss: 0.01543159
[150/200] Training loss: 0.01456261
[200/200] Training loss: 0.01397526
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16942.0008263487 ----------
[1/200] Training loss: 0.14136508
[2/200] Training loss: 0.05726545
[3/200] Training loss: 0.05190303
[4/200] Training loss: 0.05091189
[5/200] Training loss: 0.04518854
[6/200] Training loss: 0.04318434
[7/200] Training loss: 0.04321200
[8/200] Training loss: 0.03786631
[9/200] Training loss: 0.03431660
[10/200] Training loss: 0.03437337
[50/200] Training loss: 0.01649325
[100/200] Training loss: 0.01348135
[150/200] Training loss: 0.01290950
[200/200] Training loss: 0.01155904
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10721.030174381565 ----------
[1/200] Training loss: 0.15083576
[2/200] Training loss: 0.06180263
[3/200] Training loss: 0.05270569
[4/200] Training loss: 0.04880810
[5/200] Training loss: 0.04976029
[6/200] Training loss: 0.04366601
[7/200] Training loss: 0.04199735
[8/200] Training loss: 0.04124491
[9/200] Training loss: 0.03854339
[10/200] Training loss: 0.03654299
[50/200] Training loss: 0.01726374
[100/200] Training loss: 0.01440029
[150/200] Training loss: 0.01285091
[200/200] Training loss: 0.01248957
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12253.706704503744 ----------
[1/200] Training loss: 0.16714890
[2/200] Training loss: 0.05723157
[3/200] Training loss: 0.05604086
[4/200] Training loss: 0.04792203
[5/200] Training loss: 0.03940731
[6/200] Training loss: 0.04092353
[7/200] Training loss: 0.03824208
[8/200] Training loss: 0.03498723
[9/200] Training loss: 0.03386022
[10/200] Training loss: 0.03124292
[50/200] Training loss: 0.01873029
[100/200] Training loss: 0.01581309
[150/200] Training loss: 0.01410833
[200/200] Training loss: 0.01358424
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11355.08978388106 ----------
[1/200] Training loss: 0.15378776
[2/200] Training loss: 0.05542727
[3/200] Training loss: 0.04995305
[4/200] Training loss: 0.05162981
[5/200] Training loss: 0.04795881
[6/200] Training loss: 0.04466932
[7/200] Training loss: 0.04362586
[8/200] Training loss: 0.04374035
[9/200] Training loss: 0.04061022
[10/200] Training loss: 0.03727146
[50/200] Training loss: 0.01865863
[100/200] Training loss: 0.01304246
[150/200] Training loss: 0.04517177
[200/200] Training loss: 0.03326010
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 25456.861079088285 ----------
[1/200] Training loss: 0.18114901
[2/200] Training loss: 0.06500322
[3/200] Training loss: 0.05931167
[4/200] Training loss: 0.05635265
[5/200] Training loss: 0.05350247
[6/200] Training loss: 0.05515522
[7/200] Training loss: 0.05164425
[8/200] Training loss: 0.05061005
[9/200] Training loss: 0.04712075
[10/200] Training loss: 0.04676273
[50/200] Training loss: 0.01863840
[100/200] Training loss: 0.01515948
[150/200] Training loss: 0.01347342
[200/200] Training loss: 0.01273635
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18803.55115397089 ----------
[1/200] Training loss: 0.15277638
[2/200] Training loss: 0.05708920
[3/200] Training loss: 0.05209180
[4/200] Training loss: 0.05092192
[5/200] Training loss: 0.04507234
[6/200] Training loss: 0.04539057
[7/200] Training loss: 0.04053413
[8/200] Training loss: 0.03711904
[9/200] Training loss: 0.03687785
[10/200] Training loss: 0.03290415
[50/200] Training loss: 0.01792664
[100/200] Training loss: 0.01407290
[150/200] Training loss: 0.01294454
[200/200] Training loss: 0.01229779
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13239.774016198313 ----------
[1/200] Training loss: 0.16329015
[2/200] Training loss: 0.05613495
[3/200] Training loss: 0.05455109
[4/200] Training loss: 0.04487221
[5/200] Training loss: 0.04539100
[6/200] Training loss: 0.04364379
[7/200] Training loss: 0.03931791
[8/200] Training loss: 0.03380620
[9/200] Training loss: 0.03410291
[10/200] Training loss: 0.03094649
[50/200] Training loss: 0.01918584
[100/200] Training loss: 0.01503256
[150/200] Training loss: 0.01415069
[200/200] Training loss: 0.01269103
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13096.118508932332 ----------
[1/200] Training loss: 0.22004905
[2/200] Training loss: 0.06572839
[3/200] Training loss: 0.05846867
[4/200] Training loss: 0.05632171
[5/200] Training loss: 0.05397417
[6/200] Training loss: 0.05061029
[7/200] Training loss: 0.04800181
[8/200] Training loss: 0.04600041
[9/200] Training loss: 0.04405194
[10/200] Training loss: 0.04324776
[50/200] Training loss: 0.02018013
[100/200] Training loss: 0.01710590
[150/200] Training loss: 0.01566916
[200/200] Training loss: 0.01462809
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15512.511595483176 ----------
[1/200] Training loss: 0.18324860
[2/200] Training loss: 0.06018683
[3/200] Training loss: 0.05706623
[4/200] Training loss: 0.05509654
[5/200] Training loss: 0.05307831
[6/200] Training loss: 0.05175479
[7/200] Training loss: 0.04912746
[8/200] Training loss: 0.04915375
[9/200] Training loss: 0.04715492
[10/200] Training loss: 0.04499328
[50/200] Training loss: 0.02174567
[100/200] Training loss: 0.01728023
[150/200] Training loss: 0.01375258
[200/200] Training loss: 0.01149948
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14041.471147995853 ----------
[1/200] Training loss: 0.19982137
[2/200] Training loss: 0.06519789
[3/200] Training loss: 0.05477106
[4/200] Training loss: 0.05106238
[5/200] Training loss: 0.04714600
[6/200] Training loss: 0.04758475
[7/200] Training loss: 0.04316651
[8/200] Training loss: 0.04150860
[9/200] Training loss: 0.04153463
[10/200] Training loss: 0.03861191
[50/200] Training loss: 0.01897039
[100/200] Training loss: 0.01584120
[150/200] Training loss: 0.01427486
[200/200] Training loss: 0.01348774
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13410.203279592744 ----------
[1/200] Training loss: 0.16194022
[2/200] Training loss: 0.06024363
[3/200] Training loss: 0.05609388
[4/200] Training loss: 0.05297582
[5/200] Training loss: 0.05086233
[6/200] Training loss: 0.04643529
[7/200] Training loss: 0.04602470
[8/200] Training loss: 0.04312628
[9/200] Training loss: 0.04139500
[10/200] Training loss: 0.03989940
[50/200] Training loss: 0.01941402
[100/200] Training loss: 0.01645913
[150/200] Training loss: 0.01506127
[200/200] Training loss: 0.01350328
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16203.773881414168 ----------
[1/200] Training loss: 0.15821602
[2/200] Training loss: 0.06862430
[3/200] Training loss: 0.05599862
[4/200] Training loss: 0.05142495
[5/200] Training loss: 0.04705753
[6/200] Training loss: 0.04021980
[7/200] Training loss: 0.03767469
[8/200] Training loss: 0.03626409
[9/200] Training loss: 0.03414715
[10/200] Training loss: 0.03197342
[50/200] Training loss: 0.01722849
[100/200] Training loss: 0.01419256
[150/200] Training loss: 0.01289623
[200/200] Training loss: 0.01152767
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21816.200952503164 ----------
[1/200] Training loss: 0.17819994
[2/200] Training loss: 0.05601774
[3/200] Training loss: 0.05242743
[4/200] Training loss: 0.05079954
[5/200] Training loss: 0.04691381
[6/200] Training loss: 0.04462298
[7/200] Training loss: 0.04150215
[8/200] Training loss: 0.04139204
[9/200] Training loss: 0.03756955
[10/200] Training loss: 0.03845310
[50/200] Training loss: 0.01747617
[100/200] Training loss: 0.01515992
[150/200] Training loss: 0.01439916
[200/200] Training loss: 0.01322006
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15525.432039077044 ----------
[1/200] Training loss: 0.15723479
[2/200] Training loss: 0.05764755
[3/200] Training loss: 0.05275121
[4/200] Training loss: 0.04602538
[5/200] Training loss: 0.04508978
[6/200] Training loss: 0.04173347
[7/200] Training loss: 0.03884827
[8/200] Training loss: 0.03694487
[9/200] Training loss: 0.03348755
[10/200] Training loss: 0.03404317
[50/200] Training loss: 0.01774625
[100/200] Training loss: 0.01570490
[150/200] Training loss: 0.01414629
[200/200] Training loss: 0.01265302
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 25260.35945904175 ----------
[1/200] Training loss: 0.14472454
[2/200] Training loss: 0.05523753
[3/200] Training loss: 0.05229091
[4/200] Training loss: 0.04407473
[5/200] Training loss: 0.04240454
[6/200] Training loss: 0.03778273
[7/200] Training loss: 0.03637009
[8/200] Training loss: 0.03234800
[9/200] Training loss: 0.02996426
[10/200] Training loss: 0.03028483
[50/200] Training loss: 0.01869083
[100/200] Training loss: 0.01598594
[150/200] Training loss: 0.01480302
[200/200] Training loss: 0.01389850
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10155.366266166868 ----------
[1/200] Training loss: 0.16434875
[2/200] Training loss: 0.06080403
[3/200] Training loss: 0.05221756
[4/200] Training loss: 0.05026038
[5/200] Training loss: 0.04697086
[6/200] Training loss: 0.04334258
[7/200] Training loss: 0.04182652
[8/200] Training loss: 0.03911170
[9/200] Training loss: 0.03716517
[10/200] Training loss: 0.03390652
[50/200] Training loss: 0.01907684
[100/200] Training loss: 0.01602245
[150/200] Training loss: 0.01458970
[200/200] Training loss: 0.01437935
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18032.34959732092 ----------
[1/200] Training loss: 0.17218567
[2/200] Training loss: 0.05481804
[3/200] Training loss: 0.04561534
[4/200] Training loss: 0.04256517
[5/200] Training loss: 0.03951602
[6/200] Training loss: 0.03791060
[7/200] Training loss: 0.03388798
[8/200] Training loss: 0.03426237
[9/200] Training loss: 0.03030177
[10/200] Training loss: 0.03161140
[50/200] Training loss: 0.01767126
[100/200] Training loss: 0.01551587
[150/200] Training loss: 0.01443140
[200/200] Training loss: 0.01347553
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8949.528702674796 ----------
[1/200] Training loss: 0.15640284
[2/200] Training loss: 0.05356827
[3/200] Training loss: 0.04805372
[4/200] Training loss: 0.04829992
[5/200] Training loss: 0.04481316
[6/200] Training loss: 0.03884414
[7/200] Training loss: 0.03685967
[8/200] Training loss: 0.03780243
[9/200] Training loss: 0.03547441
[10/200] Training loss: 0.03319059
[50/200] Training loss: 0.01823026
[100/200] Training loss: 0.01565965
[150/200] Training loss: 0.01382486
[200/200] Training loss: 0.01222543
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11097.566940550527 ----------
[1/200] Training loss: 0.14612528
[2/200] Training loss: 0.05378349
[3/200] Training loss: 0.04673841
[4/200] Training loss: 0.04125123
[5/200] Training loss: 0.04094265
[6/200] Training loss: 0.03385583
[7/200] Training loss: 0.03470350
[8/200] Training loss: 0.03182076
[9/200] Training loss: 0.03312159
[10/200] Training loss: 0.02924203
[50/200] Training loss: 0.01812911
[100/200] Training loss: 0.01466943
[150/200] Training loss: 0.01198313
[200/200] Training loss: 0.01171679
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13186.070225810266 ----------
[1/200] Training loss: 0.14215007
[2/200] Training loss: 0.05987167
[3/200] Training loss: 0.05087016
[4/200] Training loss: 0.04792268
[5/200] Training loss: 0.04562056
[6/200] Training loss: 0.04082268
[7/200] Training loss: 0.04216776
[8/200] Training loss: 0.03532659
[9/200] Training loss: 0.03273211
[10/200] Training loss: 0.03199967
[50/200] Training loss: 0.01710279
[100/200] Training loss: 0.01454047
[150/200] Training loss: 0.01282010
[200/200] Training loss: 0.01208722
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 25574.90019530868 ----------
[1/200] Training loss: 0.13663972
[2/200] Training loss: 0.05680048
[3/200] Training loss: 0.04868482
[4/200] Training loss: 0.04856430
[5/200] Training loss: 0.03934908
[6/200] Training loss: 0.04313470
[7/200] Training loss: 0.03651537
[8/200] Training loss: 0.03445043
[9/200] Training loss: 0.03211963
[10/200] Training loss: 0.03055669
[50/200] Training loss: 0.01744415
[100/200] Training loss: 0.01348495
[150/200] Training loss: 0.01209603
[200/200] Training loss: 0.01146428
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18952.764020057864 ----------
[1/200] Training loss: 0.15887286
[2/200] Training loss: 0.05539975
[3/200] Training loss: 0.04820151
[4/200] Training loss: 0.04526193
[5/200] Training loss: 0.04341297
[6/200] Training loss: 0.03895147
[7/200] Training loss: 0.04010243
[8/200] Training loss: 0.03750523
[9/200] Training loss: 0.03421605
[10/200] Training loss: 0.03126232
[50/200] Training loss: 0.01832954
[100/200] Training loss: 0.01388480
[150/200] Training loss: 0.01211302
[200/200] Training loss: 0.01087250
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13672.447622865484 ----------
[1/200] Training loss: 0.15068576
[2/200] Training loss: 0.05720047
[3/200] Training loss: 0.05371136
[4/200] Training loss: 0.04601183
[5/200] Training loss: 0.04473406
[6/200] Training loss: 0.04137951
[7/200] Training loss: 0.03674001
[8/200] Training loss: 0.03684831
[9/200] Training loss: 0.03518310
[10/200] Training loss: 0.03239406
[50/200] Training loss: 0.01966779
[100/200] Training loss: 0.01553329
[150/200] Training loss: 0.01365951
[200/200] Training loss: 0.01293820
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20377.10794003899 ----------
[1/200] Training loss: 0.15714951
[2/200] Training loss: 0.05740124
[3/200] Training loss: 0.05133915
[4/200] Training loss: 0.04659366
[5/200] Training loss: 0.04421356
[6/200] Training loss: 0.03565702
[7/200] Training loss: 0.03654322
[8/200] Training loss: 0.03224937
[9/200] Training loss: 0.03176670
[10/200] Training loss: 0.02802439
[50/200] Training loss: 0.01710946
[100/200] Training loss: 0.01409059
[150/200] Training loss: 0.01301934
[200/200] Training loss: 0.01177572
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22568.694778387162 ----------
[1/200] Training loss: 0.20222645
[2/200] Training loss: 0.06730673
[3/200] Training loss: 0.05703986
[4/200] Training loss: 0.04955147
[5/200] Training loss: 0.05135878
[6/200] Training loss: 0.04857989
[7/200] Training loss: 0.04654620
[8/200] Training loss: 0.04634301
[9/200] Training loss: 0.04347063
[10/200] Training loss: 0.04285766
[50/200] Training loss: 0.02303760
[100/200] Training loss: 0.01634918
[150/200] Training loss: 0.01181924
[200/200] Training loss: 0.01149905
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4953.3594660593735 ----------
[1/200] Training loss: 0.16092649
[2/200] Training loss: 0.05810541
[3/200] Training loss: 0.05264036
[4/200] Training loss: 0.04683826
[5/200] Training loss: 0.04438178
[6/200] Training loss: 0.04152020
[7/200] Training loss: 0.04002690
[8/200] Training loss: 0.03737885
[9/200] Training loss: 0.03547316
[10/200] Training loss: 0.03349523
[50/200] Training loss: 0.01825644
[100/200] Training loss: 0.01610273
[150/200] Training loss: 0.01432140
[200/200] Training loss: 0.01346081
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15495.651519055273 ----------
[1/200] Training loss: 0.17386391
[2/200] Training loss: 0.06139833
[3/200] Training loss: 0.05513789
[4/200] Training loss: 0.05684083
[5/200] Training loss: 0.05087694
[6/200] Training loss: 0.05146142
[7/200] Training loss: 0.04618153
[8/200] Training loss: 0.04313609
[9/200] Training loss: 0.04296225
[10/200] Training loss: 0.04098644
[50/200] Training loss: 0.01814417
[100/200] Training loss: 0.01580998
[150/200] Training loss: 0.01432273
[200/200] Training loss: 0.01300640
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16871.481262770023 ----------
[1/200] Training loss: 0.16664660
[2/200] Training loss: 0.05873749
[3/200] Training loss: 0.05268333
[4/200] Training loss: 0.04825817
[5/200] Training loss: 0.04618679
[6/200] Training loss: 0.04237991
[7/200] Training loss: 0.04200729
[8/200] Training loss: 0.03813127
[9/200] Training loss: 0.03670756
[10/200] Training loss: 0.03326357
[50/200] Training loss: 0.01668725
[100/200] Training loss: 0.01495205
[150/200] Training loss: 0.01349585
[200/200] Training loss: 0.01278324
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9824.537037438457 ----------
[1/200] Training loss: 0.14092807
[2/200] Training loss: 0.05787437
[3/200] Training loss: 0.05093414
[4/200] Training loss: 0.04822239
[5/200] Training loss: 0.04567616
[6/200] Training loss: 0.04096003
[7/200] Training loss: 0.03687603
[8/200] Training loss: 0.03728478
[9/200] Training loss: 0.03592697
[10/200] Training loss: 0.03304528
[50/200] Training loss: 0.01886907
[100/200] Training loss: 0.01551729
[150/200] Training loss: 0.01387685
[200/200] Training loss: 0.01230125
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14835.388501822255 ----------
[1/200] Training loss: 0.15494762
[2/200] Training loss: 0.05100219
[3/200] Training loss: 0.04748540
[4/200] Training loss: 0.04244455
[5/200] Training loss: 0.03763838
[6/200] Training loss: 0.03558447
[7/200] Training loss: 0.03293801
[8/200] Training loss: 0.03293884
[9/200] Training loss: 0.02874699
[10/200] Training loss: 0.02859719
[50/200] Training loss: 0.01706623
[100/200] Training loss: 0.01443787
[150/200] Training loss: 0.01401073
[200/200] Training loss: 0.01250751
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8076.15626396617 ----------
[1/200] Training loss: 0.19175719
[2/200] Training loss: 0.05928400
[3/200] Training loss: 0.05396054
[4/200] Training loss: 0.04961064
[5/200] Training loss: 0.04793261
[6/200] Training loss: 0.04548416
[7/200] Training loss: 0.03874440
[8/200] Training loss: 0.03873800
[9/200] Training loss: 0.03911789
[10/200] Training loss: 0.03507559
[50/200] Training loss: 0.01786994
[100/200] Training loss: 0.01457938
[150/200] Training loss: 0.01448917
[200/200] Training loss: 0.01270437
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13580.805867105237 ----------
[1/200] Training loss: 0.16683183
[2/200] Training loss: 0.05703642
[3/200] Training loss: 0.05111502
[4/200] Training loss: 0.04540587
[5/200] Training loss: 0.04086267
[6/200] Training loss: 0.03842310
[7/200] Training loss: 0.03591103
[8/200] Training loss: 0.03331051
[9/200] Training loss: 0.03171278
[10/200] Training loss: 0.03062915
[50/200] Training loss: 0.01850483
[100/200] Training loss: 0.01645735
[150/200] Training loss: 0.01493348
[200/200] Training loss: 0.01309542
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21732.73586090808 ----------
[1/200] Training loss: 0.17412885
[2/200] Training loss: 0.05857208
[3/200] Training loss: 0.05106281
[4/200] Training loss: 0.04818408
[5/200] Training loss: 0.04185717
[6/200] Training loss: 0.04081070
[7/200] Training loss: 0.03945179
[8/200] Training loss: 0.03657625
[9/200] Training loss: 0.03346280
[10/200] Training loss: 0.03423385
[50/200] Training loss: 0.01638811
[100/200] Training loss: 0.01368796
[150/200] Training loss: 0.01226485
[200/200] Training loss: 0.01102908
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21460.50959320398 ----------
[1/200] Training loss: 0.15574443
[2/200] Training loss: 0.05788401
[3/200] Training loss: 0.05029807
[4/200] Training loss: 0.04805765
[5/200] Training loss: 0.04646854
[6/200] Training loss: 0.04297760
[7/200] Training loss: 0.03916907
[8/200] Training loss: 0.03816981
[9/200] Training loss: 0.03312834
[10/200] Training loss: 0.03510013
[50/200] Training loss: 0.01724564
[100/200] Training loss: 0.01516659
[150/200] Training loss: 0.01341566
[200/200] Training loss: 0.01206801
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8544.144661696688 ----------
[1/200] Training loss: 0.15296884
[2/200] Training loss: 0.05686846
[3/200] Training loss: 0.05359440
[4/200] Training loss: 0.05093908
[5/200] Training loss: 0.04723557
[6/200] Training loss: 0.04124109
[7/200] Training loss: 0.04096054
[8/200] Training loss: 0.04150427
[9/200] Training loss: 0.03964750
[10/200] Training loss: 0.03663918
[50/200] Training loss: 0.01898638
[100/200] Training loss: 0.01543013
[150/200] Training loss: 0.01373477
[200/200] Training loss: 0.01353427
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 29843.70808059883 ----------
[1/200] Training loss: 0.16954464
[2/200] Training loss: 0.06088530
[3/200] Training loss: 0.05464824
[4/200] Training loss: 0.04574505
[5/200] Training loss: 0.04409612
[6/200] Training loss: 0.04195204
[7/200] Training loss: 0.03951827
[8/200] Training loss: 0.03741842
[9/200] Training loss: 0.03702489
[10/200] Training loss: 0.03390868
[50/200] Training loss: 0.01895421
[100/200] Training loss: 0.01364527
[150/200] Training loss: 0.01303475
[200/200] Training loss: 0.01280431
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9778.278376074186 ----------
[1/200] Training loss: 0.14407421
[2/200] Training loss: 0.05606569
[3/200] Training loss: 0.04937336
[4/200] Training loss: 0.04309121
[5/200] Training loss: 0.04122486
[6/200] Training loss: 0.03718235
[7/200] Training loss: 0.03485560
[8/200] Training loss: 0.03207834
[9/200] Training loss: 0.03112073
[10/200] Training loss: 0.02971669
[50/200] Training loss: 0.01546745
[100/200] Training loss: 0.01368214
[150/200] Training loss: 0.01195453
[200/200] Training loss: 0.01164583
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18263.891370680016 ----------
[1/200] Training loss: 0.14689565
[2/200] Training loss: 0.05119157
[3/200] Training loss: 0.04660791
[4/200] Training loss: 0.04652384
[5/200] Training loss: 0.04057568
[6/200] Training loss: 0.03814445
[7/200] Training loss: 0.03663846
[8/200] Training loss: 0.03277016
[9/200] Training loss: 0.03317758
[10/200] Training loss: 0.03070272
[50/200] Training loss: 0.01715490
[100/200] Training loss: 0.01417840
[150/200] Training loss: 0.01215753
[200/200] Training loss: 0.01138207
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12081.770068992375 ----------
[1/200] Training loss: 0.16211181
[2/200] Training loss: 0.05903068
[3/200] Training loss: 0.05371077
[4/200] Training loss: 0.05431294
[5/200] Training loss: 0.05015198
[6/200] Training loss: 0.04623831
[7/200] Training loss: 0.04286493
[8/200] Training loss: 0.04166608
[9/200] Training loss: 0.03878890
[10/200] Training loss: 0.03478035
[50/200] Training loss: 0.01553893
[100/200] Training loss: 0.01330728
[150/200] Training loss: 0.01165949
[200/200] Training loss: 0.01102926
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6975.911983389698 ----------
[1/200] Training loss: 0.15370414
[2/200] Training loss: 0.05505516
[3/200] Training loss: 0.05104669
[4/200] Training loss: 0.04259467
[5/200] Training loss: 0.04246341
[6/200] Training loss: 0.03951487
[7/200] Training loss: 0.03978635
[8/200] Training loss: 0.03417708
[9/200] Training loss: 0.03410923
[10/200] Training loss: 0.03293741
[50/200] Training loss: 0.01800022
[100/200] Training loss: 0.01569591
[150/200] Training loss: 0.01315324
[200/200] Training loss: 0.01198605
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10251.371030257367 ----------
[1/200] Training loss: 0.14412556
[2/200] Training loss: 0.05466442
[3/200] Training loss: 0.05331294
[4/200] Training loss: 0.04702583
[5/200] Training loss: 0.04572170
[6/200] Training loss: 0.04178012
[7/200] Training loss: 0.04009048
[8/200] Training loss: 0.03921701
[9/200] Training loss: 0.03711479
[10/200] Training loss: 0.03580415
[50/200] Training loss: 0.01638756
[100/200] Training loss: 0.01401627
[150/200] Training loss: 0.01323137
[200/200] Training loss: 0.01247119
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17438.44213225482 ----------
[1/200] Training loss: 0.15383992
[2/200] Training loss: 0.05918554
[3/200] Training loss: 0.05310435
[4/200] Training loss: 0.04784800
[5/200] Training loss: 0.04764690
[6/200] Training loss: 0.04395894
[7/200] Training loss: 0.04134920
[8/200] Training loss: 0.03876555
[9/200] Training loss: 0.03689680
[10/200] Training loss: 0.03460781
[50/200] Training loss: 0.01857881
[100/200] Training loss: 0.01476696
[150/200] Training loss: 0.01355746
[200/200] Training loss: 0.01258487
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21021.15353637854 ----------
[1/200] Training loss: 0.15214348
[2/200] Training loss: 0.05793445
[3/200] Training loss: 0.05553011
[4/200] Training loss: 0.04837954
[5/200] Training loss: 0.04293624
[6/200] Training loss: 0.04128443
[7/200] Training loss: 0.04032284
[8/200] Training loss: 0.03862968
[9/200] Training loss: 0.03632470
[10/200] Training loss: 0.03411112
[50/200] Training loss: 0.01848871
[100/200] Training loss: 0.01590418
[150/200] Training loss: 0.01275353
[200/200] Training loss: 0.01153668
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6769.324633964603 ----------
[1/200] Training loss: 0.16740204
[2/200] Training loss: 0.06255663
[3/200] Training loss: 0.05259490
[4/200] Training loss: 0.05260183
[5/200] Training loss: 0.04854021
[6/200] Training loss: 0.04872016
[7/200] Training loss: 0.04413335
[8/200] Training loss: 0.04177612
[9/200] Training loss: 0.04012261
[10/200] Training loss: 0.03884845
[50/200] Training loss: 0.01704549
[100/200] Training loss: 0.01360964
[150/200] Training loss: 0.01205897
[200/200] Training loss: 0.01153259
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7118.18684778645 ----------
[1/200] Training loss: 0.15457034
[2/200] Training loss: 0.05620595
[3/200] Training loss: 0.05012535
[4/200] Training loss: 0.04394065
[5/200] Training loss: 0.04218495
[6/200] Training loss: 0.03766477
[7/200] Training loss: 0.03316653
[8/200] Training loss: 0.03499835
[9/200] Training loss: 0.03076626
[10/200] Training loss: 0.02955880
[50/200] Training loss: 0.01722744
[100/200] Training loss: 0.01454568
[150/200] Training loss: 0.01366446
[200/200] Training loss: 0.01206185
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 32 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14952.097377960057 ----------
[1/200] Training loss: 0.17345421
[2/200] Training loss: 0.06134412
[3/200] Training loss: 0.05267189
[4/200] Training loss: 0.05118202
[5/200] Training loss: 0.04749905
[6/200] Training loss: 0.04222269
[7/200] Training loss: 0.03932702
[8/200] Training loss: 0.03945666
[9/200] Training loss: 0.03726441
[10/200] Training loss: 0.03312061
[50/200] Training loss: 0.01780643
[100/200] Training loss: 0.01494659
[150/200] Training loss: 0.01368829
[200/200] Training loss: 0.01265127
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22300.49039819528 ----------
[1/200] Training loss: 0.13975922
[2/200] Training loss: 0.06230950
[3/200] Training loss: 0.04888108
[4/200] Training loss: 0.04928811
[5/200] Training loss: 0.04527375
[6/200] Training loss: 0.04225042
[7/200] Training loss: 0.03953808
[8/200] Training loss: 0.03567603
[9/200] Training loss: 0.03264036
[10/200] Training loss: 0.03263906
[50/200] Training loss: 0.01749766
[100/200] Training loss: 0.01472314
[150/200] Training loss: 0.01466571
[200/200] Training loss: 0.01216705
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16550.651467540483 ----------
[1/200] Training loss: 0.18346987
[2/200] Training loss: 0.06243595
[3/200] Training loss: 0.05325673
[4/200] Training loss: 0.05431861
[5/200] Training loss: 0.04985498
[6/200] Training loss: 0.04727586
[7/200] Training loss: 0.04656066
[8/200] Training loss: 0.04407566
[9/200] Training loss: 0.04096433
[10/200] Training loss: 0.03913070
[50/200] Training loss: 0.01863089
[100/200] Training loss: 0.01426100
[150/200] Training loss: 0.01226944
[200/200] Training loss: 0.01179487
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11801.210785338935 ----------
[1/200] Training loss: 0.19572000
[2/200] Training loss: 0.06390672
[3/200] Training loss: 0.05742092
[4/200] Training loss: 0.05258701
[5/200] Training loss: 0.04838811
[6/200] Training loss: 0.04892751
[7/200] Training loss: 0.04773573
[8/200] Training loss: 0.04318132
[9/200] Training loss: 0.04479549
[10/200] Training loss: 0.04066693
[50/200] Training loss: 0.01877543
[100/200] Training loss: 0.01576302
[150/200] Training loss: 0.01400909
[200/200] Training loss: 0.01271499
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4785.753023297379 ----------
[1/200] Training loss: 0.13244738
[2/200] Training loss: 0.05524362
[3/200] Training loss: 0.04921622
[4/200] Training loss: 0.04701829
[5/200] Training loss: 0.04482013
[6/200] Training loss: 0.04121469
[7/200] Training loss: 0.04101650
[8/200] Training loss: 0.03649492
[9/200] Training loss: 0.03490835
[10/200] Training loss: 0.03608055
[50/200] Training loss: 0.01886841
[100/200] Training loss: 0.01463698
[150/200] Training loss: 0.01229076
[200/200] Training loss: 0.01086874
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15297.369185582205 ----------
[1/200] Training loss: 0.19326073
[2/200] Training loss: 0.06042534
[3/200] Training loss: 0.05668286
[4/200] Training loss: 0.05335296
[5/200] Training loss: 0.05010967
[6/200] Training loss: 0.04676405
[7/200] Training loss: 0.04531230
[8/200] Training loss: 0.04513534
[9/200] Training loss: 0.04259737
[10/200] Training loss: 0.04044016
[50/200] Training loss: 0.01993671
[100/200] Training loss: 0.01599201
[150/200] Training loss: 0.01432912
[200/200] Training loss: 0.01354049
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5587.057722988013 ----------
[1/200] Training loss: 0.16742346
[2/200] Training loss: 0.06220305
[3/200] Training loss: 0.05699675
[4/200] Training loss: 0.04970233
[5/200] Training loss: 0.04354101
[6/200] Training loss: 0.04116908
[7/200] Training loss: 0.03880846
[8/200] Training loss: 0.03609009
[9/200] Training loss: 0.03286933
[10/200] Training loss: 0.03268326
[50/200] Training loss: 0.01867254
[100/200] Training loss: 0.01596116
[150/200] Training loss: 0.01461249
[200/200] Training loss: 0.01263645
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18137.080691224815 ----------
[1/200] Training loss: 0.12980748
[2/200] Training loss: 0.05709261
[3/200] Training loss: 0.05264557
[4/200] Training loss: 0.04725461
[5/200] Training loss: 0.04447686
[6/200] Training loss: 0.04130312
[7/200] Training loss: 0.03649682
[8/200] Training loss: 0.03435132
[9/200] Training loss: 0.03294691
[10/200] Training loss: 0.03027109
[50/200] Training loss: 0.01605075
[100/200] Training loss: 0.01350508
[150/200] Training loss: 0.01344774
[200/200] Training loss: 0.01227408
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4414.483208711978 ----------
[1/200] Training loss: 0.17339411
[2/200] Training loss: 0.06379252
[3/200] Training loss: 0.05507764
[4/200] Training loss: 0.05344569
[5/200] Training loss: 0.04841485
[6/200] Training loss: 0.04573962
[7/200] Training loss: 0.04651353
[8/200] Training loss: 0.04208339
[9/200] Training loss: 0.04049506
[10/200] Training loss: 0.04098577
[50/200] Training loss: 0.01773974
[100/200] Training loss: 0.01557933
[150/200] Training loss: 0.01477745
[200/200] Training loss: 0.01368646
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22563.286285468257 ----------
[1/200] Training loss: 0.15613507
[2/200] Training loss: 0.05666998
[3/200] Training loss: 0.05186446
[4/200] Training loss: 0.04665834
[5/200] Training loss: 0.04656374
[6/200] Training loss: 0.04233891
[7/200] Training loss: 0.04237924
[8/200] Training loss: 0.04028908
[9/200] Training loss: 0.03779851
[10/200] Training loss: 0.03428873
[50/200] Training loss: 0.01833319
[100/200] Training loss: 0.01408444
[150/200] Training loss: 0.01312467
[200/200] Training loss: 0.01243722
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14400.357773333273 ----------
[1/200] Training loss: 0.17593743
[2/200] Training loss: 0.06282412
[3/200] Training loss: 0.05527005
[4/200] Training loss: 0.05112027
[5/200] Training loss: 0.04981611
[6/200] Training loss: 0.04069644
[7/200] Training loss: 0.04226865
[8/200] Training loss: 0.03881090
[9/200] Training loss: 0.03678856
[10/200] Training loss: 0.03565496
[50/200] Training loss: 0.01669245
[100/200] Training loss: 0.01247052
[150/200] Training loss: 0.01234223
[200/200] Training loss: 0.01088567
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23907.61820006334 ----------
[1/200] Training loss: 0.12799073
[2/200] Training loss: 0.06023498
[3/200] Training loss: 0.04846489
[4/200] Training loss: 0.04827305
[5/200] Training loss: 0.04215452
[6/200] Training loss: 0.03968608
[7/200] Training loss: 0.03747170
[8/200] Training loss: 0.03346753
[9/200] Training loss: 0.03516712
[10/200] Training loss: 0.03117959
[50/200] Training loss: 0.01571608
[100/200] Training loss: 0.01368182
[150/200] Training loss: 0.01282619
[200/200] Training loss: 0.01132461
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14183.83587045479 ----------
[1/200] Training loss: 0.17114753
[2/200] Training loss: 0.05611043
[3/200] Training loss: 0.05160733
[4/200] Training loss: 0.04805702
[5/200] Training loss: 0.04385179
[6/200] Training loss: 0.03984679
[7/200] Training loss: 0.03845914
[8/200] Training loss: 0.03629624
[9/200] Training loss: 0.03520208
[10/200] Training loss: 0.03367185
[50/200] Training loss: 0.01915615
[100/200] Training loss: 0.01574622
[150/200] Training loss: 0.01517361
[200/200] Training loss: 0.01295831
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11957.15685269705 ----------
[1/200] Training loss: 0.16366744
[2/200] Training loss: 0.06260998
[3/200] Training loss: 0.05472849
[4/200] Training loss: 0.05210071
[5/200] Training loss: 0.04993050
[6/200] Training loss: 0.04727079
[7/200] Training loss: 0.04508151
[8/200] Training loss: 0.04499271
[9/200] Training loss: 0.04352241
[10/200] Training loss: 0.03943278
[50/200] Training loss: 0.01827789
[100/200] Training loss: 0.01586047
[150/200] Training loss: 0.01572511
[200/200] Training loss: 0.01391584
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13635.03839378533 ----------
[1/200] Training loss: 0.15925964
[2/200] Training loss: 0.05561041
[3/200] Training loss: 0.05063282
[4/200] Training loss: 0.04822011
[5/200] Training loss: 0.04031569
[6/200] Training loss: 0.04047003
[7/200] Training loss: 0.03413004
[8/200] Training loss: 0.03421067
[9/200] Training loss: 0.03208650
[10/200] Training loss: 0.02932938
[50/200] Training loss: 0.01857162
[100/200] Training loss: 0.01383677
[150/200] Training loss: 0.01296007
[200/200] Training loss: 0.01176440
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17390.218860037385 ----------
[1/200] Training loss: 0.17201482
[2/200] Training loss: 0.06354863
[3/200] Training loss: 0.05644875
[4/200] Training loss: 0.05270945
[5/200] Training loss: 0.04951659
[6/200] Training loss: 0.04668033
[7/200] Training loss: 0.04462357
[8/200] Training loss: 0.04217379
[9/200] Training loss: 0.03928715
[10/200] Training loss: 0.03928755
[50/200] Training loss: 0.02063763
[100/200] Training loss: 0.01798546
[150/200] Training loss: 0.01518528
[200/200] Training loss: 0.01369851
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17013.36415880175 ----------
[1/200] Training loss: 0.17560239
[2/200] Training loss: 0.05873160
[3/200] Training loss: 0.05350515
[4/200] Training loss: 0.04854289
[5/200] Training loss: 0.04552600
[6/200] Training loss: 0.04366590
[7/200] Training loss: 0.04240855
[8/200] Training loss: 0.04058297
[9/200] Training loss: 0.03707526
[10/200] Training loss: 0.03467270
[50/200] Training loss: 0.01898836
[100/200] Training loss: 0.01565814
[150/200] Training loss: 0.01438627
[200/200] Training loss: 0.01335431
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15052.168481650742 ----------
[1/200] Training loss: 0.14712457
[2/200] Training loss: 0.05703520
[3/200] Training loss: 0.05071788
[4/200] Training loss: 0.04540095
[5/200] Training loss: 0.04422406
[6/200] Training loss: 0.03858215
[7/200] Training loss: 0.03943119
[8/200] Training loss: 0.03325664
[9/200] Training loss: 0.03169215
[10/200] Training loss: 0.03327129
[50/200] Training loss: 0.01932972
[100/200] Training loss: 0.01552468
[150/200] Training loss: 0.01406610
[200/200] Training loss: 0.01315331
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12916.350568175207 ----------
[1/200] Training loss: 0.16589155
[2/200] Training loss: 0.05745127
[3/200] Training loss: 0.05153384
[4/200] Training loss: 0.04955345
[5/200] Training loss: 0.04648539
[6/200] Training loss: 0.04186502
[7/200] Training loss: 0.04160851
[8/200] Training loss: 0.03594544
[9/200] Training loss: 0.03892988
[10/200] Training loss: 0.03310369
[50/200] Training loss: 0.01898028
[100/200] Training loss: 0.01619399
[150/200] Training loss: 0.01481475
[200/200] Training loss: 0.01373259
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17804.44169301582 ----------
[1/200] Training loss: 0.14509675
[2/200] Training loss: 0.05935365
[3/200] Training loss: 0.05386794
[4/200] Training loss: 0.05313167
[5/200] Training loss: 0.04835789
[6/200] Training loss: 0.04469021
[7/200] Training loss: 0.04225111
[8/200] Training loss: 0.04537548
[9/200] Training loss: 0.04164722
[10/200] Training loss: 0.03805120
[50/200] Training loss: 0.01837956
[100/200] Training loss: 0.01491655
[150/200] Training loss: 0.01416933
[200/200] Training loss: 0.01321238
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9710.668360107866 ----------
[1/200] Training loss: 0.16742488
[2/200] Training loss: 0.05916149
[3/200] Training loss: 0.05403086
[4/200] Training loss: 0.04987782
[5/200] Training loss: 0.04919343
[6/200] Training loss: 0.04916144
[7/200] Training loss: 0.04120545
[8/200] Training loss: 0.04141701
[9/200] Training loss: 0.03853947
[10/200] Training loss: 0.03816222
[50/200] Training loss: 0.01756790
[100/200] Training loss: 0.01635040
[150/200] Training loss: 0.01372592
[200/200] Training loss: 0.01297948
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23059.57224234656 ----------
[1/200] Training loss: 0.17504561
[2/200] Training loss: 0.06312866
[3/200] Training loss: 0.05961995
[4/200] Training loss: 0.05501865
[5/200] Training loss: 0.05378083
[6/200] Training loss: 0.05065070
[7/200] Training loss: 0.04949898
[8/200] Training loss: 0.04943507
[9/200] Training loss: 0.04901712
[10/200] Training loss: 0.04505354
[50/200] Training loss: 0.01876364
[100/200] Training loss: 0.01600433
[150/200] Training loss: 0.01412076
[200/200] Training loss: 0.01295901
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11538.483435876658 ----------
[1/200] Training loss: 0.15128984
[2/200] Training loss: 0.05804159
[3/200] Training loss: 0.05025673
[4/200] Training loss: 0.04208319
[5/200] Training loss: 0.04176781
[6/200] Training loss: 0.03770700
[7/200] Training loss: 0.03479186
[8/200] Training loss: 0.03486953
[9/200] Training loss: 0.03151159
[10/200] Training loss: 0.02809420
[50/200] Training loss: 0.01662657
[100/200] Training loss: 0.01453624
[150/200] Training loss: 0.01372081
[200/200] Training loss: 0.01305339
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6912.192126959435 ----------
[1/200] Training loss: 0.16540920
[2/200] Training loss: 0.05803200
[3/200] Training loss: 0.05173062
[4/200] Training loss: 0.04934561
[5/200] Training loss: 0.04784817
[6/200] Training loss: 0.04496360
[7/200] Training loss: 0.04341576
[8/200] Training loss: 0.04035699
[9/200] Training loss: 0.03798084
[10/200] Training loss: 0.03497574
[50/200] Training loss: 0.02038873
[100/200] Training loss: 0.01723981
[150/200] Training loss: 0.01585699
[200/200] Training loss: 0.01525414
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15105.231411666622 ----------
[1/200] Training loss: 0.17836915
[2/200] Training loss: 0.06371641
[3/200] Training loss: 0.05753300
[4/200] Training loss: 0.05088708
[5/200] Training loss: 0.05017500
[6/200] Training loss: 0.04693713
[7/200] Training loss: 0.04541838
[8/200] Training loss: 0.04327353
[9/200] Training loss: 0.04357956
[10/200] Training loss: 0.04056198
[50/200] Training loss: 0.02061501
[100/200] Training loss: 0.01608712
[150/200] Training loss: 0.01494087
[200/200] Training loss: 0.01376365
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14576.559813618576 ----------
[1/200] Training loss: 0.15564303
[2/200] Training loss: 0.05534133
[3/200] Training loss: 0.05560279
[4/200] Training loss: 0.04931797
[5/200] Training loss: 0.04668226
[6/200] Training loss: 0.04218735
[7/200] Training loss: 0.03764308
[8/200] Training loss: 0.03613058
[9/200] Training loss: 0.03478102
[10/200] Training loss: 0.03253401
[50/200] Training loss: 0.01713427
[100/200] Training loss: 0.01494429
[150/200] Training loss: 0.01330269
[200/200] Training loss: 0.01303370
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22238.097760375098 ----------
[1/200] Training loss: 0.17494808
[2/200] Training loss: 0.05774825
[3/200] Training loss: 0.04838526
[4/200] Training loss: 0.04540412
[5/200] Training loss: 0.04310785
[6/200] Training loss: 0.03806402
[7/200] Training loss: 0.03494851
[8/200] Training loss: 0.03646898
[9/200] Training loss: 0.03315760
[10/200] Training loss: 0.03173406
[50/200] Training loss: 0.02005062
[100/200] Training loss: 0.01527881
[150/200] Training loss: 0.01388516
[200/200] Training loss: 0.01181010
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20948.979545553048 ----------
[1/200] Training loss: 0.16120925
[2/200] Training loss: 0.05937415
[3/200] Training loss: 0.05643744
[4/200] Training loss: 0.05257307
[5/200] Training loss: 0.04885716
[6/200] Training loss: 0.04485792
[7/200] Training loss: 0.04579900
[8/200] Training loss: 0.04089573
[9/200] Training loss: 0.03833527
[10/200] Training loss: 0.03935322
[50/200] Training loss: 0.01921177
[100/200] Training loss: 0.01429542
[150/200] Training loss: 0.01325467
[200/200] Training loss: 0.01151140
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10819.73382297365 ----------
[1/200] Training loss: 0.15937563
[2/200] Training loss: 0.05737640
[3/200] Training loss: 0.05251365
[4/200] Training loss: 0.04930458
[5/200] Training loss: 0.04750437
[6/200] Training loss: 0.04453185
[7/200] Training loss: 0.04076299
[8/200] Training loss: 0.03723695
[9/200] Training loss: 0.03708043
[10/200] Training loss: 0.03444818
[50/200] Training loss: 0.01750922
[100/200] Training loss: 0.01454698
[150/200] Training loss: 0.01215873
[200/200] Training loss: 0.01161830
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11252.569128870082 ----------
[1/200] Training loss: 0.15309062
[2/200] Training loss: 0.06148088
[3/200] Training loss: 0.05478345
[4/200] Training loss: 0.04816677
[5/200] Training loss: 0.04399297
[6/200] Training loss: 0.04215952
[7/200] Training loss: 0.04076121
[8/200] Training loss: 0.03560569
[9/200] Training loss: 0.03575011
[10/200] Training loss: 0.03329146
[50/200] Training loss: 0.01825921
[100/200] Training loss: 0.01532493
[150/200] Training loss: 0.01387741
[200/200] Training loss: 0.01254521
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20367.22465138537 ----------
[1/200] Training loss: 0.17997205
[2/200] Training loss: 0.06236765
[3/200] Training loss: 0.05801267
[4/200] Training loss: 0.05521087
[5/200] Training loss: 0.05083809
[6/200] Training loss: 0.04931187
[7/200] Training loss: 0.04837261
[8/200] Training loss: 0.04531993
[9/200] Training loss: 0.04506188
[10/200] Training loss: 0.04403596
[50/200] Training loss: 0.01994212
[100/200] Training loss: 0.01597022
[150/200] Training loss: 0.01501935
[200/200] Training loss: 0.01390339
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9328.105059442672 ----------
[1/200] Training loss: 0.17975830
[2/200] Training loss: 0.05957487
[3/200] Training loss: 0.05306947
[4/200] Training loss: 0.05135818
[5/200] Training loss: 0.04425573
[6/200] Training loss: 0.04544979
[7/200] Training loss: 0.04518783
[8/200] Training loss: 0.04100403
[9/200] Training loss: 0.03587006
[10/200] Training loss: 0.03414153
[50/200] Training loss: 0.01865016
[100/200] Training loss: 0.01508476
[150/200] Training loss: 0.01278235
[200/200] Training loss: 0.01239409
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20423.084194117204 ----------
[1/200] Training loss: 0.16630973
[2/200] Training loss: 0.06330100
[3/200] Training loss: 0.05584266
[4/200] Training loss: 0.04945087
[5/200] Training loss: 0.04893493
[6/200] Training loss: 0.04354827
[7/200] Training loss: 0.04084897
[8/200] Training loss: 0.03970949
[9/200] Training loss: 0.03576839
[10/200] Training loss: 0.03077259
[50/200] Training loss: 0.01792888
[100/200] Training loss: 0.01533227
[150/200] Training loss: 0.01360318
[200/200] Training loss: 0.01264803
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12226.569101755405 ----------
[1/100] Training loss: 0.15628483
[2/100] Training loss: 0.05662612
[3/100] Training loss: 0.04916898
[4/100] Training loss: 0.04390519
[5/100] Training loss: 0.04397777
[6/100] Training loss: 0.03992356
[7/100] Training loss: 0.03962414
[8/100] Training loss: 0.03644030
[9/100] Training loss: 0.03546416
[10/100] Training loss: 0.03337318
[50/100] Training loss: 0.01954885
[100/100] Training loss: 0.01528215
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4509.75054742499 ----------
[1/200] Training loss: 0.18369966
[2/200] Training loss: 0.06486175
[3/200] Training loss: 0.05503436
[4/200] Training loss: 0.04853268
[5/200] Training loss: 0.04509810
[6/200] Training loss: 0.04334649
[7/200] Training loss: 0.04060112
[8/200] Training loss: 0.03949335
[9/200] Training loss: 0.03743046
[10/200] Training loss: 0.03399183
[50/200] Training loss: 0.01899615
[100/200] Training loss: 0.01730962
[150/200] Training loss: 0.01593311
[200/200] Training loss: 0.01486903
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16321.948903240691 ----------
[1/200] Training loss: 0.16698982
[2/200] Training loss: 0.06314833
[3/200] Training loss: 0.05518068
[4/200] Training loss: 0.05111968
[5/200] Training loss: 0.04803718
[6/200] Training loss: 0.04284786
[7/200] Training loss: 0.03923526
[8/200] Training loss: 0.03724176
[9/200] Training loss: 0.03702987
[10/200] Training loss: 0.03500450
[50/200] Training loss: 0.01960879
[100/200] Training loss: 0.01729600
[150/200] Training loss: 0.01593535
[200/200] Training loss: 0.01510881
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9352.952902693352 ----------
[1/200] Training loss: 0.17005801
[2/200] Training loss: 0.05501064
[3/200] Training loss: 0.04939490
[4/200] Training loss: 0.04693672
[5/200] Training loss: 0.04374772
[6/200] Training loss: 0.04322309
[7/200] Training loss: 0.04212674
[8/200] Training loss: 0.03946168
[9/200] Training loss: 0.03829392
[10/200] Training loss: 0.03361581
[50/200] Training loss: 0.01879185
[100/200] Training loss: 0.01709161
[150/200] Training loss: 0.01585155
[200/200] Training loss: 0.01465015
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12452.935396925497 ----------
[1/200] Training loss: 0.13005981
[2/200] Training loss: 0.05098231
[3/200] Training loss: 0.04537091
[4/200] Training loss: 0.04288090
[5/200] Training loss: 0.03935455
[6/200] Training loss: 0.03633609
[7/200] Training loss: 0.03350524
[8/200] Training loss: 0.03145851
[9/200] Training loss: 0.03013786
[10/200] Training loss: 0.02884183
[50/200] Training loss: 0.01795756
[100/200] Training loss: 0.01616166
[150/200] Training loss: 0.01485438
[200/200] Training loss: 0.01421463
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5009928064813822 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18192.277042745365 ----------
[1/200] Training loss: 0.16282649
[2/200] Training loss: 0.06180689
[3/200] Training loss: 0.05297531
[4/200] Training loss: 0.04946673
[5/200] Training loss: 0.04631325
[6/200] Training loss: 0.04313871
[7/200] Training loss: 0.03938009
[8/200] Training loss: 0.04068555
[9/200] Training loss: 0.03727758
[10/200] Training loss: 0.03580758
[50/200] Training loss: 0.01895106
[100/200] Training loss: 0.01644558
[150/200] Training loss: 0.01565248
[200/200] Training loss: 0.01378423
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7658.181768540102 ----------
[1/200] Training loss: 0.15600363
[2/200] Training loss: 0.05612035
[3/200] Training loss: 0.04857484
[4/200] Training loss: 0.04700558
[5/200] Training loss: 0.04326911
[6/200] Training loss: 0.04128414
[7/200] Training loss: 0.03648067
[8/200] Training loss: 0.03667255
[9/200] Training loss: 0.03628692
[10/200] Training loss: 0.03336338
[50/200] Training loss: 0.01726945
[100/200] Training loss: 0.01420545
[150/200] Training loss: 0.01210306
[200/200] Training loss: 0.01142640
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8249.336943051847 ----------
[1/200] Training loss: 0.16693039
[2/200] Training loss: 0.06391979
[3/200] Training loss: 0.05525231
[4/200] Training loss: 0.05349969
[5/200] Training loss: 0.04929632
[6/200] Training loss: 0.04405629
[7/200] Training loss: 0.03970850
[8/200] Training loss: 0.04092036
[9/200] Training loss: 0.03799979
[10/200] Training loss: 0.03554156
[50/200] Training loss: 0.02015624
[100/200] Training loss: 0.01694879
[150/200] Training loss: 0.01534929
[200/200] Training loss: 0.01447453
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8976.219694281106 ----------
[1/200] Training loss: 0.17695271
[2/200] Training loss: 0.06336496
[3/200] Training loss: 0.05435444
[4/200] Training loss: 0.05345031
[5/200] Training loss: 0.04984559
[6/200] Training loss: 0.04810384
[7/200] Training loss: 0.04700467
[8/200] Training loss: 0.04229831
[9/200] Training loss: 0.03880726
[10/200] Training loss: 0.03571934
[50/200] Training loss: 0.01853543
[100/200] Training loss: 0.01568504
[150/200] Training loss: 0.01388649
[200/200] Training loss: 0.01329130
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19332.578100191397 ----------
[1/200] Training loss: 0.15266718
[2/200] Training loss: 0.05856674
[3/200] Training loss: 0.05419131
[4/200] Training loss: 0.05289067
[5/200] Training loss: 0.04899016
[6/200] Training loss: 0.04803563
[7/200] Training loss: 0.04138085
[8/200] Training loss: 0.04087869
[9/200] Training loss: 0.03645573
[10/200] Training loss: 0.03512910
[50/200] Training loss: 0.01805781
[100/200] Training loss: 0.01521474
[150/200] Training loss: 0.01359482
[200/200] Training loss: 0.01269929
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14650.136927687741 ----------
[1/200] Training loss: 0.16794238
[2/200] Training loss: 0.06193538
[3/200] Training loss: 0.05174563
[4/200] Training loss: 0.04812064
[5/200] Training loss: 0.04514317
[6/200] Training loss: 0.04197061
[7/200] Training loss: 0.03838721
[8/200] Training loss: 0.03476226
[9/200] Training loss: 0.03439847
[10/200] Training loss: 0.03314374
[50/200] Training loss: 0.01895188
[100/200] Training loss: 0.01645722
[150/200] Training loss: 0.01577283
[200/200] Training loss: 0.01468166
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11815.149597021613 ----------
[1/200] Training loss: 0.26163074
[2/200] Training loss: 0.08333709
[3/200] Training loss: 0.05990527
[4/200] Training loss: 0.05753794
[5/200] Training loss: 0.04955307
[6/200] Training loss: 0.04868855
[7/200] Training loss: 0.05068051
[8/200] Training loss: 0.04423118
[9/200] Training loss: 0.04638109
[10/200] Training loss: 0.04561646
[50/200] Training loss: 0.02439764
[100/200] Training loss: 0.01739378
[150/200] Training loss: 0.01575473
[200/200] Training loss: 0.01553810
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10387.669228465065 ----------
[1/200] Training loss: 0.16721223
[2/200] Training loss: 0.05545807
[3/200] Training loss: 0.05221111
[4/200] Training loss: 0.05011769
[5/200] Training loss: 0.04261584
[6/200] Training loss: 0.04112820
[7/200] Training loss: 0.03995283
[8/200] Training loss: 0.03481742
[9/200] Training loss: 0.03473139
[10/200] Training loss: 0.03520806
[50/200] Training loss: 0.01923086
[100/200] Training loss: 0.01514514
[150/200] Training loss: 0.01462819
[200/200] Training loss: 0.01318347
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8544.490154479668 ----------
[1/200] Training loss: 0.14869798
[2/200] Training loss: 0.05801398
[3/200] Training loss: 0.05288633
[4/200] Training loss: 0.04973600
[5/200] Training loss: 0.05052196
[6/200] Training loss: 0.04635152
[7/200] Training loss: 0.04413095
[8/200] Training loss: 0.03995722
[9/200] Training loss: 0.03840449
[10/200] Training loss: 0.03755271
[50/200] Training loss: 0.01865996
[100/200] Training loss: 0.01616680
[150/200] Training loss: 0.01459701
[200/200] Training loss: 0.01270409
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7118.1222242948315 ----------
[1/200] Training loss: 0.16970130
[2/200] Training loss: 0.05726767
[3/200] Training loss: 0.05110359
[4/200] Training loss: 0.04918187
[5/200] Training loss: 0.04564016
[6/200] Training loss: 0.04232880
[7/200] Training loss: 0.03991431
[8/200] Training loss: 0.03991008
[9/200] Training loss: 0.03607419
[10/200] Training loss: 0.03639737
[50/200] Training loss: 0.01911766
[100/200] Training loss: 0.01569528
[150/200] Training loss: 0.01437758
[200/200] Training loss: 0.01268757
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 3628.464275695711 ----------
[1/200] Training loss: 0.13867360
[2/200] Training loss: 0.05201688
[3/200] Training loss: 0.05257032
[4/200] Training loss: 0.04780773
[5/200] Training loss: 0.04486104
[6/200] Training loss: 0.04314996
[7/200] Training loss: 0.04027638
[8/200] Training loss: 0.03670088
[9/200] Training loss: 0.03591627
[10/200] Training loss: 0.03285761
[50/200] Training loss: 0.01692218
[100/200] Training loss: 0.01434536
[150/200] Training loss: 0.01237978
[200/200] Training loss: 0.01198882
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13023.512275880114 ----------
[1/200] Training loss: 0.16392502
[2/200] Training loss: 0.05956980
[3/200] Training loss: 0.05137493
[4/200] Training loss: 0.04589042
[5/200] Training loss: 0.04147736
[6/200] Training loss: 0.04458662
[7/200] Training loss: 0.03726713
[8/200] Training loss: 0.03633921
[9/200] Training loss: 0.03439554
[10/200] Training loss: 0.03148892
[50/200] Training loss: 0.01755948
[100/200] Training loss: 0.01513294
[150/200] Training loss: 0.01298769
[200/200] Training loss: 0.01210755
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16963.44210353547 ----------
[1/200] Training loss: 0.16393088
[2/200] Training loss: 0.05944467
[3/200] Training loss: 0.05143471
[4/200] Training loss: 0.04599137
[5/200] Training loss: 0.04310507
[6/200] Training loss: 0.04140986
[7/200] Training loss: 0.04214918
[8/200] Training loss: 0.03358925
[9/200] Training loss: 0.03347118
[10/200] Training loss: 0.03319536
[50/200] Training loss: 0.01855954
[100/200] Training loss: 0.01502782
[150/200] Training loss: 0.01360144
[200/200] Training loss: 0.01311705
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5353.752702544263 ----------
[1/200] Training loss: 0.15264632
[2/200] Training loss: 0.05714521
[3/200] Training loss: 0.05101687
[4/200] Training loss: 0.04693937
[5/200] Training loss: 0.04028665
[6/200] Training loss: 0.03987969
[7/200] Training loss: 0.03606687
[8/200] Training loss: 0.03388177
[9/200] Training loss: 0.03144740
[10/200] Training loss: 0.02940639
[50/200] Training loss: 0.01738842
[100/200] Training loss: 0.01372375
[150/200] Training loss: 0.01228495
[200/200] Training loss: 0.01241057
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19808.434567123168 ----------
[1/200] Training loss: 0.15626795
[2/200] Training loss: 0.06144550
[3/200] Training loss: 0.05553753
[4/200] Training loss: 0.04823179
[5/200] Training loss: 0.04603153
[6/200] Training loss: 0.04379381
[7/200] Training loss: 0.03990034
[8/200] Training loss: 0.04093645
[9/200] Training loss: 0.03708207
[10/200] Training loss: 0.03492936
[50/200] Training loss: 0.01720224
[100/200] Training loss: 0.01384400
[150/200] Training loss: 0.01143096
[200/200] Training loss: 0.01178609
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12087.737257237188 ----------
[1/200] Training loss: 0.17727083
[2/200] Training loss: 0.06470614
[3/200] Training loss: 0.05409888
[4/200] Training loss: 0.04956215
[5/200] Training loss: 0.04682682
[6/200] Training loss: 0.04312141
[7/200] Training loss: 0.04012205
[8/200] Training loss: 0.04065509
[9/200] Training loss: 0.03514677
[10/200] Training loss: 0.03527296
[50/200] Training loss: 0.01931847
[100/200] Training loss: 0.01653963
[150/200] Training loss: 0.01499025
[200/200] Training loss: 0.01409170
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13479.798811555016 ----------
[1/100] Training loss: 0.17497478
[2/100] Training loss: 0.06100308
[3/100] Training loss: 0.05384952
[4/100] Training loss: 0.05035005
[5/100] Training loss: 0.04905694
[6/100] Training loss: 0.04789148
[7/100] Training loss: 0.04378388
[8/100] Training loss: 0.04265382
[9/100] Training loss: 0.03994997
[10/100] Training loss: 0.03904500
[50/100] Training loss: 0.01940695
[100/100] Training loss: 0.01622807
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17038.97931215365 ----------
[1/200] Training loss: 0.13791733
[2/200] Training loss: 0.05816935
[3/200] Training loss: 0.04958731
[4/200] Training loss: 0.04564582
[5/200] Training loss: 0.04387936
[6/200] Training loss: 0.03967355
[7/200] Training loss: 0.03689051
[8/200] Training loss: 0.03502641
[9/200] Training loss: 0.03060122
[10/200] Training loss: 0.02987680
[50/200] Training loss: 0.01606895
[100/200] Training loss: 0.01257464
[150/200] Training loss: 0.01171832
[200/200] Training loss: 0.01093629
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14849.525783674037 ----------
[1/200] Training loss: 0.18375541
[2/200] Training loss: 0.06419179
[3/200] Training loss: 0.05494195
[4/200] Training loss: 0.05032154
[5/200] Training loss: 0.05057230
[6/200] Training loss: 0.04889385
[7/200] Training loss: 0.04382067
[8/200] Training loss: 0.04267072
[9/200] Training loss: 0.03915420
[10/200] Training loss: 0.04004785
[50/200] Training loss: 0.02108144
[100/200] Training loss: 0.01652659
[150/200] Training loss: 0.01450497
[200/200] Training loss: 0.01316570
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11098.86300483072 ----------
[1/200] Training loss: 0.14698325
[2/200] Training loss: 0.05281918
[3/200] Training loss: 0.04745528
[4/200] Training loss: 0.03875064
[5/200] Training loss: 0.03745574
[6/200] Training loss: 0.03469003
[7/200] Training loss: 0.03256655
[8/200] Training loss: 0.03011052
[9/200] Training loss: 0.02843455
[10/200] Training loss: 0.02904185
[50/200] Training loss: 0.01686820
[100/200] Training loss: 0.01366372
[150/200] Training loss: 0.01168928
[200/200] Training loss: 0.01085088
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10246.018934200736 ----------
[1/200] Training loss: 0.14226144
[2/200] Training loss: 0.05984966
[3/200] Training loss: 0.05753621
[4/200] Training loss: 0.04845065
[5/200] Training loss: 0.04764505
[6/200] Training loss: 0.04592690
[7/200] Training loss: 0.04200372
[8/200] Training loss: 0.04113719
[9/200] Training loss: 0.03909676
[10/200] Training loss: 0.03462537
[50/200] Training loss: 0.01829163
[100/200] Training loss: 0.01387793
[150/200] Training loss: 0.01287552
[200/200] Training loss: 0.01213993
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14292.912929140792 ----------
[1/100] Training loss: 0.16178850
[2/100] Training loss: 0.05972924
[3/100] Training loss: 0.05230562
[4/100] Training loss: 0.04588626
[5/100] Training loss: 0.04621399
[6/100] Training loss: 0.04131931
[7/100] Training loss: 0.04002613
[8/100] Training loss: 0.03570490
[9/100] Training loss: 0.03315779
[10/100] Training loss: 0.03300932
[50/100] Training loss: 0.01578066
[100/100] Training loss: 0.01272371
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17970.401442371844 ----------
[1/100] Training loss: 0.14288116
[2/100] Training loss: 0.05559437
[3/100] Training loss: 0.04596729
[4/100] Training loss: 0.04520769
[5/100] Training loss: 0.04522195
[6/100] Training loss: 0.03799207
[7/100] Training loss: 0.03674966
[8/100] Training loss: 0.03680817
[9/100] Training loss: 0.03479588
[10/100] Training loss: 0.03151989
[50/100] Training loss: 0.01740108
[100/100] Training loss: 0.01459464
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5929.865091214133 ----------
[1/200] Training loss: 0.17792072
[2/200] Training loss: 0.06067035
[3/200] Training loss: 0.04894247
[4/200] Training loss: 0.05198663
[5/200] Training loss: 0.04577494
[6/200] Training loss: 0.04328435
[7/200] Training loss: 0.04302016
[8/200] Training loss: 0.03896256
[9/200] Training loss: 0.03760243
[10/200] Training loss: 0.03505857
[50/200] Training loss: 0.01838435
[100/200] Training loss: 0.01553416
[150/200] Training loss: 0.01430154
[200/200] Training loss: 0.01319518
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16907.620057240463 ----------
[1/200] Training loss: 0.20992511
[2/200] Training loss: 0.06817250
[3/200] Training loss: 0.05959797
[4/200] Training loss: 0.05248919
[5/200] Training loss: 0.04911985
[6/200] Training loss: 0.04396792
[7/200] Training loss: 0.04116644
[8/200] Training loss: 0.03893876
[9/200] Training loss: 0.03276859
[10/200] Training loss: 0.03510583
[50/200] Training loss: 0.01791234
[100/200] Training loss: 0.01646700
[150/200] Training loss: 0.01418675
[200/200] Training loss: 0.01304711
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9327.231958089174 ----------
[1/200] Training loss: 0.17325030
[2/200] Training loss: 0.06073485
[3/200] Training loss: 0.05247204
[4/200] Training loss: 0.05016331
[5/200] Training loss: 0.04906414
[6/200] Training loss: 0.04240980
[7/200] Training loss: 0.04159579
[8/200] Training loss: 0.03929657
[9/200] Training loss: 0.03837089
[10/200] Training loss: 0.03436865
[50/200] Training loss: 0.01722064
[100/200] Training loss: 0.01515969
[150/200] Training loss: 0.01367463
[200/200] Training loss: 0.01287250
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7159.866479201969 ----------
[1/200] Training loss: 0.16113765
[2/200] Training loss: 0.06493389
[3/200] Training loss: 0.05464056
[4/200] Training loss: 0.05051312
[5/200] Training loss: 0.04667090
[6/200] Training loss: 0.04741737
[7/200] Training loss: 0.04171706
[8/200] Training loss: 0.04179207
[9/200] Training loss: 0.03907013
[10/200] Training loss: 0.03489259
[50/200] Training loss: 0.01918032
[100/200] Training loss: 0.01504949
[150/200] Training loss: 0.01321436
[200/200] Training loss: 0.01250890
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6414.794774581646 ----------
[1/200] Training loss: 0.16418243
[2/200] Training loss: 0.05816274
[3/200] Training loss: 0.05597061
[4/200] Training loss: 0.05032813
[5/200] Training loss: 0.04345068
[6/200] Training loss: 0.04283918
[7/200] Training loss: 0.04220764
[8/200] Training loss: 0.04007292
[9/200] Training loss: 0.03970753
[10/200] Training loss: 0.03566563
[50/200] Training loss: 0.01901509
[100/200] Training loss: 0.01574967
[150/200] Training loss: 0.01298825
[200/200] Training loss: 0.01201061
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9747.420992242 ----------
[1/200] Training loss: 0.17502547
[2/200] Training loss: 0.06892014
[3/200] Training loss: 0.06045572
[4/200] Training loss: 0.05348620
[5/200] Training loss: 0.05050109
[6/200] Training loss: 0.04923548
[7/200] Training loss: 0.04600606
[8/200] Training loss: 0.04564817
[9/200] Training loss: 0.04138751
[10/200] Training loss: 0.04121173
[50/200] Training loss: 0.02005920
[100/200] Training loss: 0.01580501
[150/200] Training loss: 0.01457580
[200/200] Training loss: 0.01331494
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15402.676131114358 ----------
[1/200] Training loss: 0.16309288
[2/200] Training loss: 0.05830343
[3/200] Training loss: 0.05143647
[4/200] Training loss: 0.04669958
[5/200] Training loss: 0.04328363
[6/200] Training loss: 0.04034598
[7/200] Training loss: 0.04138642
[8/200] Training loss: 0.03521010
[9/200] Training loss: 0.03532112
[10/200] Training loss: 0.03145416
[50/200] Training loss: 0.01631742
[100/200] Training loss: 0.01312163
[150/200] Training loss: 0.01121688
[200/200] Training loss: 0.01096975
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10145.519208005078 ----------
[1/200] Training loss: 0.17591904
[2/200] Training loss: 0.05662124
[3/200] Training loss: 0.05230971
[4/200] Training loss: 0.04372797
[5/200] Training loss: 0.03825507
[6/200] Training loss: 0.03709382
[7/200] Training loss: 0.03421141
[8/200] Training loss: 0.03160741
[9/200] Training loss: 0.03037959
[10/200] Training loss: 0.02995948
[50/200] Training loss: 0.01549946
[100/200] Training loss: 0.01451505
[150/200] Training loss: 0.01380513
[200/200] Training loss: 0.01242188
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11194.299263464418 ----------
[1/200] Training loss: 0.16713407
[2/200] Training loss: 0.06419289
[3/200] Training loss: 0.05450820
[4/200] Training loss: 0.04948957
[5/200] Training loss: 0.04984615
[6/200] Training loss: 0.04675273
[7/200] Training loss: 0.04052175
[8/200] Training loss: 0.04212309
[9/200] Training loss: 0.03614631
[10/200] Training loss: 0.03457713
[50/200] Training loss: 0.01792669
[100/200] Training loss: 0.01586934
[150/200] Training loss: 0.01435746
[200/200] Training loss: 0.01390930
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9992.536014445983 ----------
[1/200] Training loss: 0.15921629
[2/200] Training loss: 0.05895163
[3/200] Training loss: 0.05550934
[4/200] Training loss: 0.05113919
[5/200] Training loss: 0.04728747
[6/200] Training loss: 0.04248631
[7/200] Training loss: 0.04302714
[8/200] Training loss: 0.03928528
[9/200] Training loss: 0.03859290
[10/200] Training loss: 0.03637595
[50/200] Training loss: 0.01947852
[100/200] Training loss: 0.01698960
[150/200] Training loss: 0.01530898
[200/200] Training loss: 0.01490412
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7698.253568180253 ----------
[1/200] Training loss: 0.21904611
[2/200] Training loss: 0.06930709
[3/200] Training loss: 0.05740247
[4/200] Training loss: 0.05282847
[5/200] Training loss: 0.05272047
[6/200] Training loss: 0.05174995
[7/200] Training loss: 0.04674299
[8/200] Training loss: 0.04320243
[9/200] Training loss: 0.04430193
[10/200] Training loss: 0.04179487
[50/200] Training loss: 0.02217055
[100/200] Training loss: 0.01730464
[150/200] Training loss: 0.01523485
[200/200] Training loss: 0.01570043
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5216.317475000923 ----------
[1/200] Training loss: 0.13413510
[2/200] Training loss: 0.06026713
[3/200] Training loss: 0.04962314
[4/200] Training loss: 0.04729161
[5/200] Training loss: 0.04343563
[6/200] Training loss: 0.04177092
[7/200] Training loss: 0.03627619
[8/200] Training loss: 0.03560017
[9/200] Training loss: 0.03337714
[10/200] Training loss: 0.03112909
[50/200] Training loss: 0.01764229
[100/200] Training loss: 0.01464952
[150/200] Training loss: 0.01298598
[200/200] Training loss: 0.01166522
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21357.739955341716 ----------
[1/200] Training loss: 0.14400662
[2/200] Training loss: 0.05532108
[3/200] Training loss: 0.05400737
[4/200] Training loss: 0.04948541
[5/200] Training loss: 0.04676840
[6/200] Training loss: 0.04156655
[7/200] Training loss: 0.04346976
[8/200] Training loss: 0.03712960
[9/200] Training loss: 0.04014988
[10/200] Training loss: 0.03672444
[50/200] Training loss: 0.01630608
[100/200] Training loss: 0.01366948
[150/200] Training loss: 0.01252117
[200/200] Training loss: 0.01158592
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21395.482887749928 ----------
[1/200] Training loss: 0.17693996
[2/200] Training loss: 0.05821460
[3/200] Training loss: 0.05423844
[4/200] Training loss: 0.04963747
[5/200] Training loss: 0.04583558
[6/200] Training loss: 0.04393181
[7/200] Training loss: 0.04076087
[8/200] Training loss: 0.04045740
[9/200] Training loss: 0.03699667
[10/200] Training loss: 0.03489179
[50/200] Training loss: 0.01976599
[100/200] Training loss: 0.01512202
[150/200] Training loss: 0.01413965
[200/200] Training loss: 0.01373287
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14441.867469271418 ----------
[1/200] Training loss: 0.16607034
[2/200] Training loss: 0.05783101
[3/200] Training loss: 0.05571049
[4/200] Training loss: 0.05153369
[5/200] Training loss: 0.04579428
[6/200] Training loss: 0.04550311
[7/200] Training loss: 0.04133711
[8/200] Training loss: 0.04157716
[9/200] Training loss: 0.03777237
[10/200] Training loss: 0.03156395
[50/200] Training loss: 0.01629640
[100/200] Training loss: 0.01431425
[150/200] Training loss: 0.01308876
[200/200] Training loss: 0.01232297
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13424.079260791035 ----------
[1/200] Training loss: 0.15197816
[2/200] Training loss: 0.05732471
[3/200] Training loss: 0.05355857
[4/200] Training loss: 0.04882986
[5/200] Training loss: 0.04905110
[6/200] Training loss: 0.04325664
[7/200] Training loss: 0.04157933
[8/200] Training loss: 0.03884987
[9/200] Training loss: 0.03609087
[10/200] Training loss: 0.03318611
[50/200] Training loss: 0.01728970
[100/200] Training loss: 0.01490521
[150/200] Training loss: 0.01418793
[200/200] Training loss: 0.01326376
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21142.276887790493 ----------
[1/200] Training loss: 0.14147954
[2/200] Training loss: 0.05378589
[3/200] Training loss: 0.04830572
[4/200] Training loss: 0.04608767
[5/200] Training loss: 0.03858246
[6/200] Training loss: 0.03619583
[7/200] Training loss: 0.03383283
[8/200] Training loss: 0.03091573
[9/200] Training loss: 0.03029466
[10/200] Training loss: 0.02955629
[50/200] Training loss: 0.01746378
[100/200] Training loss: 0.01512565
[150/200] Training loss: 0.01342011
[200/200] Training loss: 0.01369908
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15517.260582976623 ----------
[1/200] Training loss: 0.18167666
[2/200] Training loss: 0.05941557
[3/200] Training loss: 0.05372524
[4/200] Training loss: 0.04584342
[5/200] Training loss: 0.04230831
[6/200] Training loss: 0.04136118
[7/200] Training loss: 0.03913389
[8/200] Training loss: 0.03575144
[9/200] Training loss: 0.03503764
[10/200] Training loss: 0.03361664
[50/200] Training loss: 0.01718582
[100/200] Training loss: 0.01390802
[150/200] Training loss: 0.01331437
[200/200] Training loss: 0.01167892
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7003.326066948475 ----------
[1/200] Training loss: 0.15650471
[2/200] Training loss: 0.05414055
[3/200] Training loss: 0.05139197
[4/200] Training loss: 0.04608005
[5/200] Training loss: 0.04662913
[6/200] Training loss: 0.04236309
[7/200] Training loss: 0.03634591
[8/200] Training loss: 0.03589402
[9/200] Training loss: 0.03392302
[10/200] Training loss: 0.03179632
[50/200] Training loss: 0.01843056
[100/200] Training loss: 0.01666059
[150/200] Training loss: 0.01450285
[200/200] Training loss: 0.01451844
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9268.334909788273 ----------
[1/200] Training loss: 0.15579976
[2/200] Training loss: 0.05785268
[3/200] Training loss: 0.05443296
[4/200] Training loss: 0.04890839
[5/200] Training loss: 0.05025766
[6/200] Training loss: 0.04726644
[7/200] Training loss: 0.04483319
[8/200] Training loss: 0.04138163
[9/200] Training loss: 0.04044244
[10/200] Training loss: 0.03839027
[50/200] Training loss: 0.01794149
[100/200] Training loss: 0.01607732
[150/200] Training loss: 0.01407422
[200/200] Training loss: 0.01299572
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12272.628406335783 ----------
[1/200] Training loss: 0.17805105
[2/200] Training loss: 0.06205385
[3/200] Training loss: 0.05337763
[4/200] Training loss: 0.05237132
[5/200] Training loss: 0.04805215
[6/200] Training loss: 0.04580135
[7/200] Training loss: 0.04507410
[8/200] Training loss: 0.04197191
[9/200] Training loss: 0.04101217
[10/200] Training loss: 0.03930057
[50/200] Training loss: 0.01952937
[100/200] Training loss: 0.01660990
[150/200] Training loss: 0.01519210
[200/200] Training loss: 0.01406637
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16876.40814865533 ----------
[1/200] Training loss: 0.29091558
[2/200] Training loss: 0.07912353
[3/200] Training loss: 0.05910078
[4/200] Training loss: 0.05514570
[5/200] Training loss: 0.05406485
[6/200] Training loss: 0.05033442
[7/200] Training loss: 0.05470019
[8/200] Training loss: 0.05263864
[9/200] Training loss: 0.04484267
[10/200] Training loss: 0.04430438
[50/200] Training loss: 0.02161593
[100/200] Training loss: 0.01702652
[150/200] Training loss: 0.01514070
[200/200] Training loss: 0.01390694
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11294.500962857986 ----------
[1/200] Training loss: 0.12760034
[2/200] Training loss: 0.06154167
[3/200] Training loss: 0.05278849
[4/200] Training loss: 0.04878440
[5/200] Training loss: 0.04512307
[6/200] Training loss: 0.04074028
[7/200] Training loss: 0.03821837
[8/200] Training loss: 0.03678177
[9/200] Training loss: 0.03183385
[10/200] Training loss: 0.03271602
[50/200] Training loss: 0.01974320
[100/200] Training loss: 0.01641055
[150/200] Training loss: 0.01432952
[200/200] Training loss: 0.01208803
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15126.56894341873 ----------
[1/200] Training loss: 0.25325608
[2/200] Training loss: 0.07461080
[3/200] Training loss: 0.05843970
[4/200] Training loss: 0.05969972
[5/200] Training loss: 0.04914918
[6/200] Training loss: 0.05049995
[7/200] Training loss: 0.04420359
[8/200] Training loss: 0.04915719
[9/200] Training loss: 0.04395443
[10/200] Training loss: 0.04398069
[50/200] Training loss: 0.02295123
[100/200] Training loss: 0.01581024
[150/200] Training loss: 0.01452445
[200/200] Training loss: 0.01244851
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10625.06357628038 ----------
[1/200] Training loss: 0.18850442
[2/200] Training loss: 0.06159735
[3/200] Training loss: 0.05105559
[4/200] Training loss: 0.04472410
[5/200] Training loss: 0.04183577
[6/200] Training loss: 0.04052417
[7/200] Training loss: 0.04182192
[8/200] Training loss: 0.03865945
[9/200] Training loss: 0.03598615
[10/200] Training loss: 0.03619889
[50/200] Training loss: 0.01911292
[100/200] Training loss: 0.01631996
[150/200] Training loss: 0.01506341
[200/200] Training loss: 0.01314879
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8595.133972196129 ----------
[1/200] Training loss: 0.14600972
[2/200] Training loss: 0.05791707
[3/200] Training loss: 0.05356201
[4/200] Training loss: 0.04851455
[5/200] Training loss: 0.04588246
[6/200] Training loss: 0.04083637
[7/200] Training loss: 0.03785863
[8/200] Training loss: 0.03723292
[9/200] Training loss: 0.03365627
[10/200] Training loss: 0.03215673
[50/200] Training loss: 0.01834381
[100/200] Training loss: 0.01693880
[150/200] Training loss: 0.01565779
[200/200] Training loss: 0.01504276
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14753.412079922393 ----------
[1/200] Training loss: 0.15926655
[2/200] Training loss: 0.06036914
[3/200] Training loss: 0.05136070
[4/200] Training loss: 0.04497533
[5/200] Training loss: 0.04530030
[6/200] Training loss: 0.03734203
[7/200] Training loss: 0.03568553
[8/200] Training loss: 0.03599375
[9/200] Training loss: 0.03244887
[10/200] Training loss: 0.03372704
[50/200] Training loss: 0.01979456
[100/200] Training loss: 0.01572592
[150/200] Training loss: 0.01487879
[200/200] Training loss: 0.01281813
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23091.68854804689 ----------
[1/200] Training loss: 0.17936621
[2/200] Training loss: 0.06221999
[3/200] Training loss: 0.05252899
[4/200] Training loss: 0.05154561
[5/200] Training loss: 0.04814445
[6/200] Training loss: 0.04511141
[7/200] Training loss: 0.04486706
[8/200] Training loss: 0.04258444
[9/200] Training loss: 0.03985853
[10/200] Training loss: 0.03820904
[50/200] Training loss: 0.01782007
[100/200] Training loss: 0.01546820
[150/200] Training loss: 0.01380691
[200/200] Training loss: 0.01287270
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7213.973939514891 ----------
[1/200] Training loss: 0.18395241
[2/200] Training loss: 0.05884314
[3/200] Training loss: 0.05229919
[4/200] Training loss: 0.05357808
[5/200] Training loss: 0.05043978
[6/200] Training loss: 0.04992962
[7/200] Training loss: 0.04802579
[8/200] Training loss: 0.04561418
[9/200] Training loss: 0.04321237
[10/200] Training loss: 0.04292212
[50/200] Training loss: 0.01969369
[100/200] Training loss: 0.01656076
[150/200] Training loss: 0.01545248
[200/200] Training loss: 0.01472986
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11552.086911030405 ----------
[1/200] Training loss: 0.17652344
[2/200] Training loss: 0.06097333
[3/200] Training loss: 0.06102678
[4/200] Training loss: 0.05000706
[5/200] Training loss: 0.04386743
[6/200] Training loss: 0.04575274
[7/200] Training loss: 0.03758731
[8/200] Training loss: 0.03871573
[9/200] Training loss: 0.03788508
[10/200] Training loss: 0.03412794
[50/200] Training loss: 0.01832328
[100/200] Training loss: 0.01458663
[150/200] Training loss: 0.01305405
[200/200] Training loss: 0.01224503
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9954.06851493398 ----------
[1/200] Training loss: 0.17327881
[2/200] Training loss: 0.06051161
[3/200] Training loss: 0.05023865
[4/200] Training loss: 0.04109676
[5/200] Training loss: 0.04389997
[6/200] Training loss: 0.03925457
[7/200] Training loss: 0.03537563
[8/200] Training loss: 0.03531608
[9/200] Training loss: 0.03106295
[10/200] Training loss: 0.02720791
[50/200] Training loss: 0.01821529
[100/200] Training loss: 0.01629720
[150/200] Training loss: 0.01483811
[200/200] Training loss: 0.01430727
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13046.744268207298 ----------
[1/200] Training loss: 0.14648447
[2/200] Training loss: 0.05620184
[3/200] Training loss: 0.05306683
[4/200] Training loss: 0.04888782
[5/200] Training loss: 0.04232643
[6/200] Training loss: 0.03930562
[7/200] Training loss: 0.03630533
[8/200] Training loss: 0.03449996
[9/200] Training loss: 0.03051644
[10/200] Training loss: 0.03207082
[50/200] Training loss: 0.01724628
[100/200] Training loss: 0.01486768
[150/200] Training loss: 0.01313154
[200/200] Training loss: 0.01215509
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15126.53562452421 ----------
[1/200] Training loss: 0.14720892
[2/200] Training loss: 0.05472592
[3/200] Training loss: 0.04488752
[4/200] Training loss: 0.04289220
[5/200] Training loss: 0.03804265
[6/200] Training loss: 0.03603641
[7/200] Training loss: 0.03314119
[8/200] Training loss: 0.03082395
[9/200] Training loss: 0.02934971
[10/200] Training loss: 0.02728199
[50/200] Training loss: 0.01753609
[100/200] Training loss: 0.01468401
[150/200] Training loss: 0.01390741
[200/200] Training loss: 0.01223450
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13402.675553784027 ----------
[1/200] Training loss: 0.18575021
[2/200] Training loss: 0.05773960
[3/200] Training loss: 0.05036024
[4/200] Training loss: 0.05271161
[5/200] Training loss: 0.04359172
[6/200] Training loss: 0.04257790
[7/200] Training loss: 0.04107000
[8/200] Training loss: 0.03697950
[9/200] Training loss: 0.03761686
[10/200] Training loss: 0.03499042
[50/200] Training loss: 0.01805804
[100/200] Training loss: 0.01570570
[150/200] Training loss: 0.01451682
[200/200] Training loss: 0.01318357
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10523.258050622915 ----------
[1/200] Training loss: 0.08951832
[2/200] Training loss: 0.05006884
[3/200] Training loss: 0.04118146
[4/200] Training loss: 0.03426374
[5/200] Training loss: 0.03062950
[6/200] Training loss: 0.02709771
[7/200] Training loss: 0.02593209
[8/200] Training loss: 0.02445077
[9/200] Training loss: 0.02402297
[10/200] Training loss: 0.02240323
[50/200] Training loss: 0.01512395
[100/200] Training loss: 0.01293350
[150/200] Training loss: 0.01185881
[200/200] Training loss: 0.01079583
---batch_size---: 4 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14005.9387404058 ----------
[1/200] Training loss: 0.18704892
[2/200] Training loss: 0.06511276
[3/200] Training loss: 0.05562179
[4/200] Training loss: 0.05227340
[5/200] Training loss: 0.04912209
[6/200] Training loss: 0.04798694
[7/200] Training loss: 0.04550091
[8/200] Training loss: 0.04430484
[9/200] Training loss: 0.04091900
[10/200] Training loss: 0.03976243
[50/200] Training loss: 0.02140932
[100/200] Training loss: 0.01721696
[150/200] Training loss: 0.01574380
[200/200] Training loss: 0.01401650
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18466.066608782716 ----------
[1/200] Training loss: 0.26665889
[2/200] Training loss: 0.08028400
[3/200] Training loss: 0.06548603
[4/200] Training loss: 0.05450093
[5/200] Training loss: 0.05761780
[6/200] Training loss: 0.05285152
[7/200] Training loss: 0.05320362
[8/200] Training loss: 0.04903156
[9/200] Training loss: 0.05099290
[10/200] Training loss: 0.04621770
[50/200] Training loss: 0.02269499
[100/200] Training loss: 0.01806664
[150/200] Training loss: 0.01745430
[200/200] Training loss: 0.01510672
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16690.6728444362 ----------
[1/200] Training loss: 0.14690962
[2/200] Training loss: 0.06446497
[3/200] Training loss: 0.05261193
[4/200] Training loss: 0.05162860
[5/200] Training loss: 0.04769618
[6/200] Training loss: 0.04532360
[7/200] Training loss: 0.04352181
[8/200] Training loss: 0.04107610
[9/200] Training loss: 0.04037911
[10/200] Training loss: 0.03792925
[50/200] Training loss: 0.01760581
[100/200] Training loss: 0.01385341
[150/200] Training loss: 0.01265741
[200/200] Training loss: 0.01124281
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23574.954846192177 ----------
[1/200] Training loss: 0.16293337
[2/200] Training loss: 0.05520750
[3/200] Training loss: 0.04774988
[4/200] Training loss: 0.04447410
[5/200] Training loss: 0.04058997
[6/200] Training loss: 0.03945428
[7/200] Training loss: 0.03737564
[8/200] Training loss: 0.03527513
[9/200] Training loss: 0.03352268
[10/200] Training loss: 0.03168520
[50/200] Training loss: 0.01730590
[100/200] Training loss: 0.01535980
[150/200] Training loss: 0.01313612
[200/200] Training loss: 0.01294206
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12791.354580340583 ----------
[1/200] Training loss: 0.16481996
[2/200] Training loss: 0.05921774
[3/200] Training loss: 0.05625530
[4/200] Training loss: 0.05099890
[5/200] Training loss: 0.04958089
[6/200] Training loss: 0.04824198
[7/200] Training loss: 0.04653507
[8/200] Training loss: 0.04526415
[9/200] Training loss: 0.04315644
[10/200] Training loss: 0.04041943
[50/200] Training loss: 0.01867610
[100/200] Training loss: 0.01534360
[150/200] Training loss: 0.01265913
[200/200] Training loss: 0.01152456
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11394.008601014833 ----------
[1/200] Training loss: 0.15528700
[2/200] Training loss: 0.05928518
[3/200] Training loss: 0.05155741
[4/200] Training loss: 0.04779544
[5/200] Training loss: 0.04446958
[6/200] Training loss: 0.04243619
[7/200] Training loss: 0.04024440
[8/200] Training loss: 0.03836106
[9/200] Training loss: 0.03638423
[10/200] Training loss: 0.03516331
[50/200] Training loss: 0.01759339
[100/200] Training loss: 0.01419233
[150/200] Training loss: 0.01334856
[200/200] Training loss: 0.01182445
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15329.468092533412 ----------
[1/200] Training loss: 0.15145584
[2/200] Training loss: 0.06316535
[3/200] Training loss: 0.05510343
[4/200] Training loss: 0.04700241
[5/200] Training loss: 0.04453138
[6/200] Training loss: 0.04145887
[7/200] Training loss: 0.04293745
[8/200] Training loss: 0.03626033
[9/200] Training loss: 0.03553077
[10/200] Training loss: 0.03769025
[50/200] Training loss: 0.01818712
[100/200] Training loss: 0.01409242
[150/200] Training loss: 0.01278802
[200/200] Training loss: 0.01299627
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17239.12294752839 ----------
[1/200] Training loss: 0.19626830
[2/200] Training loss: 0.06225842
[3/200] Training loss: 0.05501476
[4/200] Training loss: 0.05140276
[5/200] Training loss: 0.04781647
[6/200] Training loss: 0.04714664
[7/200] Training loss: 0.04480654
[8/200] Training loss: 0.04469063
[9/200] Training loss: 0.04206045
[10/200] Training loss: 0.03897809
[50/200] Training loss: 0.02240705
[100/200] Training loss: 0.01613666
[150/200] Training loss: 0.01393130
[200/200] Training loss: 0.01290377
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8671.186308689255 ----------
[1/200] Training loss: 0.19058417
[2/200] Training loss: 0.06483929
[3/200] Training loss: 0.05509873
[4/200] Training loss: 0.04750780
[5/200] Training loss: 0.04730515
[6/200] Training loss: 0.04625580
[7/200] Training loss: 0.04101716
[8/200] Training loss: 0.03670757
[9/200] Training loss: 0.03524318
[10/200] Training loss: 0.03363201
[50/200] Training loss: 0.01759193
[100/200] Training loss: 0.01547767
[150/200] Training loss: 0.01393852
[200/200] Training loss: 0.01280732
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20766.72723372655 ----------
[1/200] Training loss: 0.15588052
[2/200] Training loss: 0.06026191
[3/200] Training loss: 0.05504906
[4/200] Training loss: 0.04663866
[5/200] Training loss: 0.04417748
[6/200] Training loss: 0.04351801
[7/200] Training loss: 0.03958682
[8/200] Training loss: 0.03566146
[9/200] Training loss: 0.03492807
[10/200] Training loss: 0.03164155
[50/200] Training loss: 0.01762042
[100/200] Training loss: 0.01559836
[150/200] Training loss: 0.01285196
[200/200] Training loss: 0.01208660
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14160.824269794468 ----------
[1/200] Training loss: 0.13903991
[2/200] Training loss: 0.05712286
[3/200] Training loss: 0.05099542
[4/200] Training loss: 0.04549680
[5/200] Training loss: 0.04337381
[6/200] Training loss: 0.04023224
[7/200] Training loss: 0.03777333
[8/200] Training loss: 0.03292579
[9/200] Training loss: 0.03162996
[10/200] Training loss: 0.03096184
[50/200] Training loss: 0.01549043
[100/200] Training loss: 0.01351680
[150/200] Training loss: 0.01237313
[200/200] Training loss: 0.01141021
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12386.69350553246 ----------
[1/200] Training loss: 0.14290911
[2/200] Training loss: 0.05625502
[3/200] Training loss: 0.04911234
[4/200] Training loss: 0.04630294
[5/200] Training loss: 0.04329733
[6/200] Training loss: 0.04042444
[7/200] Training loss: 0.04020708
[8/200] Training loss: 0.03500016
[9/200] Training loss: 0.03594935
[10/200] Training loss: 0.03373419
[50/200] Training loss: 0.01735138
[100/200] Training loss: 0.01488668
[150/200] Training loss: 0.01301044
[200/200] Training loss: 0.01193694
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17266.16575850006 ----------
[1/200] Training loss: 0.11930461
[2/200] Training loss: 0.05204233
[3/200] Training loss: 0.05066473
[4/200] Training loss: 0.04806038
[5/200] Training loss: 0.04385348
[6/200] Training loss: 0.04251204
[7/200] Training loss: 0.03894336
[8/200] Training loss: 0.03501948
[9/200] Training loss: 0.03646043
[10/200] Training loss: 0.03501446
[50/200] Training loss: 0.01654646
[100/200] Training loss: 0.01398815
[150/200] Training loss: 0.01248190
[200/200] Training loss: 0.01176369
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8565.563612512606 ----------
[1/200] Training loss: 0.14335806
[2/200] Training loss: 0.05749117
[3/200] Training loss: 0.04972827
[4/200] Training loss: 0.04888931
[5/200] Training loss: 0.04187773
[6/200] Training loss: 0.03909983
[7/200] Training loss: 0.03574475
[8/200] Training loss: 0.03334956
[9/200] Training loss: 0.03174732
[10/200] Training loss: 0.03038762
[50/200] Training loss: 0.01604547
[100/200] Training loss: 0.01436923
[150/200] Training loss: 0.01335608
[200/200] Training loss: 0.01105634
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16661.932661009047 ----------
[1/200] Training loss: 0.15954158
[2/200] Training loss: 0.05805560
[3/200] Training loss: 0.05046469
[4/200] Training loss: 0.04494299
[5/200] Training loss: 0.04419160
[6/200] Training loss: 0.04161847
[7/200] Training loss: 0.04038137
[8/200] Training loss: 0.03811269
[9/200] Training loss: 0.03634794
[10/200] Training loss: 0.03514763
[50/200] Training loss: 0.01785209
[100/200] Training loss: 0.01545280
[150/200] Training loss: 0.01366933
[200/200] Training loss: 0.01219551
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11274.627798734644 ----------
[1/200] Training loss: 0.16086782
[2/200] Training loss: 0.05729592
[3/200] Training loss: 0.05395060
[4/200] Training loss: 0.04620309
[5/200] Training loss: 0.04458165
[6/200] Training loss: 0.04368627
[7/200] Training loss: 0.04009831
[8/200] Training loss: 0.03720123
[9/200] Training loss: 0.03435976
[10/200] Training loss: 0.03318631
[50/200] Training loss: 0.01619676
[100/200] Training loss: 0.01450453
[150/200] Training loss: 0.01278101
[200/200] Training loss: 0.01216855
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6237.581582632807 ----------
[1/200] Training loss: 0.16342175
[2/200] Training loss: 0.05892508
[3/200] Training loss: 0.05005001
[4/200] Training loss: 0.05020752
[5/200] Training loss: 0.04726552
[6/200] Training loss: 0.04167566
[7/200] Training loss: 0.04517077
[8/200] Training loss: 0.04093973
[9/200] Training loss: 0.03861259
[10/200] Training loss: 0.03556949
[50/200] Training loss: 0.01722289
[100/200] Training loss: 0.01565621
[150/200] Training loss: 0.01317832
[200/200] Training loss: 0.01293300
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13995.781078596507 ----------
[1/200] Training loss: 0.14400150
[2/200] Training loss: 0.05607527
[3/200] Training loss: 0.05184694
[4/200] Training loss: 0.04842290
[5/200] Training loss: 0.04554310
[6/200] Training loss: 0.04388486
[7/200] Training loss: 0.03937817
[8/200] Training loss: 0.03830362
[9/200] Training loss: 0.03712263
[10/200] Training loss: 0.03549885
[50/200] Training loss: 0.01850997
[100/200] Training loss: 0.01518017
[150/200] Training loss: 0.01306737
[200/200] Training loss: 0.01229668
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11160.077060665844 ----------
[1/200] Training loss: 0.15226230
[2/200] Training loss: 0.05908263
[3/200] Training loss: 0.04913805
[4/200] Training loss: 0.04545285
[5/200] Training loss: 0.04511237
[6/200] Training loss: 0.04179410
[7/200] Training loss: 0.03830227
[8/200] Training loss: 0.03679194
[9/200] Training loss: 0.03439440
[10/200] Training loss: 0.03233967
[50/200] Training loss: 0.01708128
[100/200] Training loss: 0.01509184
[150/200] Training loss: 0.01246599
[200/200] Training loss: 0.01168148
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12449.27885461644 ----------
[1/200] Training loss: 0.16544324
[2/200] Training loss: 0.06344183
[3/200] Training loss: 0.05787151
[4/200] Training loss: 0.05212925
[5/200] Training loss: 0.04881228
[6/200] Training loss: 0.04896334
[7/200] Training loss: 0.04313098
[8/200] Training loss: 0.04132099
[9/200] Training loss: 0.04162575
[10/200] Training loss: 0.03684718
[50/200] Training loss: 0.01821051
[100/200] Training loss: 0.01591089
[150/200] Training loss: 0.01356094
[200/200] Training loss: 0.01316874
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11060.214464466771 ----------
[1/200] Training loss: 0.16608663
[2/200] Training loss: 0.05865527
[3/200] Training loss: 0.05489520
[4/200] Training loss: 0.05449178
[5/200] Training loss: 0.05004940
[6/200] Training loss: 0.04662940
[7/200] Training loss: 0.04574325
[8/200] Training loss: 0.04109503
[9/200] Training loss: 0.03943654
[10/200] Training loss: 0.03818929
[50/200] Training loss: 0.01799307
[100/200] Training loss: 0.01543344
[150/200] Training loss: 0.01385265
[200/200] Training loss: 0.01293918
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 24363.440151177336 ----------
[1/200] Training loss: 0.16361677
[2/200] Training loss: 0.05591003
[3/200] Training loss: 0.05279572
[4/200] Training loss: 0.04792494
[5/200] Training loss: 0.04424797
[6/200] Training loss: 0.04190504
[7/200] Training loss: 0.04058426
[8/200] Training loss: 0.03822515
[9/200] Training loss: 0.03539868
[10/200] Training loss: 0.03497845
[50/200] Training loss: 0.01774880
[100/200] Training loss: 0.01511411
[150/200] Training loss: 0.01421554
[200/200] Training loss: 0.01248453
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16321.81166415052 ----------
[1/200] Training loss: 0.14311703
[2/200] Training loss: 0.05820278
[3/200] Training loss: 0.04842405
[4/200] Training loss: 0.04507308
[5/200] Training loss: 0.04349606
[6/200] Training loss: 0.04083683
[7/200] Training loss: 0.03942763
[8/200] Training loss: 0.03559753
[9/200] Training loss: 0.03309251
[10/200] Training loss: 0.03256435
[50/200] Training loss: 0.01891500
[100/200] Training loss: 0.01532318
[150/200] Training loss: 0.01313566
[200/200] Training loss: 0.01269998
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13940.018364406842 ----------
[1/200] Training loss: 0.15530211
[2/200] Training loss: 0.06016179
[3/200] Training loss: 0.05313719
[4/200] Training loss: 0.04800962
[5/200] Training loss: 0.04735557
[6/200] Training loss: 0.04359380
[7/200] Training loss: 0.04021513
[8/200] Training loss: 0.03729986
[9/200] Training loss: 0.03496629
[10/200] Training loss: 0.03511126
[50/200] Training loss: 0.01814450
[100/200] Training loss: 0.01485559
[150/200] Training loss: 0.01302954
[200/200] Training loss: 0.01222479
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10021.357193514259 ----------
[1/200] Training loss: 0.17329429
[2/200] Training loss: 0.06163237
[3/200] Training loss: 0.05554472
[4/200] Training loss: 0.05517671
[5/200] Training loss: 0.05112151
[6/200] Training loss: 0.04748783
[7/200] Training loss: 0.04482623
[8/200] Training loss: 0.04493150
[9/200] Training loss: 0.04459822
[10/200] Training loss: 0.03812473
[50/200] Training loss: 0.01761422
[100/200] Training loss: 0.01415095
[150/200] Training loss: 0.01325361
[200/200] Training loss: 0.01173975
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7007.848171871306 ----------
[1/200] Training loss: 0.10150245
[2/200] Training loss: 0.05374268
[3/200] Training loss: 0.04964847
[4/200] Training loss: 0.04296669
[5/200] Training loss: 0.04150322
[6/200] Training loss: 0.03759847
[7/200] Training loss: 0.03571521
[8/200] Training loss: 0.03347739
[9/200] Training loss: 0.03044045
[10/200] Training loss: 0.02935725
[50/200] Training loss: 0.01923631
[100/200] Training loss: 0.01693781
[150/200] Training loss: 0.01552138
[200/200] Training loss: 0.01434435
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10896.227236984369 ----------
[1/200] Training loss: 0.16430327
[2/200] Training loss: 0.06404436
[3/200] Training loss: 0.05866069
[4/200] Training loss: 0.04757155
[5/200] Training loss: 0.04869015
[6/200] Training loss: 0.04527257
[7/200] Training loss: 0.04388040
[8/200] Training loss: 0.03919191
[9/200] Training loss: 0.03613164
[10/200] Training loss: 0.03494525
[50/200] Training loss: 0.01935512
[100/200] Training loss: 0.01528559
[150/200] Training loss: 0.01376215
[200/200] Training loss: 0.01317251
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14764.863697305167 ----------
[1/200] Training loss: 0.16449292
[2/200] Training loss: 0.05247921
[3/200] Training loss: 0.04821631
[4/200] Training loss: 0.04651295
[5/200] Training loss: 0.04306815
[6/200] Training loss: 0.03875145
[7/200] Training loss: 0.03757202
[8/200] Training loss: 0.03895804
[9/200] Training loss: 0.03350602
[10/200] Training loss: 0.03360523
[50/200] Training loss: 0.01833996
[100/200] Training loss: 0.01529560
[150/200] Training loss: 0.01313934
[200/200] Training loss: 0.01222636
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15537.762001009025 ----------
[1/200] Training loss: 0.14936754
[2/200] Training loss: 0.05830132
[3/200] Training loss: 0.04761696
[4/200] Training loss: 0.04617973
[5/200] Training loss: 0.04202218
[6/200] Training loss: 0.03746173
[7/200] Training loss: 0.03467867
[8/200] Training loss: 0.03444731
[9/200] Training loss: 0.03288242
[10/200] Training loss: 0.03045953
[50/200] Training loss: 0.01834285
[100/200] Training loss: 0.01505716
[150/200] Training loss: 0.01381286
[200/200] Training loss: 0.01248584
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10821.562918543697 ----------
[1/200] Training loss: 0.16288049
[2/200] Training loss: 0.05728842
[3/200] Training loss: 0.05464708
[4/200] Training loss: 0.05003241
[5/200] Training loss: 0.04781574
[6/200] Training loss: 0.04645406
[7/200] Training loss: 0.04298254
[8/200] Training loss: 0.04158894
[9/200] Training loss: 0.03998928
[10/200] Training loss: 0.03921948
[50/200] Training loss: 0.01947303
[100/200] Training loss: 0.01724404
[150/200] Training loss: 0.01550024
[200/200] Training loss: 0.01396909
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13005.221105386867 ----------
[1/200] Training loss: 0.15666149
[2/200] Training loss: 0.05847012
[3/200] Training loss: 0.05497395
[4/200] Training loss: 0.04848332
[5/200] Training loss: 0.04484789
[6/200] Training loss: 0.04159397
[7/200] Training loss: 0.04000799
[8/200] Training loss: 0.03706702
[9/200] Training loss: 0.03278334
[10/200] Training loss: 0.03319590
[50/200] Training loss: 0.01784361
[100/200] Training loss: 0.01531094
[150/200] Training loss: 0.01410817
[200/200] Training loss: 0.01355151
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4730.607360582782 ----------
[1/200] Training loss: 0.12823620
[2/200] Training loss: 0.05834178
[3/200] Training loss: 0.05107680
[4/200] Training loss: 0.04944313
[5/200] Training loss: 0.04988508
[6/200] Training loss: 0.04539795
[7/200] Training loss: 0.04261884
[8/200] Training loss: 0.04177877
[9/200] Training loss: 0.03895198
[10/200] Training loss: 0.03665623
[50/200] Training loss: 0.01835401
[100/200] Training loss: 0.01533494
[150/200] Training loss: 0.01351862
[200/200] Training loss: 0.01223161
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6645.867889147361 ----------
[1/200] Training loss: 0.16410952
[2/200] Training loss: 0.06251384
[3/200] Training loss: 0.05287411
[4/200] Training loss: 0.05184462
[5/200] Training loss: 0.04652176
[6/200] Training loss: 0.04661387
[7/200] Training loss: 0.04397275
[8/200] Training loss: 0.04435405
[9/200] Training loss: 0.04184905
[10/200] Training loss: 0.03921822
[50/200] Training loss: 0.02008796
[100/200] Training loss: 0.01566204
[150/200] Training loss: 0.01366789
[200/200] Training loss: 0.01230673
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14500.418200865794 ----------
[1/200] Training loss: 0.16289414
[2/200] Training loss: 0.05395574
[3/200] Training loss: 0.05104690
[4/200] Training loss: 0.05025105
[5/200] Training loss: 0.04486641
[6/200] Training loss: 0.04435212
[7/200] Training loss: 0.04199113
[8/200] Training loss: 0.03800143
[9/200] Training loss: 0.03852410
[10/200] Training loss: 0.03702424
[50/200] Training loss: 0.01941283
[100/200] Training loss: 0.01564983
[150/200] Training loss: 0.01461868
[200/200] Training loss: 0.01367139
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9636.171438906636 ----------
[1/200] Training loss: 0.17887750
[2/200] Training loss: 0.06020237
[3/200] Training loss: 0.05568320
[4/200] Training loss: 0.04874644
[5/200] Training loss: 0.04692222
[6/200] Training loss: 0.04374417
[7/200] Training loss: 0.03962129
[8/200] Training loss: 0.03954092
[9/200] Training loss: 0.03722040
[10/200] Training loss: 0.03438871
[50/200] Training loss: 0.01834187
[100/200] Training loss: 0.01511457
[150/200] Training loss: 0.01487891
[200/200] Training loss: 0.01366160
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20951.309648802388 ----------
[1/200] Training loss: 0.13892464
[2/200] Training loss: 0.05742121
[3/200] Training loss: 0.04816797
[4/200] Training loss: 0.04760059
[5/200] Training loss: 0.04406332
[6/200] Training loss: 0.04375310
[7/200] Training loss: 0.03919340
[8/200] Training loss: 0.03734689
[9/200] Training loss: 0.03497519
[10/200] Training loss: 0.03155513
[50/200] Training loss: 0.01773689
[100/200] Training loss: 0.01415023
[150/200] Training loss: 0.01254095
[200/200] Training loss: 0.01210730
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23115.164892338536 ----------
[1/200] Training loss: 0.19474116
[2/200] Training loss: 0.05446359
[3/200] Training loss: 0.05303485
[4/200] Training loss: 0.04832534
[5/200] Training loss: 0.04643900
[6/200] Training loss: 0.04233230
[7/200] Training loss: 0.03844892
[8/200] Training loss: 0.03767872
[9/200] Training loss: 0.03441726
[10/200] Training loss: 0.03113076
[50/200] Training loss: 0.01670246
[100/200] Training loss: 0.01432411
[150/200] Training loss: 0.01254384
[200/200] Training loss: 0.01250739
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11446.11410042727 ----------
[1/200] Training loss: 0.13854948
[2/200] Training loss: 0.05519662
[3/200] Training loss: 0.05090908
[4/200] Training loss: 0.04648066
[5/200] Training loss: 0.04518069
[6/200] Training loss: 0.04121699
[7/200] Training loss: 0.03824428
[8/200] Training loss: 0.03737471
[9/200] Training loss: 0.03555312
[10/200] Training loss: 0.03235446
[50/200] Training loss: 0.01796393
[100/200] Training loss: 0.01406915
[150/200] Training loss: 0.01182455
[200/200] Training loss: 0.01101303
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12328.97400435251 ----------
[1/200] Training loss: 0.16125497
[2/200] Training loss: 0.05467249
[3/200] Training loss: 0.04956791
[4/200] Training loss: 0.04850742
[5/200] Training loss: 0.04264358
[6/200] Training loss: 0.04091335
[7/200] Training loss: 0.03858713
[8/200] Training loss: 0.03541312
[9/200] Training loss: 0.03238845
[10/200] Training loss: 0.03388449
[50/200] Training loss: 0.01676736
[100/200] Training loss: 0.01524375
[150/200] Training loss: 0.01447979
[200/200] Training loss: 0.01255686
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16196.79770818911 ----------
[1/200] Training loss: 0.12779776
[2/200] Training loss: 0.05773229
[3/200] Training loss: 0.05122676
[4/200] Training loss: 0.04789693
[5/200] Training loss: 0.04385647
[6/200] Training loss: 0.04348512
[7/200] Training loss: 0.04016259
[8/200] Training loss: 0.03763894
[9/200] Training loss: 0.03374583
[10/200] Training loss: 0.03206550
[50/200] Training loss: 0.01848533
[100/200] Training loss: 0.01468812
[150/200] Training loss: 0.01346499
[200/200] Training loss: 0.01184456
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 25173.045902313846 ----------
[1/200] Training loss: 0.15931247
[2/200] Training loss: 0.06169401
[3/200] Training loss: 0.05731688
[4/200] Training loss: 0.05229972
[5/200] Training loss: 0.05309197
[6/200] Training loss: 0.04999174
[7/200] Training loss: 0.04811213
[8/200] Training loss: 0.04452508
[9/200] Training loss: 0.04444616
[10/200] Training loss: 0.04012236
[50/200] Training loss: 0.01827467
[100/200] Training loss: 0.01442081
[150/200] Training loss: 0.01312229
[200/200] Training loss: 0.01238779
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15045.961052721092 ----------
[1/200] Training loss: 0.15751796
[2/200] Training loss: 0.05467753
[3/200] Training loss: 0.05468584
[4/200] Training loss: 0.04499160
[5/200] Training loss: 0.04055966
[6/200] Training loss: 0.03990613
[7/200] Training loss: 0.03817865
[8/200] Training loss: 0.03372246
[9/200] Training loss: 0.03551426
[10/200] Training loss: 0.03347696
[50/200] Training loss: 0.01655380
[100/200] Training loss: 0.01373204
[150/200] Training loss: 0.01183521
[200/200] Training loss: 0.01122225
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5881.863310210464 ----------
[1/200] Training loss: 0.17008526
[2/200] Training loss: 0.05857835
[3/200] Training loss: 0.05399675
[4/200] Training loss: 0.05153928
[5/200] Training loss: 0.05006425
[6/200] Training loss: 0.04875725
[7/200] Training loss: 0.04325610
[8/200] Training loss: 0.04126900
[9/200] Training loss: 0.04131037
[10/200] Training loss: 0.03497664
[50/200] Training loss: 0.01832556
[100/200] Training loss: 0.01639654
[150/200] Training loss: 0.01461138
[200/200] Training loss: 0.01329370
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13832.371886267372 ----------
[1/200] Training loss: 0.12013864
[2/200] Training loss: 0.05197441
[3/200] Training loss: 0.04697275
[4/200] Training loss: 0.04422243
[5/200] Training loss: 0.04095327
[6/200] Training loss: 0.03652483
[7/200] Training loss: 0.03278961
[8/200] Training loss: 0.03008668
[9/200] Training loss: 0.03017819
[10/200] Training loss: 0.02878501
[50/200] Training loss: 0.01763563
[100/200] Training loss: 0.01447020
[150/200] Training loss: 0.01335536
[200/200] Training loss: 0.01232618
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7322.236270429957 ----------
[1/200] Training loss: 0.15106955
[2/200] Training loss: 0.05781372
[3/200] Training loss: 0.05243508
[4/200] Training loss: 0.05070708
[5/200] Training loss: 0.04899713
[6/200] Training loss: 0.04456573
[7/200] Training loss: 0.04021983
[8/200] Training loss: 0.03842082
[9/200] Training loss: 0.03500173
[10/200] Training loss: 0.03252542
[50/200] Training loss: 0.01686178
[100/200] Training loss: 0.01391215
[150/200] Training loss: 0.01293419
[200/200] Training loss: 0.01224979
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11628.883351379873 ----------
[1/200] Training loss: 0.15220774
[2/200] Training loss: 0.05628656
[3/200] Training loss: 0.04906330
[4/200] Training loss: 0.04980559
[5/200] Training loss: 0.04470853
[6/200] Training loss: 0.04405285
[7/200] Training loss: 0.04004306
[8/200] Training loss: 0.04056228
[9/200] Training loss: 0.03698744
[10/200] Training loss: 0.03554474
[50/200] Training loss: 0.01947650
[100/200] Training loss: 0.01579249
[150/200] Training loss: 0.01470832
[200/200] Training loss: 0.01392699
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6291.678313455003 ----------
[1/200] Training loss: 0.12861990
[2/200] Training loss: 0.05751013
[3/200] Training loss: 0.05112557
[4/200] Training loss: 0.04532546
[5/200] Training loss: 0.04156431
[6/200] Training loss: 0.03778331
[7/200] Training loss: 0.03439052
[8/200] Training loss: 0.03242030
[9/200] Training loss: 0.02889138
[10/200] Training loss: 0.02846140
[50/200] Training loss: 0.01634025
[100/200] Training loss: 0.01411298
[150/200] Training loss: 0.01343384
[200/200] Training loss: 0.01263805
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14119.960339887644 ----------
[1/200] Training loss: 0.17152532
[2/200] Training loss: 0.06076076
[3/200] Training loss: 0.05368058
[4/200] Training loss: 0.05174437
[5/200] Training loss: 0.04958292
[6/200] Training loss: 0.04615412
[7/200] Training loss: 0.04443774
[8/200] Training loss: 0.04210820
[9/200] Training loss: 0.03954604
[10/200] Training loss: 0.03788078
[50/200] Training loss: 0.01739872
[100/200] Training loss: 0.01531666
[150/200] Training loss: 0.01398999
[200/200] Training loss: 0.01308293
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9609.452429769346 ----------
[1/200] Training loss: 0.14208857
[2/200] Training loss: 0.05619793
[3/200] Training loss: 0.04621115
[4/200] Training loss: 0.04509941
[5/200] Training loss: 0.03892061
[6/200] Training loss: 0.03635006
[7/200] Training loss: 0.03438454
[8/200] Training loss: 0.03124346
[9/200] Training loss: 0.03097915
[10/200] Training loss: 0.02734989
[50/200] Training loss: 0.01884347
[100/200] Training loss: 0.01581936
[150/200] Training loss: 0.01470851
[200/200] Training loss: 0.01359420
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10274.98866179423 ----------
[1/200] Training loss: 0.14216698
[2/200] Training loss: 0.06105279
[3/200] Training loss: 0.05248857
[4/200] Training loss: 0.04445059
[5/200] Training loss: 0.04351666
[6/200] Training loss: 0.03852736
[7/200] Training loss: 0.03403194
[8/200] Training loss: 0.03124789
[9/200] Training loss: 0.02943428
[10/200] Training loss: 0.02685333
[50/200] Training loss: 0.01478307
[100/200] Training loss: 0.01263164
[150/200] Training loss: 0.01131885
[200/200] Training loss: 0.01046748
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12673.912734432093 ----------
[1/200] Training loss: 0.15816949
[2/200] Training loss: 0.05911406
[3/200] Training loss: 0.05215447
[4/200] Training loss: 0.04747419
[5/200] Training loss: 0.04511380
[6/200] Training loss: 0.04150922
[7/200] Training loss: 0.03958456
[8/200] Training loss: 0.03761084
[9/200] Training loss: 0.03311763
[10/200] Training loss: 0.03161722
[50/200] Training loss: 0.01683100
[100/200] Training loss: 0.01452104
[150/200] Training loss: 0.01352524
[200/200] Training loss: 0.01290447
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4668.643914457388 ----------
[1/200] Training loss: 0.15684169
[2/200] Training loss: 0.05346302
[3/200] Training loss: 0.05108692
[4/200] Training loss: 0.04614320
[5/200] Training loss: 0.04339876
[6/200] Training loss: 0.04059220
[7/200] Training loss: 0.03790191
[8/200] Training loss: 0.04004938
[9/200] Training loss: 0.03635732
[10/200] Training loss: 0.03536298
[50/200] Training loss: 0.01913087
[100/200] Training loss: 0.01440852
[150/200] Training loss: 0.01289537
[200/200] Training loss: 0.01346385
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9363.012335781685 ----------
[1/200] Training loss: 0.14854617
[2/200] Training loss: 0.06227522
[3/200] Training loss: 0.05424427
[4/200] Training loss: 0.05005710
[5/200] Training loss: 0.04777171
[6/200] Training loss: 0.04770446
[7/200] Training loss: 0.04264567
[8/200] Training loss: 0.04162989
[9/200] Training loss: 0.04077606
[10/200] Training loss: 0.03828120
[50/200] Training loss: 0.01769273
[100/200] Training loss: 0.01465020
[150/200] Training loss: 0.01388776
[200/200] Training loss: 0.01268496
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4031.8918636292815 ----------
[1/200] Training loss: 0.14198536
[2/200] Training loss: 0.05630470
[3/200] Training loss: 0.05118113
[4/200] Training loss: 0.04776964
[5/200] Training loss: 0.04696011
[6/200] Training loss: 0.04395901
[7/200] Training loss: 0.04088878
[8/200] Training loss: 0.04014596
[9/200] Training loss: 0.03802656
[10/200] Training loss: 0.03652796
[50/200] Training loss: 0.01839822
[100/200] Training loss: 0.01545576
[150/200] Training loss: 0.01328920
[200/200] Training loss: 0.01346223
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6888.005807198481 ----------
[1/200] Training loss: 0.14810800
[2/200] Training loss: 0.05660078
[3/200] Training loss: 0.05270624
[4/200] Training loss: 0.04948195
[5/200] Training loss: 0.04790401
[6/200] Training loss: 0.04525391
[7/200] Training loss: 0.04344517
[8/200] Training loss: 0.03809891
[9/200] Training loss: 0.03756631
[10/200] Training loss: 0.03437133
[50/200] Training loss: 0.01892692
[100/200] Training loss: 0.01490212
[150/200] Training loss: 0.01433770
[200/200] Training loss: 0.01234712
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4125.63134562457 ----------
[1/200] Training loss: 0.14896643
[2/200] Training loss: 0.05757167
[3/200] Training loss: 0.05082411
[4/200] Training loss: 0.04497444
[5/200] Training loss: 0.04497876
[6/200] Training loss: 0.04103943
[7/200] Training loss: 0.04002580
[8/200] Training loss: 0.03642986
[9/200] Training loss: 0.03796746
[10/200] Training loss: 0.03375805
[50/200] Training loss: 0.01841752
[100/200] Training loss: 0.01462530
[150/200] Training loss: 0.01348765
[200/200] Training loss: 0.01244364
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9544.953431002165 ----------
[1/200] Training loss: 0.13537287
[2/200] Training loss: 0.05484462
[3/200] Training loss: 0.04857443
[4/200] Training loss: 0.04303975
[5/200] Training loss: 0.04055132
[6/200] Training loss: 0.03700880
[7/200] Training loss: 0.03593519
[8/200] Training loss: 0.03390073
[9/200] Training loss: 0.03346491
[10/200] Training loss: 0.03051573
[50/200] Training loss: 0.01970324
[100/200] Training loss: 0.01706096
[150/200] Training loss: 0.01605011
[200/200] Training loss: 0.01486652
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14003.531554575795 ----------
[1/200] Training loss: 0.14840359
[2/200] Training loss: 0.06069608
[3/200] Training loss: 0.04718945
[4/200] Training loss: 0.04320504
[5/200] Training loss: 0.04035384
[6/200] Training loss: 0.03923753
[7/200] Training loss: 0.03547440
[8/200] Training loss: 0.03395245
[9/200] Training loss: 0.03309061
[10/200] Training loss: 0.03127269
[50/200] Training loss: 0.01755414
[100/200] Training loss: 0.01514509
[150/200] Training loss: 0.01342355
[200/200] Training loss: 0.01181113
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14959.394105377396 ----------
[1/200] Training loss: 0.11910252
[2/200] Training loss: 0.05404091
[3/200] Training loss: 0.04765091
[4/200] Training loss: 0.04243267
[5/200] Training loss: 0.04455049
[6/200] Training loss: 0.03907882
[7/200] Training loss: 0.03908775
[8/200] Training loss: 0.03706970
[9/200] Training loss: 0.03558947
[10/200] Training loss: 0.02947458
[50/200] Training loss: 0.02017130
[100/200] Training loss: 0.01616946
[150/200] Training loss: 0.01357319
[200/200] Training loss: 0.01209152
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9982.58483560245 ----------
[1/200] Training loss: 0.17727203
[2/200] Training loss: 0.06605662
[3/200] Training loss: 0.05653285
[4/200] Training loss: 0.05339207
[5/200] Training loss: 0.05009701
[6/200] Training loss: 0.04812342
[7/200] Training loss: 0.04453959
[8/200] Training loss: 0.04453974
[9/200] Training loss: 0.03749299
[10/200] Training loss: 0.04250725
[50/200] Training loss: 0.01938524
[100/200] Training loss: 0.01607910
[150/200] Training loss: 0.01477181
[200/200] Training loss: 0.01365485
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10100.443158594577 ----------
[1/200] Training loss: 0.16892454
[2/200] Training loss: 0.05627494
[3/200] Training loss: 0.05270344
[4/200] Training loss: 0.04935188
[5/200] Training loss: 0.04609183
[6/200] Training loss: 0.04328003
[7/200] Training loss: 0.03935094
[8/200] Training loss: 0.03934712
[9/200] Training loss: 0.03552315
[10/200] Training loss: 0.03523203
[50/200] Training loss: 0.01778472
[100/200] Training loss: 0.01501607
[150/200] Training loss: 0.01299432
[200/200] Training loss: 0.01304761
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14924.18332774025 ----------
[1/200] Training loss: 0.17112100
[2/200] Training loss: 0.05789220
[3/200] Training loss: 0.05622202
[4/200] Training loss: 0.05170592
[5/200] Training loss: 0.04972912
[6/200] Training loss: 0.04665644
[7/200] Training loss: 0.04547809
[8/200] Training loss: 0.04420360
[9/200] Training loss: 0.04266750
[10/200] Training loss: 0.04042435
[50/200] Training loss: 0.01955511
[100/200] Training loss: 0.01697216
[150/200] Training loss: 0.01501269
[200/200] Training loss: 0.01345468
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19104.936325463113 ----------
[1/200] Training loss: 0.16082382
[2/200] Training loss: 0.06430485
[3/200] Training loss: 0.05235961
[4/200] Training loss: 0.04882262
[5/200] Training loss: 0.04471231
[6/200] Training loss: 0.04047268
[7/200] Training loss: 0.03558287
[8/200] Training loss: 0.03312369
[9/200] Training loss: 0.03274681
[10/200] Training loss: 0.03109694
[50/200] Training loss: 0.01919929
[100/200] Training loss: 0.01623782
[150/200] Training loss: 0.01460155
[200/200] Training loss: 0.01346619
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10107.12303279227 ----------
[1/200] Training loss: 0.16482658
[2/200] Training loss: 0.05618478
[3/200] Training loss: 0.05184018
[4/200] Training loss: 0.04853322
[5/200] Training loss: 0.04611326
[6/200] Training loss: 0.04221386
[7/200] Training loss: 0.03826834
[8/200] Training loss: 0.03650759
[9/200] Training loss: 0.03491969
[10/200] Training loss: 0.03469084
[50/200] Training loss: 0.01838016
[100/200] Training loss: 0.01638902
[150/200] Training loss: 0.01542577
[200/200] Training loss: 0.01407751
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13021.277356695848 ----------
[1/200] Training loss: 0.03682235
[2/200] Training loss: 0.00250872
[3/200] Training loss: 0.00227896
[4/200] Training loss: 0.00216171
[5/200] Training loss: 0.00194242
[6/200] Training loss: 0.00176906
[7/200] Training loss: 0.00167068
[8/200] Training loss: 0.00148185
[9/200] Training loss: 0.00132110
[10/200] Training loss: 0.00118871
[50/200] Training loss: 0.00028753
[100/200] Training loss: 0.00017442
[150/200] Training loss: 0.00014876
[200/200] Training loss: 0.00014965
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13289.97276144688 ----------
[1/200] Training loss: 0.19363130
[2/200] Training loss: 0.06672283
[3/200] Training loss: 0.05505023
[4/200] Training loss: 0.04788950
[5/200] Training loss: 0.04501032
[6/200] Training loss: 0.04299207
[7/200] Training loss: 0.03741352
[8/200] Training loss: 0.03822083
[9/200] Training loss: 0.03508384
[10/200] Training loss: 0.03367658
[50/200] Training loss: 0.01935490
[100/200] Training loss: 0.01656466
[150/200] Training loss: 0.01546324
[200/200] Training loss: 0.01449391
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17346.20557931907 ----------
[1/200] Training loss: 0.16529009
[2/200] Training loss: 0.06061802
[3/200] Training loss: 0.05304373
[4/200] Training loss: 0.04982842
[5/200] Training loss: 0.04444554
[6/200] Training loss: 0.04286503
[7/200] Training loss: 0.03999977
[8/200] Training loss: 0.03735102
[9/200] Training loss: 0.03688057
[10/200] Training loss: 0.03491379
[50/200] Training loss: 0.01961320
[100/200] Training loss: 0.01695729
[150/200] Training loss: 0.01562553
[200/200] Training loss: 0.01476394
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16116.614532835361 ----------
[1/200] Training loss: 0.17192285
[2/200] Training loss: 0.06907191
[3/200] Training loss: 0.05886318
[4/200] Training loss: 0.05003638
[5/200] Training loss: 0.04615537
[6/200] Training loss: 0.04380928
[7/200] Training loss: 0.04151374
[8/200] Training loss: 0.03959735
[9/200] Training loss: 0.03401998
[10/200] Training loss: 0.03448297
[50/200] Training loss: 0.01903542
[100/200] Training loss: 0.01584735
[150/200] Training loss: 0.01442139
[200/200] Training loss: 0.01310608
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19123.563684627403 ----------
[1/200] Training loss: 0.13219305
[2/200] Training loss: 0.05455586
[3/200] Training loss: 0.04804039
[4/200] Training loss: 0.04171584
[5/200] Training loss: 0.04099962
[6/200] Training loss: 0.03620910
[7/200] Training loss: 0.03314050
[8/200] Training loss: 0.03078992
[9/200] Training loss: 0.03090058
[10/200] Training loss: 0.02780330
[50/200] Training loss: 0.01959110
[100/200] Training loss: 0.01745589
[150/200] Training loss: 0.01641188
[200/200] Training loss: 0.01574719
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17070.71972706482 ----------
[1/200] Training loss: 0.14437276
[2/200] Training loss: 0.05484852
[3/200] Training loss: 0.04714159
[4/200] Training loss: 0.04548610
[5/200] Training loss: 0.03859718
[6/200] Training loss: 0.03657230
[7/200] Training loss: 0.03663840
[8/200] Training loss: 0.03145145
[9/200] Training loss: 0.03136136
[10/200] Training loss: 0.03047398
[50/200] Training loss: 0.01586954
[100/200] Training loss: 0.01448976
[150/200] Training loss: 0.01322179
[200/200] Training loss: 0.01242862
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7320.577572842187 ----------
[1/200] Training loss: 0.14553074
[2/200] Training loss: 0.05820642
[3/200] Training loss: 0.05168555
[4/200] Training loss: 0.04576479
[5/200] Training loss: 0.04217729
[6/200] Training loss: 0.03879121
[7/200] Training loss: 0.03731970
[8/200] Training loss: 0.03352181
[9/200] Training loss: 0.03101881
[10/200] Training loss: 0.03052135
[50/200] Training loss: 0.01888417
[100/200] Training loss: 0.01467576
[150/200] Training loss: 0.01330775
[200/200] Training loss: 0.01199777
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15157.37074825314 ----------
[1/200] Training loss: 0.16618612
[2/200] Training loss: 0.05927679
[3/200] Training loss: 0.04298454
[4/200] Training loss: 0.04350529
[5/200] Training loss: 0.04117948
[6/200] Training loss: 0.03442283
[7/200] Training loss: 0.03441831
[8/200] Training loss: 0.03214885
[9/200] Training loss: 0.02922510
[10/200] Training loss: 0.02877643
[50/200] Training loss: 0.01737955
[100/200] Training loss: 0.01533113
[150/200] Training loss: 0.01391614
[200/200] Training loss: 0.01277291
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17291.18249281986 ----------
[1/200] Training loss: 0.17750425
[2/200] Training loss: 0.06673239
[3/200] Training loss: 0.05666267
[4/200] Training loss: 0.04688231
[5/200] Training loss: 0.05213038
[6/200] Training loss: 0.04816518
[7/200] Training loss: 0.04545782
[8/200] Training loss: 0.03972695
[9/200] Training loss: 0.04073566
[10/200] Training loss: 0.03653014
[50/200] Training loss: 0.02008873
[100/200] Training loss: 0.01525798
[150/200] Training loss: 0.01334118
[200/200] Training loss: 0.01281713
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9130.69241624095 ----------
[1/200] Training loss: 0.14504964
[2/200] Training loss: 0.04958669
[3/200] Training loss: 0.04592885
[4/200] Training loss: 0.03604262
[5/200] Training loss: 0.02695235
[6/200] Training loss: 0.02751178
[7/200] Training loss: 0.03842067
[8/200] Training loss: 0.02645635
[9/200] Training loss: 0.02685951
[10/200] Training loss: 0.02234239
[50/200] Training loss: 0.01544611
[100/200] Training loss: 0.01269628
[150/200] Training loss: 0.01033516
[200/200] Training loss: 0.01348428
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adamax ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22408.244911192844 ----------
[1/200] Training loss: 0.13758204
[2/200] Training loss: 0.05469581
[3/200] Training loss: 0.05019784
[4/200] Training loss: 0.04601800
[5/200] Training loss: 0.04259404
[6/200] Training loss: 0.04107049
[7/200] Training loss: 0.03813435
[8/200] Training loss: 0.03464180
[9/200] Training loss: 0.03471895
[10/200] Training loss: 0.03086456
[50/200] Training loss: 0.01820584
[100/200] Training loss: 0.01595571
[150/200] Training loss: 0.01522013
[200/200] Training loss: 0.01474327
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6928.362865785827 ----------
[1/200] Training loss: 0.14690088
[2/200] Training loss: 0.05581936
[3/200] Training loss: 0.05157152
[4/200] Training loss: 0.04631139
[5/200] Training loss: 0.04353266
[6/200] Training loss: 0.03786850
[7/200] Training loss: 0.03824789
[8/200] Training loss: 0.03346932
[9/200] Training loss: 0.03150913
[10/200] Training loss: 0.03016082
[50/200] Training loss: 0.01745089
[100/200] Training loss: 0.01529401
[150/200] Training loss: 0.01383482
[200/200] Training loss: 0.01325222
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10558.141124269934 ----------
[1/200] Training loss: 0.12705974
[2/200] Training loss: 0.05729372
[3/200] Training loss: 0.05184037
[4/200] Training loss: 0.05016563
[5/200] Training loss: 0.04401377
[6/200] Training loss: 0.04128742
[7/200] Training loss: 0.03629374
[8/200] Training loss: 0.03177760
[9/200] Training loss: 0.03031287
[10/200] Training loss: 0.02724758
[50/200] Training loss: 0.01652869
[100/200] Training loss: 0.01369470
[150/200] Training loss: 0.01305887
[200/200] Training loss: 0.01153670
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 26312.722398110007 ----------
[1/200] Training loss: 0.20136197
[2/200] Training loss: 0.06521469
[3/200] Training loss: 0.06257661
[4/200] Training loss: 0.05602473
[5/200] Training loss: 0.05493841
[6/200] Training loss: 0.05372242
[7/200] Training loss: 0.05070491
[8/200] Training loss: 0.04967090
[9/200] Training loss: 0.04857863
[10/200] Training loss: 0.04718870
[50/200] Training loss: 0.01917069
[100/200] Training loss: 0.01713372
[150/200] Training loss: 0.01505360
[200/200] Training loss: 0.01412848
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.02221947058327604
----FITNESS-----------RMSE---- 18256.013146358106 ----------
[1/200] Training loss: 0.11182966
[2/200] Training loss: 0.05535017
[3/200] Training loss: 0.05104535
[4/200] Training loss: 0.05053982
[5/200] Training loss: 0.04693275
[6/200] Training loss: 0.04300663
[7/200] Training loss: 0.04158460
[8/200] Training loss: 0.03986325
[9/200] Training loss: 0.03839230
[10/200] Training loss: 0.03593532
[50/200] Training loss: 0.01920989
[100/200] Training loss: 0.01548503
[150/200] Training loss: 0.01430341
[200/200] Training loss: 0.01291727
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19141.486671625065 ----------
[1/200] Training loss: 0.12057234
[2/200] Training loss: 0.05436355
[3/200] Training loss: 0.04809539
[4/200] Training loss: 0.04348347
[5/200] Training loss: 0.04051549
[6/200] Training loss: 0.03639680
[7/200] Training loss: 0.03403386
[8/200] Training loss: 0.02964727
[9/200] Training loss: 0.02825411
[10/200] Training loss: 0.02772532
[50/200] Training loss: 0.01751172
[100/200] Training loss: 0.01543652
[150/200] Training loss: 0.01431767
[200/200] Training loss: 0.01344953
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12219.060520350982 ----------
[1/200] Training loss: 0.13686706
[2/200] Training loss: 0.05361194
[3/200] Training loss: 0.04916106
[4/200] Training loss: 0.04795444
[5/200] Training loss: 0.04217388
[6/200] Training loss: 0.03957303
[7/200] Training loss: 0.03741965
[8/200] Training loss: 0.03341850
[9/200] Training loss: 0.03207959
[10/200] Training loss: 0.03029658
[50/200] Training loss: 0.01739927
[100/200] Training loss: 0.01536168
[150/200] Training loss: 0.01414190
[200/200] Training loss: 0.01286635
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12663.729624403704 ----------
[1/200] Training loss: 0.15226615
[2/200] Training loss: 0.05499468
[3/200] Training loss: 0.05192148
[4/200] Training loss: 0.04816976
[5/200] Training loss: 0.04615863
[6/200] Training loss: 0.04484002
[7/200] Training loss: 0.04056458
[8/200] Training loss: 0.03806252
[9/200] Training loss: 0.03787589
[10/200] Training loss: 0.03697578
[50/200] Training loss: 0.01872952
[100/200] Training loss: 0.01652051
[150/200] Training loss: 0.01529159
[200/200] Training loss: 0.01454252
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17238.397141265774 ----------
[1/200] Training loss: 0.13208328
[2/200] Training loss: 0.05391992
[3/200] Training loss: 0.05178035
[4/200] Training loss: 0.05057782
[5/200] Training loss: 0.04459201
[6/200] Training loss: 0.04413502
[7/200] Training loss: 0.04161925
[8/200] Training loss: 0.03803871
[9/200] Training loss: 0.03613911
[10/200] Training loss: 0.03575176
[50/200] Training loss: 0.01770835
[100/200] Training loss: 0.01537173
[150/200] Training loss: 0.01441360
[200/200] Training loss: 0.01382367
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12405.780588096824 ----------
[1/200] Training loss: 0.14795820
[2/200] Training loss: 0.05514015
[3/200] Training loss: 0.05293968
[4/200] Training loss: 0.05078909
[5/200] Training loss: 0.04567495
[6/200] Training loss: 0.04435304
[7/200] Training loss: 0.04048802
[8/200] Training loss: 0.04001203
[9/200] Training loss: 0.03697488
[10/200] Training loss: 0.03676663
[50/200] Training loss: 0.01793913
[100/200] Training loss: 0.01361141
[150/200] Training loss: 0.01241687
[200/200] Training loss: 0.01142741
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14077.699812114193 ----------
[1/200] Training loss: 0.13986926
[2/200] Training loss: 0.05573375
[3/200] Training loss: 0.04801291
[4/200] Training loss: 0.04312943
[5/200] Training loss: 0.04348927
[6/200] Training loss: 0.03937306
[7/200] Training loss: 0.03596320
[8/200] Training loss: 0.03410116
[9/200] Training loss: 0.03134540
[10/200] Training loss: 0.03174900
[50/200] Training loss: 0.01772194
[100/200] Training loss: 0.01452689
[150/200] Training loss: 0.01371560
[200/200] Training loss: 0.01222190
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7247.564556456189 ----------
[1/200] Training loss: 0.17793552
[2/200] Training loss: 0.06617432
[3/200] Training loss: 0.05283104
[4/200] Training loss: 0.05112740
[5/200] Training loss: 0.04904889
[6/200] Training loss: 0.04718565
[7/200] Training loss: 0.04304058
[8/200] Training loss: 0.04230794
[9/200] Training loss: 0.04163444
[10/200] Training loss: 0.03625426
[50/200] Training loss: 0.01900900
[100/200] Training loss: 0.01654507
[150/200] Training loss: 0.01496840
[200/200] Training loss: 0.01350503
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13583.473488029489 ----------
[1/200] Training loss: 0.13034969
[2/200] Training loss: 0.05800032
[3/200] Training loss: 0.05041687
[4/200] Training loss: 0.04978349
[5/200] Training loss: 0.04853669
[6/200] Training loss: 0.04524622
[7/200] Training loss: 0.04410718
[8/200] Training loss: 0.04008511
[9/200] Training loss: 0.03720200
[10/200] Training loss: 0.03370693
[50/200] Training loss: 0.01712816
[100/200] Training loss: 0.01403148
[150/200] Training loss: 0.01280267
[200/200] Training loss: 0.01132308
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 29135.224591548973 ----------
[1/200] Training loss: 0.15746212
[2/200] Training loss: 0.05945945
[3/200] Training loss: 0.05123905
[4/200] Training loss: 0.04568661
[5/200] Training loss: 0.04221979
[6/200] Training loss: 0.03963755
[7/200] Training loss: 0.03462146
[8/200] Training loss: 0.03543437
[9/200] Training loss: 0.03314620
[10/200] Training loss: 0.03158822
[50/200] Training loss: 0.01781865
[100/200] Training loss: 0.01488655
[150/200] Training loss: 0.01335189
[200/200] Training loss: 0.01248755
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8667.390380039427 ----------
[1/200] Training loss: 0.16013790
[2/200] Training loss: 0.05634140
[3/200] Training loss: 0.05225449
[4/200] Training loss: 0.04873819
[5/200] Training loss: 0.04591536
[6/200] Training loss: 0.04213203
[7/200] Training loss: 0.04183657
[8/200] Training loss: 0.03869020
[9/200] Training loss: 0.03163999
[10/200] Training loss: 0.03113739
[50/200] Training loss: 0.01788060
[100/200] Training loss: 0.01560018
[150/200] Training loss: 0.01388612
[200/200] Training loss: 0.01152216
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12059.274936744747 ----------
[1/200] Training loss: 0.12963128
[2/200] Training loss: 0.05791979
[3/200] Training loss: 0.05014128
[4/200] Training loss: 0.04712080
[5/200] Training loss: 0.04405713
[6/200] Training loss: 0.03992746
[7/200] Training loss: 0.03612952
[8/200] Training loss: 0.03817614
[9/200] Training loss: 0.03747264
[10/200] Training loss: 0.03005834
[50/200] Training loss: 0.01872277
[100/200] Training loss: 0.01522228
[150/200] Training loss: 0.01387817
[200/200] Training loss: 0.01312486
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10656.82166501814 ----------
[1/200] Training loss: 0.12708923
[2/200] Training loss: 0.05502356
[3/200] Training loss: 0.04986796
[4/200] Training loss: 0.04523685
[5/200] Training loss: 0.04123641
[6/200] Training loss: 0.04022309
[7/200] Training loss: 0.03766259
[8/200] Training loss: 0.03587424
[9/200] Training loss: 0.03289763
[10/200] Training loss: 0.03164676
[50/200] Training loss: 0.01678695
[100/200] Training loss: 0.01439124
[150/200] Training loss: 0.01292794
[200/200] Training loss: 0.01196837
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7668.244649201015 ----------
[1/200] Training loss: 0.17423339
[2/200] Training loss: 0.06011304
[3/200] Training loss: 0.05344794
[4/200] Training loss: 0.05230060
[5/200] Training loss: 0.04826756
[6/200] Training loss: 0.04609583
[7/200] Training loss: 0.04355725
[8/200] Training loss: 0.04317104
[9/200] Training loss: 0.04079839
[10/200] Training loss: 0.03706841
[50/200] Training loss: 0.01829239
[100/200] Training loss: 0.01450406
[150/200] Training loss: 0.01340712
[200/200] Training loss: 0.01345672
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7608.117506978977 ----------
[1/200] Training loss: 0.18068848
[2/200] Training loss: 0.06118879
[3/200] Training loss: 0.05527735
[4/200] Training loss: 0.04887991
[5/200] Training loss: 0.04864132
[6/200] Training loss: 0.04301240
[7/200] Training loss: 0.04022341
[8/200] Training loss: 0.03633186
[9/200] Training loss: 0.03433716
[10/200] Training loss: 0.03246251
[50/200] Training loss: 0.01769762
[100/200] Training loss: 0.01235851
[150/200] Training loss: 0.01104877
[200/200] Training loss: 0.01082657
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7835.076259999005 ----------
[1/200] Training loss: 0.15513882
[2/200] Training loss: 0.05990378
[3/200] Training loss: 0.05468716
[4/200] Training loss: 0.04769026
[5/200] Training loss: 0.04704358
[6/200] Training loss: 0.04138651
[7/200] Training loss: 0.03840596
[8/200] Training loss: 0.03797119
[9/200] Training loss: 0.03497915
[10/200] Training loss: 0.03327302
[50/200] Training loss: 0.01719425
[100/200] Training loss: 0.01499787
[150/200] Training loss: 0.01327424
[200/200] Training loss: 0.01323132
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21667.583898533772 ----------
[1/200] Training loss: 0.18089204
[2/200] Training loss: 0.06101827
[3/200] Training loss: 0.05349795
[4/200] Training loss: 0.04959182
[5/200] Training loss: 0.04705250
[6/200] Training loss: 0.04475226
[7/200] Training loss: 0.03963890
[8/200] Training loss: 0.03807146
[9/200] Training loss: 0.03414138
[10/200] Training loss: 0.03377402
[50/200] Training loss: 0.01683918
[100/200] Training loss: 0.01466485
[150/200] Training loss: 0.01336549
[200/200] Training loss: 0.01245897
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15926.850787271162 ----------
[1/200] Training loss: 0.16488252
[2/200] Training loss: 0.05633485
[3/200] Training loss: 0.05296038
[4/200] Training loss: 0.05014528
[5/200] Training loss: 0.04802730
[6/200] Training loss: 0.04591034
[7/200] Training loss: 0.04397195
[8/200] Training loss: 0.04211822
[9/200] Training loss: 0.04132896
[10/200] Training loss: 0.03842466
[50/200] Training loss: 0.01874878
[100/200] Training loss: 0.01554484
[150/200] Training loss: 0.01502290
[200/200] Training loss: 0.01332865
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14577.328150247562 ----------
[1/200] Training loss: 0.17461697
[2/200] Training loss: 0.06466273
[3/200] Training loss: 0.05753751
[4/200] Training loss: 0.05454734
[5/200] Training loss: 0.05263694
[6/200] Training loss: 0.05082585
[7/200] Training loss: 0.04960639
[8/200] Training loss: 0.04519904
[9/200] Training loss: 0.04459739
[10/200] Training loss: 0.04321217
[50/200] Training loss: 0.01911072
[100/200] Training loss: 0.01609845
[150/200] Training loss: 0.01512077
[200/200] Training loss: 0.01374067
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13439.951785627803 ----------
[1/200] Training loss: 0.18020187
[2/200] Training loss: 0.06231253
[3/200] Training loss: 0.05169048
[4/200] Training loss: 0.04593069
[5/200] Training loss: 0.04480963
[6/200] Training loss: 0.04049221
[7/200] Training loss: 0.03978234
[8/200] Training loss: 0.03781797
[9/200] Training loss: 0.03565205
[10/200] Training loss: 0.03363825
[50/200] Training loss: 0.01812441
[100/200] Training loss: 0.01532751
[150/200] Training loss: 0.01385845
[200/200] Training loss: 0.01269371
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5018.734302590644 ----------
[1/200] Training loss: 0.17147895
[2/200] Training loss: 0.05883086
[3/200] Training loss: 0.05064949
[4/200] Training loss: 0.05053241
[5/200] Training loss: 0.04708253
[6/200] Training loss: 0.04367808
[7/200] Training loss: 0.04472964
[8/200] Training loss: 0.03822892
[9/200] Training loss: 0.03817558
[10/200] Training loss: 0.03491649
[50/200] Training loss: 0.01763441
[100/200] Training loss: 0.01603551
[150/200] Training loss: 0.01417499
[200/200] Training loss: 0.01291542
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12791.35270407317 ----------
[1/200] Training loss: 0.15643415
[2/200] Training loss: 0.05955614
[3/200] Training loss: 0.05234001
[4/200] Training loss: 0.04700665
[5/200] Training loss: 0.04370640
[6/200] Training loss: 0.03845073
[7/200] Training loss: 0.03679506
[8/200] Training loss: 0.03606024
[9/200] Training loss: 0.03168742
[10/200] Training loss: 0.03097305
[50/200] Training loss: 0.01733311
[100/200] Training loss: 0.01373494
[150/200] Training loss: 0.01340204
[200/200] Training loss: 0.01220637
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18636.178148966057 ----------
[1/200] Training loss: 0.15818890
[2/200] Training loss: 0.05908787
[3/200] Training loss: 0.05312131
[4/200] Training loss: 0.04913226
[5/200] Training loss: 0.04734102
[6/200] Training loss: 0.04204478
[7/200] Training loss: 0.04000789
[8/200] Training loss: 0.04087185
[9/200] Training loss: 0.03425405
[10/200] Training loss: 0.03257034
[50/200] Training loss: 0.01726523
[100/200] Training loss: 0.01486470
[150/200] Training loss: 0.01403141
[200/200] Training loss: 0.01260870
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5276.611791670864 ----------
[1/200] Training loss: 0.14044965
[2/200] Training loss: 0.05566428
[3/200] Training loss: 0.04968141
[4/200] Training loss: 0.04317546
[5/200] Training loss: 0.04270654
[6/200] Training loss: 0.03958458
[7/200] Training loss: 0.03486486
[8/200] Training loss: 0.03189730
[9/200] Training loss: 0.03108558
[10/200] Training loss: 0.03096990
[50/200] Training loss: 0.01802205
[100/200] Training loss: 0.01503612
[150/200] Training loss: 0.01316979
[200/200] Training loss: 0.01237511
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11960.801979800519 ----------
[1/200] Training loss: 0.12058743
[2/200] Training loss: 0.05644844
[3/200] Training loss: 0.05139661
[4/200] Training loss: 0.04659582
[5/200] Training loss: 0.04482967
[6/200] Training loss: 0.04105034
[7/200] Training loss: 0.03985616
[8/200] Training loss: 0.03854183
[9/200] Training loss: 0.03723808
[10/200] Training loss: 0.03414937
[50/200] Training loss: 0.01755147
[100/200] Training loss: 0.01446692
[150/200] Training loss: 0.01287813
[200/200] Training loss: 0.01201798
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12317.113947674594 ----------
[1/200] Training loss: 0.17827041
[2/200] Training loss: 0.06074972
[3/200] Training loss: 0.05263838
[4/200] Training loss: 0.04884283
[5/200] Training loss: 0.04296335
[6/200] Training loss: 0.04036610
[7/200] Training loss: 0.03648777
[8/200] Training loss: 0.03593077
[9/200] Training loss: 0.03182908
[10/200] Training loss: 0.03189102
[50/200] Training loss: 0.01925977
[100/200] Training loss: 0.01661692
[150/200] Training loss: 0.01479979
[200/200] Training loss: 0.01350273
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10555.48236699773 ----------
[1/200] Training loss: 0.18961641
[2/200] Training loss: 0.06295860
[3/200] Training loss: 0.05371914
[4/200] Training loss: 0.05176365
[5/200] Training loss: 0.05001923
[6/200] Training loss: 0.04765075
[7/200] Training loss: 0.04553727
[8/200] Training loss: 0.04343902
[9/200] Training loss: 0.04254892
[10/200] Training loss: 0.03753522
[50/200] Training loss: 0.01940938
[100/200] Training loss: 0.01627989
[150/200] Training loss: 0.01507905
[200/200] Training loss: 0.01450645
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16652.085995454145 ----------
[1/200] Training loss: 0.14334819
[2/200] Training loss: 0.05728277
[3/200] Training loss: 0.05287545
[4/200] Training loss: 0.04926554
[5/200] Training loss: 0.04295508
[6/200] Training loss: 0.03803383
[7/200] Training loss: 0.03571982
[8/200] Training loss: 0.03274006
[9/200] Training loss: 0.03211454
[10/200] Training loss: 0.02909682
[50/200] Training loss: 0.01713047
[100/200] Training loss: 0.01508442
[150/200] Training loss: 0.01357632
[200/200] Training loss: 0.01349255
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16086.165981985887 ----------
[1/200] Training loss: 0.16071827
[2/200] Training loss: 0.05345966
[3/200] Training loss: 0.04823442
[4/200] Training loss: 0.04247618
[5/200] Training loss: 0.04080689
[6/200] Training loss: 0.03971752
[7/200] Training loss: 0.03556824
[8/200] Training loss: 0.03216753
[9/200] Training loss: 0.03644508
[10/200] Training loss: 0.02946918
[50/200] Training loss: 0.01928653
[100/200] Training loss: 0.01601252
[150/200] Training loss: 0.01430581
[200/200] Training loss: 0.01327072
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8263.856242699288 ----------
[1/200] Training loss: 0.16947755
[2/200] Training loss: 0.05613611
[3/200] Training loss: 0.05333940
[4/200] Training loss: 0.04894542
[5/200] Training loss: 0.04563329
[6/200] Training loss: 0.04288407
[7/200] Training loss: 0.04162990
[8/200] Training loss: 0.03934857
[9/200] Training loss: 0.03551010
[10/200] Training loss: 0.03327891
[50/200] Training loss: 0.01744184
[100/200] Training loss: 0.01496942
[150/200] Training loss: 0.01372357
[200/200] Training loss: 0.01312131
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4519.783844389021 ----------
[1/200] Training loss: 0.15149944
[2/200] Training loss: 0.05872754
[3/200] Training loss: 0.05576484
[4/200] Training loss: 0.05102580
[5/200] Training loss: 0.04700290
[6/200] Training loss: 0.04410374
[7/200] Training loss: 0.04200186
[8/200] Training loss: 0.03832980
[9/200] Training loss: 0.03787229
[10/200] Training loss: 0.03445606
[50/200] Training loss: 0.01784250
[100/200] Training loss: 0.01394471
[150/200] Training loss: 0.01291820
[200/200] Training loss: 0.01203892
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14052.38769746978 ----------
[1/200] Training loss: 0.15948148
[2/200] Training loss: 0.05705027
[3/200] Training loss: 0.05390236
[4/200] Training loss: 0.05156571
[5/200] Training loss: 0.04973251
[6/200] Training loss: 0.04559728
[7/200] Training loss: 0.04457553
[8/200] Training loss: 0.04155665
[9/200] Training loss: 0.04115854
[10/200] Training loss: 0.03553759
[50/200] Training loss: 0.01805760
[100/200] Training loss: 0.01386593
[150/200] Training loss: 0.01325884
[200/200] Training loss: 0.01243146
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8012.5281902780225 ----------
[1/200] Training loss: 0.14570834
[2/200] Training loss: 0.05518155
[3/200] Training loss: 0.05250212
[4/200] Training loss: 0.04811523
[5/200] Training loss: 0.04767312
[6/200] Training loss: 0.04084676
[7/200] Training loss: 0.03733706
[8/200] Training loss: 0.03414084
[9/200] Training loss: 0.03137171
[10/200] Training loss: 0.02889112
[50/200] Training loss: 0.01646948
[100/200] Training loss: 0.01436972
[150/200] Training loss: 0.01311104
[200/200] Training loss: 0.01181521
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5853.13112786652 ----------
[1/200] Training loss: 0.14622464
[2/200] Training loss: 0.05877331
[3/200] Training loss: 0.05110782
[4/200] Training loss: 0.04666821
[5/200] Training loss: 0.04456435
[6/200] Training loss: 0.04094709
[7/200] Training loss: 0.03783448
[8/200] Training loss: 0.03818354
[9/200] Training loss: 0.03264096
[10/200] Training loss: 0.03430106
[50/200] Training loss: 0.01729984
[100/200] Training loss: 0.01375017
[150/200] Training loss: 0.01268470
[200/200] Training loss: 0.01174673
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 3326.5700052757043 ----------
[1/200] Training loss: 0.17209016
[2/200] Training loss: 0.06266655
[3/200] Training loss: 0.05689989
[4/200] Training loss: 0.05211988
[5/200] Training loss: 0.04836964
[6/200] Training loss: 0.04639608
[7/200] Training loss: 0.04450181
[8/200] Training loss: 0.04097936
[9/200] Training loss: 0.04095027
[10/200] Training loss: 0.03679064
[50/200] Training loss: 0.01833964
[100/200] Training loss: 0.01581466
[150/200] Training loss: 0.01456748
[200/200] Training loss: 0.01410140
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16274.415258312662 ----------
[1/200] Training loss: 0.15483357
[2/200] Training loss: 0.05789434
[3/200] Training loss: 0.05208405
[4/200] Training loss: 0.05020047
[5/200] Training loss: 0.04654351
[6/200] Training loss: 0.04265943
[7/200] Training loss: 0.03939894
[8/200] Training loss: 0.03698973
[9/200] Training loss: 0.03506821
[10/200] Training loss: 0.03283933
[50/200] Training loss: 0.01692398
[100/200] Training loss: 0.01267436
[150/200] Training loss: 0.01127476
[200/200] Training loss: 0.01042103
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15182.087866956903 ----------
[1/200] Training loss: 0.17196334
[2/200] Training loss: 0.05557346
[3/200] Training loss: 0.04804173
[4/200] Training loss: 0.04313936
[5/200] Training loss: 0.04170975
[6/200] Training loss: 0.03778655
[7/200] Training loss: 0.03401451
[8/200] Training loss: 0.03226814
[9/200] Training loss: 0.02946521
[10/200] Training loss: 0.02864844
[50/200] Training loss: 0.01808246
[100/200] Training loss: 0.01488707
[150/200] Training loss: 0.01312413
[200/200] Training loss: 0.01256015
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15670.85958076327 ----------
[1/200] Training loss: 0.18611046
[2/200] Training loss: 0.06159521
[3/200] Training loss: 0.05442080
[4/200] Training loss: 0.05546612
[5/200] Training loss: 0.04971773
[6/200] Training loss: 0.04943562
[7/200] Training loss: 0.04461218
[8/200] Training loss: 0.04134911
[9/200] Training loss: 0.04010860
[10/200] Training loss: 0.03592165
[50/200] Training loss: 0.01804503
[100/200] Training loss: 0.01627844
[150/200] Training loss: 0.01353725
[200/200] Training loss: 0.01283794
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20192.34785754247 ----------
[1/200] Training loss: 0.17992695
[2/200] Training loss: 0.05601422
[3/200] Training loss: 0.04907632
[4/200] Training loss: 0.04537073
[5/200] Training loss: 0.04416368
[6/200] Training loss: 0.04254839
[7/200] Training loss: 0.04028529
[8/200] Training loss: 0.04050606
[9/200] Training loss: 0.03872571
[10/200] Training loss: 0.03459267
[50/200] Training loss: 0.01868953
[100/200] Training loss: 0.01618113
[150/200] Training loss: 0.01455991
[200/200] Training loss: 0.01359172
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22311.308702090966 ----------
[1/200] Training loss: 0.14396281
[2/200] Training loss: 0.05944854
[3/200] Training loss: 0.05335180
[4/200] Training loss: 0.04850566
[5/200] Training loss: 0.04745302
[6/200] Training loss: 0.04316247
[7/200] Training loss: 0.04068249
[8/200] Training loss: 0.03870042
[9/200] Training loss: 0.03514744
[10/200] Training loss: 0.03762951
[50/200] Training loss: 0.01976021
[100/200] Training loss: 0.01652018
[150/200] Training loss: 0.01514071
[200/200] Training loss: 0.01394120
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19658.527310050467 ----------
[1/200] Training loss: 0.13640345
[2/200] Training loss: 0.06029809
[3/200] Training loss: 0.05276646
[4/200] Training loss: 0.05096197
[5/200] Training loss: 0.04722586
[6/200] Training loss: 0.04528440
[7/200] Training loss: 0.04181874
[8/200] Training loss: 0.03928707
[9/200] Training loss: 0.03569184
[10/200] Training loss: 0.03312460
[50/200] Training loss: 0.01628752
[100/200] Training loss: 0.01427530
[150/200] Training loss: 0.01334676
[200/200] Training loss: 0.01205133
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15786.263902519811 ----------
[1/200] Training loss: 0.14106025
[2/200] Training loss: 0.05414955
[3/200] Training loss: 0.04795419
[4/200] Training loss: 0.04093752
[5/200] Training loss: 0.03700289
[6/200] Training loss: 0.03379901
[7/200] Training loss: 0.03250382
[8/200] Training loss: 0.03001288
[9/200] Training loss: 0.02978123
[10/200] Training loss: 0.02660574
[50/200] Training loss: 0.01620129
[100/200] Training loss: 0.01497133
[150/200] Training loss: 0.01356952
[200/200] Training loss: 0.01352713
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11718.802669214974 ----------
[1/200] Training loss: 0.17113342
[2/200] Training loss: 0.06314877
[3/200] Training loss: 0.05455079
[4/200] Training loss: 0.04864138
[5/200] Training loss: 0.04714395
[6/200] Training loss: 0.04413011
[7/200] Training loss: 0.04099209
[8/200] Training loss: 0.03989695
[9/200] Training loss: 0.03819768
[10/200] Training loss: 0.03180806
[50/200] Training loss: 0.01723934
[100/200] Training loss: 0.01524683
[150/200] Training loss: 0.01394138
[200/200] Training loss: 0.01292623
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20256.52605951968 ----------
[1/200] Training loss: 0.15115157
[2/200] Training loss: 0.05937991
[3/200] Training loss: 0.05191261
[4/200] Training loss: 0.04820054
[5/200] Training loss: 0.04378693
[6/200] Training loss: 0.03985589
[7/200] Training loss: 0.03933044
[8/200] Training loss: 0.03812155
[9/200] Training loss: 0.03439429
[10/200] Training loss: 0.03312720
[50/200] Training loss: 0.01727079
[100/200] Training loss: 0.01510877
[150/200] Training loss: 0.01282006
[200/200] Training loss: 0.01183075
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11051.621419502208 ----------
[1/200] Training loss: 0.12195357
[2/200] Training loss: 0.05802052
[3/200] Training loss: 0.05425395
[4/200] Training loss: 0.04910123
[5/200] Training loss: 0.04709536
[6/200] Training loss: 0.04302337
[7/200] Training loss: 0.04093966
[8/200] Training loss: 0.03721979
[9/200] Training loss: 0.03653480
[10/200] Training loss: 0.03446439
[50/200] Training loss: 0.01711248
[100/200] Training loss: 0.01449560
[150/200] Training loss: 0.01252952
[200/200] Training loss: 0.01154346
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14984.724488625074 ----------
[1/200] Training loss: 0.16869576
[2/200] Training loss: 0.05598637
[3/200] Training loss: 0.04505382
[4/200] Training loss: 0.04822921
[5/200] Training loss: 0.04150748
[6/200] Training loss: 0.03692994
[7/200] Training loss: 0.03730830
[8/200] Training loss: 0.03494861
[9/200] Training loss: 0.03618647
[10/200] Training loss: 0.03303166
[50/200] Training loss: 0.01787649
[100/200] Training loss: 0.01363312
[150/200] Training loss: 0.01226499
[200/200] Training loss: 0.01167419
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6710.295969627569 ----------
[1/200] Training loss: 0.16410427
[2/200] Training loss: 0.05850029
[3/200] Training loss: 0.05262046
[4/200] Training loss: 0.04826559
[5/200] Training loss: 0.04551559
[6/200] Training loss: 0.04028789
[7/200] Training loss: 0.04026446
[8/200] Training loss: 0.03775964
[9/200] Training loss: 0.03292717
[10/200] Training loss: 0.03173007
[50/200] Training loss: 0.01795374
[100/200] Training loss: 0.01635801
[150/200] Training loss: 0.01538372
[200/200] Training loss: 0.01361217
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9835.489209998657 ----------
[1/200] Training loss: 0.12468851
[2/200] Training loss: 0.05595020
[3/200] Training loss: 0.05140107
[4/200] Training loss: 0.04859536
[5/200] Training loss: 0.04415945
[6/200] Training loss: 0.04482102
[7/200] Training loss: 0.04063548
[8/200] Training loss: 0.03883034
[9/200] Training loss: 0.03790364
[10/200] Training loss: 0.03602684
[50/200] Training loss: 0.01870919
[100/200] Training loss: 0.01508044
[150/200] Training loss: 0.01406025
[200/200] Training loss: 0.01229800
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6424.925836147839 ----------
[1/200] Training loss: 0.11546865
[2/200] Training loss: 0.05627620
[3/200] Training loss: 0.04769089
[4/200] Training loss: 0.04103330
[5/200] Training loss: 0.03937508
[6/200] Training loss: 0.03892405
[7/200] Training loss: 0.03509361
[8/200] Training loss: 0.03381560
[9/200] Training loss: 0.03383255
[10/200] Training loss: 0.03309896
[50/200] Training loss: 0.01571691
[100/200] Training loss: 0.01371737
[150/200] Training loss: 0.01236905
[200/200] Training loss: 0.01123558
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6524.388400455632 ----------
[1/200] Training loss: 0.14584969
[2/200] Training loss: 0.06088037
[3/200] Training loss: 0.05016738
[4/200] Training loss: 0.04898655
[5/200] Training loss: 0.04345669
[6/200] Training loss: 0.04214478
[7/200] Training loss: 0.03889092
[8/200] Training loss: 0.03934172
[9/200] Training loss: 0.03384324
[10/200] Training loss: 0.03568586
[50/200] Training loss: 0.01778429
[100/200] Training loss: 0.01457482
[150/200] Training loss: 0.01399079
[200/200] Training loss: 0.01207199
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 25361.55704999202 ----------
[1/200] Training loss: 0.16228534
[2/200] Training loss: 0.05470656
[3/200] Training loss: 0.05062336
[4/200] Training loss: 0.04534315
[5/200] Training loss: 0.04342757
[6/200] Training loss: 0.03977411
[7/200] Training loss: 0.03534249
[8/200] Training loss: 0.03368653
[9/200] Training loss: 0.03234795
[10/200] Training loss: 0.03227121
[50/200] Training loss: 0.01734298
[100/200] Training loss: 0.01562659
[150/200] Training loss: 0.01371727
[200/200] Training loss: 0.01292942
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13216.792349129195 ----------
[1/200] Training loss: 0.12136426
[2/200] Training loss: 0.05708470
[3/200] Training loss: 0.05005780
[4/200] Training loss: 0.04322713
[5/200] Training loss: 0.04354099
[6/200] Training loss: 0.03852924
[7/200] Training loss: 0.03642164
[8/200] Training loss: 0.03373312
[9/200] Training loss: 0.02927965
[10/200] Training loss: 0.02568178
[50/200] Training loss: 0.01820209
[100/200] Training loss: 0.01488722
[150/200] Training loss: 0.01339768
[200/200] Training loss: 0.01320372
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14064.02275311015 ----------
[1/200] Training loss: 0.11558498
[2/200] Training loss: 0.04941650
[3/200] Training loss: 0.04469811
[4/200] Training loss: 0.03816721
[5/200] Training loss: 0.03694323
[6/200] Training loss: 0.03330932
[7/200] Training loss: 0.03058302
[8/200] Training loss: 0.02696565
[9/200] Training loss: 0.02838928
[10/200] Training loss: 0.02679524
[50/200] Training loss: 0.01778301
[100/200] Training loss: 0.01532857
[150/200] Training loss: 0.01461446
[200/200] Training loss: 0.01349549
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9468.95728155957 ----------
[1/200] Training loss: 0.15170443
[2/200] Training loss: 0.05253480
[3/200] Training loss: 0.05062469
[4/200] Training loss: 0.03943718
[5/200] Training loss: 0.04104122
[6/200] Training loss: 0.03540411
[7/200] Training loss: 0.03467693
[8/200] Training loss: 0.03254606
[9/200] Training loss: 0.03048998
[10/200] Training loss: 0.02804124
[50/200] Training loss: 0.01723190
[100/200] Training loss: 0.01469563
[150/200] Training loss: 0.01242232
[200/200] Training loss: 0.01107734
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12007.816121177073 ----------
[1/200] Training loss: 0.14054655
[2/200] Training loss: 0.05320579
[3/200] Training loss: 0.04719573
[4/200] Training loss: 0.04326636
[5/200] Training loss: 0.03961337
[6/200] Training loss: 0.04074724
[7/200] Training loss: 0.03523786
[8/200] Training loss: 0.03435181
[9/200] Training loss: 0.03236903
[10/200] Training loss: 0.03069870
[50/200] Training loss: 0.01787990
[100/200] Training loss: 0.01599636
[150/200] Training loss: 0.01410712
[200/200] Training loss: 0.01349667
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9337.11818496478 ----------
[1/200] Training loss: 0.15120265
[2/200] Training loss: 0.05562294
[3/200] Training loss: 0.04754021
[4/200] Training loss: 0.04563043
[5/200] Training loss: 0.04060947
[6/200] Training loss: 0.03838655
[7/200] Training loss: 0.03631839
[8/200] Training loss: 0.03258247
[9/200] Training loss: 0.03253878
[10/200] Training loss: 0.03338556
[50/200] Training loss: 0.01685230
[100/200] Training loss: 0.01446830
[150/200] Training loss: 0.01323211
[200/200] Training loss: 0.01228603
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8313.528252192327 ----------
[1/200] Training loss: 0.11586043
[2/200] Training loss: 0.05149554
[3/200] Training loss: 0.04839296
[4/200] Training loss: 0.04416723
[5/200] Training loss: 0.03961726
[6/200] Training loss: 0.03822995
[7/200] Training loss: 0.03749084
[8/200] Training loss: 0.03434515
[9/200] Training loss: 0.03320014
[10/200] Training loss: 0.03131101
[50/200] Training loss: 0.01780879
[100/200] Training loss: 0.01538227
[150/200] Training loss: 0.01339728
[200/200] Training loss: 0.01198160
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17403.90576853368 ----------
[1/200] Training loss: 0.17334460
[2/200] Training loss: 0.06276509
[3/200] Training loss: 0.05279419
[4/200] Training loss: 0.05051282
[5/200] Training loss: 0.04545391
[6/200] Training loss: 0.04169101
[7/200] Training loss: 0.03569855
[8/200] Training loss: 0.03489537
[9/200] Training loss: 0.03125986
[10/200] Training loss: 0.02997601
[50/200] Training loss: 0.01508176
[100/200] Training loss: 0.01263137
[150/200] Training loss: 0.01139035
[200/200] Training loss: 0.01025741
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 24578.60402870757 ----------
[1/200] Training loss: 0.13486234
[2/200] Training loss: 0.05464750
[3/200] Training loss: 0.05069173
[4/200] Training loss: 0.04572259
[5/200] Training loss: 0.04432833
[6/200] Training loss: 0.04019220
[7/200] Training loss: 0.03714176
[8/200] Training loss: 0.03541498
[9/200] Training loss: 0.03409927
[10/200] Training loss: 0.02959206
[50/200] Training loss: 0.01784779
[100/200] Training loss: 0.01509197
[150/200] Training loss: 0.01463418
[200/200] Training loss: 0.01325443
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16408.92635122725 ----------
[1/200] Training loss: 0.12939814
[2/200] Training loss: 0.05740421
[3/200] Training loss: 0.05166560
[4/200] Training loss: 0.05108871
[5/200] Training loss: 0.04957961
[6/200] Training loss: 0.04424400
[7/200] Training loss: 0.04335547
[8/200] Training loss: 0.04295245
[9/200] Training loss: 0.03972358
[10/200] Training loss: 0.03743779
[50/200] Training loss: 0.01912964
[100/200] Training loss: 0.01608102
[150/200] Training loss: 0.01506202
[200/200] Training loss: 0.01400469
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16337.779530890972 ----------
[1/200] Training loss: 0.16052804
[2/200] Training loss: 0.06042163
[3/200] Training loss: 0.05594103
[4/200] Training loss: 0.05200244
[5/200] Training loss: 0.05111082
[6/200] Training loss: 0.04595630
[7/200] Training loss: 0.04381895
[8/200] Training loss: 0.03920348
[9/200] Training loss: 0.03781597
[10/200] Training loss: 0.03616068
[50/200] Training loss: 0.01798971
[100/200] Training loss: 0.01683426
[150/200] Training loss: 0.01478451
[200/200] Training loss: 0.01362782
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15868.275772748595 ----------
[1/200] Training loss: 0.17882914
[2/200] Training loss: 0.06483430
[3/200] Training loss: 0.05721330
[4/200] Training loss: 0.05238441
[5/200] Training loss: 0.04854320
[6/200] Training loss: 0.04586608
[7/200] Training loss: 0.04028656
[8/200] Training loss: 0.04150903
[9/200] Training loss: 0.03820668
[10/200] Training loss: 0.03603070
[50/200] Training loss: 0.01798869
[100/200] Training loss: 0.01596751
[150/200] Training loss: 0.01531702
[200/200] Training loss: 0.01458478
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5213.993287299093 ----------
[1/200] Training loss: 0.14930360
[2/200] Training loss: 0.05665047
[3/200] Training loss: 0.05024807
[4/200] Training loss: 0.04816752
[5/200] Training loss: 0.04588952
[6/200] Training loss: 0.04072996
[7/200] Training loss: 0.04202764
[8/200] Training loss: 0.03631531
[9/200] Training loss: 0.03420608
[10/200] Training loss: 0.03281252
[50/200] Training loss: 0.01809741
[100/200] Training loss: 0.01521297
[150/200] Training loss: 0.01412836
[200/200] Training loss: 0.01307654
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6836.596229118698 ----------
[1/200] Training loss: 0.15641988
[2/200] Training loss: 0.05615259
[3/200] Training loss: 0.04815723
[4/200] Training loss: 0.04769112
[5/200] Training loss: 0.04092046
[6/200] Training loss: 0.03968287
[7/200] Training loss: 0.03840206
[8/200] Training loss: 0.03511077
[9/200] Training loss: 0.03294296
[10/200] Training loss: 0.03227925
[50/200] Training loss: 0.01778111
[100/200] Training loss: 0.01422624
[150/200] Training loss: 0.01260893
[200/200] Training loss: 0.01141900
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11272.39424434756 ----------
[1/200] Training loss: 0.12196315
[2/200] Training loss: 0.05780394
[3/200] Training loss: 0.05357307
[4/200] Training loss: 0.04892047
[5/200] Training loss: 0.04880368
[6/200] Training loss: 0.04494351
[7/200] Training loss: 0.04131274
[8/200] Training loss: 0.03824940
[9/200] Training loss: 0.03640100
[10/200] Training loss: 0.03567701
[50/200] Training loss: 0.01773108
[100/200] Training loss: 0.01541189
[150/200] Training loss: 0.01382371
[200/200] Training loss: 0.01284956
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18536.612851327503 ----------
[1/200] Training loss: 0.16367278
[2/200] Training loss: 0.05890522
[3/200] Training loss: 0.05346778
[4/200] Training loss: 0.04790109
[5/200] Training loss: 0.04617289
[6/200] Training loss: 0.04661413
[7/200] Training loss: 0.03930730
[8/200] Training loss: 0.03759609
[9/200] Training loss: 0.03281257
[10/200] Training loss: 0.03204959
[50/200] Training loss: 0.01900609
[100/200] Training loss: 0.01616820
[150/200] Training loss: 0.01537824
[200/200] Training loss: 0.01399442
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13708.969326685357 ----------
[1/200] Training loss: 0.12607104
[2/200] Training loss: 0.05543224
[3/200] Training loss: 0.05239179
[4/200] Training loss: 0.04497766
[5/200] Training loss: 0.04520276
[6/200] Training loss: 0.04497667
[7/200] Training loss: 0.04062069
[8/200] Training loss: 0.03948235
[9/200] Training loss: 0.03462539
[10/200] Training loss: 0.03562128
[50/200] Training loss: 0.01929567
[100/200] Training loss: 0.01642386
[150/200] Training loss: 0.01502562
[200/200] Training loss: 0.01363292
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7381.235397953381 ----------
[1/200] Training loss: 0.18126767
[2/200] Training loss: 0.06657587
[3/200] Training loss: 0.05508645
[4/200] Training loss: 0.05024085
[5/200] Training loss: 0.04506300
[6/200] Training loss: 0.04219538
[7/200] Training loss: 0.04227728
[8/200] Training loss: 0.03752059
[9/200] Training loss: 0.03720642
[10/200] Training loss: 0.03360369
[50/200] Training loss: 0.01980301
[100/200] Training loss: 0.01740244
[150/200] Training loss: 0.01547587
[200/200] Training loss: 0.01517752
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11197.908733330523 ----------
[1/200] Training loss: 0.17253781
[2/200] Training loss: 0.06351341
[3/200] Training loss: 0.05476067
[4/200] Training loss: 0.05082751
[5/200] Training loss: 0.04527324
[6/200] Training loss: 0.04263410
[7/200] Training loss: 0.03831543
[8/200] Training loss: 0.03471314
[9/200] Training loss: 0.03492459
[10/200] Training loss: 0.03208226
[50/200] Training loss: 0.01801332
[100/200] Training loss: 0.01596772
[150/200] Training loss: 0.01534198
[200/200] Training loss: 0.01396445
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21385.018307216855 ----------
[1/200] Training loss: 0.13397667
[2/200] Training loss: 0.05747087
[3/200] Training loss: 0.05061464
[4/200] Training loss: 0.04276330
[5/200] Training loss: 0.04113408
[6/200] Training loss: 0.03873723
[7/200] Training loss: 0.03663110
[8/200] Training loss: 0.03448372
[9/200] Training loss: 0.03605238
[10/200] Training loss: 0.03340230
[50/200] Training loss: 0.01823294
[100/200] Training loss: 0.01582371
[150/200] Training loss: 0.01505064
[200/200] Training loss: 0.01335202
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10862.982279282242 ----------
[1/200] Training loss: 0.12456340
[2/200] Training loss: 0.05963326
[3/200] Training loss: 0.05543053
[4/200] Training loss: 0.05028386
[5/200] Training loss: 0.04911851
[6/200] Training loss: 0.04285523
[7/200] Training loss: 0.04056099
[8/200] Training loss: 0.03610334
[9/200] Training loss: 0.03363649
[10/200] Training loss: 0.03091051
[50/200] Training loss: 0.01863764
[100/200] Training loss: 0.01569052
[150/200] Training loss: 0.01353683
[200/200] Training loss: 0.01260650
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 27533.81862364899 ----------
[1/200] Training loss: 0.13842289
[2/200] Training loss: 0.06054310
[3/200] Training loss: 0.05454467
[4/200] Training loss: 0.05177935
[5/200] Training loss: 0.05126780
[6/200] Training loss: 0.04614269
[7/200] Training loss: 0.04202249
[8/200] Training loss: 0.04128251
[9/200] Training loss: 0.03597871
[10/200] Training loss: 0.03597866
[50/200] Training loss: 0.01752037
[100/200] Training loss: 0.01618859
[150/200] Training loss: 0.01427517
[200/200] Training loss: 0.01297836
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16352.332188406643 ----------
[1/200] Training loss: 0.14018182
[2/200] Training loss: 0.05425107
[3/200] Training loss: 0.05222798
[4/200] Training loss: 0.04797754
[5/200] Training loss: 0.04488842
[6/200] Training loss: 0.04449486
[7/200] Training loss: 0.04094127
[8/200] Training loss: 0.03592200
[9/200] Training loss: 0.03858235
[10/200] Training loss: 0.03175490
[50/200] Training loss: 0.01824471
[100/200] Training loss: 0.01416422
[150/200] Training loss: 0.01310348
[200/200] Training loss: 0.01274867
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8215.411858208936 ----------
[1/200] Training loss: 0.11485517
[2/200] Training loss: 0.05472892
[3/200] Training loss: 0.05234265
[4/200] Training loss: 0.04964997
[5/200] Training loss: 0.04475472
[6/200] Training loss: 0.04287000
[7/200] Training loss: 0.03909124
[8/200] Training loss: 0.03601528
[9/200] Training loss: 0.03563599
[10/200] Training loss: 0.03513534
[50/200] Training loss: 0.01792807
[100/200] Training loss: 0.01434693
[150/200] Training loss: 0.01299917
[200/200] Training loss: 0.01237652
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11893.888178388092 ----------
[1/200] Training loss: 0.14484592
[2/200] Training loss: 0.05598592
[3/200] Training loss: 0.04898396
[4/200] Training loss: 0.04672695
[5/200] Training loss: 0.04553535
[6/200] Training loss: 0.04496508
[7/200] Training loss: 0.03861228
[8/200] Training loss: 0.03723566
[9/200] Training loss: 0.03555780
[10/200] Training loss: 0.03445594
[50/200] Training loss: 0.01877508
[100/200] Training loss: 0.01542231
[150/200] Training loss: 0.01430577
[200/200] Training loss: 0.01301163
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12889.383538400896 ----------
[1/200] Training loss: 0.18049506
[2/200] Training loss: 0.06099260
[3/200] Training loss: 0.05434867
[4/200] Training loss: 0.04903408
[5/200] Training loss: 0.04688855
[6/200] Training loss: 0.04167440
[7/200] Training loss: 0.04062027
[8/200] Training loss: 0.03797773
[9/200] Training loss: 0.03517658
[10/200] Training loss: 0.03052865
[50/200] Training loss: 0.01884542
[100/200] Training loss: 0.01575666
[150/200] Training loss: 0.01434409
[200/200] Training loss: 0.01331997
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18511.07690006176 ----------
[1/200] Training loss: 0.17595066
[2/200] Training loss: 0.06144608
[3/200] Training loss: 0.05793135
[4/200] Training loss: 0.05205897
[5/200] Training loss: 0.04697988
[6/200] Training loss: 0.04182860
[7/200] Training loss: 0.04086888
[8/200] Training loss: 0.03770440
[9/200] Training loss: 0.03583081
[10/200] Training loss: 0.03350863
[50/200] Training loss: 0.01845251
[100/200] Training loss: 0.01668575
[150/200] Training loss: 0.01496772
[200/200] Training loss: 0.01424848
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10782.449443424253 ----------
[1/200] Training loss: 0.13367626
[2/200] Training loss: 0.05826659
[3/200] Training loss: 0.05090472
[4/200] Training loss: 0.05035546
[5/200] Training loss: 0.04430229
[6/200] Training loss: 0.03982739
[7/200] Training loss: 0.03578910
[8/200] Training loss: 0.03572861
[9/200] Training loss: 0.03316903
[10/200] Training loss: 0.03161599
[50/200] Training loss: 0.01876502
[100/200] Training loss: 0.01603964
[150/200] Training loss: 0.01394416
[200/200] Training loss: 0.01327196
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13888.995356036376 ----------
[1/200] Training loss: 0.13105563
[2/200] Training loss: 0.05445859
[3/200] Training loss: 0.04758866
[4/200] Training loss: 0.04340438
[5/200] Training loss: 0.03852900
[6/200] Training loss: 0.03747874
[7/200] Training loss: 0.03481100
[8/200] Training loss: 0.03308686
[9/200] Training loss: 0.03083400
[10/200] Training loss: 0.02898143
[50/200] Training loss: 0.01813576
[100/200] Training loss: 0.01621755
[150/200] Training loss: 0.01455837
[200/200] Training loss: 0.01346991
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8912.51973349849 ----------
[1/200] Training loss: 0.12947013
[2/200] Training loss: 0.05806238
[3/200] Training loss: 0.05342421
[4/200] Training loss: 0.04777792
[5/200] Training loss: 0.04230667
[6/200] Training loss: 0.03564788
[7/200] Training loss: 0.03251532
[8/200] Training loss: 0.03161524
[9/200] Training loss: 0.03230732
[10/200] Training loss: 0.02887885
[50/200] Training loss: 0.01593622
[100/200] Training loss: 0.01406882
[150/200] Training loss: 0.01348597
[200/200] Training loss: 0.01217065
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14265.340935287877 ----------
[1/200] Training loss: 0.17574840
[2/200] Training loss: 0.05672390
[3/200] Training loss: 0.04930735
[4/200] Training loss: 0.04912104
[5/200] Training loss: 0.04455790
[6/200] Training loss: 0.04384534
[7/200] Training loss: 0.03981523
[8/200] Training loss: 0.03818942
[9/200] Training loss: 0.03345213
[10/200] Training loss: 0.03470792
[50/200] Training loss: 0.01660832
[100/200] Training loss: 0.01387222
[150/200] Training loss: 0.01252086
[200/200] Training loss: 0.01215789
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18947.428743763623 ----------
[1/200] Training loss: 0.12854072
[2/200] Training loss: 0.05244392
[3/200] Training loss: 0.05110639
[4/200] Training loss: 0.04335992
[5/200] Training loss: 0.04518946
[6/200] Training loss: 0.03761221
[7/200] Training loss: 0.03697488
[8/200] Training loss: 0.03635730
[9/200] Training loss: 0.03425133
[10/200] Training loss: 0.03245921
[50/200] Training loss: 0.01710982
[100/200] Training loss: 0.01475221
[150/200] Training loss: 0.01385848
[200/200] Training loss: 0.01299965
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7251.0142738792065 ----------
[1/200] Training loss: 0.14866630
[2/200] Training loss: 0.05738664
[3/200] Training loss: 0.05311391
[4/200] Training loss: 0.05033046
[5/200] Training loss: 0.04743572
[6/200] Training loss: 0.04366899
[7/200] Training loss: 0.04175580
[8/200] Training loss: 0.03643262
[9/200] Training loss: 0.03388121
[10/200] Training loss: 0.03432694
[50/200] Training loss: 0.01838098
[100/200] Training loss: 0.01589550
[150/200] Training loss: 0.01460395
[200/200] Training loss: 0.01330506
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13178.048413934439 ----------
[1/200] Training loss: 0.12845990
[2/200] Training loss: 0.05745897
[3/200] Training loss: 0.05059070
[4/200] Training loss: 0.04869636
[5/200] Training loss: 0.04551457
[6/200] Training loss: 0.04107469
[7/200] Training loss: 0.03800545
[8/200] Training loss: 0.03429488
[9/200] Training loss: 0.03443961
[10/200] Training loss: 0.03180993
[50/200] Training loss: 0.01898804
[100/200] Training loss: 0.01720261
[150/200] Training loss: 0.01571737
[200/200] Training loss: 0.01345594
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17818.619475144533 ----------
[1/200] Training loss: 0.14100476
[2/200] Training loss: 0.05638427
[3/200] Training loss: 0.05076120
[4/200] Training loss: 0.05139568
[5/200] Training loss: 0.04802273
[6/200] Training loss: 0.04750630
[7/200] Training loss: 0.04499980
[8/200] Training loss: 0.04566126
[9/200] Training loss: 0.04257703
[10/200] Training loss: 0.03919048
[50/200] Training loss: 0.01975690
[100/200] Training loss: 0.01550275
[150/200] Training loss: 0.01274890
[200/200] Training loss: 0.01092319
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6768.2863414604435 ----------
[1/200] Training loss: 0.15619127
[2/200] Training loss: 0.06117569
[3/200] Training loss: 0.05688777
[4/200] Training loss: 0.05209447
[5/200] Training loss: 0.05090364
[6/200] Training loss: 0.04838954
[7/200] Training loss: 0.04688141
[8/200] Training loss: 0.04804324
[9/200] Training loss: 0.04395587
[10/200] Training loss: 0.04284849
[50/200] Training loss: 0.01897730
[100/200] Training loss: 0.01572313
[150/200] Training loss: 0.01372998
[200/200] Training loss: 0.01240344
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14320.079329389206 ----------
[1/200] Training loss: 0.12527383
[2/200] Training loss: 0.05362538
[3/200] Training loss: 0.04828824
[4/200] Training loss: 0.04844949
[5/200] Training loss: 0.04192668
[6/200] Training loss: 0.04022691
[7/200] Training loss: 0.03893465
[8/200] Training loss: 0.03781212
[9/200] Training loss: 0.03455633
[10/200] Training loss: 0.03380383
[50/200] Training loss: 0.01843153
[100/200] Training loss: 0.01656466
[150/200] Training loss: 0.01545728
[200/200] Training loss: 0.01375953
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9579.182846151336 ----------
[1/200] Training loss: 0.17239625
[2/200] Training loss: 0.06305957
[3/200] Training loss: 0.05708922
[4/200] Training loss: 0.05268984
[5/200] Training loss: 0.04929454
[6/200] Training loss: 0.04634082
[7/200] Training loss: 0.04458621
[8/200] Training loss: 0.04188016
[9/200] Training loss: 0.04102862
[10/200] Training loss: 0.03741076
[50/200] Training loss: 0.01870963
[100/200] Training loss: 0.01539996
[150/200] Training loss: 0.01369017
[200/200] Training loss: 0.01255581
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12984.566839136376 ----------
[1/200] Training loss: 0.21184725
[2/200] Training loss: 0.06450297
[3/200] Training loss: 0.06091233
[4/200] Training loss: 0.05700253
[5/200] Training loss: 0.05015872
[6/200] Training loss: 0.05000174
[7/200] Training loss: 0.04933170
[8/200] Training loss: 0.04593334
[9/200] Training loss: 0.04393210
[10/200] Training loss: 0.04422468
[50/200] Training loss: 0.01959190
[100/200] Training loss: 0.01575198
[150/200] Training loss: 0.01469837
[200/200] Training loss: 0.01352116
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11929.234677882734 ----------
[1/200] Training loss: 0.16803314
[2/200] Training loss: 0.05810659
[3/200] Training loss: 0.05099883
[4/200] Training loss: 0.04745661
[5/200] Training loss: 0.04409088
[6/200] Training loss: 0.04033275
[7/200] Training loss: 0.03772551
[8/200] Training loss: 0.03579720
[9/200] Training loss: 0.03269129
[10/200] Training loss: 0.03251812
[50/200] Training loss: 0.01865855
[100/200] Training loss: 0.01526160
[150/200] Training loss: 0.01359991
[200/200] Training loss: 0.01219109
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10919.761535857822 ----------
[1/200] Training loss: 0.12702279
[2/200] Training loss: 0.05350938
[3/200] Training loss: 0.04900333
[4/200] Training loss: 0.04554051
[5/200] Training loss: 0.04323220
[6/200] Training loss: 0.03972202
[7/200] Training loss: 0.03816791
[8/200] Training loss: 0.03320543
[9/200] Training loss: 0.03351709
[10/200] Training loss: 0.03141956
[50/200] Training loss: 0.01861361
[100/200] Training loss: 0.01613570
[150/200] Training loss: 0.01484397
[200/200] Training loss: 0.01381334
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13061.122156997078 ----------
[1/200] Training loss: 0.13167158
[2/200] Training loss: 0.06211327
[3/200] Training loss: 0.05060344
[4/200] Training loss: 0.04911248
[5/200] Training loss: 0.04606008
[6/200] Training loss: 0.04684927
[7/200] Training loss: 0.04229720
[8/200] Training loss: 0.04019050
[9/200] Training loss: 0.03835353
[10/200] Training loss: 0.03585425
[50/200] Training loss: 0.01770543
[100/200] Training loss: 0.01598179
[150/200] Training loss: 0.01356837
[200/200] Training loss: 0.01253179
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21854.844771812037 ----------
[1/200] Training loss: 0.13485342
[2/200] Training loss: 0.05717334
[3/200] Training loss: 0.05512327
[4/200] Training loss: 0.05153864
[5/200] Training loss: 0.04943418
[6/200] Training loss: 0.04525602
[7/200] Training loss: 0.04685019
[8/200] Training loss: 0.04410005
[9/200] Training loss: 0.04093595
[10/200] Training loss: 0.04128162
[50/200] Training loss: 0.01878923
[100/200] Training loss: 0.01313731
[150/200] Training loss: 0.01153077
[200/200] Training loss: 0.01103894
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12962.044900400553 ----------
[1/200] Training loss: 0.12620215
[2/200] Training loss: 0.05385614
[3/200] Training loss: 0.05264346
[4/200] Training loss: 0.04920869
[5/200] Training loss: 0.04511337
[6/200] Training loss: 0.04206829
[7/200] Training loss: 0.03989619
[8/200] Training loss: 0.03852204
[9/200] Training loss: 0.03538754
[10/200] Training loss: 0.03342557
[50/200] Training loss: 0.01810526
[100/200] Training loss: 0.01650483
[150/200] Training loss: 0.01502602
[200/200] Training loss: 0.01383144
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10584.939489671162 ----------
[1/200] Training loss: 0.16893030
[2/200] Training loss: 0.06402779
[3/200] Training loss: 0.05016193
[4/200] Training loss: 0.04730622
[5/200] Training loss: 0.04732753
[6/200] Training loss: 0.04335667
[7/200] Training loss: 0.04357834
[8/200] Training loss: 0.03626605
[9/200] Training loss: 0.03645432
[10/200] Training loss: 0.03524893
[50/200] Training loss: 0.01977218
[100/200] Training loss: 0.01670379
[150/200] Training loss: 0.01516897
[200/200] Training loss: 0.01414259
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10587.028289373746 ----------
[1/200] Training loss: 0.14908934
[2/200] Training loss: 0.06201303
[3/200] Training loss: 0.05527328
[4/200] Training loss: 0.04991711
[5/200] Training loss: 0.04574155
[6/200] Training loss: 0.03839705
[7/200] Training loss: 0.03927416
[8/200] Training loss: 0.03400237
[9/200] Training loss: 0.03092261
[10/200] Training loss: 0.02921065
[50/200] Training loss: 0.01749744
[100/200] Training loss: 0.01508649
[150/200] Training loss: 0.01383126
[200/200] Training loss: 0.01254474
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 27747.047122171396 ----------
[1/200] Training loss: 0.12727314
[2/200] Training loss: 0.05562649
[3/200] Training loss: 0.05373854
[4/200] Training loss: 0.04674071
[5/200] Training loss: 0.04361609
[6/200] Training loss: 0.04096002
[7/200] Training loss: 0.03604882
[8/200] Training loss: 0.03576256
[9/200] Training loss: 0.03308884
[10/200] Training loss: 0.03221376
[50/200] Training loss: 0.01748325
[100/200] Training loss: 0.01427235
[150/200] Training loss: 0.01265445
[200/200] Training loss: 0.01192618
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12083.841773211034 ----------
[1/200] Training loss: 0.16419763
[2/200] Training loss: 0.06155651
[3/200] Training loss: 0.04927503
[4/200] Training loss: 0.04816336
[5/200] Training loss: 0.04676560
[6/200] Training loss: 0.04092383
[7/200] Training loss: 0.04098066
[8/200] Training loss: 0.03751488
[9/200] Training loss: 0.03421285
[10/200] Training loss: 0.03534973
[50/200] Training loss: 0.01973681
[100/200] Training loss: 0.01694616
[150/200] Training loss: 0.01478964
[200/200] Training loss: 0.01400814
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14752.895309057134 ----------
[1/200] Training loss: 0.11305154
[2/200] Training loss: 0.05675286
[3/200] Training loss: 0.05138892
[4/200] Training loss: 0.04588174
[5/200] Training loss: 0.04228307
[6/200] Training loss: 0.03727857
[7/200] Training loss: 0.03392466
[8/200] Training loss: 0.03505237
[9/200] Training loss: 0.02895758
[10/200] Training loss: 0.03011776
[50/200] Training loss: 0.01630461
[100/200] Training loss: 0.01359003
[150/200] Training loss: 0.01241898
[200/200] Training loss: 0.01162355
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10618.22358024166 ----------
[1/200] Training loss: 0.14018643
[2/200] Training loss: 0.06127990
[3/200] Training loss: 0.05356764
[4/200] Training loss: 0.05129025
[5/200] Training loss: 0.04754257
[6/200] Training loss: 0.04314611
[7/200] Training loss: 0.03829906
[8/200] Training loss: 0.03643160
[9/200] Training loss: 0.03311939
[10/200] Training loss: 0.03124871
[50/200] Training loss: 0.01612287
[100/200] Training loss: 0.01447557
[150/200] Training loss: 0.01249777
[200/200] Training loss: 0.01177983
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9760.680713966623 ----------
[1/200] Training loss: 0.13198284
[2/200] Training loss: 0.05409739
[3/200] Training loss: 0.05267740
[4/200] Training loss: 0.04998783
[5/200] Training loss: 0.04824981
[6/200] Training loss: 0.04623052
[7/200] Training loss: 0.04372201
[8/200] Training loss: 0.04169301
[9/200] Training loss: 0.03749415
[10/200] Training loss: 0.03606763
[50/200] Training loss: 0.01861126
[100/200] Training loss: 0.01593890
[150/200] Training loss: 0.01404497
[200/200] Training loss: 0.01303459
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16086.87365525073 ----------
[1/200] Training loss: 0.18322101
[2/200] Training loss: 0.06014906
[3/200] Training loss: 0.05333922
[4/200] Training loss: 0.05152837
[5/200] Training loss: 0.04694904
[6/200] Training loss: 0.04395484
[7/200] Training loss: 0.04050196
[8/200] Training loss: 0.03732458
[9/200] Training loss: 0.03632370
[10/200] Training loss: 0.03343460
[50/200] Training loss: 0.01913108
[100/200] Training loss: 0.01704202
[150/200] Training loss: 0.01478555
[200/200] Training loss: 0.01397709
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10490.17788219056 ----------
[1/200] Training loss: 0.13020531
[2/200] Training loss: 0.05654248
[3/200] Training loss: 0.05050503
[4/200] Training loss: 0.04145366
[5/200] Training loss: 0.03737267
[6/200] Training loss: 0.03457756
[7/200] Training loss: 0.03433409
[8/200] Training loss: 0.03177733
[9/200] Training loss: 0.03088337
[10/200] Training loss: 0.03093624
[50/200] Training loss: 0.01649062
[100/200] Training loss: 0.01477801
[150/200] Training loss: 0.01349564
[200/200] Training loss: 0.01294443
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8612.940032300237 ----------
[1/200] Training loss: 0.13617697
[2/200] Training loss: 0.05634596
[3/200] Training loss: 0.05139900
[4/200] Training loss: 0.04579419
[5/200] Training loss: 0.04380427
[6/200] Training loss: 0.03853840
[7/200] Training loss: 0.03689390
[8/200] Training loss: 0.03255511
[9/200] Training loss: 0.03109097
[10/200] Training loss: 0.03234226
[50/200] Training loss: 0.01816106
[100/200] Training loss: 0.01531863
[150/200] Training loss: 0.01433061
[200/200] Training loss: 0.01348902
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10713.925517754918 ----------
[1/200] Training loss: 0.13317308
[2/200] Training loss: 0.05696844
[3/200] Training loss: 0.05215569
[4/200] Training loss: 0.04783150
[5/200] Training loss: 0.04366185
[6/200] Training loss: 0.04204649
[7/200] Training loss: 0.03894516
[8/200] Training loss: 0.03549749
[9/200] Training loss: 0.03366554
[10/200] Training loss: 0.03343456
[50/200] Training loss: 0.01895739
[100/200] Training loss: 0.01575204
[150/200] Training loss: 0.01474090
[200/200] Training loss: 0.01334849
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.18623040572254934 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16927.493376161754 ----------
[1/200] Training loss: 0.13487514
[2/200] Training loss: 0.05742740
[3/200] Training loss: 0.04917122
[4/200] Training loss: 0.04935123
[5/200] Training loss: 0.04093780
[6/200] Training loss: 0.03968527
[7/200] Training loss: 0.03850988
[8/200] Training loss: 0.03485878
[9/200] Training loss: 0.03308262
[10/200] Training loss: 0.03206419
[50/200] Training loss: 0.01646365
[100/200] Training loss: 0.01372053
[150/200] Training loss: 0.01235370
[200/200] Training loss: 0.01164555
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21087.58649063472 ----------
[1/200] Training loss: 0.11971199
[2/200] Training loss: 0.05258865
[3/200] Training loss: 0.04738662
[4/200] Training loss: 0.04592619
[5/200] Training loss: 0.04094813
[6/200] Training loss: 0.04165140
[7/200] Training loss: 0.03887795
[8/200] Training loss: 0.03293514
[9/200] Training loss: 0.03276217
[10/200] Training loss: 0.03264475
[50/200] Training loss: 0.01894109
[100/200] Training loss: 0.01481377
[150/200] Training loss: 0.01302703
[200/200] Training loss: 0.01189273
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6334.178084013742 ----------
[1/200] Training loss: 0.19241778
[2/200] Training loss: 0.06505837
[3/200] Training loss: 0.05431758
[4/200] Training loss: 0.04962084
[5/200] Training loss: 0.04790632
[6/200] Training loss: 0.04567259
[7/200] Training loss: 0.04251232
[8/200] Training loss: 0.03790001
[9/200] Training loss: 0.03678468
[10/200] Training loss: 0.03585226
[50/200] Training loss: 0.01863903
[100/200] Training loss: 0.01529072
[150/200] Training loss: 0.01349359
[200/200] Training loss: 0.01294745
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10485.585534437263 ----------
[1/200] Training loss: 0.16679448
[2/200] Training loss: 0.05413539
[3/200] Training loss: 0.05627986
[4/200] Training loss: 0.05120845
[5/200] Training loss: 0.04642249
[6/200] Training loss: 0.04194746
[7/200] Training loss: 0.04256036
[8/200] Training loss: 0.03987982
[9/200] Training loss: 0.03375998
[10/200] Training loss: 0.03319561
[50/200] Training loss: 0.01820986
[100/200] Training loss: 0.01595722
[150/200] Training loss: 0.01399561
[200/200] Training loss: 0.01288412
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9182.50510481753 ----------
[1/100] Training loss: 0.08344877
[2/100] Training loss: 0.04337050
[3/100] Training loss: 0.03880580
[4/100] Training loss: 0.03297400
[5/100] Training loss: 0.02943815
[6/100] Training loss: 0.02863261
[7/100] Training loss: 0.02600978
[8/100] Training loss: 0.02563266
[9/100] Training loss: 0.02485252
[10/100] Training loss: 0.02353571
[50/100] Training loss: 0.01486322
[100/100] Training loss: 0.01201337
---batch_size---: 4 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19559.095275600044 ----------
[1/200] Training loss: 0.16022809
[2/200] Training loss: 0.06244569
[3/200] Training loss: 0.05342196
[4/200] Training loss: 0.05169186
[5/200] Training loss: 0.04835824
[6/200] Training loss: 0.04484453
[7/200] Training loss: 0.04095810
[8/200] Training loss: 0.03833443
[9/200] Training loss: 0.03730397
[10/200] Training loss: 0.03589842
[50/200] Training loss: 0.01804994
[100/200] Training loss: 0.01519506
[150/200] Training loss: 0.01365247
[200/200] Training loss: 0.01230127
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5500.043999824002 ----------
[1/200] Training loss: 0.12584799
[2/200] Training loss: 0.05342506
[3/200] Training loss: 0.04787541
[4/200] Training loss: 0.04858890
[5/200] Training loss: 0.04288060
[6/200] Training loss: 0.03807357
[7/200] Training loss: 0.03570537
[8/200] Training loss: 0.03377070
[9/200] Training loss: 0.03248417
[10/200] Training loss: 0.03089697
[50/200] Training loss: 0.01907399
[100/200] Training loss: 0.01606167
[150/200] Training loss: 0.01454124
[200/200] Training loss: 0.01292279
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22442.615177380732 ----------
[1/200] Training loss: 0.13686534
[2/200] Training loss: 0.05604680
[3/200] Training loss: 0.05307986
[4/200] Training loss: 0.04793812
[5/200] Training loss: 0.04658028
[6/200] Training loss: 0.04371812
[7/200] Training loss: 0.04093969
[8/200] Training loss: 0.03707373
[9/200] Training loss: 0.03275479
[10/200] Training loss: 0.03056937
[50/200] Training loss: 0.01805126
[100/200] Training loss: 0.01590688
[150/200] Training loss: 0.01469465
[200/200] Training loss: 0.01360976
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19800.52847779574 ----------
[1/200] Training loss: 0.13228595
[2/200] Training loss: 0.05885029
[3/200] Training loss: 0.05166392
[4/200] Training loss: 0.05037174
[5/200] Training loss: 0.04826892
[6/200] Training loss: 0.04246234
[7/200] Training loss: 0.04366150
[8/200] Training loss: 0.03982439
[9/200] Training loss: 0.03580412
[10/200] Training loss: 0.03538142
[50/200] Training loss: 0.01666108
[100/200] Training loss: 0.01412072
[150/200] Training loss: 0.01275930
[200/200] Training loss: 0.01179892
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11519.313521212973 ----------
[1/200] Training loss: 0.11021054
[2/200] Training loss: 0.05574678
[3/200] Training loss: 0.04914708
[4/200] Training loss: 0.04361630
[5/200] Training loss: 0.04235912
[6/200] Training loss: 0.03990604
[7/200] Training loss: 0.03755213
[8/200] Training loss: 0.03460771
[9/200] Training loss: 0.03239858
[10/200] Training loss: 0.03173582
[50/200] Training loss: 0.01730781
[100/200] Training loss: 0.01462754
[150/200] Training loss: 0.01433022
[200/200] Training loss: 0.01294575
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5963.15118037435 ----------
[1/200] Training loss: 0.17514738
[2/200] Training loss: 0.06715917
[3/200] Training loss: 0.05631201
[4/200] Training loss: 0.04973447
[5/200] Training loss: 0.04536383
[6/200] Training loss: 0.04048423
[7/200] Training loss: 0.03930362
[8/200] Training loss: 0.03782629
[9/200] Training loss: 0.03713150
[10/200] Training loss: 0.03561525
[50/200] Training loss: 0.01896452
[100/200] Training loss: 0.01667528
[150/200] Training loss: 0.01457163
[200/200] Training loss: 0.01383799
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8687.333768193783 ----------
[1/200] Training loss: 0.15123427
[2/200] Training loss: 0.05911188
[3/200] Training loss: 0.05413542
[4/200] Training loss: 0.05128689
[5/200] Training loss: 0.04639026
[6/200] Training loss: 0.04522215
[7/200] Training loss: 0.04327617
[8/200] Training loss: 0.04130955
[9/200] Training loss: 0.03734529
[10/200] Training loss: 0.03593097
[50/200] Training loss: 0.01683428
[100/200] Training loss: 0.01482688
[150/200] Training loss: 0.01318320
[200/200] Training loss: 0.01224109
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13183.716016358969 ----------
[1/200] Training loss: 0.14220070
[2/200] Training loss: 0.05633331
[3/200] Training loss: 0.05166728
[4/200] Training loss: 0.04836115
[5/200] Training loss: 0.04509274
[6/200] Training loss: 0.04412351
[7/200] Training loss: 0.04424746
[8/200] Training loss: 0.03762924
[9/200] Training loss: 0.03883833
[10/200] Training loss: 0.03545306
[50/200] Training loss: 0.01779838
[100/200] Training loss: 0.01425875
[150/200] Training loss: 0.01358884
[200/200] Training loss: 0.01209767
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22425.14160490408 ----------
[1/200] Training loss: 0.16649778
[2/200] Training loss: 0.05666841
[3/200] Training loss: 0.05271584
[4/200] Training loss: 0.04724585
[5/200] Training loss: 0.04425531
[6/200] Training loss: 0.04366451
[7/200] Training loss: 0.04040277
[8/200] Training loss: 0.03668987
[9/200] Training loss: 0.03359030
[10/200] Training loss: 0.03514953
[50/200] Training loss: 0.01915359
[100/200] Training loss: 0.01593416
[150/200] Training loss: 0.01420399
[200/200] Training loss: 0.01253975
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7328.776978459639 ----------
[1/200] Training loss: 0.16329643
[2/200] Training loss: 0.06106746
[3/200] Training loss: 0.05583819
[4/200] Training loss: 0.05311308
[5/200] Training loss: 0.05042251
[6/200] Training loss: 0.04813803
[7/200] Training loss: 0.04536254
[8/200] Training loss: 0.04477578
[9/200] Training loss: 0.04380093
[10/200] Training loss: 0.03726139
[50/200] Training loss: 0.01776818
[100/200] Training loss: 0.01430081
[150/200] Training loss: 0.01327168
[200/200] Training loss: 0.01215317
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13564.086110018618 ----------
[1/200] Training loss: 0.12203233
[2/200] Training loss: 0.05258362
[3/200] Training loss: 0.04790316
[4/200] Training loss: 0.04534255
[5/200] Training loss: 0.04157351
[6/200] Training loss: 0.04046347
[7/200] Training loss: 0.03859323
[8/200] Training loss: 0.03609084
[9/200] Training loss: 0.03498208
[10/200] Training loss: 0.03419721
[50/200] Training loss: 0.01695894
[100/200] Training loss: 0.01468566
[150/200] Training loss: 0.01319035
[200/200] Training loss: 0.01135525
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8254.563828573864 ----------
[1/200] Training loss: 0.12225441
[2/200] Training loss: 0.05741844
[3/200] Training loss: 0.05050345
[4/200] Training loss: 0.04909485
[5/200] Training loss: 0.04697602
[6/200] Training loss: 0.04307234
[7/200] Training loss: 0.04132437
[8/200] Training loss: 0.04118083
[9/200] Training loss: 0.03933501
[10/200] Training loss: 0.03658256
[50/200] Training loss: 0.01815600
[100/200] Training loss: 0.01411509
[150/200] Training loss: 0.01296850
[200/200] Training loss: 0.01181913
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10150.066797809757 ----------
[1/200] Training loss: 0.13089396
[2/200] Training loss: 0.05732367
[3/200] Training loss: 0.05191014
[4/200] Training loss: 0.04898764
[5/200] Training loss: 0.04474634
[6/200] Training loss: 0.04205837
[7/200] Training loss: 0.03804731
[8/200] Training loss: 0.03677504
[9/200] Training loss: 0.03463071
[10/200] Training loss: 0.02931579
[50/200] Training loss: 0.01607473
[100/200] Training loss: 0.01408489
[150/200] Training loss: 0.01305358
[200/200] Training loss: 0.01204624
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9351.529072830817 ----------
[1/200] Training loss: 0.16904567
[2/200] Training loss: 0.06117298
[3/200] Training loss: 0.05164826
[4/200] Training loss: 0.04952693
[5/200] Training loss: 0.04844186
[6/200] Training loss: 0.04252595
[7/200] Training loss: 0.04124501
[8/200] Training loss: 0.03964069
[9/200] Training loss: 0.03959288
[10/200] Training loss: 0.03770183
[50/200] Training loss: 0.01921433
[100/200] Training loss: 0.01658107
[150/200] Training loss: 0.01317306
[200/200] Training loss: 0.01263217
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7829.973180030695 ----------
[1/200] Training loss: 0.20593975
[2/200] Training loss: 0.05956465
[3/200] Training loss: 0.05448340
[4/200] Training loss: 0.05292312
[5/200] Training loss: 0.05146693
[6/200] Training loss: 0.05110015
[7/200] Training loss: 0.04988338
[8/200] Training loss: 0.05171613
[9/200] Training loss: 0.04833294
[10/200] Training loss: 0.04993341
[50/200] Training loss: 0.03583104
[100/200] Training loss: 0.02868534
[150/200] Training loss: 0.02231344
[200/200] Training loss: 0.02076189
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: SGD ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15417.104786567419 ----------
[1/200] Training loss: 0.17275671
[2/200] Training loss: 0.05768692
[3/200] Training loss: 0.05072151
[4/200] Training loss: 0.04723251
[5/200] Training loss: 0.04500127
[6/200] Training loss: 0.04299639
[7/200] Training loss: 0.03883444
[8/200] Training loss: 0.03804515
[9/200] Training loss: 0.03510868
[10/200] Training loss: 0.03379250
[50/200] Training loss: 0.01791967
[100/200] Training loss: 0.01608676
[150/200] Training loss: 0.01481106
[200/200] Training loss: 0.01392792
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12912.138784879908 ----------
[1/200] Training loss: 0.14181249
[2/200] Training loss: 0.05590834
[3/200] Training loss: 0.04964592
[4/200] Training loss: 0.04725623
[5/200] Training loss: 0.04079486
[6/200] Training loss: 0.03964688
[7/200] Training loss: 0.03785640
[8/200] Training loss: 0.03276053
[9/200] Training loss: 0.03097240
[10/200] Training loss: 0.03140763
[50/200] Training loss: 0.01847658
[100/200] Training loss: 0.01490260
[150/200] Training loss: 0.01365054
[200/200] Training loss: 0.01296984
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9825.541817121333 ----------
[1/200] Training loss: 0.18919266
[2/200] Training loss: 0.06413112
[3/200] Training loss: 0.05931836
[4/200] Training loss: 0.05270534
[5/200] Training loss: 0.05002238
[6/200] Training loss: 0.04955013
[7/200] Training loss: 0.04629230
[8/200] Training loss: 0.04397698
[9/200] Training loss: 0.04178748
[10/200] Training loss: 0.04149731
[50/200] Training loss: 0.01769618
[100/200] Training loss: 0.01551922
[150/200] Training loss: 0.01462444
[200/200] Training loss: 0.01407137
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17224.39758017679 ----------
[1/200] Training loss: 0.18230143
[2/200] Training loss: 0.06777580
[3/200] Training loss: 0.05807124
[4/200] Training loss: 0.05463493
[5/200] Training loss: 0.05191591
[6/200] Training loss: 0.04949060
[7/200] Training loss: 0.04320423
[8/200] Training loss: 0.03794529
[9/200] Training loss: 0.03756399
[10/200] Training loss: 0.03493875
[50/200] Training loss: 0.01871288
[100/200] Training loss: 0.01664703
[150/200] Training loss: 0.01509377
[200/200] Training loss: 0.01435539
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14397.420880143776 ----------
[1/200] Training loss: 0.17424958
[2/200] Training loss: 0.06174439
[3/200] Training loss: 0.05180681
[4/200] Training loss: 0.05196945
[5/200] Training loss: 0.04713050
[6/200] Training loss: 0.04315086
[7/200] Training loss: 0.04284410
[8/200] Training loss: 0.03946399
[9/200] Training loss: 0.03904821
[10/200] Training loss: 0.03867823
[50/200] Training loss: 0.01909860
[100/200] Training loss: 0.01688641
[150/200] Training loss: 0.01500473
[200/200] Training loss: 0.01445552
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12899.618599012918 ----------
[1/200] Training loss: 0.16207958
[2/200] Training loss: 0.05911752
[3/200] Training loss: 0.05364229
[4/200] Training loss: 0.05055525
[5/200] Training loss: 0.04617039
[6/200] Training loss: 0.04641488
[7/200] Training loss: 0.04485064
[8/200] Training loss: 0.04221329
[9/200] Training loss: 0.04271987
[10/200] Training loss: 0.03825164
[50/200] Training loss: 0.01850337
[100/200] Training loss: 0.01586684
[150/200] Training loss: 0.01522125
[200/200] Training loss: 0.01372594
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20403.865516122183 ----------
[1/200] Training loss: 0.14323747
[2/200] Training loss: 0.05851507
[3/200] Training loss: 0.04950412
[4/200] Training loss: 0.04723204
[5/200] Training loss: 0.04361418
[6/200] Training loss: 0.04138319
[7/200] Training loss: 0.03742026
[8/200] Training loss: 0.03584016
[9/200] Training loss: 0.03452216
[10/200] Training loss: 0.03169281
[50/200] Training loss: 0.01914041
[100/200] Training loss: 0.01500735
[150/200] Training loss: 0.01404208
[200/200] Training loss: 0.01324131
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17975.206034980518 ----------
[1/200] Training loss: 0.17569517
[2/200] Training loss: 0.05448515
[3/200] Training loss: 0.05130857
[4/200] Training loss: 0.04231106
[5/200] Training loss: 0.03945961
[6/200] Training loss: 0.03651070
[7/200] Training loss: 0.03410354
[8/200] Training loss: 0.02927603
[9/200] Training loss: 0.03030120
[10/200] Training loss: 0.02982066
[50/200] Training loss: 0.01751525
[100/200] Training loss: 0.01556987
[150/200] Training loss: 0.01433251
[200/200] Training loss: 0.01310602
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15202.558205775764 ----------
[1/200] Training loss: 0.11849542
[2/200] Training loss: 0.05233376
[3/200] Training loss: 0.05026129
[4/200] Training loss: 0.04384470
[5/200] Training loss: 0.04289139
[6/200] Training loss: 0.03784940
[7/200] Training loss: 0.03573929
[8/200] Training loss: 0.03307440
[9/200] Training loss: 0.02912525
[10/200] Training loss: 0.02689219
[50/200] Training loss: 0.01713850
[100/200] Training loss: 0.01427784
[150/200] Training loss: 0.01310094
[200/200] Training loss: 0.01187825
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14488.555483553217 ----------
[1/200] Training loss: 0.15343067
[2/200] Training loss: 0.05973095
[3/200] Training loss: 0.05251431
[4/200] Training loss: 0.04867487
[5/200] Training loss: 0.04614835
[6/200] Training loss: 0.04268862
[7/200] Training loss: 0.04010896
[8/200] Training loss: 0.03788783
[9/200] Training loss: 0.03387749
[10/200] Training loss: 0.03393570
[50/200] Training loss: 0.01849323
[100/200] Training loss: 0.01469488
[150/200] Training loss: 0.01317760
[200/200] Training loss: 0.01094176
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23805.34259362801 ----------
[1/200] Training loss: 0.14042429
[2/200] Training loss: 0.05997666
[3/200] Training loss: 0.05135916
[4/200] Training loss: 0.04968004
[5/200] Training loss: 0.04593226
[6/200] Training loss: 0.04379872
[7/200] Training loss: 0.04250835
[8/200] Training loss: 0.04057641
[9/200] Training loss: 0.03848604
[10/200] Training loss: 0.03487416
[50/200] Training loss: 0.01861990
[100/200] Training loss: 0.01554759
[150/200] Training loss: 0.01477596
[200/200] Training loss: 0.01368242
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11884.529102997729 ----------
[1/200] Training loss: 0.18000281
[2/200] Training loss: 0.06349306
[3/200] Training loss: 0.05378070
[4/200] Training loss: 0.05109472
[5/200] Training loss: 0.04526736
[6/200] Training loss: 0.04483027
[7/200] Training loss: 0.04381789
[8/200] Training loss: 0.04029015
[9/200] Training loss: 0.03803305
[10/200] Training loss: 0.03929612
[50/200] Training loss: 0.01707236
[100/200] Training loss: 0.01507657
[150/200] Training loss: 0.01323014
[200/200] Training loss: 0.01298951
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6201.6720326054 ----------
[1/200] Training loss: 0.15193475
[2/200] Training loss: 0.05921720
[3/200] Training loss: 0.05008845
[4/200] Training loss: 0.04922709
[5/200] Training loss: 0.04536250
[6/200] Training loss: 0.04183919
[7/200] Training loss: 0.04241854
[8/200] Training loss: 0.03567987
[9/200] Training loss: 0.03662706
[10/200] Training loss: 0.03349307
[50/200] Training loss: 0.01757786
[100/200] Training loss: 0.01483691
[150/200] Training loss: 0.01354474
[200/200] Training loss: 0.01279104
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13108.213608268672 ----------
[1/200] Training loss: 0.18178724
[2/200] Training loss: 0.06493145
[3/200] Training loss: 0.05828313
[4/200] Training loss: 0.05267035
[5/200] Training loss: 0.05355021
[6/200] Training loss: 0.05231734
[7/200] Training loss: 0.04806991
[8/200] Training loss: 0.04708744
[9/200] Training loss: 0.04560739
[10/200] Training loss: 0.04205886
[50/200] Training loss: 0.01700606
[100/200] Training loss: 0.01452074
[150/200] Training loss: 0.01333085
[200/200] Training loss: 0.01301794
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12192.465214221445 ----------
[1/200] Training loss: 0.17065240
[2/200] Training loss: 0.05523637
[3/200] Training loss: 0.05202793
[4/200] Training loss: 0.04931540
[5/200] Training loss: 0.04545840
[6/200] Training loss: 0.04131968
[7/200] Training loss: 0.04152810
[8/200] Training loss: 0.04117554
[9/200] Training loss: 0.03711556
[10/200] Training loss: 0.03570778
[50/200] Training loss: 0.01856916
[100/200] Training loss: 0.01455897
[150/200] Training loss: 0.01412165
[200/200] Training loss: 0.01244791
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4771.204250501125 ----------
[1/200] Training loss: 0.11594734
[2/200] Training loss: 0.05429790
[3/200] Training loss: 0.04854502
[4/200] Training loss: 0.04612558
[5/200] Training loss: 0.04380132
[6/200] Training loss: 0.04073942
[7/200] Training loss: 0.03890891
[8/200] Training loss: 0.03858631
[9/200] Training loss: 0.03614981
[10/200] Training loss: 0.03493149
[50/200] Training loss: 0.01779108
[100/200] Training loss: 0.01614426
[150/200] Training loss: 0.01555926
[200/200] Training loss: 0.01381271
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10026.979206121852 ----------
[1/200] Training loss: 0.15013837
[2/200] Training loss: 0.05688892
[3/200] Training loss: 0.05028192
[4/200] Training loss: 0.04556253
[5/200] Training loss: 0.04181624
[6/200] Training loss: 0.04191833
[7/200] Training loss: 0.04080987
[8/200] Training loss: 0.03579578
[9/200] Training loss: 0.03689702
[10/200] Training loss: 0.03327453
[50/200] Training loss: 0.01758577
[100/200] Training loss: 0.01465402
[150/200] Training loss: 0.01224980
[200/200] Training loss: 0.01151650
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11291.249355142238 ----------
[1/200] Training loss: 0.16196766
[2/200] Training loss: 0.05760882
[3/200] Training loss: 0.05281714
[4/200] Training loss: 0.05103239
[5/200] Training loss: 0.04236103
[6/200] Training loss: 0.04376106
[7/200] Training loss: 0.03735976
[8/200] Training loss: 0.03755264
[9/200] Training loss: 0.03445083
[10/200] Training loss: 0.03176653
[50/200] Training loss: 0.01750660
[100/200] Training loss: 0.01513560
[150/200] Training loss: 0.01380415
[200/200] Training loss: 0.01302458
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11838.138367158917 ----------
[1/200] Training loss: 0.18239931
[2/200] Training loss: 0.06210506
[3/200] Training loss: 0.05279004
[4/200] Training loss: 0.05006950
[5/200] Training loss: 0.04786214
[6/200] Training loss: 0.04692480
[7/200] Training loss: 0.04467895
[8/200] Training loss: 0.04246710
[9/200] Training loss: 0.03886229
[10/200] Training loss: 0.03783223
[50/200] Training loss: 0.02127447
[100/200] Training loss: 0.01687709
[150/200] Training loss: 0.01484179
[200/200] Training loss: 0.01349941
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9703.653332637146 ----------
[1/200] Training loss: 0.16886470
[2/200] Training loss: 0.06640333
[3/200] Training loss: 0.05280077
[4/200] Training loss: 0.04690501
[5/200] Training loss: 0.04718634
[6/200] Training loss: 0.03817718
[7/200] Training loss: 0.03554775
[8/200] Training loss: 0.03137799
[9/200] Training loss: 0.03146515
[10/200] Training loss: 0.02844702
[50/200] Training loss: 0.01726144
[100/200] Training loss: 0.01442840
[150/200] Training loss: 0.01372336
[200/200] Training loss: 0.01261827
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12858.422298244835 ----------
[1/200] Training loss: 0.16397721
[2/200] Training loss: 0.05998473
[3/200] Training loss: 0.04985205
[4/200] Training loss: 0.04979299
[5/200] Training loss: 0.04540230
[6/200] Training loss: 0.04143355
[7/200] Training loss: 0.03953623
[8/200] Training loss: 0.03686686
[9/200] Training loss: 0.03977242
[10/200] Training loss: 0.03293728
[50/200] Training loss: 0.01914589
[100/200] Training loss: 0.01464417
[150/200] Training loss: 0.01292876
[200/200] Training loss: 0.01204832
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17518.86206350173 ----------
[1/200] Training loss: 0.15188979
[2/200] Training loss: 0.05158319
[3/200] Training loss: 0.04535798
[4/200] Training loss: 0.04483829
[5/200] Training loss: 0.04388689
[6/200] Training loss: 0.04345606
[7/200] Training loss: 0.04165561
[8/200] Training loss: 0.03979581
[9/200] Training loss: 0.03896698
[10/200] Training loss: 0.03899865
[50/200] Training loss: 0.02286781
[100/200] Training loss: 0.02097146
[150/200] Training loss: 0.01936082
[200/200] Training loss: 0.01845042
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.0028968250439960685
----FITNESS-----------RMSE---- 17454.30147556756 ----------
[1/200] Training loss: 0.18829250
[2/200] Training loss: 0.05562722
[3/200] Training loss: 0.05168032
[4/200] Training loss: 0.04749574
[5/200] Training loss: 0.04340535
[6/200] Training loss: 0.04010909
[7/200] Training loss: 0.03765323
[8/200] Training loss: 0.03417105
[9/200] Training loss: 0.03597066
[10/200] Training loss: 0.03051314
[50/200] Training loss: 0.01668223
[100/200] Training loss: 0.01443524
[150/200] Training loss: 0.01262894
[200/200] Training loss: 0.01208295
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10097.453936512908 ----------
[1/200] Training loss: 0.17328302
[2/200] Training loss: 0.06365701
[3/200] Training loss: 0.05502590
[4/200] Training loss: 0.05046061
[5/200] Training loss: 0.04567378
[6/200] Training loss: 0.04670439
[7/200] Training loss: 0.03823973
[8/200] Training loss: 0.03654293
[9/200] Training loss: 0.03413860
[10/200] Training loss: 0.03262832
[50/200] Training loss: 0.01755121
[100/200] Training loss: 0.01467789
[150/200] Training loss: 0.01357423
[200/200] Training loss: 0.01219245
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9137.037156540408 ----------
[1/200] Training loss: 0.18199634
[2/200] Training loss: 0.06917766
[3/200] Training loss: 0.05899889
[4/200] Training loss: 0.05558241
[5/200] Training loss: 0.05283217
[6/200] Training loss: 0.04860959
[7/200] Training loss: 0.04440646
[8/200] Training loss: 0.03932251
[9/200] Training loss: 0.03927242
[10/200] Training loss: 0.03341209
[50/200] Training loss: 0.01807360
[100/200] Training loss: 0.01512603
[150/200] Training loss: 0.01371249
[200/200] Training loss: 0.01237669
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17391.824286140887 ----------
[1/200] Training loss: 0.18139207
[2/200] Training loss: 0.06526651
[3/200] Training loss: 0.05735113
[4/200] Training loss: 0.05116305
[5/200] Training loss: 0.04933531
[6/200] Training loss: 0.04803374
[7/200] Training loss: 0.04562438
[8/200] Training loss: 0.03985746
[9/200] Training loss: 0.03715802
[10/200] Training loss: 0.03635593
[50/200] Training loss: 0.01774314
[100/200] Training loss: 0.01410125
[150/200] Training loss: 0.01333007
[200/200] Training loss: 0.01219124
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18695.560114636843 ----------
[1/200] Training loss: 0.17050822
[2/200] Training loss: 0.05860896
[3/200] Training loss: 0.05362984
[4/200] Training loss: 0.05077585
[5/200] Training loss: 0.04881972
[6/200] Training loss: 0.04668867
[7/200] Training loss: 0.04526620
[8/200] Training loss: 0.04290608
[9/200] Training loss: 0.04107274
[10/200] Training loss: 0.03736448
[50/200] Training loss: 0.01761940
[100/200] Training loss: 0.01534055
[150/200] Training loss: 0.01368167
[200/200] Training loss: 0.01189365
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13437.586092747462 ----------
[1/200] Training loss: 0.11934786
[2/200] Training loss: 0.05466934
[3/200] Training loss: 0.04872462
[4/200] Training loss: 0.04438863
[5/200] Training loss: 0.04332871
[6/200] Training loss: 0.04034181
[7/200] Training loss: 0.03629794
[8/200] Training loss: 0.03278100
[9/200] Training loss: 0.03491995
[10/200] Training loss: 0.03172464
[50/200] Training loss: 0.01749980
[100/200] Training loss: 0.01412171
[150/200] Training loss: 0.01354442
[200/200] Training loss: 0.01246783
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10556.224703936536 ----------
[1/200] Training loss: 0.16990613
[2/200] Training loss: 0.05808123
[3/200] Training loss: 0.05566988
[4/200] Training loss: 0.05000983
[5/200] Training loss: 0.05014103
[6/200] Training loss: 0.04822178
[7/200] Training loss: 0.04365152
[8/200] Training loss: 0.04207871
[9/200] Training loss: 0.04028691
[10/200] Training loss: 0.03875798
[50/200] Training loss: 0.01765108
[100/200] Training loss: 0.01535068
[150/200] Training loss: 0.01340134
[200/200] Training loss: 0.01281165
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16491.784136351045 ----------
[1/200] Training loss: 0.14596058
[2/200] Training loss: 0.06071954
[3/200] Training loss: 0.05169544
[4/200] Training loss: 0.05280614
[5/200] Training loss: 0.04766974
[6/200] Training loss: 0.04330602
[7/200] Training loss: 0.04075853
[8/200] Training loss: 0.03600818
[9/200] Training loss: 0.03430837
[10/200] Training loss: 0.03409480
[50/200] Training loss: 0.01592914
[100/200] Training loss: 0.01419670
[150/200] Training loss: 0.01281969
[200/200] Training loss: 0.01215508
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10616.079502339835 ----------
[1/200] Training loss: 0.13743276
[2/200] Training loss: 0.05112814
[3/200] Training loss: 0.04627453
[4/200] Training loss: 0.04539296
[5/200] Training loss: 0.04052492
[6/200] Training loss: 0.03948432
[7/200] Training loss: 0.03439789
[8/200] Training loss: 0.03576531
[9/200] Training loss: 0.03196067
[10/200] Training loss: 0.02990018
[50/200] Training loss: 0.01675599
[100/200] Training loss: 0.01345199
[150/200] Training loss: 0.01225721
[200/200] Training loss: 0.01192062
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7373.328691981662 ----------
[1/200] Training loss: 0.17124198
[2/200] Training loss: 0.05985897
[3/200] Training loss: 0.05755877
[4/200] Training loss: 0.04941152
[5/200] Training loss: 0.04811672
[6/200] Training loss: 0.04701076
[7/200] Training loss: 0.04166736
[8/200] Training loss: 0.04356133
[9/200] Training loss: 0.03892321
[10/200] Training loss: 0.03509864
[50/200] Training loss: 0.01846289
[100/200] Training loss: 0.01510858
[150/200] Training loss: 0.01374545
[200/200] Training loss: 0.01330486
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16215.193862547558 ----------
[1/200] Training loss: 0.17006178
[2/200] Training loss: 0.05480267
[3/200] Training loss: 0.05408217
[4/200] Training loss: 0.04956768
[5/200] Training loss: 0.04794425
[6/200] Training loss: 0.04435410
[7/200] Training loss: 0.04472768
[8/200] Training loss: 0.04101570
[9/200] Training loss: 0.03748008
[10/200] Training loss: 0.03885168
[50/200] Training loss: 0.01809419
[100/200] Training loss: 0.01563951
[150/200] Training loss: 0.01428009
[200/200] Training loss: 0.01304150
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13008.514134980982 ----------
[1/200] Training loss: 0.14430171
[2/200] Training loss: 0.05120772
[3/200] Training loss: 0.04864752
[4/200] Training loss: 0.04653695
[5/200] Training loss: 0.04345032
[6/200] Training loss: 0.04008547
[7/200] Training loss: 0.03858879
[8/200] Training loss: 0.03752537
[9/200] Training loss: 0.03234313
[10/200] Training loss: 0.03064410
[50/200] Training loss: 0.01770874
[100/200] Training loss: 0.01510081
[150/200] Training loss: 0.01414792
[200/200] Training loss: 0.01237514
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 33524.90274407966 ----------
[1/200] Training loss: 0.14321336
[2/200] Training loss: 0.05356130
[3/200] Training loss: 0.05085398
[4/200] Training loss: 0.04712326
[5/200] Training loss: 0.04326417
[6/200] Training loss: 0.04071673
[7/200] Training loss: 0.03877479
[8/200] Training loss: 0.03547618
[9/200] Training loss: 0.03709286
[10/200] Training loss: 0.03278694
[50/200] Training loss: 0.01795116
[100/200] Training loss: 0.01437100
[150/200] Training loss: 0.01353427
[200/200] Training loss: 0.01239359
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5188.767869157378 ----------
[1/200] Training loss: 0.14158506
[2/200] Training loss: 0.05865320
[3/200] Training loss: 0.05126182
[4/200] Training loss: 0.04748161
[5/200] Training loss: 0.04214755
[6/200] Training loss: 0.04052660
[7/200] Training loss: 0.03648979
[8/200] Training loss: 0.03365171
[9/200] Training loss: 0.03226245
[10/200] Training loss: 0.02961499
[50/200] Training loss: 0.01712992
[100/200] Training loss: 0.01609029
[150/200] Training loss: 0.01446132
[200/200] Training loss: 0.01360706
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14137.097297536011 ----------
[1/200] Training loss: 0.17041573
[2/200] Training loss: 0.06147874
[3/200] Training loss: 0.05507153
[4/200] Training loss: 0.05188722
[5/200] Training loss: 0.05025427
[6/200] Training loss: 0.04735869
[7/200] Training loss: 0.04597781
[8/200] Training loss: 0.04360413
[9/200] Training loss: 0.04188613
[10/200] Training loss: 0.04011842
[50/200] Training loss: 0.01837873
[100/200] Training loss: 0.01599725
[150/200] Training loss: 0.01498087
[200/200] Training loss: 0.01448036
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19068.82523911738 ----------
[1/200] Training loss: 0.14944722
[2/200] Training loss: 0.06273823
[3/200] Training loss: 0.05225520
[4/200] Training loss: 0.04994381
[5/200] Training loss: 0.04436787
[6/200] Training loss: 0.04334768
[7/200] Training loss: 0.03886346
[8/200] Training loss: 0.03804328
[9/200] Training loss: 0.03553987
[10/200] Training loss: 0.03451422
[50/200] Training loss: 0.01722740
[100/200] Training loss: 0.01400933
[150/200] Training loss: 0.01308540
[200/200] Training loss: 0.01253458
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.018331189995319217
----FITNESS-----------RMSE---- 42189.927897544454 ----------
[1/200] Training loss: 0.15279708
[2/200] Training loss: 0.06016687
[3/200] Training loss: 0.04922457
[4/200] Training loss: 0.04821156
[5/200] Training loss: 0.04322368
[6/200] Training loss: 0.04302656
[7/200] Training loss: 0.04138423
[8/200] Training loss: 0.03690820
[9/200] Training loss: 0.03853976
[10/200] Training loss: 0.03380675
[50/200] Training loss: 0.01656635
[100/200] Training loss: 0.01348431
[150/200] Training loss: 0.01325742
[200/200] Training loss: 0.01266831
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14608.516967851323 ----------
[1/200] Training loss: 0.14670109
[2/200] Training loss: 0.05759907
[3/200] Training loss: 0.05107397
[4/200] Training loss: 0.05067127
[5/200] Training loss: 0.05012562
[6/200] Training loss: 0.04628064
[7/200] Training loss: 0.04338112
[8/200] Training loss: 0.04337271
[9/200] Training loss: 0.04265073
[10/200] Training loss: 0.03714579
[50/200] Training loss: 0.01827602
[100/200] Training loss: 0.01522521
[150/200] Training loss: 0.01270947
[200/200] Training loss: 0.01208396
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7396.970190557753 ----------
[1/200] Training loss: 0.16756901
[2/200] Training loss: 0.06014758
[3/200] Training loss: 0.05137289
[4/200] Training loss: 0.04915292
[5/200] Training loss: 0.04537202
[6/200] Training loss: 0.04463253
[7/200] Training loss: 0.04195393
[8/200] Training loss: 0.03993677
[9/200] Training loss: 0.03827096
[10/200] Training loss: 0.03500817
[50/200] Training loss: 0.01718455
[100/200] Training loss: 0.01529517
[150/200] Training loss: 0.01406403
[200/200] Training loss: 0.01321071
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11011.051175977705 ----------
[1/200] Training loss: 0.18013962
[2/200] Training loss: 0.05809025
[3/200] Training loss: 0.05340129
[4/200] Training loss: 0.04976204
[5/200] Training loss: 0.04908897
[6/200] Training loss: 0.04340960
[7/200] Training loss: 0.04090399
[8/200] Training loss: 0.04093434
[9/200] Training loss: 0.03525437
[10/200] Training loss: 0.03594226
[50/200] Training loss: 0.01732694
[100/200] Training loss: 0.01462447
[150/200] Training loss: 0.01405098
[200/200] Training loss: 0.01308755
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10441.675344502912 ----------
[1/200] Training loss: 0.13245094
[2/200] Training loss: 0.05550592
[3/200] Training loss: 0.05273529
[4/200] Training loss: 0.04689703
[5/200] Training loss: 0.04405711
[6/200] Training loss: 0.04223068
[7/200] Training loss: 0.04042045
[8/200] Training loss: 0.03965449
[9/200] Training loss: 0.03784132
[10/200] Training loss: 0.03633394
[50/200] Training loss: 0.01851868
[100/200] Training loss: 0.01636309
[150/200] Training loss: 0.01415940
[200/200] Training loss: 0.01298587
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21276.886990347062 ----------
[1/200] Training loss: 0.14400248
[2/200] Training loss: 0.06226455
[3/200] Training loss: 0.05664071
[4/200] Training loss: 0.05243991
[5/200] Training loss: 0.05153275
[6/200] Training loss: 0.04887863
[7/200] Training loss: 0.04716634
[8/200] Training loss: 0.04416348
[9/200] Training loss: 0.04325240
[10/200] Training loss: 0.04194227
[50/200] Training loss: 0.01912180
[100/200] Training loss: 0.01561855
[150/200] Training loss: 0.01432375
[200/200] Training loss: 0.01326929
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 3429.259832675267 ----------
[1/200] Training loss: 0.16527773
[2/200] Training loss: 0.05776300
[3/200] Training loss: 0.05662013
[4/200] Training loss: 0.05083255
[5/200] Training loss: 0.04608757
[6/200] Training loss: 0.04278548
[7/200] Training loss: 0.03999502
[8/200] Training loss: 0.03624080
[9/200] Training loss: 0.03484624
[10/200] Training loss: 0.03239314
[50/200] Training loss: 0.01869621
[100/200] Training loss: 0.01612379
[150/200] Training loss: 0.01463211
[200/200] Training loss: 0.01270630
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18670.834582310454 ----------
[1/200] Training loss: 0.02619037
[2/200] Training loss: 0.00193977
[3/200] Training loss: 0.00161751
[4/200] Training loss: 0.00129557
[5/200] Training loss: 0.00102995
[6/200] Training loss: 0.00077164
[7/200] Training loss: 0.00066011
[8/200] Training loss: 0.00064982
[9/200] Training loss: 0.00048310
[10/200] Training loss: 0.00047020
[50/200] Training loss: 0.00024098
[100/200] Training loss: 0.00019316
[150/200] Training loss: 0.00017115
[200/200] Training loss: 0.00016088
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15032.45342583838 ----------
[1/200] Training loss: 0.17680922
[2/200] Training loss: 0.05671938
[3/200] Training loss: 0.05020051
[4/200] Training loss: 0.04422296
[5/200] Training loss: 0.04080872
[6/200] Training loss: 0.03824440
[7/200] Training loss: 0.03618964
[8/200] Training loss: 0.03380246
[9/200] Training loss: 0.03169211
[10/200] Training loss: 0.03117298
[50/200] Training loss: 0.01618106
[100/200] Training loss: 0.01419441
[150/200] Training loss: 0.01298195
[200/200] Training loss: 0.01208838
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.02873359962507423
----FITNESS-----------RMSE---- 6816.862621470378 ----------
[1/200] Training loss: 0.14741246
[2/200] Training loss: 0.06186817
[3/200] Training loss: 0.05271161
[4/200] Training loss: 0.04837398
[5/200] Training loss: 0.04457144
[6/200] Training loss: 0.04511271
[7/200] Training loss: 0.04114136
[8/200] Training loss: 0.03839742
[9/200] Training loss: 0.03885112
[10/200] Training loss: 0.03449151
[50/200] Training loss: 0.01739261
[100/200] Training loss: 0.01428571
[150/200] Training loss: 0.01352977
[200/200] Training loss: 0.01250903
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4761.3141043203605 ----------
[1/200] Training loss: 0.18027519
[2/200] Training loss: 0.06063590
[3/200] Training loss: 0.05316811
[4/200] Training loss: 0.05280112
[5/200] Training loss: 0.04962870
[6/200] Training loss: 0.04885299
[7/200] Training loss: 0.04373240
[8/200] Training loss: 0.04211045
[9/200] Training loss: 0.04289801
[10/200] Training loss: 0.03764485
[50/200] Training loss: 0.02024610
[100/200] Training loss: 0.01759114
[150/200] Training loss: 0.01603333
[200/200] Training loss: 0.01482975
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15759.351763318185 ----------
[1/200] Training loss: 0.16446787
[2/200] Training loss: 0.05841650
[3/200] Training loss: 0.05319682
[4/200] Training loss: 0.04747817
[5/200] Training loss: 0.04623052
[6/200] Training loss: 0.04172673
[7/200] Training loss: 0.03656438
[8/200] Training loss: 0.03368234
[9/200] Training loss: 0.03340752
[10/200] Training loss: 0.03269860
[50/200] Training loss: 0.01776667
[100/200] Training loss: 0.01440559
[150/200] Training loss: 0.01302358
[200/200] Training loss: 0.01264186
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13581.232344673292 ----------
[1/200] Training loss: 0.16570889
[2/200] Training loss: 0.06400110
[3/200] Training loss: 0.05005063
[4/200] Training loss: 0.04891369
[5/200] Training loss: 0.04550489
[6/200] Training loss: 0.03839575
[7/200] Training loss: 0.03686840
[8/200] Training loss: 0.03216483
[9/200] Training loss: 0.03149599
[10/200] Training loss: 0.02907841
[50/200] Training loss: 0.01635408
[100/200] Training loss: 0.01355023
[150/200] Training loss: 0.01277632
[200/200] Training loss: 0.01262684
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18945.435756403178 ----------
[1/200] Training loss: 0.14176113
[2/200] Training loss: 0.05666790
[3/200] Training loss: 0.05257320
[4/200] Training loss: 0.05039195
[5/200] Training loss: 0.04478447
[6/200] Training loss: 0.04403221
[7/200] Training loss: 0.03984850
[8/200] Training loss: 0.03866131
[9/200] Training loss: 0.03598885
[10/200] Training loss: 0.03431902
[50/200] Training loss: 0.02114908
[100/200] Training loss: 0.01735029
[150/200] Training loss: 0.01500241
[200/200] Training loss: 0.01371857
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 27056.19278464729 ----------
[1/200] Training loss: 0.18284966
[2/200] Training loss: 0.05867586
[3/200] Training loss: 0.05008028
[4/200] Training loss: 0.04801410
[5/200] Training loss: 0.04532672
[6/200] Training loss: 0.03913453
[7/200] Training loss: 0.03746681
[8/200] Training loss: 0.03513161
[9/200] Training loss: 0.03297581
[10/200] Training loss: 0.03085049
[50/200] Training loss: 0.01936808
[100/200] Training loss: 0.01621462
[150/200] Training loss: 0.01448203
[200/200] Training loss: 0.01383870
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15605.489290631038 ----------
[1/200] Training loss: 0.16639775
[2/200] Training loss: 0.05624728
[3/200] Training loss: 0.05276646
[4/200] Training loss: 0.04842944
[5/200] Training loss: 0.04651071
[6/200] Training loss: 0.04227014
[7/200] Training loss: 0.03759940
[8/200] Training loss: 0.03771899
[9/200] Training loss: 0.03484608
[10/200] Training loss: 0.03237437
[50/200] Training loss: 0.01721901
[100/200] Training loss: 0.01400714
[150/200] Training loss: 0.01329143
[200/200] Training loss: 0.01220090
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10035.546821175218 ----------
[1/200] Training loss: 0.17379009
[2/200] Training loss: 0.05826519
[3/200] Training loss: 0.05420798
[4/200] Training loss: 0.04656064
[5/200] Training loss: 0.04261347
[6/200] Training loss: 0.03923596
[7/200] Training loss: 0.03711486
[8/200] Training loss: 0.03568723
[9/200] Training loss: 0.03181036
[10/200] Training loss: 0.03144558
[50/200] Training loss: 0.01952480
[100/200] Training loss: 0.01668417
[150/200] Training loss: 0.01525097
[200/200] Training loss: 0.01431953
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11511.833563772541 ----------
[1/200] Training loss: 0.14504697
[2/200] Training loss: 0.06127296
[3/200] Training loss: 0.05438952
[4/200] Training loss: 0.04829971
[5/200] Training loss: 0.04609202
[6/200] Training loss: 0.04508918
[7/200] Training loss: 0.04169041
[8/200] Training loss: 0.03920750
[9/200] Training loss: 0.03722529
[10/200] Training loss: 0.03619357
[50/200] Training loss: 0.01794772
[100/200] Training loss: 0.01531058
[150/200] Training loss: 0.01249115
[200/200] Training loss: 0.01190287
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19815.564791345212 ----------
[1/200] Training loss: 0.16265892
[2/200] Training loss: 0.05650818
[3/200] Training loss: 0.04911515
[4/200] Training loss: 0.04388437
[5/200] Training loss: 0.04533617
[6/200] Training loss: 0.03901561
[7/200] Training loss: 0.03372455
[8/200] Training loss: 0.03827775
[9/200] Training loss: 0.03506442
[10/200] Training loss: 0.03164355
[50/200] Training loss: 0.01733803
[100/200] Training loss: 0.01497374
[150/200] Training loss: 0.01304914
[200/200] Training loss: 0.01208181
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14040.940709226003 ----------
[1/200] Training loss: 0.16958716
[2/200] Training loss: 0.05661529
[3/200] Training loss: 0.05056327
[4/200] Training loss: 0.05065801
[5/200] Training loss: 0.04232455
[6/200] Training loss: 0.04181629
[7/200] Training loss: 0.03537528
[8/200] Training loss: 0.03267072
[9/200] Training loss: 0.03277316
[10/200] Training loss: 0.03182170
[50/200] Training loss: 0.01758718
[100/200] Training loss: 0.01547287
[150/200] Training loss: 0.01377484
[200/200] Training loss: 0.01233046
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19803.645724966907 ----------
[1/200] Training loss: 0.14614760
[2/200] Training loss: 0.05946803
[3/200] Training loss: 0.05704100
[4/200] Training loss: 0.05291744
[5/200] Training loss: 0.04903274
[6/200] Training loss: 0.04836029
[7/200] Training loss: 0.04414102
[8/200] Training loss: 0.04048107
[9/200] Training loss: 0.03931644
[10/200] Training loss: 0.03655922
[50/200] Training loss: 0.01997231
[100/200] Training loss: 0.01487403
[150/200] Training loss: 0.01317647
[200/200] Training loss: 0.01231723
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.29992489173655756 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19977.890979780623 ----------
[1/200] Training loss: 0.16238449
[2/200] Training loss: 0.05896177
[3/200] Training loss: 0.05515527
[4/200] Training loss: 0.04979955
[5/200] Training loss: 0.04268194
[6/200] Training loss: 0.04140416
[7/200] Training loss: 0.03839485
[8/200] Training loss: 0.03697943
[9/200] Training loss: 0.03539343
[10/200] Training loss: 0.03272366
[50/200] Training loss: 0.01950927
[100/200] Training loss: 0.01695517
[150/200] Training loss: 0.01584095
[200/200] Training loss: 0.01436903
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14657.255950552273 ----------
[1/200] Training loss: 0.16745388
[2/200] Training loss: 0.06709902
[3/200] Training loss: 0.05836668
[4/200] Training loss: 0.05235848
[5/200] Training loss: 0.04903301
[6/200] Training loss: 0.04532496
[7/200] Training loss: 0.04386669
[8/200] Training loss: 0.04173172
[9/200] Training loss: 0.03940789
[10/200] Training loss: 0.03665817
[50/200] Training loss: 0.01785984
[100/200] Training loss: 0.01459059
[150/200] Training loss: 0.01361215
[200/200] Training loss: 0.01192671
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 37669.66312565059 ----------
[1/200] Training loss: 0.15669035
[2/200] Training loss: 0.05515719
[3/200] Training loss: 0.04512670
[4/200] Training loss: 0.04634130
[5/200] Training loss: 0.04445845
[6/200] Training loss: 0.04021519
[7/200] Training loss: 0.03656849
[8/200] Training loss: 0.03829307
[9/200] Training loss: 0.03215742
[10/200] Training loss: 0.03158836
[50/200] Training loss: 0.01721600
[100/200] Training loss: 0.01371546
[150/200] Training loss: 0.01258634
[200/200] Training loss: 0.01191917
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8719.89403605342 ----------
[1/200] Training loss: 0.15489783
[2/200] Training loss: 0.06013985
[3/200] Training loss: 0.05328003
[4/200] Training loss: 0.04937422
[5/200] Training loss: 0.04754555
[6/200] Training loss: 0.04106023
[7/200] Training loss: 0.03975173
[8/200] Training loss: 0.03593940
[9/200] Training loss: 0.03326323
[10/200] Training loss: 0.03296133
[50/200] Training loss: 0.01756932
[100/200] Training loss: 0.01451426
[150/200] Training loss: 0.01247345
[200/200] Training loss: 0.01174803
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19735.169824452994 ----------
[1/200] Training loss: 0.13679673
[2/200] Training loss: 0.05493027
[3/200] Training loss: 0.05419712
[4/200] Training loss: 0.04776282
[5/200] Training loss: 0.04159017
[6/200] Training loss: 0.04161972
[7/200] Training loss: 0.04158098
[8/200] Training loss: 0.03915624
[9/200] Training loss: 0.03583181
[10/200] Training loss: 0.03391092
[50/200] Training loss: 0.01736485
[100/200] Training loss: 0.01455604
[150/200] Training loss: 0.01453817
[200/200] Training loss: 0.01339802
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16858.040692797014 ----------
[1/200] Training loss: 0.15809064
[2/200] Training loss: 0.05285257
[3/200] Training loss: 0.05350011
[4/200] Training loss: 0.04644102
[5/200] Training loss: 0.04725526
[6/200] Training loss: 0.04273317
[7/200] Training loss: 0.04158407
[8/200] Training loss: 0.03836924
[9/200] Training loss: 0.03616091
[10/200] Training loss: 0.03639138
[50/200] Training loss: 0.01900844
[100/200] Training loss: 0.01503579
[150/200] Training loss: 0.01347916
[200/200] Training loss: 0.01245364
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5029.833198029533 ----------
[1/200] Training loss: 0.16822800
[2/200] Training loss: 0.05911965
[3/200] Training loss: 0.05578250
[4/200] Training loss: 0.05204751
[5/200] Training loss: 0.04859881
[6/200] Training loss: 0.04711509
[7/200] Training loss: 0.04568213
[8/200] Training loss: 0.04600834
[9/200] Training loss: 0.04136217
[10/200] Training loss: 0.03635672
[50/200] Training loss: 0.01738835
[100/200] Training loss: 0.01414386
[150/200] Training loss: 0.01305949
[200/200] Training loss: 0.01215562
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15722.947306405375 ----------
[1/200] Training loss: 0.15147858
[2/200] Training loss: 0.06041222
[3/200] Training loss: 0.05557757
[4/200] Training loss: 0.04946679
[5/200] Training loss: 0.04435735
[6/200] Training loss: 0.04361794
[7/200] Training loss: 0.03975105
[8/200] Training loss: 0.03892301
[9/200] Training loss: 0.03567198
[10/200] Training loss: 0.03419673
[50/200] Training loss: 0.01763528
[100/200] Training loss: 0.01405629
[150/200] Training loss: 0.01260123
[200/200] Training loss: 0.01142222
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10051.538389719257 ----------
[1/200] Training loss: 0.17975204
[2/200] Training loss: 0.07806873
[3/200] Training loss: 0.05917221
[4/200] Training loss: 0.04974036
[5/200] Training loss: 0.04076899
[6/200] Training loss: 0.03526467
[7/200] Training loss: 0.03651836
[8/200] Training loss: 0.03234300
[9/200] Training loss: 0.03246227
[10/200] Training loss: 0.02970945
[50/200] Training loss: 0.01819451
[100/200] Training loss: 0.01419100
[150/200] Training loss: 0.01290447
[200/200] Training loss: 0.01130593
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 27443.368889405687 ----------
[1/200] Training loss: 0.18206468
[2/200] Training loss: 0.06233014
[3/200] Training loss: 0.06360391
[4/200] Training loss: 0.05699308
[5/200] Training loss: 0.05478312
[6/200] Training loss: 0.05263496
[7/200] Training loss: 0.04898658
[8/200] Training loss: 0.04857911
[9/200] Training loss: 0.04624040
[10/200] Training loss: 0.04265463
[50/200] Training loss: 0.01900456
[100/200] Training loss: 0.01709324
[150/200] Training loss: 0.01469029
[200/200] Training loss: 0.01372971
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22207.44091515274 ----------
[1/200] Training loss: 0.19976607
[2/200] Training loss: 0.06441239
[3/200] Training loss: 0.05553181
[4/200] Training loss: 0.04948580
[5/200] Training loss: 0.04793746
[6/200] Training loss: 0.04383830
[7/200] Training loss: 0.04289906
[8/200] Training loss: 0.04090048
[9/200] Training loss: 0.03779853
[10/200] Training loss: 0.03600526
[50/200] Training loss: 0.01805248
[100/200] Training loss: 0.01337586
[150/200] Training loss: 0.01327328
[200/200] Training loss: 0.01244013
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14055.612971336397 ----------
[1/200] Training loss: 0.18167347
[2/200] Training loss: 0.06701653
[3/200] Training loss: 0.05698371
[4/200] Training loss: 0.05442501
[5/200] Training loss: 0.04990758
[6/200] Training loss: 0.04328494
[7/200] Training loss: 0.04046376
[8/200] Training loss: 0.03873801
[9/200] Training loss: 0.03795628
[10/200] Training loss: 0.03406575
[50/200] Training loss: 0.01734228
[100/200] Training loss: 0.01549393
[150/200] Training loss: 0.01406051
[200/200] Training loss: 0.01363052
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13890.501502825591 ----------
[1/200] Training loss: 0.15489278
[2/200] Training loss: 0.05678096
[3/200] Training loss: 0.05124389
[4/200] Training loss: 0.04741848
[5/200] Training loss: 0.04472611
[6/200] Training loss: 0.04011876
[7/200] Training loss: 0.03929798
[8/200] Training loss: 0.03614631
[9/200] Training loss: 0.03292314
[10/200] Training loss: 0.03177938
[50/200] Training loss: 0.01819115
[100/200] Training loss: 0.01410592
[150/200] Training loss: 0.01272113
[200/200] Training loss: 0.01138086
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.02873359962507423
----FITNESS-----------RMSE---- 23579.859880838987 ----------
[1/200] Training loss: 0.19584544
[2/200] Training loss: 0.06268369
[3/200] Training loss: 0.05924550
[4/200] Training loss: 0.05644295
[5/200] Training loss: 0.04898733
[6/200] Training loss: 0.04984665
[7/200] Training loss: 0.04794639
[8/200] Training loss: 0.04503065
[9/200] Training loss: 0.04177575
[10/200] Training loss: 0.04159949
[50/200] Training loss: 0.02258556
[100/200] Training loss: 0.01658788
[150/200] Training loss: 0.01449639
[200/200] Training loss: 0.01401638
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12187.353445272685 ----------
[1/200] Training loss: 0.13136736
[2/200] Training loss: 0.05588253
[3/200] Training loss: 0.04815051
[4/200] Training loss: 0.04092809
[5/200] Training loss: 0.03967629
[6/200] Training loss: 0.03559308
[7/200] Training loss: 0.03488044
[8/200] Training loss: 0.03171649
[9/200] Training loss: 0.02904647
[10/200] Training loss: 0.02792644
[50/200] Training loss: 0.01648824
[100/200] Training loss: 0.01430457
[150/200] Training loss: 0.01317110
[200/200] Training loss: 0.01212389
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8587.609679066696 ----------
[1/200] Training loss: 0.16267892
[2/200] Training loss: 0.06236296
[3/200] Training loss: 0.05068000
[4/200] Training loss: 0.04941843
[5/200] Training loss: 0.04560774
[6/200] Training loss: 0.04341642
[7/200] Training loss: 0.03925833
[8/200] Training loss: 0.03874125
[9/200] Training loss: 0.03596261
[10/200] Training loss: 0.03022765
[50/200] Training loss: 0.01899756
[100/200] Training loss: 0.01506061
[150/200] Training loss: 0.01431304
[200/200] Training loss: 0.01207855
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10205.716045432579 ----------
[1/200] Training loss: 0.16515134
[2/200] Training loss: 0.06297497
[3/200] Training loss: 0.05292751
[4/200] Training loss: 0.04960555
[5/200] Training loss: 0.04553409
[6/200] Training loss: 0.04385776
[7/200] Training loss: 0.03760233
[8/200] Training loss: 0.03573418
[9/200] Training loss: 0.03668924
[10/200] Training loss: 0.03436460
[50/200] Training loss: 0.01767022
[100/200] Training loss: 0.01532557
[150/200] Training loss: 0.01346063
[200/200] Training loss: 0.01291239
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19896.83391899324 ----------
[1/200] Training loss: 0.17544216
[2/200] Training loss: 0.06469740
[3/200] Training loss: 0.05378397
[4/200] Training loss: 0.05083252
[5/200] Training loss: 0.04758564
[6/200] Training loss: 0.04624470
[7/200] Training loss: 0.04436672
[8/200] Training loss: 0.04122141
[9/200] Training loss: 0.04267620
[10/200] Training loss: 0.03788271
[50/200] Training loss: 0.02041691
[100/200] Training loss: 0.01602337
[150/200] Training loss: 0.01473897
[200/200] Training loss: 0.01329814
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6161.298889033058 ----------
[1/200] Training loss: 0.13683724
[2/200] Training loss: 0.06485584
[3/200] Training loss: 0.05178155
[4/200] Training loss: 0.04960121
[5/200] Training loss: 0.04322966
[6/200] Training loss: 0.03903774
[7/200] Training loss: 0.04047118
[8/200] Training loss: 0.03443637
[9/200] Training loss: 0.03237668
[10/200] Training loss: 0.03124445
[50/200] Training loss: 0.01869929
[100/200] Training loss: 0.01555344
[150/200] Training loss: 0.01455342
[200/200] Training loss: 0.01341080
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7487.128154372676 ----------
[1/200] Training loss: 0.17296876
[2/200] Training loss: 0.06212923
[3/200] Training loss: 0.05153288
[4/200] Training loss: 0.05105738
[5/200] Training loss: 0.04912251
[6/200] Training loss: 0.04546104
[7/200] Training loss: 0.04291819
[8/200] Training loss: 0.04126365
[9/200] Training loss: 0.03803257
[10/200] Training loss: 0.03807251
[50/200] Training loss: 0.01821971
[100/200] Training loss: 0.01614591
[150/200] Training loss: 0.01500999
[200/200] Training loss: 0.01396139
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10527.68920513899 ----------
[1/200] Training loss: 0.13091973
[2/200] Training loss: 0.05196276
[3/200] Training loss: 0.04986153
[4/200] Training loss: 0.04591942
[5/200] Training loss: 0.04361067
[6/200] Training loss: 0.04106867
[7/200] Training loss: 0.03896599
[8/200] Training loss: 0.03297990
[9/200] Training loss: 0.03476577
[10/200] Training loss: 0.03289706
[50/200] Training loss: 0.01813542
[100/200] Training loss: 0.01496334
[150/200] Training loss: 0.01304316
[200/200] Training loss: 0.01222938
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5236.205496349432 ----------
[1/200] Training loss: 0.15652317
[2/200] Training loss: 0.06133912
[3/200] Training loss: 0.05269209
[4/200] Training loss: 0.04899742
[5/200] Training loss: 0.04477846
[6/200] Training loss: 0.04253122
[7/200] Training loss: 0.03998643
[8/200] Training loss: 0.03586087
[9/200] Training loss: 0.03278053
[10/200] Training loss: 0.03123887
[50/200] Training loss: 0.01769244
[100/200] Training loss: 0.01576414
[150/200] Training loss: 0.01443598
[200/200] Training loss: 0.01372853
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20208.799667471594 ----------
[1/200] Training loss: 0.17666231
[2/200] Training loss: 0.05727582
[3/200] Training loss: 0.05404251
[4/200] Training loss: 0.04905870
[5/200] Training loss: 0.04535508
[6/200] Training loss: 0.04376153
[7/200] Training loss: 0.04168206
[8/200] Training loss: 0.04118000
[9/200] Training loss: 0.03857999
[10/200] Training loss: 0.03638601
[50/200] Training loss: 0.01732106
[100/200] Training loss: 0.01512632
[150/200] Training loss: 0.01338624
[200/200] Training loss: 0.01225227
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9900.974901493288 ----------
[1/200] Training loss: 0.14628278
[2/200] Training loss: 0.05897801
[3/200] Training loss: 0.05461438
[4/200] Training loss: 0.05140049
[5/200] Training loss: 0.04915233
[6/200] Training loss: 0.04624556
[7/200] Training loss: 0.04245067
[8/200] Training loss: 0.04137752
[9/200] Training loss: 0.03784133
[10/200] Training loss: 0.03329548
[50/200] Training loss: 0.01651895
[100/200] Training loss: 0.01453990
[150/200] Training loss: 0.01288983
[200/200] Training loss: 0.01145031
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20842.630160322857 ----------
[1/200] Training loss: 0.14590420
[2/200] Training loss: 0.05693908
[3/200] Training loss: 0.05033336
[4/200] Training loss: 0.04787132
[5/200] Training loss: 0.04281919
[6/200] Training loss: 0.03948248
[7/200] Training loss: 0.03782021
[8/200] Training loss: 0.03586188
[9/200] Training loss: 0.03275132
[10/200] Training loss: 0.03360498
[50/200] Training loss: 0.01831617
[100/200] Training loss: 0.01559203
[150/200] Training loss: 0.01401510
[200/200] Training loss: 0.01275093
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18164.48226622493 ----------
[1/200] Training loss: 0.13607038
[2/200] Training loss: 0.05558359
[3/200] Training loss: 0.05472581
[4/200] Training loss: 0.04867639
[5/200] Training loss: 0.04599602
[6/200] Training loss: 0.04360593
[7/200] Training loss: 0.03945853
[8/200] Training loss: 0.03884207
[9/200] Training loss: 0.03210090
[10/200] Training loss: 0.03013290
[50/200] Training loss: 0.01641774
[100/200] Training loss: 0.01421786
[150/200] Training loss: 0.01294934
[200/200] Training loss: 0.01208096
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15669.79463809274 ----------
[1/200] Training loss: 0.16090476
[2/200] Training loss: 0.05740703
[3/200] Training loss: 0.05002767
[4/200] Training loss: 0.05014726
[5/200] Training loss: 0.04404204
[6/200] Training loss: 0.04296171
[7/200] Training loss: 0.03881121
[8/200] Training loss: 0.04020325
[9/200] Training loss: 0.03429891
[10/200] Training loss: 0.03448762
[50/200] Training loss: 0.01852824
[100/200] Training loss: 0.01540585
[150/200] Training loss: 0.01364174
[200/200] Training loss: 0.01273797
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11189.252700694537 ----------
[1/200] Training loss: 0.09281455
[2/200] Training loss: 0.05322151
[3/200] Training loss: 0.05093108
[4/200] Training loss: 0.04793913
[5/200] Training loss: 0.04646912
[6/200] Training loss: 0.04441643
[7/200] Training loss: 0.04503837
[8/200] Training loss: 0.04077166
[9/200] Training loss: 0.03790680
[10/200] Training loss: 0.03753417
[50/200] Training loss: 0.01851747
[100/200] Training loss: 0.01383416
[150/200] Training loss: 0.01277984
[200/200] Training loss: 0.01210777
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01269111532361182
----FITNESS-----------RMSE---- 8235.130600057293 ----------
[1/200] Training loss: 0.15524906
[2/200] Training loss: 0.06294973
[3/200] Training loss: 0.05155403
[4/200] Training loss: 0.04824040
[5/200] Training loss: 0.04495119
[6/200] Training loss: 0.04274135
[7/200] Training loss: 0.04021754
[8/200] Training loss: 0.04167473
[9/200] Training loss: 0.03437560
[10/200] Training loss: 0.03365277
[50/200] Training loss: 0.01765688
[100/200] Training loss: 0.01502450
[150/200] Training loss: 0.01377107
[200/200] Training loss: 0.01234583
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15168.039556910444 ----------
[1/200] Training loss: 0.16971044
[2/200] Training loss: 0.05499539
[3/200] Training loss: 0.05452988
[4/200] Training loss: 0.04760607
[5/200] Training loss: 0.05025140
[6/200] Training loss: 0.04644096
[7/200] Training loss: 0.04468428
[8/200] Training loss: 0.04073138
[9/200] Training loss: 0.03836525
[10/200] Training loss: 0.03640618
[50/200] Training loss: 0.01981214
[100/200] Training loss: 0.01608794
[150/200] Training loss: 0.01424831
[200/200] Training loss: 0.01314348
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6785.8803408253525 ----------
[1/200] Training loss: 0.15289498
[2/200] Training loss: 0.05362191
[3/200] Training loss: 0.04923023
[4/200] Training loss: 0.04481224
[5/200] Training loss: 0.04411029
[6/200] Training loss: 0.04183456
[7/200] Training loss: 0.03879414
[8/200] Training loss: 0.03595984
[9/200] Training loss: 0.03606904
[10/200] Training loss: 0.03341507
[50/200] Training loss: 0.01823278
[100/200] Training loss: 0.01424612
[150/200] Training loss: 0.01266439
[200/200] Training loss: 0.01173581
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4570.675442426425 ----------
[1/200] Training loss: 0.24561079
[2/200] Training loss: 0.06358879
[3/200] Training loss: 0.05103131
[4/200] Training loss: 0.05387508
[5/200] Training loss: 0.05098796
[6/200] Training loss: 0.04798002
[7/200] Training loss: 0.05179057
[8/200] Training loss: 0.04765362
[9/200] Training loss: 0.05040285
[10/200] Training loss: 0.03777340
[50/200] Training loss: 0.01840773
[100/200] Training loss: 0.01485805
[150/200] Training loss: 0.01379030
[200/200] Training loss: 0.01388179
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5971.265862444914 ----------
[1/200] Training loss: 1.29354649
[2/200] Training loss: 0.63314190
[3/200] Training loss: 0.52716624
[4/200] Training loss: 0.49241483
[5/200] Training loss: 0.47143171
[6/200] Training loss: 0.46138820
[7/200] Training loss: 0.45278185
[8/200] Training loss: 0.45551545
[9/200] Training loss: 0.44567770
[10/200] Training loss: 0.45260880
[50/200] Training loss: 0.45265900
[100/200] Training loss: 0.45253286
[150/200] Training loss: 0.45601400
[200/200] Training loss: 0.44682081
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: RMSprop ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22942.3123507636 ----------
[1/200] Training loss: 0.12247468
[2/200] Training loss: 0.05794490
[3/200] Training loss: 0.04786045
[4/200] Training loss: 0.03467875
[5/200] Training loss: 0.03240220
[6/200] Training loss: 0.02842253
[7/200] Training loss: 0.02334926
[8/200] Training loss: 0.02788962
[9/200] Training loss: 0.02740480
[10/200] Training loss: 0.03145581
[50/200] Training loss: 0.01631670
[100/200] Training loss: 0.01277537
[150/200] Training loss: 0.01112912
[200/200] Training loss: 0.00964834
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adamax ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 30403.900802364158 ----------
[1/200] Training loss: 0.15783477
[2/200] Training loss: 0.05931600
[3/200] Training loss: 0.05341634
[4/200] Training loss: 0.05275540
[5/200] Training loss: 0.04689860
[6/200] Training loss: 0.04338104
[7/200] Training loss: 0.04125167
[8/200] Training loss: 0.03784206
[9/200] Training loss: 0.03699962
[10/200] Training loss: 0.03360395
[50/200] Training loss: 0.01733633
[100/200] Training loss: 0.01406961
[150/200] Training loss: 0.01303741
[200/200] Training loss: 0.01230927
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12403.682678946603 ----------
[1/200] Training loss: 0.16748213
[2/200] Training loss: 0.06014577
[3/200] Training loss: 0.05255698
[4/200] Training loss: 0.04806550
[5/200] Training loss: 0.04310505
[6/200] Training loss: 0.04670316
[7/200] Training loss: 0.03825466
[8/200] Training loss: 0.03835859
[9/200] Training loss: 0.03557565
[10/200] Training loss: 0.03555009
[50/200] Training loss: 0.01898682
[100/200] Training loss: 0.01538690
[150/200] Training loss: 0.01398998
[200/200] Training loss: 0.01350868
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 29423.278947119405 ----------
[1/200] Training loss: 0.14275914
[2/200] Training loss: 0.05835591
[3/200] Training loss: 0.04988627
[4/200] Training loss: 0.04890915
[5/200] Training loss: 0.04989416
[6/200] Training loss: 0.04422757
[7/200] Training loss: 0.04018455
[8/200] Training loss: 0.03822653
[9/200] Training loss: 0.03488186
[10/200] Training loss: 0.03280404
[50/200] Training loss: 0.01703383
[100/200] Training loss: 0.01488329
[150/200] Training loss: 0.01354761
[200/200] Training loss: 0.01246376
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 24721.369541350254 ----------
[1/200] Training loss: 0.17482276
[2/200] Training loss: 0.06382910
[3/200] Training loss: 0.04892951
[4/200] Training loss: 0.05031969
[5/200] Training loss: 0.04222642
[6/200] Training loss: 0.04126877
[7/200] Training loss: 0.03765228
[8/200] Training loss: 0.03833933
[9/200] Training loss: 0.03415029
[10/200] Training loss: 0.03237993
[50/200] Training loss: 0.01826055
[100/200] Training loss: 0.01511688
[150/200] Training loss: 0.01370218
[200/200] Training loss: 0.01306570
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8496.636510996572 ----------
[1/200] Training loss: 0.15387999
[2/200] Training loss: 0.06043964
[3/200] Training loss: 0.04919058
[4/200] Training loss: 0.04848275
[5/200] Training loss: 0.04438839
[6/200] Training loss: 0.04237650
[7/200] Training loss: 0.03632344
[8/200] Training loss: 0.03751268
[9/200] Training loss: 0.03271700
[10/200] Training loss: 0.03427495
[50/200] Training loss: 0.01760035
[100/200] Training loss: 0.01518340
[150/200] Training loss: 0.01425002
[200/200] Training loss: 0.01286038
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18308.03364646242 ----------
[1/200] Training loss: 0.18958098
[2/200] Training loss: 0.05888215
[3/200] Training loss: 0.05476786
[4/200] Training loss: 0.05310321
[5/200] Training loss: 0.04878738
[6/200] Training loss: 0.04627710
[7/200] Training loss: 0.04300609
[8/200] Training loss: 0.04081332
[9/200] Training loss: 0.03625037
[10/200] Training loss: 0.03524311
[50/200] Training loss: 0.01844874
[100/200] Training loss: 0.01543240
[150/200] Training loss: 0.01418563
[200/200] Training loss: 0.01249086
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17209.791631510245 ----------
[1/200] Training loss: 0.19411595
[2/200] Training loss: 0.06552231
[3/200] Training loss: 0.05895833
[4/200] Training loss: 0.05606914
[5/200] Training loss: 0.05489511
[6/200] Training loss: 0.05292178
[7/200] Training loss: 0.05142174
[8/200] Training loss: 0.05172230
[9/200] Training loss: 0.04862877
[10/200] Training loss: 0.04714690
[50/200] Training loss: 0.01866305
[100/200] Training loss: 0.01654427
[150/200] Training loss: 0.01513327
[200/200] Training loss: 0.01391151
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15637.840260087069 ----------
[1/200] Training loss: 0.15957243
[2/200] Training loss: 0.06483267
[3/200] Training loss: 0.05628526
[4/200] Training loss: 0.05257383
[5/200] Training loss: 0.04664007
[6/200] Training loss: 0.04664212
[7/200] Training loss: 0.04482077
[8/200] Training loss: 0.03897168
[9/200] Training loss: 0.03548447
[10/200] Training loss: 0.03754617
[50/200] Training loss: 0.01751236
[100/200] Training loss: 0.01336018
[150/200] Training loss: 0.01258446
[200/200] Training loss: 0.01211431
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17521.42459961518 ----------
[1/200] Training loss: 0.15754587
[2/200] Training loss: 0.05801238
[3/200] Training loss: 0.05371603
[4/200] Training loss: 0.04947191
[5/200] Training loss: 0.04585031
[6/200] Training loss: 0.04255425
[7/200] Training loss: 0.04191029
[8/200] Training loss: 0.04213362
[9/200] Training loss: 0.03788803
[10/200] Training loss: 0.03680749
[50/200] Training loss: 0.02257176
[100/200] Training loss: 0.01754178
[150/200] Training loss: 0.01482005
[200/200] Training loss: 0.01412566
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7132.893662462661 ----------
[1/200] Training loss: 0.14484270
[2/200] Training loss: 0.05582728
[3/200] Training loss: 0.04970339
[4/200] Training loss: 0.04678342
[5/200] Training loss: 0.04450477
[6/200] Training loss: 0.04083621
[7/200] Training loss: 0.03754326
[8/200] Training loss: 0.03955988
[9/200] Training loss: 0.03578890
[10/200] Training loss: 0.03130981
[50/200] Training loss: 0.01743420
[100/200] Training loss: 0.01415801
[150/200] Training loss: 0.01305456
[200/200] Training loss: 0.01234016
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17150.45702014964 ----------
[1/200] Training loss: 0.13335975
[2/200] Training loss: 0.06324435
[3/200] Training loss: 0.05439318
[4/200] Training loss: 0.04944262
[5/200] Training loss: 0.04702729
[6/200] Training loss: 0.04344928
[7/200] Training loss: 0.03988900
[8/200] Training loss: 0.03768055
[9/200] Training loss: 0.03540488
[10/200] Training loss: 0.03031414
[50/200] Training loss: 0.01804574
[100/200] Training loss: 0.01504027
[150/200] Training loss: 0.01341005
[200/200] Training loss: 0.01278467
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11603.912443654512 ----------
[1/200] Training loss: 0.25874513
[2/200] Training loss: 0.07254385
[3/200] Training loss: 0.06045289
[4/200] Training loss: 0.05948777
[5/200] Training loss: 0.05067329
[6/200] Training loss: 0.05456910
[7/200] Training loss: 0.05732857
[8/200] Training loss: 0.05371817
[9/200] Training loss: 0.04816190
[10/200] Training loss: 0.04874473
[50/200] Training loss: 0.02128545
[100/200] Training loss: 0.01817520
[150/200] Training loss: 0.01592240
[200/200] Training loss: 0.01513457
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16199.197017136374 ----------
[1/200] Training loss: 0.18874427
[2/200] Training loss: 0.06341712
[3/200] Training loss: 0.05665267
[4/200] Training loss: 0.05218515
[5/200] Training loss: 0.04957873
[6/200] Training loss: 0.04876416
[7/200] Training loss: 0.04760512
[8/200] Training loss: 0.04366799
[9/200] Training loss: 0.03964361
[10/200] Training loss: 0.03844498
[50/200] Training loss: 0.01789509
[100/200] Training loss: 0.01649608
[150/200] Training loss: 0.01522080
[200/200] Training loss: 0.01429276
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9414.199488007464 ----------
[1/200] Training loss: 0.19857930
[2/200] Training loss: 0.06492644
[3/200] Training loss: 0.05217533
[4/200] Training loss: 0.04958084
[5/200] Training loss: 0.04591194
[6/200] Training loss: 0.04356622
[7/200] Training loss: 0.04091227
[8/200] Training loss: 0.03916963
[9/200] Training loss: 0.03622165
[10/200] Training loss: 0.03503272
[50/200] Training loss: 0.01871025
[100/200] Training loss: 0.01557180
[150/200] Training loss: 0.01449181
[200/200] Training loss: 0.01287495
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11624.445965292281 ----------
[1/200] Training loss: 0.12502169
[2/200] Training loss: 0.05795110
[3/200] Training loss: 0.04647387
[4/200] Training loss: 0.04581428
[5/200] Training loss: 0.03958930
[6/200] Training loss: 0.03661769
[7/200] Training loss: 0.03465256
[8/200] Training loss: 0.02945098
[9/200] Training loss: 0.02916471
[10/200] Training loss: 0.02905284
[50/200] Training loss: 0.01661119
[100/200] Training loss: 0.01515344
[150/200] Training loss: 0.01437259
[200/200] Training loss: 0.01334579
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10263.521033251698 ----------
[1/200] Training loss: 0.15025954
[2/200] Training loss: 0.06001809
[3/200] Training loss: 0.05440847
[4/200] Training loss: 0.04836926
[5/200] Training loss: 0.04266257
[6/200] Training loss: 0.04218271
[7/200] Training loss: 0.03918682
[8/200] Training loss: 0.03762963
[9/200] Training loss: 0.04021405
[10/200] Training loss: 0.03539775
[50/200] Training loss: 0.01850247
[100/200] Training loss: 0.01527050
[150/200] Training loss: 0.01400427
[200/200] Training loss: 0.01240273
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12393.435036340812 ----------
[1/200] Training loss: 0.16722608
[2/200] Training loss: 0.06545433
[3/200] Training loss: 0.05790400
[4/200] Training loss: 0.05217136
[5/200] Training loss: 0.05014213
[6/200] Training loss: 0.04897177
[7/200] Training loss: 0.04660521
[8/200] Training loss: 0.04257734
[9/200] Training loss: 0.04149521
[10/200] Training loss: 0.04115263
[50/200] Training loss: 0.01986477
[100/200] Training loss: 0.01491484
[150/200] Training loss: 0.01372505
[200/200] Training loss: 0.01233722
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13281.843245574011 ----------
[1/200] Training loss: 0.17566797
[2/200] Training loss: 0.05811232
[3/200] Training loss: 0.05001649
[4/200] Training loss: 0.04616918
[5/200] Training loss: 0.04454693
[6/200] Training loss: 0.04206564
[7/200] Training loss: 0.03828488
[8/200] Training loss: 0.03289030
[9/200] Training loss: 0.03282923
[10/200] Training loss: 0.03047643
[50/200] Training loss: 0.01907783
[100/200] Training loss: 0.01539264
[150/200] Training loss: 0.01443949
[200/200] Training loss: 0.01340260
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17899.130705148786 ----------
[1/200] Training loss: 0.14051874
[2/200] Training loss: 0.05983802
[3/200] Training loss: 0.05510542
[4/200] Training loss: 0.04781675
[5/200] Training loss: 0.04669836
[6/200] Training loss: 0.04272421
[7/200] Training loss: 0.03794750
[8/200] Training loss: 0.03571863
[9/200] Training loss: 0.03422024
[10/200] Training loss: 0.03319010
[50/200] Training loss: 0.01732066
[100/200] Training loss: 0.01432960
[150/200] Training loss: 0.01333789
[200/200] Training loss: 0.01235585
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18661.425454664495 ----------
[1/200] Training loss: 0.14964620
[2/200] Training loss: 0.05546937
[3/200] Training loss: 0.04950732
[4/200] Training loss: 0.04335516
[5/200] Training loss: 0.04128384
[6/200] Training loss: 0.03731582
[7/200] Training loss: 0.03543294
[8/200] Training loss: 0.03275767
[9/200] Training loss: 0.03065658
[10/200] Training loss: 0.03024627
[50/200] Training loss: 0.01725378
[100/200] Training loss: 0.01449915
[150/200] Training loss: 0.01298461
[200/200] Training loss: 0.01222871
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20108.581252788572 ----------
[1/200] Training loss: 0.19072648
[2/200] Training loss: 0.05834372
[3/200] Training loss: 0.05489343
[4/200] Training loss: 0.05134834
[5/200] Training loss: 0.04699526
[6/200] Training loss: 0.04504096
[7/200] Training loss: 0.04386066
[8/200] Training loss: 0.04123492
[9/200] Training loss: 0.03549352
[10/200] Training loss: 0.03735778
[50/200] Training loss: 0.01825813
[100/200] Training loss: 0.01662284
[150/200] Training loss: 0.01459872
[200/200] Training loss: 0.01326616
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10941.043460292076 ----------
[1/200] Training loss: 0.17048693
[2/200] Training loss: 0.06122920
[3/200] Training loss: 0.05634603
[4/200] Training loss: 0.05421971
[5/200] Training loss: 0.05125937
[6/200] Training loss: 0.04964018
[7/200] Training loss: 0.04619535
[8/200] Training loss: 0.04461175
[9/200] Training loss: 0.04240019
[10/200] Training loss: 0.03831385
[50/200] Training loss: 0.01893171
[100/200] Training loss: 0.01639681
[150/200] Training loss: 0.01399267
[200/200] Training loss: 0.01310921
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16487.7263441628 ----------
[1/200] Training loss: 0.12027320
[2/200] Training loss: 0.05355883
[3/200] Training loss: 0.04709599
[4/200] Training loss: 0.04390952
[5/200] Training loss: 0.04184718
[6/200] Training loss: 0.04077423
[7/200] Training loss: 0.03936531
[8/200] Training loss: 0.03871638
[9/200] Training loss: 0.03493397
[10/200] Training loss: 0.03510713
[50/200] Training loss: 0.01912653
[100/200] Training loss: 0.01495700
[150/200] Training loss: 0.01299314
[200/200] Training loss: 0.01263960
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01269111532361182
----FITNESS-----------RMSE---- 5205.4459943409265 ----------
[1/200] Training loss: 0.15282567
[2/200] Training loss: 0.05607163
[3/200] Training loss: 0.05115811
[4/200] Training loss: 0.04417697
[5/200] Training loss: 0.04153710
[6/200] Training loss: 0.03418217
[7/200] Training loss: 0.03276949
[8/200] Training loss: 0.03078815
[9/200] Training loss: 0.02977048
[10/200] Training loss: 0.02915468
[50/200] Training loss: 0.01663932
[100/200] Training loss: 0.01469774
[150/200] Training loss: 0.01344954
[200/200] Training loss: 0.01280336
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9607.726057710013 ----------
[1/200] Training loss: 0.02834703
[2/200] Training loss: 0.00238265
[3/200] Training loss: 0.00221950
[4/200] Training loss: 0.00185228
[5/200] Training loss: 0.00176719
[6/200] Training loss: 0.00163239
[7/200] Training loss: 0.00136707
[8/200] Training loss: 0.00123158
[9/200] Training loss: 0.00105458
[10/200] Training loss: 0.00067677
[50/200] Training loss: 0.00021832
[100/200] Training loss: 0.00017037
[150/200] Training loss: 0.00014262
[200/200] Training loss: 0.00013876
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11203.516947815984 ----------
[1/200] Training loss: 0.24506165
[2/200] Training loss: 0.07796586
[3/200] Training loss: 0.06099383
[4/200] Training loss: 0.05397728
[5/200] Training loss: 0.05417509
[6/200] Training loss: 0.05214137
[7/200] Training loss: 0.04950779
[8/200] Training loss: 0.04926057
[9/200] Training loss: 0.04496099
[10/200] Training loss: 0.04463245
[50/200] Training loss: 0.02001439
[100/200] Training loss: 0.01868124
[150/200] Training loss: 0.01699192
[200/200] Training loss: 0.01568023
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20081.779204044644 ----------
[1/200] Training loss: 0.14006648
[2/200] Training loss: 0.05463124
[3/200] Training loss: 0.04935854
[4/200] Training loss: 0.04487516
[5/200] Training loss: 0.04187972
[6/200] Training loss: 0.03841493
[7/200] Training loss: 0.03559308
[8/200] Training loss: 0.03486645
[9/200] Training loss: 0.03158306
[10/200] Training loss: 0.03161367
[50/200] Training loss: 0.01761661
[100/200] Training loss: 0.01495952
[150/200] Training loss: 0.01386112
[200/200] Training loss: 0.01317752
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12768.38470598376 ----------
[1/200] Training loss: 0.17671073
[2/200] Training loss: 0.06447781
[3/200] Training loss: 0.05459418
[4/200] Training loss: 0.05070952
[5/200] Training loss: 0.05132153
[6/200] Training loss: 0.04969514
[7/200] Training loss: 0.04634765
[8/200] Training loss: 0.04090714
[9/200] Training loss: 0.04003924
[10/200] Training loss: 0.03705822
[50/200] Training loss: 0.02005785
[100/200] Training loss: 0.01598654
[150/200] Training loss: 0.01453151
[200/200] Training loss: 0.01369234
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10433.770171898555 ----------
[1/200] Training loss: 0.15817159
[2/200] Training loss: 0.05983301
[3/200] Training loss: 0.05304667
[4/200] Training loss: 0.04932010
[5/200] Training loss: 0.04901920
[6/200] Training loss: 0.04448044
[7/200] Training loss: 0.03963755
[8/200] Training loss: 0.04062820
[9/200] Training loss: 0.03647118
[10/200] Training loss: 0.03460264
[50/200] Training loss: 0.01837377
[100/200] Training loss: 0.01558281
[150/200] Training loss: 0.01382116
[200/200] Training loss: 0.01278586
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7175.906353904014 ----------
[1/200] Training loss: 0.13351091
[2/200] Training loss: 0.05771754
[3/200] Training loss: 0.05031560
[4/200] Training loss: 0.04844663
[5/200] Training loss: 0.04683842
[6/200] Training loss: 0.04243360
[7/200] Training loss: 0.04030628
[8/200] Training loss: 0.03619423
[9/200] Training loss: 0.03328588
[10/200] Training loss: 0.03353676
[50/200] Training loss: 0.01700589
[100/200] Training loss: 0.01494855
[150/200] Training loss: 0.01388501
[200/200] Training loss: 0.01190768
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16511.79651037403 ----------
[1/200] Training loss: 0.11628909
[2/200] Training loss: 0.05614279
[3/200] Training loss: 0.05249987
[4/200] Training loss: 0.04707565
[5/200] Training loss: 0.04644554
[6/200] Training loss: 0.04503661
[7/200] Training loss: 0.04023658
[8/200] Training loss: 0.03984749
[9/200] Training loss: 0.03641795
[10/200] Training loss: 0.03497873
[50/200] Training loss: 0.01865374
[100/200] Training loss: 0.01656978
[150/200] Training loss: 0.01521991
[200/200] Training loss: 0.01433872
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01269111532361182
----FITNESS-----------RMSE---- 17344.394829454268 ----------
[1/200] Training loss: 0.10608934
[2/200] Training loss: 0.05395113
[3/200] Training loss: 0.05007996
[4/200] Training loss: 0.04662491
[5/200] Training loss: 0.04469215
[6/200] Training loss: 0.04302332
[7/200] Training loss: 0.03778252
[8/200] Training loss: 0.03699726
[9/200] Training loss: 0.03629642
[10/200] Training loss: 0.03246855
[50/200] Training loss: 0.01800342
[100/200] Training loss: 0.01413377
[150/200] Training loss: 0.01252508
[200/200] Training loss: 0.01201861
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01269111532361182
----FITNESS-----------RMSE---- 17852.937909487053 ----------
[1/200] Training loss: 0.10321791
[2/200] Training loss: 0.05423955
[3/200] Training loss: 0.04898141
[4/200] Training loss: 0.04589014
[5/200] Training loss: 0.04318995
[6/200] Training loss: 0.04080882
[7/200] Training loss: 0.03928653
[8/200] Training loss: 0.03846685
[9/200] Training loss: 0.03704505
[10/200] Training loss: 0.03473392
[50/200] Training loss: 0.01860392
[100/200] Training loss: 0.01670624
[150/200] Training loss: 0.01486832
[200/200] Training loss: 0.01397736
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01269111532361182
----FITNESS-----------RMSE---- 10358.956318085331 ----------
[1/200] Training loss: 0.17747377
[2/200] Training loss: 0.05588219
[3/200] Training loss: 0.05539784
[4/200] Training loss: 0.05208254
[5/200] Training loss: 0.04291700
[6/200] Training loss: 0.03878610
[7/200] Training loss: 0.04019404
[8/200] Training loss: 0.03623538
[9/200] Training loss: 0.03561393
[10/200] Training loss: 0.03849671
[50/200] Training loss: 0.01897647
[100/200] Training loss: 0.01614209
[150/200] Training loss: 0.01367049
[200/200] Training loss: 0.01370633
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8134.484126236894 ----------
[1/200] Training loss: 0.11616793
[2/200] Training loss: 0.04924594
[3/200] Training loss: 0.04485963
[4/200] Training loss: 0.04203209
[5/200] Training loss: 0.03654008
[6/200] Training loss: 0.03745452
[7/200] Training loss: 0.03617389
[8/200] Training loss: 0.03329291
[9/200] Training loss: 0.03202932
[10/200] Training loss: 0.02994863
[50/200] Training loss: 0.01675889
[100/200] Training loss: 0.01465889
[150/200] Training loss: 0.01367069
[200/200] Training loss: 0.01281205
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5011.606728385618 ----------
[1/200] Training loss: 0.12492259
[2/200] Training loss: 0.05459381
[3/200] Training loss: 0.04918852
[4/200] Training loss: 0.04411965
[5/200] Training loss: 0.04364294
[6/200] Training loss: 0.04050917
[7/200] Training loss: 0.03854755
[8/200] Training loss: 0.03695059
[9/200] Training loss: 0.03938521
[10/200] Training loss: 0.03644802
[50/200] Training loss: 0.01904692
[100/200] Training loss: 0.01533187
[150/200] Training loss: 0.01274966
[200/200] Training loss: 0.01175671
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01269111532361182
----FITNESS-----------RMSE---- 9965.106722960874 ----------
[1/200] Training loss: 0.15860104
[2/200] Training loss: 0.05869557
[3/200] Training loss: 0.05747021
[4/200] Training loss: 0.05419275
[5/200] Training loss: 0.04644421
[6/200] Training loss: 0.04699777
[7/200] Training loss: 0.04318823
[8/200] Training loss: 0.04053370
[9/200] Training loss: 0.04003952
[10/200] Training loss: 0.03351419
[50/200] Training loss: 0.01778381
[100/200] Training loss: 0.01530033
[150/200] Training loss: 0.01278517
[200/200] Training loss: 0.01154530
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10552.485204917371 ----------
[1/200] Training loss: 0.17865513
[2/200] Training loss: 0.06222054
[3/200] Training loss: 0.05901214
[4/200] Training loss: 0.05075817
[5/200] Training loss: 0.04935855
[6/200] Training loss: 0.04729065
[7/200] Training loss: 0.04421392
[8/200] Training loss: 0.03998146
[9/200] Training loss: 0.03874214
[10/200] Training loss: 0.03532761
[50/200] Training loss: 0.02042614
[100/200] Training loss: 0.01528885
[150/200] Training loss: 0.01435224
[200/200] Training loss: 0.01254063
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9598.853681560106 ----------
[1/200] Training loss: 0.15235564
[2/200] Training loss: 0.06061023
[3/200] Training loss: 0.05067898
[4/200] Training loss: 0.04919437
[5/200] Training loss: 0.04753080
[6/200] Training loss: 0.04491712
[7/200] Training loss: 0.03819536
[8/200] Training loss: 0.03777804
[9/200] Training loss: 0.03552658
[10/200] Training loss: 0.03302056
[50/200] Training loss: 0.01878104
[100/200] Training loss: 0.01497888
[150/200] Training loss: 0.01338219
[200/200] Training loss: 0.01249683
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15792.313066805635 ----------
[1/200] Training loss: 0.12138609
[2/200] Training loss: 0.05276174
[3/200] Training loss: 0.04890637
[4/200] Training loss: 0.04855214
[5/200] Training loss: 0.04534871
[6/200] Training loss: 0.04259648
[7/200] Training loss: 0.04108764
[8/200] Training loss: 0.03771975
[9/200] Training loss: 0.03732671
[10/200] Training loss: 0.03666424
[50/200] Training loss: 0.02138929
[100/200] Training loss: 0.01644436
[150/200] Training loss: 0.01482530
[200/200] Training loss: 0.01404553
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01269111532361182
----FITNESS-----------RMSE---- 6970.671990561598 ----------
[1/200] Training loss: 0.09978973
[2/200] Training loss: 0.05074260
[3/200] Training loss: 0.04745434
[4/200] Training loss: 0.04383225
[5/200] Training loss: 0.04132229
[6/200] Training loss: 0.04076690
[7/200] Training loss: 0.04041926
[8/200] Training loss: 0.03714634
[9/200] Training loss: 0.03494926
[10/200] Training loss: 0.03372409
[50/200] Training loss: 0.01860760
[100/200] Training loss: 0.01556226
[150/200] Training loss: 0.01437637
[200/200] Training loss: 0.01268386
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01269111532361182
----FITNESS-----------RMSE---- 8122.905391545564 ----------
[1/200] Training loss: 0.11027297
[2/200] Training loss: 0.05194824
[3/200] Training loss: 0.04860752
[4/200] Training loss: 0.04392711
[5/200] Training loss: 0.04316698
[6/200] Training loss: 0.03940234
[7/200] Training loss: 0.03764071
[8/200] Training loss: 0.03555903
[9/200] Training loss: 0.03333018
[10/200] Training loss: 0.03183794
[50/200] Training loss: 0.01877844
[100/200] Training loss: 0.01597373
[150/200] Training loss: 0.01446582
[200/200] Training loss: 0.01355599
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01269111532361182
----FITNESS-----------RMSE---- 15753.767295475707 ----------
[1/100] Training loss: 0.12635193
[2/100] Training loss: 0.06057556
[3/100] Training loss: 0.05546705
[4/100] Training loss: 0.05475408
[5/100] Training loss: 0.05171983
[6/100] Training loss: 0.05099406
[7/100] Training loss: 0.04854520
[8/100] Training loss: 0.04890358
[9/100] Training loss: 0.04768127
[10/100] Training loss: 0.04584068
[50/100] Training loss: 0.02187839
[100/100] Training loss: 0.01684419
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01269111532361182
----FITNESS-----------RMSE---- 15188.868555623227 ----------
[1/200] Training loss: 0.17863908
[2/200] Training loss: 0.06206581
[3/200] Training loss: 0.05690310
[4/200] Training loss: 0.05202029
[5/200] Training loss: 0.04972974
[6/200] Training loss: 0.04669652
[7/200] Training loss: 0.04191523
[8/200] Training loss: 0.03645828
[9/200] Training loss: 0.03644614
[10/200] Training loss: 0.03242868
[50/200] Training loss: 0.01866264
[100/200] Training loss: 0.01576979
[150/200] Training loss: 0.01424084
[200/200] Training loss: 0.01370296
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 37202.73194269474 ----------
[1/200] Training loss: 0.10684863
[2/200] Training loss: 0.05639269
[3/200] Training loss: 0.05071818
[4/200] Training loss: 0.04701211
[5/200] Training loss: 0.04578102
[6/200] Training loss: 0.04191763
[7/200] Training loss: 0.03920857
[8/200] Training loss: 0.03448970
[9/200] Training loss: 0.03531753
[10/200] Training loss: 0.03163927
[50/200] Training loss: 0.01786434
[100/200] Training loss: 0.01486211
[150/200] Training loss: 0.01322874
[200/200] Training loss: 0.01278601
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01269111532361182
----FITNESS-----------RMSE---- 25610.44536902863 ----------
[1/200] Training loss: 0.11750548
[2/200] Training loss: 0.04969320
[3/200] Training loss: 0.04264366
[4/200] Training loss: 0.04138175
[5/200] Training loss: 0.03684105
[6/200] Training loss: 0.03528627
[7/200] Training loss: 0.03265299
[8/200] Training loss: 0.03210410
[9/200] Training loss: 0.03057260
[10/200] Training loss: 0.02768424
[50/200] Training loss: 0.01990096
[100/200] Training loss: 0.01533189
[150/200] Training loss: 0.01393943
[200/200] Training loss: 0.01269735
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01269111532361182
----FITNESS-----------RMSE---- 9553.357524975185 ----------
[1/200] Training loss: 0.16533749
[2/200] Training loss: 0.05625691
[3/200] Training loss: 0.05207039
[4/200] Training loss: 0.04927602
[5/200] Training loss: 0.04632438
[6/200] Training loss: 0.04452758
[7/200] Training loss: 0.04164880
[8/200] Training loss: 0.03871670
[9/200] Training loss: 0.03578396
[10/200] Training loss: 0.03344943
[50/200] Training loss: 0.01744548
[100/200] Training loss: 0.01516434
[150/200] Training loss: 0.01346142
[200/200] Training loss: 0.01201206
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8622.541156758836 ----------
[1/200] Training loss: 0.14896865
[2/200] Training loss: 0.05334827
[3/200] Training loss: 0.05097869
[4/200] Training loss: 0.04504962
[5/200] Training loss: 0.04226760
[6/200] Training loss: 0.03832755
[7/200] Training loss: 0.03559921
[8/200] Training loss: 0.03466444
[9/200] Training loss: 0.03279498
[10/200] Training loss: 0.03112780
[50/200] Training loss: 0.01886715
[100/200] Training loss: 0.01654567
[150/200] Training loss: 0.01384857
[200/200] Training loss: 0.01232166
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15124.437973028947 ----------
[1/200] Training loss: 0.10276103
[2/200] Training loss: 0.05214292
[3/200] Training loss: 0.04961853
[4/200] Training loss: 0.04616681
[5/200] Training loss: 0.04663587
[6/200] Training loss: 0.04207917
[7/200] Training loss: 0.04002921
[8/200] Training loss: 0.03698391
[9/200] Training loss: 0.03441367
[10/200] Training loss: 0.03253757
[50/200] Training loss: 0.01689375
[100/200] Training loss: 0.01415816
[150/200] Training loss: 0.01328414
[200/200] Training loss: 0.01262984
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01269111532361182
----FITNESS-----------RMSE---- 17357.332513955018 ----------
[1/200] Training loss: 0.18998651
[2/200] Training loss: 0.05939705
[3/200] Training loss: 0.05042859
[4/200] Training loss: 0.04855618
[5/200] Training loss: 0.04082793
[6/200] Training loss: 0.03827717
[7/200] Training loss: 0.03763445
[8/200] Training loss: 0.03639099
[9/200] Training loss: 0.03477225
[10/200] Training loss: 0.03448142
[50/200] Training loss: 0.01884859
[100/200] Training loss: 0.01644065
[150/200] Training loss: 0.01532972
[200/200] Training loss: 0.01361629
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9442.593287863245 ----------
[1/200] Training loss: 0.11091737
[2/200] Training loss: 0.05485162
[3/200] Training loss: 0.05106258
[4/200] Training loss: 0.04769958
[5/200] Training loss: 0.04604104
[6/200] Training loss: 0.04342289
[7/200] Training loss: 0.04174506
[8/200] Training loss: 0.03928310
[9/200] Training loss: 0.03688197
[10/200] Training loss: 0.03408595
[50/200] Training loss: 0.01733592
[100/200] Training loss: 0.01453825
[150/200] Training loss: 0.01339423
[200/200] Training loss: 0.01184747
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12107.044560915765 ----------
[1/200] Training loss: 0.14598098
[2/200] Training loss: 0.05768202
[3/200] Training loss: 0.05333661
[4/200] Training loss: 0.04709642
[5/200] Training loss: 0.04413207
[6/200] Training loss: 0.04413108
[7/200] Training loss: 0.04242491
[8/200] Training loss: 0.04015223
[9/200] Training loss: 0.03803619
[10/200] Training loss: 0.03715600
[50/200] Training loss: 0.01882101
[100/200] Training loss: 0.01515238
[150/200] Training loss: 0.01403758
[200/200] Training loss: 0.01281745
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01269111532361182
----FITNESS-----------RMSE---- 8638.55450871267 ----------
[1/200] Training loss: 0.15768781
[2/200] Training loss: 0.06172074
[3/200] Training loss: 0.05602746
[4/200] Training loss: 0.05229527
[5/200] Training loss: 0.05012960
[6/200] Training loss: 0.04661768
[7/200] Training loss: 0.04386330
[8/200] Training loss: 0.03902291
[9/200] Training loss: 0.03580622
[10/200] Training loss: 0.03589898
[50/200] Training loss: 0.01830887
[100/200] Training loss: 0.01575099
[150/200] Training loss: 0.01307783
[200/200] Training loss: 0.01191578
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13803.01416358036 ----------
[1/200] Training loss: 0.17347361
[2/200] Training loss: 0.06674546
[3/200] Training loss: 0.05518029
[4/200] Training loss: 0.05412991
[5/200] Training loss: 0.05110265
[6/200] Training loss: 0.04932537
[7/200] Training loss: 0.04720511
[8/200] Training loss: 0.04639202
[9/200] Training loss: 0.04423948
[10/200] Training loss: 0.04573928
[50/200] Training loss: 0.01802114
[100/200] Training loss: 0.01588971
[150/200] Training loss: 0.01477534
[200/200] Training loss: 0.01409789
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6833.600807773308 ----------
[1/200] Training loss: 0.12110837
[2/200] Training loss: 0.05250144
[3/200] Training loss: 0.05016217
[4/200] Training loss: 0.04210388
[5/200] Training loss: 0.03823140
[6/200] Training loss: 0.03457896
[7/200] Training loss: 0.03206009
[8/200] Training loss: 0.03076189
[9/200] Training loss: 0.02924508
[10/200] Training loss: 0.02764189
[50/200] Training loss: 0.01658557
[100/200] Training loss: 0.01407903
[150/200] Training loss: 0.01308186
[200/200] Training loss: 0.01266058
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14833.661179897565 ----------
[1/200] Training loss: 0.18537593
[2/200] Training loss: 0.05960706
[3/200] Training loss: 0.05493150
[4/200] Training loss: 0.05049309
[5/200] Training loss: 0.04830713
[6/200] Training loss: 0.04383108
[7/200] Training loss: 0.04294416
[8/200] Training loss: 0.03911413
[9/200] Training loss: 0.03922317
[10/200] Training loss: 0.03384895
[50/200] Training loss: 0.01808508
[100/200] Training loss: 0.01523951
[150/200] Training loss: 0.01339372
[200/200] Training loss: 0.01213256
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5227.664488086434 ----------
[1/200] Training loss: 0.12771316
[2/200] Training loss: 0.05633931
[3/200] Training loss: 0.04761250
[4/200] Training loss: 0.03914682
[5/200] Training loss: 0.03545547
[6/200] Training loss: 0.03663060
[7/200] Training loss: 0.03521582
[8/200] Training loss: 0.03351123
[9/200] Training loss: 0.03304859
[10/200] Training loss: 0.03250536
[50/200] Training loss: 0.01757502
[100/200] Training loss: 0.01422206
[150/200] Training loss: 0.01308084
[200/200] Training loss: 0.01221307
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10335.374981102525 ----------
[1/200] Training loss: 0.16840373
[2/200] Training loss: 0.05711514
[3/200] Training loss: 0.05410101
[4/200] Training loss: 0.05197639
[5/200] Training loss: 0.04599199
[6/200] Training loss: 0.04567715
[7/200] Training loss: 0.04372783
[8/200] Training loss: 0.04073199
[9/200] Training loss: 0.03773400
[10/200] Training loss: 0.03696450
[50/200] Training loss: 0.01957106
[100/200] Training loss: 0.01492956
[150/200] Training loss: 0.01370735
[200/200] Training loss: 0.01242540
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22344.509840226972 ----------
[1/200] Training loss: 0.12437115
[2/200] Training loss: 0.05727543
[3/200] Training loss: 0.04998081
[4/200] Training loss: 0.04890713
[5/200] Training loss: 0.04527061
[6/200] Training loss: 0.04399640
[7/200] Training loss: 0.04247146
[8/200] Training loss: 0.03650665
[9/200] Training loss: 0.03659805
[10/200] Training loss: 0.03536239
[50/200] Training loss: 0.01732207
[100/200] Training loss: 0.01528261
[150/200] Training loss: 0.01297009
[200/200] Training loss: 0.01204739
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12756.351202440297 ----------
[1/200] Training loss: 0.11742606
[2/200] Training loss: 0.05520007
[3/200] Training loss: 0.05176452
[4/200] Training loss: 0.04653131
[5/200] Training loss: 0.04537795
[6/200] Training loss: 0.04313453
[7/200] Training loss: 0.04192579
[8/200] Training loss: 0.03816330
[9/200] Training loss: 0.03524834
[10/200] Training loss: 0.03267464
[50/200] Training loss: 0.01983926
[100/200] Training loss: 0.01745413
[150/200] Training loss: 0.01570273
[200/200] Training loss: 0.01491423
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01269111532361182
----FITNESS-----------RMSE---- 17844.838805660307 ----------
[1/200] Training loss: 0.13405973
[2/200] Training loss: 0.06022063
[3/200] Training loss: 0.05398156
[4/200] Training loss: 0.04840235
[5/200] Training loss: 0.04565297
[6/200] Training loss: 0.04470007
[7/200] Training loss: 0.04322208
[8/200] Training loss: 0.04043655
[9/200] Training loss: 0.04077744
[10/200] Training loss: 0.03916580
[50/200] Training loss: 0.01826618
[100/200] Training loss: 0.01379036
[150/200] Training loss: 0.01239443
[200/200] Training loss: 0.01198247
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01269111532361182
----FITNESS-----------RMSE---- 12890.002948021385 ----------
[1/200] Training loss: 0.13381051
[2/200] Training loss: 0.05735679
[3/200] Training loss: 0.05183404
[4/200] Training loss: 0.04881185
[5/200] Training loss: 0.04488190
[6/200] Training loss: 0.03861389
[7/200] Training loss: 0.03715151
[8/200] Training loss: 0.03698846
[9/200] Training loss: 0.03407574
[10/200] Training loss: 0.03254191
[50/200] Training loss: 0.01802378
[100/200] Training loss: 0.01552978
[150/200] Training loss: 0.01349918
[200/200] Training loss: 0.01194707
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8714.90768740553 ----------
[1/200] Training loss: 0.12869759
[2/200] Training loss: 0.05496228
[3/200] Training loss: 0.04714929
[4/200] Training loss: 0.04217486
[5/200] Training loss: 0.04181719
[6/200] Training loss: 0.03594486
[7/200] Training loss: 0.03556234
[8/200] Training loss: 0.03358300
[9/200] Training loss: 0.02988515
[10/200] Training loss: 0.02948700
[50/200] Training loss: 0.01902751
[100/200] Training loss: 0.01559476
[150/200] Training loss: 0.01526953
[200/200] Training loss: 0.01401804
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11820.703193972853 ----------
[1/200] Training loss: 0.16469490
[2/200] Training loss: 0.06330156
[3/200] Training loss: 0.05212113
[4/200] Training loss: 0.05204003
[5/200] Training loss: 0.05108862
[6/200] Training loss: 0.04680189
[7/200] Training loss: 0.04265618
[8/200] Training loss: 0.04363264
[9/200] Training loss: 0.04012790
[10/200] Training loss: 0.03795644
[50/200] Training loss: 0.01780128
[100/200] Training loss: 0.01511866
[150/200] Training loss: 0.01256640
[200/200] Training loss: 0.01215387
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12009.606821207763 ----------
[1/200] Training loss: 0.14626697
[2/200] Training loss: 0.06063492
[3/200] Training loss: 0.05394089
[4/200] Training loss: 0.04970075
[5/200] Training loss: 0.04713084
[6/200] Training loss: 0.04439193
[7/200] Training loss: 0.04285737
[8/200] Training loss: 0.04029994
[9/200] Training loss: 0.04012424
[10/200] Training loss: 0.03658814
[50/200] Training loss: 0.01828051
[100/200] Training loss: 0.01463312
[150/200] Training loss: 0.01394901
[200/200] Training loss: 0.01266688
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01269111532361182
----FITNESS-----------RMSE---- 11488.723514820957 ----------
[1/200] Training loss: 0.12222456
[2/200] Training loss: 0.05755896
[3/200] Training loss: 0.05182723
[4/200] Training loss: 0.05109070
[5/200] Training loss: 0.04601704
[6/200] Training loss: 0.04510502
[7/200] Training loss: 0.04000744
[8/200] Training loss: 0.03936411
[9/200] Training loss: 0.03600503
[10/200] Training loss: 0.03385341
[50/200] Training loss: 0.01921059
[100/200] Training loss: 0.01603410
[150/200] Training loss: 0.01478762
[200/200] Training loss: 0.01396625
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 29424.078303321585 ----------
[1/200] Training loss: 0.14610049
[2/200] Training loss: 0.05192378
[3/200] Training loss: 0.04565821
[4/200] Training loss: 0.04134237
[5/200] Training loss: 0.03800028
[6/200] Training loss: 0.03362307
[7/200] Training loss: 0.03416821
[8/200] Training loss: 0.03126460
[9/200] Training loss: 0.03236374
[10/200] Training loss: 0.03131616
[50/200] Training loss: 0.01754255
[100/200] Training loss: 0.01622243
[150/200] Training loss: 0.01471998
[200/200] Training loss: 0.01380704
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14025.743759244997 ----------
[1/200] Training loss: 0.16707745
[2/200] Training loss: 0.06030146
[3/200] Training loss: 0.05323615
[4/200] Training loss: 0.05066853
[5/200] Training loss: 0.04768900
[6/200] Training loss: 0.04685805
[7/200] Training loss: 0.04350036
[8/200] Training loss: 0.04010564
[9/200] Training loss: 0.03756344
[10/200] Training loss: 0.03700778
[50/200] Training loss: 0.01720335
[100/200] Training loss: 0.01434819
[150/200] Training loss: 0.01372642
[200/200] Training loss: 0.01186305
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8190.10305185472 ----------
[1/200] Training loss: 0.16546735
[2/200] Training loss: 0.06177895
[3/200] Training loss: 0.05725775
[4/200] Training loss: 0.05112056
[5/200] Training loss: 0.05229764
[6/200] Training loss: 0.04808740
[7/200] Training loss: 0.04522133
[8/200] Training loss: 0.04162318
[9/200] Training loss: 0.03906103
[10/200] Training loss: 0.03810283
[50/200] Training loss: 0.01695042
[100/200] Training loss: 0.01479472
[150/200] Training loss: 0.01356808
[200/200] Training loss: 0.01239057
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23052.198853905455 ----------
[1/200] Training loss: 0.17960321
[2/200] Training loss: 0.06686847
[3/200] Training loss: 0.05876966
[4/200] Training loss: 0.05260065
[5/200] Training loss: 0.04916642
[6/200] Training loss: 0.04987644
[7/200] Training loss: 0.04595771
[8/200] Training loss: 0.04329500
[9/200] Training loss: 0.04176018
[10/200] Training loss: 0.04123942
[50/200] Training loss: 0.01652817
[100/200] Training loss: 0.01371426
[150/200] Training loss: 0.01206919
[200/200] Training loss: 0.01140367
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4431.515090801339 ----------
[1/200] Training loss: 0.16973996
[2/200] Training loss: 0.05772551
[3/200] Training loss: 0.05146559
[4/200] Training loss: 0.05021063
[5/200] Training loss: 0.04304573
[6/200] Training loss: 0.04366473
[7/200] Training loss: 0.03979646
[8/200] Training loss: 0.03972882
[9/200] Training loss: 0.03696558
[10/200] Training loss: 0.03888889
[50/200] Training loss: 0.01985256
[100/200] Training loss: 0.01512534
[150/200] Training loss: 0.01294329
[200/200] Training loss: 0.01204337
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7089.167793189832 ----------
[1/200] Training loss: 0.16197193
[2/200] Training loss: 0.06247066
[3/200] Training loss: 0.04816927
[4/200] Training loss: 0.04655649
[5/200] Training loss: 0.04576166
[6/200] Training loss: 0.04128695
[7/200] Training loss: 0.04015751
[8/200] Training loss: 0.04029128
[9/200] Training loss: 0.03786957
[10/200] Training loss: 0.03387334
[50/200] Training loss: 0.01716371
[100/200] Training loss: 0.01401387
[150/200] Training loss: 0.01330601
[200/200] Training loss: 0.01171028
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6209.568745090113 ----------
[1/200] Training loss: 0.16513865
[2/200] Training loss: 0.05913050
[3/200] Training loss: 0.04625100
[4/200] Training loss: 0.04577092
[5/200] Training loss: 0.04360004
[6/200] Training loss: 0.04120414
[7/200] Training loss: 0.03813735
[8/200] Training loss: 0.03562393
[9/200] Training loss: 0.03562168
[10/200] Training loss: 0.03144776
[50/200] Training loss: 0.01727668
[100/200] Training loss: 0.01376381
[150/200] Training loss: 0.01202228
[200/200] Training loss: 0.01185818
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8215.928918874602 ----------
[1/200] Training loss: 0.09468263
[2/200] Training loss: 0.05189964
[3/200] Training loss: 0.04627057
[4/200] Training loss: 0.04312311
[5/200] Training loss: 0.04065861
[6/200] Training loss: 0.03841732
[7/200] Training loss: 0.03561725
[8/200] Training loss: 0.03730462
[9/200] Training loss: 0.03359285
[10/200] Training loss: 0.03202714
[50/200] Training loss: 0.01748884
[100/200] Training loss: 0.01393531
[150/200] Training loss: 0.01333367
[200/200] Training loss: 0.01176031
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01269111532361182
----FITNESS-----------RMSE---- 12058.567079052138 ----------
[1/200] Training loss: 0.14727561
[2/200] Training loss: 0.05737283
[3/200] Training loss: 0.05131869
[4/200] Training loss: 0.04985737
[5/200] Training loss: 0.04604989
[6/200] Training loss: 0.04334321
[7/200] Training loss: 0.03994158
[8/200] Training loss: 0.03738755
[9/200] Training loss: 0.03649213
[10/200] Training loss: 0.03604515
[50/200] Training loss: 0.01830456
[100/200] Training loss: 0.01515444
[150/200] Training loss: 0.01303930
[200/200] Training loss: 0.01167522
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16089.652824097853 ----------
[1/200] Training loss: 0.13927924
[2/200] Training loss: 0.05624284
[3/200] Training loss: 0.05133507
[4/200] Training loss: 0.05128912
[5/200] Training loss: 0.04657337
[6/200] Training loss: 0.04467187
[7/200] Training loss: 0.04320506
[8/200] Training loss: 0.04025235
[9/200] Training loss: 0.03900961
[10/200] Training loss: 0.03707101
[50/200] Training loss: 0.01825687
[100/200] Training loss: 0.01570015
[150/200] Training loss: 0.01290857
[200/200] Training loss: 0.01272251
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9775.422650709277 ----------
[1/200] Training loss: 0.14185288
[2/200] Training loss: 0.05430991
[3/200] Training loss: 0.04775384
[4/200] Training loss: 0.04164839
[5/200] Training loss: 0.03912233
[6/200] Training loss: 0.03634759
[7/200] Training loss: 0.03371919
[8/200] Training loss: 0.03182870
[9/200] Training loss: 0.03027337
[10/200] Training loss: 0.02887788
[50/200] Training loss: 0.01986033
[100/200] Training loss: 0.01745570
[150/200] Training loss: 0.01515344
[200/200] Training loss: 0.01355591
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15853.875740650928 ----------
[1/200] Training loss: 0.16401701
[2/200] Training loss: 0.05888258
[3/200] Training loss: 0.04995936
[4/200] Training loss: 0.04553740
[5/200] Training loss: 0.04280344
[6/200] Training loss: 0.04057409
[7/200] Training loss: 0.03687107
[8/200] Training loss: 0.03604010
[9/200] Training loss: 0.03317751
[10/200] Training loss: 0.03046315
[50/200] Training loss: 0.01801736
[100/200] Training loss: 0.01558701
[150/200] Training loss: 0.01386057
[200/200] Training loss: 0.01283210
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4761.726157602934 ----------
[1/200] Training loss: 0.16546318
[2/200] Training loss: 0.06209453
[3/200] Training loss: 0.04988890
[4/200] Training loss: 0.04794219
[5/200] Training loss: 0.04459545
[6/200] Training loss: 0.04369225
[7/200] Training loss: 0.04362473
[8/200] Training loss: 0.04056544
[9/200] Training loss: 0.03693605
[10/200] Training loss: 0.03488447
[50/200] Training loss: 0.01982065
[100/200] Training loss: 0.01681677
[150/200] Training loss: 0.01433305
[200/200] Training loss: 0.01326868
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4840.476836015228 ----------
[1/200] Training loss: 0.12285379
[2/200] Training loss: 0.05634202
[3/200] Training loss: 0.05194189
[4/200] Training loss: 0.04806778
[5/200] Training loss: 0.04581641
[6/200] Training loss: 0.04507972
[7/200] Training loss: 0.04050270
[8/200] Training loss: 0.04033984
[9/200] Training loss: 0.03969163
[10/200] Training loss: 0.03602087
[50/200] Training loss: 0.01876880
[100/200] Training loss: 0.01596084
[150/200] Training loss: 0.01442996
[200/200] Training loss: 0.01309269
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13791.680970788151 ----------
[1/200] Training loss: 0.12640908
[2/200] Training loss: 0.05492621
[3/200] Training loss: 0.04431868
[4/200] Training loss: 0.04399556
[5/200] Training loss: 0.04062488
[6/200] Training loss: 0.03898171
[7/200] Training loss: 0.03560444
[8/200] Training loss: 0.03281167
[9/200] Training loss: 0.03148957
[10/200] Training loss: 0.03134613
[50/200] Training loss: 0.01768039
[100/200] Training loss: 0.01480480
[150/200] Training loss: 0.01262945
[200/200] Training loss: 0.01209932
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19201.43994600405 ----------
[1/200] Training loss: 0.19536437
[2/200] Training loss: 0.06580358
[3/200] Training loss: 0.05716898
[4/200] Training loss: 0.05469897
[5/200] Training loss: 0.04963431
[6/200] Training loss: 0.04781575
[7/200] Training loss: 0.04530310
[8/200] Training loss: 0.04336348
[9/200] Training loss: 0.03892566
[10/200] Training loss: 0.03669866
[50/200] Training loss: 0.01895924
[100/200] Training loss: 0.01620766
[150/200] Training loss: 0.01420522
[200/200] Training loss: 0.01331958
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19541.004273066417 ----------
[1/200] Training loss: 0.14473740
[2/200] Training loss: 0.05375142
[3/200] Training loss: 0.05054812
[4/200] Training loss: 0.04871242
[5/200] Training loss: 0.04580912
[6/200] Training loss: 0.03835087
[7/200] Training loss: 0.03785346
[8/200] Training loss: 0.03577360
[9/200] Training loss: 0.03490253
[10/200] Training loss: 0.02973124
[50/200] Training loss: 0.01768467
[100/200] Training loss: 0.01493042
[150/200] Training loss: 0.01302225
[200/200] Training loss: 0.01258452
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 31436.954559880636 ----------
[1/200] Training loss: 0.17364864
[2/200] Training loss: 0.05794953
[3/200] Training loss: 0.05274617
[4/200] Training loss: 0.04815667
[5/200] Training loss: 0.04506379
[6/200] Training loss: 0.04179543
[7/200] Training loss: 0.03840782
[8/200] Training loss: 0.03622333
[9/200] Training loss: 0.03550254
[10/200] Training loss: 0.03016163
[50/200] Training loss: 0.01565059
[100/200] Training loss: 0.01211317
[150/200] Training loss: 0.01125084
[200/200] Training loss: 0.01078286
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9539.415076407988 ----------
[1/200] Training loss: 0.15601703
[2/200] Training loss: 0.05498411
[3/200] Training loss: 0.04987141
[4/200] Training loss: 0.04500849
[5/200] Training loss: 0.04305948
[6/200] Training loss: 0.03928142
[7/200] Training loss: 0.03625359
[8/200] Training loss: 0.03486850
[9/200] Training loss: 0.03388318
[10/200] Training loss: 0.03183750
[50/200] Training loss: 0.01675044
[100/200] Training loss: 0.01383727
[150/200] Training loss: 0.01246324
[200/200] Training loss: 0.01169968
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22296.920684255932 ----------
[1/200] Training loss: 0.14075767
[2/200] Training loss: 0.05360399
[3/200] Training loss: 0.05053285
[4/200] Training loss: 0.04568503
[5/200] Training loss: 0.03975132
[6/200] Training loss: 0.03771047
[7/200] Training loss: 0.03622966
[8/200] Training loss: 0.03160794
[9/200] Training loss: 0.03060585
[10/200] Training loss: 0.02748680
[50/200] Training loss: 0.01730691
[100/200] Training loss: 0.01505155
[150/200] Training loss: 0.01328898
[200/200] Training loss: 0.01352697
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16985.9603202174 ----------
[1/200] Training loss: 0.15961541
[2/200] Training loss: 0.06056778
[3/200] Training loss: 0.05466045
[4/200] Training loss: 0.04984404
[5/200] Training loss: 0.04871997
[6/200] Training loss: 0.04528201
[7/200] Training loss: 0.04294984
[8/200] Training loss: 0.04172800
[9/200] Training loss: 0.03997150
[10/200] Training loss: 0.03633806
[50/200] Training loss: 0.01879735
[100/200] Training loss: 0.01663281
[150/200] Training loss: 0.01539971
[200/200] Training loss: 0.01337042
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16878.778154830994 ----------
[1/200] Training loss: 0.15496128
[2/200] Training loss: 0.06235393
[3/200] Training loss: 0.05523105
[4/200] Training loss: 0.05009945
[5/200] Training loss: 0.04758562
[6/200] Training loss: 0.04733556
[7/200] Training loss: 0.04290743
[8/200] Training loss: 0.04022493
[9/200] Training loss: 0.03444734
[10/200] Training loss: 0.03549761
[50/200] Training loss: 0.01834238
[100/200] Training loss: 0.01659361
[150/200] Training loss: 0.01461690
[200/200] Training loss: 0.01310626
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13804.121703317454 ----------
[1/200] Training loss: 0.17317193
[2/200] Training loss: 0.05663797
[3/200] Training loss: 0.05754396
[4/200] Training loss: 0.05053655
[5/200] Training loss: 0.04465114
[6/200] Training loss: 0.04771587
[7/200] Training loss: 0.03799528
[8/200] Training loss: 0.03666586
[9/200] Training loss: 0.03476793
[10/200] Training loss: 0.03280573
[50/200] Training loss: 0.01955401
[100/200] Training loss: 0.01624988
[150/200] Training loss: 0.01445342
[200/200] Training loss: 0.01312143
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19267.94810040758 ----------
[1/200] Training loss: 0.18535212
[2/200] Training loss: 0.05923806
[3/200] Training loss: 0.04987244
[4/200] Training loss: 0.04917096
[5/200] Training loss: 0.04371870
[6/200] Training loss: 0.04306112
[7/200] Training loss: 0.03919706
[8/200] Training loss: 0.03498505
[9/200] Training loss: 0.03526933
[10/200] Training loss: 0.03157213
[50/200] Training loss: 0.01830522
[100/200] Training loss: 0.01472990
[150/200] Training loss: 0.01387881
[200/200] Training loss: 0.01260135
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4551.186878167056 ----------
[1/200] Training loss: 0.18043708
[2/200] Training loss: 0.06174834
[3/200] Training loss: 0.05580430
[4/200] Training loss: 0.05485126
[5/200] Training loss: 0.04818751
[6/200] Training loss: 0.04647125
[7/200] Training loss: 0.04616754
[8/200] Training loss: 0.04437703
[9/200] Training loss: 0.04338840
[10/200] Training loss: 0.03909122
[50/200] Training loss: 0.01864278
[100/200] Training loss: 0.01468528
[150/200] Training loss: 0.01302171
[200/200] Training loss: 0.01205812
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9175.551427570988 ----------
[1/200] Training loss: 0.18863674
[2/200] Training loss: 0.06169568
[3/200] Training loss: 0.05887058
[4/200] Training loss: 0.05184837
[5/200] Training loss: 0.04989677
[6/200] Training loss: 0.04717004
[7/200] Training loss: 0.04594045
[8/200] Training loss: 0.04219443
[9/200] Training loss: 0.04081540
[10/200] Training loss: 0.04003329
[50/200] Training loss: 0.01729318
[100/200] Training loss: 0.01507267
[150/200] Training loss: 0.01415036
[200/200] Training loss: 0.01351853
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11027.482033537846 ----------
[1/200] Training loss: 0.15918188
[2/200] Training loss: 0.05730126
[3/200] Training loss: 0.04654929
[4/200] Training loss: 0.04360137
[5/200] Training loss: 0.04280234
[6/200] Training loss: 0.04171800
[7/200] Training loss: 0.03607446
[8/200] Training loss: 0.03785629
[9/200] Training loss: 0.03627190
[10/200] Training loss: 0.03454306
[50/200] Training loss: 0.01769915
[100/200] Training loss: 0.01392970
[150/200] Training loss: 0.01267896
[200/200] Training loss: 0.01215762
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10492.39191033198 ----------
[1/200] Training loss: 0.15358241
[2/200] Training loss: 0.05656967
[3/200] Training loss: 0.05146812
[4/200] Training loss: 0.04829533
[5/200] Training loss: 0.04444247
[6/200] Training loss: 0.04482352
[7/200] Training loss: 0.04338180
[8/200] Training loss: 0.03990794
[9/200] Training loss: 0.03676571
[10/200] Training loss: 0.03545052
[50/200] Training loss: 0.01838004
[100/200] Training loss: 0.01595198
[150/200] Training loss: 0.01462931
[200/200] Training loss: 0.01373139
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9281.707170558657 ----------
[1/200] Training loss: 0.15836975
[2/200] Training loss: 0.06047790
[3/200] Training loss: 0.05105537
[4/200] Training loss: 0.04791168
[5/200] Training loss: 0.04220256
[6/200] Training loss: 0.04025545
[7/200] Training loss: 0.04104838
[8/200] Training loss: 0.03651910
[9/200] Training loss: 0.03186977
[10/200] Training loss: 0.03405006
[50/200] Training loss: 0.01875798
[100/200] Training loss: 0.01586486
[150/200] Training loss: 0.01371642
[200/200] Training loss: 0.01294782
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19932.394537536125 ----------
[1/200] Training loss: 0.16907577
[2/200] Training loss: 0.06417692
[3/200] Training loss: 0.05937053
[4/200] Training loss: 0.05470266
[5/200] Training loss: 0.05153254
[6/200] Training loss: 0.05248661
[7/200] Training loss: 0.04730898
[8/200] Training loss: 0.04594308
[9/200] Training loss: 0.04472071
[10/200] Training loss: 0.04282184
[50/200] Training loss: 0.01795275
[100/200] Training loss: 0.01596516
[150/200] Training loss: 0.01449266
[200/200] Training loss: 0.01315869
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16754.798238116746 ----------
[1/200] Training loss: 0.22577388
[2/200] Training loss: 0.07922937
[3/200] Training loss: 0.07789940
[4/200] Training loss: 0.07647871
[5/200] Training loss: 0.07351390
[6/200] Training loss: 0.06946019
[7/200] Training loss: 0.06286799
[8/200] Training loss: 0.05554309
[9/200] Training loss: 0.05222184
[10/200] Training loss: 0.04956702
[50/200] Training loss: 0.02910254
[100/200] Training loss: 0.02614847
[150/200] Training loss: 0.02322889
[200/200] Training loss: 0.02285433
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: SGD ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10309.00693568493 ----------
[1/200] Training loss: 0.17711322
[2/200] Training loss: 0.05643800
[3/200] Training loss: 0.05028485
[4/200] Training loss: 0.04893460
[5/200] Training loss: 0.04130369
[6/200] Training loss: 0.03798297
[7/200] Training loss: 0.03950338
[8/200] Training loss: 0.03532325
[9/200] Training loss: 0.03231319
[10/200] Training loss: 0.03528418
[50/200] Training loss: 0.01770880
[100/200] Training loss: 0.01352350
[150/200] Training loss: 0.01297887
[200/200] Training loss: 0.01158722
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 3585.6367635330826 ----------
[1/200] Training loss: 0.18695089
[2/200] Training loss: 0.06473905
[3/200] Training loss: 0.05582061
[4/200] Training loss: 0.05486829
[5/200] Training loss: 0.05237722
[6/200] Training loss: 0.05074652
[7/200] Training loss: 0.04600592
[8/200] Training loss: 0.04413993
[9/200] Training loss: 0.04396566
[10/200] Training loss: 0.04035465
[50/200] Training loss: 0.01709219
[100/200] Training loss: 0.01485862
[150/200] Training loss: 0.01330223
[200/200] Training loss: 0.01173808
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12042.93718326223 ----------
[1/200] Training loss: 0.14073166
[2/200] Training loss: 0.05698556
[3/200] Training loss: 0.05474215
[4/200] Training loss: 0.05244189
[5/200] Training loss: 0.04847313
[6/200] Training loss: 0.04678787
[7/200] Training loss: 0.04170568
[8/200] Training loss: 0.04019366
[9/200] Training loss: 0.03735908
[10/200] Training loss: 0.03462906
[50/200] Training loss: 0.01827744
[100/200] Training loss: 0.01402993
[150/200] Training loss: 0.01282217
[200/200] Training loss: 0.01165048
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 25544.19292128839 ----------
[1/200] Training loss: 0.14738279
[2/200] Training loss: 0.05313796
[3/200] Training loss: 0.05193604
[4/200] Training loss: 0.04984827
[5/200] Training loss: 0.04367272
[6/200] Training loss: 0.04106440
[7/200] Training loss: 0.04013900
[8/200] Training loss: 0.04003989
[9/200] Training loss: 0.03290657
[10/200] Training loss: 0.03067634
[50/200] Training loss: 0.01805786
[100/200] Training loss: 0.01504156
[150/200] Training loss: 0.01353438
[200/200] Training loss: 0.01220823
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5019.713537643358 ----------
[1/200] Training loss: 0.13741914
[2/200] Training loss: 0.05471327
[3/200] Training loss: 0.05296252
[4/200] Training loss: 0.04859917
[5/200] Training loss: 0.03999183
[6/200] Training loss: 0.03695371
[7/200] Training loss: 0.03316653
[8/200] Training loss: 0.03217440
[9/200] Training loss: 0.02917841
[10/200] Training loss: 0.02788491
[50/200] Training loss: 0.01705670
[100/200] Training loss: 0.01478847
[150/200] Training loss: 0.01308716
[200/200] Training loss: 0.01263247
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23304.667343688903 ----------
[1/200] Training loss: 0.22341538
[2/200] Training loss: 0.04801307
[3/200] Training loss: 0.04268865
[4/200] Training loss: 0.03963123
[5/200] Training loss: 0.03043891
[6/200] Training loss: 0.02739430
[7/200] Training loss: 0.02517586
[8/200] Training loss: 0.02789533
[9/200] Training loss: 0.02912436
[10/200] Training loss: 0.03595710
[50/200] Training loss: 0.02324090
[100/200] Training loss: 0.01503706
[150/200] Training loss: 0.01318329
[200/200] Training loss: 0.01561733
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adam ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 28962.45403967005 ----------
[1/200] Training loss: 0.15763756
[2/200] Training loss: 0.05528924
[3/200] Training loss: 0.05339063
[4/200] Training loss: 0.05107883
[5/200] Training loss: 0.04813361
[6/200] Training loss: 0.04609859
[7/200] Training loss: 0.04362728
[8/200] Training loss: 0.04250376
[9/200] Training loss: 0.03874507
[10/200] Training loss: 0.03937196
[50/200] Training loss: 0.01990156
[100/200] Training loss: 0.01692687
[150/200] Training loss: 0.01489498
[200/200] Training loss: 0.01340925
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14561.821864038853 ----------
[1/200] Training loss: 0.16429203
[2/200] Training loss: 0.06257740
[3/200] Training loss: 0.05444924
[4/200] Training loss: 0.05614127
[5/200] Training loss: 0.05090784
[6/200] Training loss: 0.04790021
[7/200] Training loss: 0.04155910
[8/200] Training loss: 0.04085930
[9/200] Training loss: 0.03618284
[10/200] Training loss: 0.03246772
[50/200] Training loss: 0.01757558
[100/200] Training loss: 0.01511235
[150/200] Training loss: 0.01415026
[200/200] Training loss: 0.01330153
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17385.225911675694 ----------
[1/200] Training loss: 0.17068294
[2/200] Training loss: 0.05865135
[3/200] Training loss: 0.05267532
[4/200] Training loss: 0.04949839
[5/200] Training loss: 0.04655831
[6/200] Training loss: 0.03963786
[7/200] Training loss: 0.03658978
[8/200] Training loss: 0.03365087
[9/200] Training loss: 0.03293827
[10/200] Training loss: 0.03195824
[50/200] Training loss: 0.01835698
[100/200] Training loss: 0.01640347
[150/200] Training loss: 0.01481579
[200/200] Training loss: 0.01255134
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11122.124976819852 ----------
[1/150] Training loss: 0.15403399
[2/150] Training loss: 0.05704660
[3/150] Training loss: 0.05189383
[4/150] Training loss: 0.04678565
[5/150] Training loss: 0.04488089
[6/150] Training loss: 0.04360764
[7/150] Training loss: 0.03914654
[8/150] Training loss: 0.03607386
[9/150] Training loss: 0.03649718
[10/150] Training loss: 0.03475299
[50/150] Training loss: 0.01884697
[100/150] Training loss: 0.01550180
[150/150] Training loss: 0.01359291
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 150
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21171.970904948834 ----------
[1/200] Training loss: 0.15146126
[2/200] Training loss: 0.05418669
[3/200] Training loss: 0.05162921
[4/200] Training loss: 0.05171656
[5/200] Training loss: 0.04721790
[6/200] Training loss: 0.04613928
[7/200] Training loss: 0.04239437
[8/200] Training loss: 0.03898013
[9/200] Training loss: 0.03894225
[10/200] Training loss: 0.03558287
[50/200] Training loss: 0.01783688
[100/200] Training loss: 0.01546402
[150/200] Training loss: 0.01352935
[200/200] Training loss: 0.01252031
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9457.318436004996 ----------
[1/200] Training loss: 0.12620932
[2/200] Training loss: 0.05376474
[3/200] Training loss: 0.04984192
[4/200] Training loss: 0.04399748
[5/200] Training loss: 0.03972963
[6/200] Training loss: 0.03635387
[7/200] Training loss: 0.03338857
[8/200] Training loss: 0.03171302
[9/200] Training loss: 0.02995238
[10/200] Training loss: 0.03057030
[50/200] Training loss: 0.01792508
[100/200] Training loss: 0.01482002
[150/200] Training loss: 0.01346725
[200/200] Training loss: 0.01234180
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12268.48711129453 ----------
[1/200] Training loss: 0.15550026
[2/200] Training loss: 0.05832024
[3/200] Training loss: 0.04704160
[4/200] Training loss: 0.04790470
[5/200] Training loss: 0.04156652
[6/200] Training loss: 0.03952833
[7/200] Training loss: 0.03476750
[8/200] Training loss: 0.03418709
[9/200] Training loss: 0.03221786
[10/200] Training loss: 0.03206530
[50/200] Training loss: 0.01815637
[100/200] Training loss: 0.01538605
[150/200] Training loss: 0.01377513
[200/200] Training loss: 0.01300096
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6894.69129113117 ----------
[1/200] Training loss: 0.17309206
[2/200] Training loss: 0.06082327
[3/200] Training loss: 0.05554665
[4/200] Training loss: 0.04859791
[5/200] Training loss: 0.04315156
[6/200] Training loss: 0.04360095
[7/200] Training loss: 0.04013117
[8/200] Training loss: 0.03783260
[9/200] Training loss: 0.03354646
[10/200] Training loss: 0.03015291
[50/200] Training loss: 0.01634117
[100/200] Training loss: 0.01456096
[150/200] Training loss: 0.01286554
[200/200] Training loss: 0.01139129
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16466.357459984887 ----------
[1/200] Training loss: 0.15856224
[2/200] Training loss: 0.06071868
[3/200] Training loss: 0.04762949
[4/200] Training loss: 0.04775527
[5/200] Training loss: 0.03976904
[6/200] Training loss: 0.03871700
[7/200] Training loss: 0.03469970
[8/200] Training loss: 0.03171787
[9/200] Training loss: 0.03341069
[10/200] Training loss: 0.02937182
[50/200] Training loss: 0.01844503
[100/200] Training loss: 0.01489046
[150/200] Training loss: 0.01351495
[200/200] Training loss: 0.01278051
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13878.7521052867 ----------
[1/200] Training loss: 0.17743868
[2/200] Training loss: 0.06424149
[3/200] Training loss: 0.05898316
[4/200] Training loss: 0.05239938
[5/200] Training loss: 0.05147864
[6/200] Training loss: 0.04816476
[7/200] Training loss: 0.04719569
[8/200] Training loss: 0.04515995
[9/200] Training loss: 0.04375503
[10/200] Training loss: 0.04164652
[50/200] Training loss: 0.01746975
[100/200] Training loss: 0.01541721
[150/200] Training loss: 0.01441833
[200/200] Training loss: 0.01349274
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22048.44484311762 ----------
[1/200] Training loss: 0.15829926
[2/200] Training loss: 0.06450281
[3/200] Training loss: 0.05249736
[4/200] Training loss: 0.05160148
[5/200] Training loss: 0.04929759
[6/200] Training loss: 0.04659013
[7/200] Training loss: 0.04169409
[8/200] Training loss: 0.04365234
[9/200] Training loss: 0.03876024
[10/200] Training loss: 0.03510181
[50/200] Training loss: 0.01719723
[100/200] Training loss: 0.01424653
[150/200] Training loss: 0.01265270
[200/200] Training loss: 0.01129794
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12472.037844714872 ----------
[1/200] Training loss: 0.13913207
[2/200] Training loss: 0.06088783
[3/200] Training loss: 0.04841627
[4/200] Training loss: 0.04732128
[5/200] Training loss: 0.04205025
[6/200] Training loss: 0.03950524
[7/200] Training loss: 0.03690186
[8/200] Training loss: 0.03281236
[9/200] Training loss: 0.03433806
[10/200] Training loss: 0.03068970
[50/200] Training loss: 0.01625739
[100/200] Training loss: 0.01490641
[150/200] Training loss: 0.01343872
[200/200] Training loss: 0.01239803
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11683.715847280779 ----------
[1/200] Training loss: 0.17063074
[2/200] Training loss: 0.06161507
[3/200] Training loss: 0.05100747
[4/200] Training loss: 0.04759710
[5/200] Training loss: 0.04164938
[6/200] Training loss: 0.03658638
[7/200] Training loss: 0.03452943
[8/200] Training loss: 0.03135878
[9/200] Training loss: 0.02967926
[10/200] Training loss: 0.02876435
[50/200] Training loss: 0.01780778
[100/200] Training loss: 0.01442627
[150/200] Training loss: 0.01258006
[200/200] Training loss: 0.01107234
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 27141.78358177664 ----------
[1/200] Training loss: 0.17601393
[2/200] Training loss: 0.05493889
[3/200] Training loss: 0.05065973
[4/200] Training loss: 0.04717178
[5/200] Training loss: 0.04371264
[6/200] Training loss: 0.03955772
[7/200] Training loss: 0.03854152
[8/200] Training loss: 0.03335787
[9/200] Training loss: 0.03429278
[10/200] Training loss: 0.03223437
[50/200] Training loss: 0.01873885
[100/200] Training loss: 0.01527290
[150/200] Training loss: 0.01394047
[200/200] Training loss: 0.01245458
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13015.940995563862 ----------
[1/200] Training loss: 0.15717878
[2/200] Training loss: 0.06226298
[3/200] Training loss: 0.05494325
[4/200] Training loss: 0.05372067
[5/200] Training loss: 0.05130751
[6/200] Training loss: 0.04654971
[7/200] Training loss: 0.04719020
[8/200] Training loss: 0.04522297
[9/200] Training loss: 0.04194567
[10/200] Training loss: 0.03928533
[50/200] Training loss: 0.01738479
[100/200] Training loss: 0.01490338
[150/200] Training loss: 0.01326895
[200/200] Training loss: 0.01220018
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8356.604094965849 ----------
[1/200] Training loss: 0.16799659
[2/200] Training loss: 0.05989514
[3/200] Training loss: 0.05065513
[4/200] Training loss: 0.04938004
[5/200] Training loss: 0.04414331
[6/200] Training loss: 0.04148591
[7/200] Training loss: 0.03871702
[8/200] Training loss: 0.03957704
[9/200] Training loss: 0.03714397
[10/200] Training loss: 0.03438084
[50/200] Training loss: 0.01707777
[100/200] Training loss: 0.01478280
[150/200] Training loss: 0.01265145
[200/200] Training loss: 0.01133748
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22181.764041662693 ----------
[1/200] Training loss: 0.16487324
[2/200] Training loss: 0.05501646
[3/200] Training loss: 0.04925027
[4/200] Training loss: 0.04681581
[5/200] Training loss: 0.04294920
[6/200] Training loss: 0.03934012
[7/200] Training loss: 0.03816494
[8/200] Training loss: 0.03713127
[9/200] Training loss: 0.03305190
[10/200] Training loss: 0.03128640
[50/200] Training loss: 0.01942677
[100/200] Training loss: 0.01643162
[150/200] Training loss: 0.01491552
[200/200] Training loss: 0.01435542
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10941.116944809612 ----------
[1/200] Training loss: 0.17717878
[2/200] Training loss: 0.06013998
[3/200] Training loss: 0.05505832
[4/200] Training loss: 0.04556301
[5/200] Training loss: 0.04591728
[6/200] Training loss: 0.04206313
[7/200] Training loss: 0.03996114
[8/200] Training loss: 0.03942536
[9/200] Training loss: 0.03683758
[10/200] Training loss: 0.03701272
[50/200] Training loss: 0.01889406
[100/200] Training loss: 0.01673385
[150/200] Training loss: 0.01489043
[200/200] Training loss: 0.01410881
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13199.208461116144 ----------
[1/200] Training loss: 0.15719589
[2/200] Training loss: 0.06095880
[3/200] Training loss: 0.05417958
[4/200] Training loss: 0.05435430
[5/200] Training loss: 0.04295941
[6/200] Training loss: 0.04081008
[7/200] Training loss: 0.04039243
[8/200] Training loss: 0.03926011
[9/200] Training loss: 0.03624696
[10/200] Training loss: 0.03473571
[50/200] Training loss: 0.01806938
[100/200] Training loss: 0.01455242
[150/200] Training loss: 0.01245847
[200/200] Training loss: 0.01097416
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 31682.275675841218 ----------
[1/200] Training loss: 0.15441591
[2/200] Training loss: 0.05863359
[3/200] Training loss: 0.04976629
[4/200] Training loss: 0.05170703
[5/200] Training loss: 0.04503523
[6/200] Training loss: 0.04089805
[7/200] Training loss: 0.03956409
[8/200] Training loss: 0.03494060
[9/200] Training loss: 0.03422139
[10/200] Training loss: 0.03453966
[50/200] Training loss: 0.01854939
[100/200] Training loss: 0.01502226
[150/200] Training loss: 0.01336840
[200/200] Training loss: 0.01214530
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12368.728309733382 ----------
[1/200] Training loss: 0.15648991
[2/200] Training loss: 0.05670844
[3/200] Training loss: 0.05146850
[4/200] Training loss: 0.04823944
[5/200] Training loss: 0.04535003
[6/200] Training loss: 0.04210812
[7/200] Training loss: 0.03998160
[8/200] Training loss: 0.03895179
[9/200] Training loss: 0.03655510
[10/200] Training loss: 0.03585179
[50/200] Training loss: 0.01812239
[100/200] Training loss: 0.01569481
[150/200] Training loss: 0.01445122
[200/200] Training loss: 0.01232236
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15047.122249785838 ----------
[1/200] Training loss: 0.15282261
[2/200] Training loss: 0.05547650
[3/200] Training loss: 0.05229267
[4/200] Training loss: 0.04840260
[5/200] Training loss: 0.04554873
[6/200] Training loss: 0.04436602
[7/200] Training loss: 0.04263803
[8/200] Training loss: 0.03736896
[9/200] Training loss: 0.03721735
[10/200] Training loss: 0.03677312
[50/200] Training loss: 0.01715657
[100/200] Training loss: 0.01411598
[150/200] Training loss: 0.01323331
[200/200] Training loss: 0.01219742
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18639.328743278285 ----------
[1/200] Training loss: 0.14727096
[2/200] Training loss: 0.06232708
[3/200] Training loss: 0.05453299
[4/200] Training loss: 0.05273080
[5/200] Training loss: 0.04732601
[6/200] Training loss: 0.04787914
[7/200] Training loss: 0.04681869
[8/200] Training loss: 0.04221139
[9/200] Training loss: 0.03821282
[10/200] Training loss: 0.03586290
[50/200] Training loss: 0.01757605
[100/200] Training loss: 0.01517455
[150/200] Training loss: 0.01374729
[200/200] Training loss: 0.01295257
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15032.556668777272 ----------
[1/200] Training loss: 0.16082588
[2/200] Training loss: 0.07804457
[3/200] Training loss: 0.07533808
[4/200] Training loss: 0.06951114
[5/200] Training loss: 0.06022129
[6/200] Training loss: 0.05462060
[7/200] Training loss: 0.05241683
[8/200] Training loss: 0.05086556
[9/200] Training loss: 0.04925759
[10/200] Training loss: 0.04965027
[50/200] Training loss: 0.03328213
[100/200] Training loss: 0.02654529
[150/200] Training loss: 0.02223286
[200/200] Training loss: 0.02435820
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: SGD ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10126.441823266452 ----------
[1/200] Training loss: 0.16622728
[2/200] Training loss: 0.05390477
[3/200] Training loss: 0.05288815
[4/200] Training loss: 0.04411301
[5/200] Training loss: 0.03758444
[6/200] Training loss: 0.03765961
[7/200] Training loss: 0.03769394
[8/200] Training loss: 0.03725215
[9/200] Training loss: 0.03305277
[10/200] Training loss: 0.03248359
[50/200] Training loss: 0.01744843
[100/200] Training loss: 0.01524082
[150/200] Training loss: 0.01381330
[200/200] Training loss: 0.01315338
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16470.69737442832 ----------
[1/200] Training loss: 0.17456516
[2/200] Training loss: 0.05827359
[3/200] Training loss: 0.05246781
[4/200] Training loss: 0.05043659
[5/200] Training loss: 0.04368490
[6/200] Training loss: 0.04151234
[7/200] Training loss: 0.03872857
[8/200] Training loss: 0.03837485
[9/200] Training loss: 0.03438750
[10/200] Training loss: 0.03355576
[50/200] Training loss: 0.01809443
[100/200] Training loss: 0.01621996
[150/200] Training loss: 0.01443377
[200/200] Training loss: 0.01381426
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7231.571889983533 ----------
[1/200] Training loss: 0.16627163
[2/200] Training loss: 0.06328692
[3/200] Training loss: 0.05457176
[4/200] Training loss: 0.05342103
[5/200] Training loss: 0.04686015
[6/200] Training loss: 0.04532867
[7/200] Training loss: 0.04311511
[8/200] Training loss: 0.04202948
[9/200] Training loss: 0.04065931
[10/200] Training loss: 0.03597817
[50/200] Training loss: 0.01947306
[100/200] Training loss: 0.01595098
[150/200] Training loss: 0.01455213
[200/200] Training loss: 0.01335372
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17080.955476787592 ----------
[1/200] Training loss: 0.16565948
[2/200] Training loss: 0.05956274
[3/200] Training loss: 0.05085059
[4/200] Training loss: 0.05096933
[5/200] Training loss: 0.04916332
[6/200] Training loss: 0.04621909
[7/200] Training loss: 0.04410685
[8/200] Training loss: 0.03984916
[9/200] Training loss: 0.03717326
[10/200] Training loss: 0.03254780
[50/200] Training loss: 0.01638248
[100/200] Training loss: 0.01461052
[150/200] Training loss: 0.01384580
[200/200] Training loss: 0.01296039
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15441.62685729713 ----------
[1/200] Training loss: 0.17125308
[2/200] Training loss: 0.05798712
[3/200] Training loss: 0.04757926
[4/200] Training loss: 0.04304503
[5/200] Training loss: 0.04404216
[6/200] Training loss: 0.03767666
[7/200] Training loss: 0.03334113
[8/200] Training loss: 0.03356584
[9/200] Training loss: 0.03370520
[10/200] Training loss: 0.03175358
[50/200] Training loss: 0.01862563
[100/200] Training loss: 0.01662521
[150/200] Training loss: 0.01430236
[200/200] Training loss: 0.01399680
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9281.16285817677 ----------
[1/200] Training loss: 0.17477914
[2/200] Training loss: 0.06188410
[3/200] Training loss: 0.05203043
[4/200] Training loss: 0.04565624
[5/200] Training loss: 0.04158734
[6/200] Training loss: 0.04275156
[7/200] Training loss: 0.03637857
[8/200] Training loss: 0.03650230
[9/200] Training loss: 0.03285835
[10/200] Training loss: 0.03187844
[50/200] Training loss: 0.01667632
[100/200] Training loss: 0.01472464
[150/200] Training loss: 0.01323647
[200/200] Training loss: 0.01305253
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11979.277440647245 ----------
[1/200] Training loss: 0.17842888
[2/200] Training loss: 0.05811390
[3/200] Training loss: 0.05334994
[4/200] Training loss: 0.05159095
[5/200] Training loss: 0.04578196
[6/200] Training loss: 0.04606100
[7/200] Training loss: 0.04361369
[8/200] Training loss: 0.04138630
[9/200] Training loss: 0.03913472
[10/200] Training loss: 0.03818140
[50/200] Training loss: 0.01997437
[100/200] Training loss: 0.01613969
[150/200] Training loss: 0.01456097
[200/200] Training loss: 0.01298503
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11321.41545920827 ----------
[1/200] Training loss: 0.14677299
[2/200] Training loss: 0.06049442
[3/200] Training loss: 0.05218305
[4/200] Training loss: 0.04979143
[5/200] Training loss: 0.04242010
[6/200] Training loss: 0.04187315
[7/200] Training loss: 0.03948642
[8/200] Training loss: 0.03648688
[9/200] Training loss: 0.03476530
[10/200] Training loss: 0.02945453
[50/200] Training loss: 0.01835581
[100/200] Training loss: 0.01574503
[150/200] Training loss: 0.01432037
[200/200] Training loss: 0.01365935
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17492.91605193371 ----------
[1/200] Training loss: 0.14899940
[2/200] Training loss: 0.05835422
[3/200] Training loss: 0.05154403
[4/200] Training loss: 0.04969344
[5/200] Training loss: 0.04563439
[6/200] Training loss: 0.04373470
[7/200] Training loss: 0.04090952
[8/200] Training loss: 0.03767841
[9/200] Training loss: 0.03495318
[10/200] Training loss: 0.03610754
[50/200] Training loss: 0.01705647
[100/200] Training loss: 0.01499041
[150/200] Training loss: 0.01339981
[200/200] Training loss: 0.01202482
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 32 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13366.838369637002 ----------
[1/200] Training loss: 0.11335997
[2/200] Training loss: 0.04864039
[3/200] Training loss: 0.04851954
[4/200] Training loss: 0.04406260
[5/200] Training loss: 0.03913365
[6/200] Training loss: 0.03701464
[7/200] Training loss: 0.03486464
[8/200] Training loss: 0.03292034
[9/200] Training loss: 0.03458869
[10/200] Training loss: 0.03088334
[50/200] Training loss: 0.01584990
[100/200] Training loss: 0.01325990
[150/200] Training loss: 0.01243216
[200/200] Training loss: 0.01127521
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7481.287055046077 ----------
[1/200] Training loss: 0.15731853
[2/200] Training loss: 0.06001671
[3/200] Training loss: 0.04828844
[4/200] Training loss: 0.04464286
[5/200] Training loss: 0.04307224
[6/200] Training loss: 0.03876974
[7/200] Training loss: 0.03620693
[8/200] Training loss: 0.03315325
[9/200] Training loss: 0.03211360
[10/200] Training loss: 0.03063093
[50/200] Training loss: 0.01925024
[100/200] Training loss: 0.01573748
[150/200] Training loss: 0.01514136
[200/200] Training loss: 0.01395201
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9563.329545717852 ----------
[1/200] Training loss: 0.14450370
[2/200] Training loss: 0.05509306
[3/200] Training loss: 0.05398774
[4/200] Training loss: 0.04786987
[5/200] Training loss: 0.04952751
[6/200] Training loss: 0.04425209
[7/200] Training loss: 0.04561662
[8/200] Training loss: 0.04083174
[9/200] Training loss: 0.03933104
[10/200] Training loss: 0.03616358
[50/200] Training loss: 0.01882683
[100/200] Training loss: 0.01423376
[150/200] Training loss: 0.01305032
[200/200] Training loss: 0.01142864
---batch_size---: 16 ---n_steps---: 2 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12106.478926591331 ----------
[1/200] Training loss: 0.17440876
[2/200] Training loss: 0.06038964
[3/200] Training loss: 0.05551435
[4/200] Training loss: 0.05297545
[5/200] Training loss: 0.05327124
[6/200] Training loss: 0.04838533
[7/200] Training loss: 0.04702592
[8/200] Training loss: 0.04421152
[9/200] Training loss: 0.04130425
[10/200] Training loss: 0.03899033
[50/200] Training loss: 0.01830877
[100/200] Training loss: 0.01621885
[150/200] Training loss: 0.01395868
[200/200] Training loss: 0.01245304
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20047.426967069863 ----------
[1/200] Training loss: 0.15962017
[2/200] Training loss: 0.05233337
[3/200] Training loss: 0.04986004
[4/200] Training loss: 0.04374703
[5/200] Training loss: 0.04235648
[6/200] Training loss: 0.04263818
[7/200] Training loss: 0.04072472
[8/200] Training loss: 0.03600545
[9/200] Training loss: 0.03422465
[10/200] Training loss: 0.03574764
[50/200] Training loss: 0.01854574
[100/200] Training loss: 0.01464096
[150/200] Training loss: 0.01326203
[200/200] Training loss: 0.01254998
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11765.804689862907 ----------
[1/200] Training loss: 0.17160360
[2/200] Training loss: 0.05788869
[3/200] Training loss: 0.05305504
[4/200] Training loss: 0.04679798
[5/200] Training loss: 0.04539576
[6/200] Training loss: 0.04170580
[7/200] Training loss: 0.03782236
[8/200] Training loss: 0.03886528
[9/200] Training loss: 0.03470169
[10/200] Training loss: 0.03347005
[50/200] Training loss: 0.02083847
[100/200] Training loss: 0.01544782
[150/200] Training loss: 0.01338931
[200/200] Training loss: 0.01262239
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6530.647747352478 ----------
[1/200] Training loss: 0.17366757
[2/200] Training loss: 0.06396644
[3/200] Training loss: 0.05137729
[4/200] Training loss: 0.05281530
[5/200] Training loss: 0.05018060
[6/200] Training loss: 0.04711105
[7/200] Training loss: 0.04286747
[8/200] Training loss: 0.03757720
[9/200] Training loss: 0.03741483
[10/200] Training loss: 0.03537714
[50/200] Training loss: 0.01888598
[100/200] Training loss: 0.01439936
[150/200] Training loss: 0.01249778
[200/200] Training loss: 0.01145439
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17072.15651287206 ----------
[1/200] Training loss: 0.19639016
[2/200] Training loss: 0.05978745
[3/200] Training loss: 0.05238219
[4/200] Training loss: 0.05128520
[5/200] Training loss: 0.04675193
[6/200] Training loss: 0.04424014
[7/200] Training loss: 0.04110855
[8/200] Training loss: 0.04158316
[9/200] Training loss: 0.03685583
[10/200] Training loss: 0.03644220
[50/200] Training loss: 0.01814073
[100/200] Training loss: 0.01540202
[150/200] Training loss: 0.01371509
[200/200] Training loss: 0.01305265
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13082.142943722944 ----------
[1/200] Training loss: 0.17921479
[2/200] Training loss: 0.06182461
[3/200] Training loss: 0.05503376
[4/200] Training loss: 0.05108296
[5/200] Training loss: 0.04346948
[6/200] Training loss: 0.04042754
[7/200] Training loss: 0.03864755
[8/200] Training loss: 0.03573553
[9/200] Training loss: 0.03404887
[10/200] Training loss: 0.03324361
[50/200] Training loss: 0.01644997
[100/200] Training loss: 0.01337716
[150/200] Training loss: 0.01170971
[200/200] Training loss: 0.01110115
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5012.644212389306 ----------
[1/200] Training loss: 0.16778652
[2/200] Training loss: 0.06221119
[3/200] Training loss: 0.05242679
[4/200] Training loss: 0.05200064
[5/200] Training loss: 0.04811758
[6/200] Training loss: 0.04682270
[7/200] Training loss: 0.04485683
[8/200] Training loss: 0.04228031
[9/200] Training loss: 0.04191884
[10/200] Training loss: 0.03630034
[50/200] Training loss: 0.01795334
[100/200] Training loss: 0.01558466
[150/200] Training loss: 0.01427249
[200/200] Training loss: 0.01361261
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5993.598585157334 ----------
[1/200] Training loss: 0.13772568
[2/200] Training loss: 0.05682120
[3/200] Training loss: 0.05197782
[4/200] Training loss: 0.04664741
[5/200] Training loss: 0.04465853
[6/200] Training loss: 0.04425670
[7/200] Training loss: 0.04129610
[8/200] Training loss: 0.03610706
[9/200] Training loss: 0.03338258
[10/200] Training loss: 0.03240625
[50/200] Training loss: 0.01716574
[100/200] Training loss: 0.01513796
[150/200] Training loss: 0.01395473
[200/200] Training loss: 0.01267158
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13030.78263190665 ----------
[1/200] Training loss: 0.14643213
[2/200] Training loss: 0.06186598
[3/200] Training loss: 0.05334362
[4/200] Training loss: 0.04943254
[5/200] Training loss: 0.04732777
[6/200] Training loss: 0.04537047
[7/200] Training loss: 0.03883229
[8/200] Training loss: 0.03832537
[9/200] Training loss: 0.03478113
[10/200] Training loss: 0.03165473
[50/200] Training loss: 0.01781638
[100/200] Training loss: 0.01476230
[150/200] Training loss: 0.01278673
[200/200] Training loss: 0.01175882
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15930.939457546125 ----------
[1/200] Training loss: 0.15811109
[2/200] Training loss: 0.05918945
[3/200] Training loss: 0.05267329
[4/200] Training loss: 0.04798631
[5/200] Training loss: 0.04486047
[6/200] Training loss: 0.04307503
[7/200] Training loss: 0.03813841
[8/200] Training loss: 0.03809816
[9/200] Training loss: 0.03498064
[10/200] Training loss: 0.03520609
[50/200] Training loss: 0.01771175
[100/200] Training loss: 0.01482409
[150/200] Training loss: 0.01310995
[200/200] Training loss: 0.01219935
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20665.222573202544 ----------
[1/200] Training loss: 0.15858686
[2/200] Training loss: 0.05542032
[3/200] Training loss: 0.05129390
[4/200] Training loss: 0.04564847
[5/200] Training loss: 0.04639021
[6/200] Training loss: 0.04137210
[7/200] Training loss: 0.04035764
[8/200] Training loss: 0.03783228
[9/200] Training loss: 0.03723262
[10/200] Training loss: 0.03314583
[50/200] Training loss: 0.01737210
[100/200] Training loss: 0.01446905
[150/200] Training loss: 0.01263908
[200/200] Training loss: 0.01252169
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8225.750786402417 ----------
[1/200] Training loss: 0.18084060
[2/200] Training loss: 0.06569699
[3/200] Training loss: 0.05666454
[4/200] Training loss: 0.05293523
[5/200] Training loss: 0.04773210
[6/200] Training loss: 0.04641091
[7/200] Training loss: 0.04101965
[8/200] Training loss: 0.04063780
[9/200] Training loss: 0.04005252
[10/200] Training loss: 0.03620747
[50/200] Training loss: 0.01959418
[100/200] Training loss: 0.01520102
[150/200] Training loss: 0.01357655
[200/200] Training loss: 0.01223320
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 37656.78945422724 ----------
[1/200] Training loss: 0.17257926
[2/200] Training loss: 0.05878811
[3/200] Training loss: 0.05230828
[4/200] Training loss: 0.04880782
[5/200] Training loss: 0.04363528
[6/200] Training loss: 0.04232442
[7/200] Training loss: 0.04029364
[8/200] Training loss: 0.03798134
[9/200] Training loss: 0.03727498
[10/200] Training loss: 0.03616267
[50/200] Training loss: 0.01866891
[100/200] Training loss: 0.01552017
[150/200] Training loss: 0.01446995
[200/200] Training loss: 0.01380864
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9636.592343769658 ----------
[1/200] Training loss: 0.17100837
[2/200] Training loss: 0.06108473
[3/200] Training loss: 0.05357566
[4/200] Training loss: 0.04844488
[5/200] Training loss: 0.04316403
[6/200] Training loss: 0.04292266
[7/200] Training loss: 0.03750353
[8/200] Training loss: 0.03460428
[9/200] Training loss: 0.03604094
[10/200] Training loss: 0.03120639
[50/200] Training loss: 0.01836780
[100/200] Training loss: 0.01458582
[150/200] Training loss: 0.01396510
[200/200] Training loss: 0.01203123
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7939.611325499504 ----------
[1/200] Training loss: 0.17981664
[2/200] Training loss: 0.05777713
[3/200] Training loss: 0.05641340
[4/200] Training loss: 0.04864101
[5/200] Training loss: 0.04413565
[6/200] Training loss: 0.04121723
[7/200] Training loss: 0.03481062
[8/200] Training loss: 0.03576183
[9/200] Training loss: 0.03038969
[10/200] Training loss: 0.02910782
[50/200] Training loss: 0.01636511
[100/200] Training loss: 0.01375699
[150/200] Training loss: 0.01220615
[200/200] Training loss: 0.01163152
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19675.988208982035 ----------
[1/200] Training loss: 0.14390534
[2/200] Training loss: 0.05691825
[3/200] Training loss: 0.05370396
[4/200] Training loss: 0.04735051
[5/200] Training loss: 0.04395484
[6/200] Training loss: 0.04189120
[7/200] Training loss: 0.03732528
[8/200] Training loss: 0.03403174
[9/200] Training loss: 0.03063875
[10/200] Training loss: 0.03122719
[50/200] Training loss: 0.01831136
[100/200] Training loss: 0.01677077
[150/200] Training loss: 0.01482838
[200/200] Training loss: 0.01363878
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17717.8382428557 ----------
[1/200] Training loss: 0.17641920
[2/200] Training loss: 0.06011056
[3/200] Training loss: 0.05995281
[4/200] Training loss: 0.05387339
[5/200] Training loss: 0.04952434
[6/200] Training loss: 0.05123565
[7/200] Training loss: 0.04797822
[8/200] Training loss: 0.04396837
[9/200] Training loss: 0.03737659
[10/200] Training loss: 0.03481949
[50/200] Training loss: 0.01795613
[100/200] Training loss: 0.01645948
[150/200] Training loss: 0.01453474
[200/200] Training loss: 0.01385317
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20125.59842588538 ----------
[1/200] Training loss: 0.17299857
[2/200] Training loss: 0.05527028
[3/200] Training loss: 0.04866320
[4/200] Training loss: 0.04576974
[5/200] Training loss: 0.04192006
[6/200] Training loss: 0.03864429
[7/200] Training loss: 0.03593632
[8/200] Training loss: 0.03800888
[9/200] Training loss: 0.03284926
[10/200] Training loss: 0.03292236
[50/200] Training loss: 0.01761774
[100/200] Training loss: 0.01403759
[150/200] Training loss: 0.01352973
[200/200] Training loss: 0.01198929
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6639.637038272499 ----------
[1/200] Training loss: 0.16853396
[2/200] Training loss: 0.06199276
[3/200] Training loss: 0.05745541
[4/200] Training loss: 0.05533309
[5/200] Training loss: 0.05023155
[6/200] Training loss: 0.04796324
[7/200] Training loss: 0.04558510
[8/200] Training loss: 0.04175275
[9/200] Training loss: 0.03951970
[10/200] Training loss: 0.03818351
[50/200] Training loss: 0.01758026
[100/200] Training loss: 0.01525434
[150/200] Training loss: 0.01427986
[200/200] Training loss: 0.01347600
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12717.599773542175 ----------
[1/200] Training loss: 0.07748206
[2/200] Training loss: 0.04448776
[3/200] Training loss: 0.03513169
[4/200] Training loss: 0.03063893
[5/200] Training loss: 0.02747041
[6/200] Training loss: 0.02541908
[7/200] Training loss: 0.02440857
[8/200] Training loss: 0.02374042
[9/200] Training loss: 0.02257346
[10/200] Training loss: 0.02222667
[50/200] Training loss: 0.01579672
[100/200] Training loss: 0.01333997
[150/200] Training loss: 0.01197757
[200/200] Training loss: 0.01087278
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13786.209341222118 ----------
[1/200] Training loss: 0.16278343
[2/200] Training loss: 0.06332449
[3/200] Training loss: 0.05453202
[4/200] Training loss: 0.04886914
[5/200] Training loss: 0.04745572
[6/200] Training loss: 0.04335129
[7/200] Training loss: 0.04167177
[8/200] Training loss: 0.03692828
[9/200] Training loss: 0.04157124
[10/200] Training loss: 0.03415411
[50/200] Training loss: 0.01697846
[100/200] Training loss: 0.01430770
[150/200] Training loss: 0.01319463
[200/200] Training loss: 0.01140117
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17015.455562517272 ----------
[1/200] Training loss: 0.20843379
[2/200] Training loss: 0.06663197
[3/200] Training loss: 0.05433290
[4/200] Training loss: 0.04955247
[5/200] Training loss: 0.04975156
[6/200] Training loss: 0.04617603
[7/200] Training loss: 0.04556435
[8/200] Training loss: 0.04501702
[9/200] Training loss: 0.04062387
[10/200] Training loss: 0.03985432
[50/200] Training loss: 0.01989597
[100/200] Training loss: 0.01597589
[150/200] Training loss: 0.01462290
[200/200] Training loss: 0.01398112
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17869.27284474665 ----------
[1/200] Training loss: 0.15305878
[2/200] Training loss: 0.05948656
[3/200] Training loss: 0.05059021
[4/200] Training loss: 0.04983705
[5/200] Training loss: 0.04380660
[6/200] Training loss: 0.04282834
[7/200] Training loss: 0.03977937
[8/200] Training loss: 0.03769013
[9/200] Training loss: 0.03740405
[10/200] Training loss: 0.03475252
[50/200] Training loss: 0.01745950
[100/200] Training loss: 0.01456422
[150/200] Training loss: 0.01328098
[200/200] Training loss: 0.01269711
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12426.328178508726 ----------
[1/200] Training loss: 0.16252902
[2/200] Training loss: 0.05882696
[3/200] Training loss: 0.04858291
[4/200] Training loss: 0.04425372
[5/200] Training loss: 0.04307537
[6/200] Training loss: 0.03971115
[7/200] Training loss: 0.03506898
[8/200] Training loss: 0.03419752
[9/200] Training loss: 0.03222144
[10/200] Training loss: 0.02853124
[50/200] Training loss: 0.01718855
[100/200] Training loss: 0.01377835
[150/200] Training loss: 0.01302317
[200/200] Training loss: 0.01190973
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15476.237785715235 ----------
[1/200] Training loss: 0.12844726
[2/200] Training loss: 0.05698150
[3/200] Training loss: 0.05187291
[4/200] Training loss: 0.04817327
[5/200] Training loss: 0.04497052
[6/200] Training loss: 0.04186055
[7/200] Training loss: 0.04058735
[8/200] Training loss: 0.03798790
[9/200] Training loss: 0.03492778
[10/200] Training loss: 0.03443507
[50/200] Training loss: 0.02084152
[100/200] Training loss: 0.01537152
[150/200] Training loss: 0.01374929
[200/200] Training loss: 0.01264943
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.013062663220525995
----FITNESS-----------RMSE---- 7996.848879402436 ----------
[1/200] Training loss: 0.16866964
[2/200] Training loss: 0.06334362
[3/200] Training loss: 0.05035155
[4/200] Training loss: 0.05260591
[5/200] Training loss: 0.04846504
[6/200] Training loss: 0.04549376
[7/200] Training loss: 0.04462685
[8/200] Training loss: 0.04153756
[9/200] Training loss: 0.03915986
[10/200] Training loss: 0.03911191
[50/200] Training loss: 0.01785634
[100/200] Training loss: 0.01495820
[150/200] Training loss: 0.01316617
[200/200] Training loss: 0.01197173
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10695.422946288754 ----------
[1/200] Training loss: 0.13966996
[2/200] Training loss: 0.05859082
[3/200] Training loss: 0.05018671
[4/200] Training loss: 0.04954889
[5/200] Training loss: 0.04186124
[6/200] Training loss: 0.04062913
[7/200] Training loss: 0.03853122
[8/200] Training loss: 0.03525531
[9/200] Training loss: 0.03366733
[10/200] Training loss: 0.03118239
[50/200] Training loss: 0.01754387
[100/200] Training loss: 0.01455811
[150/200] Training loss: 0.01360086
[200/200] Training loss: 0.01218048
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19452.239768211784 ----------
[1/200] Training loss: 0.14839407
[2/200] Training loss: 0.05563452
[3/200] Training loss: 0.04671942
[4/200] Training loss: 0.04390226
[5/200] Training loss: 0.03873152
[6/200] Training loss: 0.03601373
[7/200] Training loss: 0.03450428
[8/200] Training loss: 0.02966121
[9/200] Training loss: 0.03003268
[10/200] Training loss: 0.02713305
[50/200] Training loss: 0.01627216
[100/200] Training loss: 0.01417990
[150/200] Training loss: 0.01340817
[200/200] Training loss: 0.01230041
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12061.550149130915 ----------
[1/200] Training loss: 0.16548910
[2/200] Training loss: 0.06080049
[3/200] Training loss: 0.05196834
[4/200] Training loss: 0.05158048
[5/200] Training loss: 0.04903751
[6/200] Training loss: 0.04294766
[7/200] Training loss: 0.04089896
[8/200] Training loss: 0.03660567
[9/200] Training loss: 0.03494320
[10/200] Training loss: 0.03117309
[50/200] Training loss: 0.01754759
[100/200] Training loss: 0.01366083
[150/200] Training loss: 0.01354266
[200/200] Training loss: 0.01197159
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22509.168976219447 ----------
[1/200] Training loss: 0.17540898
[2/200] Training loss: 0.06165017
[3/200] Training loss: 0.05288198
[4/200] Training loss: 0.05341978
[5/200] Training loss: 0.04983339
[6/200] Training loss: 0.04702241
[7/200] Training loss: 0.04121936
[8/200] Training loss: 0.04266126
[9/200] Training loss: 0.03917152
[10/200] Training loss: 0.03729146
[50/200] Training loss: 0.02155271
[100/200] Training loss: 0.01737852
[150/200] Training loss: 0.01385630
[200/200] Training loss: 0.01311080
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9582.33791931802 ----------
[1/200] Training loss: 0.16343177
[2/200] Training loss: 0.06327049
[3/200] Training loss: 0.05575516
[4/200] Training loss: 0.05099413
[5/200] Training loss: 0.04948102
[6/200] Training loss: 0.04778449
[7/200] Training loss: 0.04215989
[8/200] Training loss: 0.03815305
[9/200] Training loss: 0.03765247
[10/200] Training loss: 0.03497328
[50/200] Training loss: 0.01875174
[100/200] Training loss: 0.01715483
[150/200] Training loss: 0.01610659
[200/200] Training loss: 0.01518573
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9901.180939665732 ----------
[1/200] Training loss: 0.15397818
[2/200] Training loss: 0.05871132
[3/200] Training loss: 0.04848017
[4/200] Training loss: 0.04767063
[5/200] Training loss: 0.04610051
[6/200] Training loss: 0.04273313
[7/200] Training loss: 0.04157929
[8/200] Training loss: 0.04008826
[9/200] Training loss: 0.03507869
[10/200] Training loss: 0.03518855
[50/200] Training loss: 0.01785887
[100/200] Training loss: 0.01418253
[150/200] Training loss: 0.01322379
[200/200] Training loss: 0.01104278
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7537.140306508829 ----------
[1/200] Training loss: 0.14151880
[2/200] Training loss: 0.05721706
[3/200] Training loss: 0.05031140
[4/200] Training loss: 0.04923220
[5/200] Training loss: 0.04565912
[6/200] Training loss: 0.04292937
[7/200] Training loss: 0.03956942
[8/200] Training loss: 0.03907588
[9/200] Training loss: 0.03273247
[10/200] Training loss: 0.03519991
[50/200] Training loss: 0.01876486
[100/200] Training loss: 0.01468724
[150/200] Training loss: 0.01350134
[200/200] Training loss: 0.01146960
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10299.433382473038 ----------
[1/200] Training loss: 0.15463053
[2/200] Training loss: 0.05871851
[3/200] Training loss: 0.04813906
[4/200] Training loss: 0.04507589
[5/200] Training loss: 0.04321903
[6/200] Training loss: 0.03964268
[7/200] Training loss: 0.03737285
[8/200] Training loss: 0.03528381
[9/200] Training loss: 0.03252761
[10/200] Training loss: 0.03308718
[50/200] Training loss: 0.01674624
[100/200] Training loss: 0.01416220
[150/200] Training loss: 0.01284638
[200/200] Training loss: 0.01199837
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13882.961643683959 ----------
[1/200] Training loss: 0.17129187
[2/200] Training loss: 0.05713389
[3/200] Training loss: 0.05285490
[4/200] Training loss: 0.04745521
[5/200] Training loss: 0.04382843
[6/200] Training loss: 0.04181152
[7/200] Training loss: 0.04053719
[8/200] Training loss: 0.03804703
[9/200] Training loss: 0.03825612
[10/200] Training loss: 0.03399197
[50/200] Training loss: 0.01918908
[100/200] Training loss: 0.01534616
[150/200] Training loss: 0.01280774
[200/200] Training loss: 0.01267061
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 29210.612865874624 ----------
[1/200] Training loss: 0.16818568
[2/200] Training loss: 0.05400185
[3/200] Training loss: 0.05192843
[4/200] Training loss: 0.04471649
[5/200] Training loss: 0.04237974
[6/200] Training loss: 0.03961395
[7/200] Training loss: 0.03735793
[8/200] Training loss: 0.03571969
[9/200] Training loss: 0.03313927
[10/200] Training loss: 0.03107823
[50/200] Training loss: 0.01828788
[100/200] Training loss: 0.01611402
[150/200] Training loss: 0.01509089
[200/200] Training loss: 0.01353295
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6999.87942753302 ----------
[1/200] Training loss: 0.16901907
[2/200] Training loss: 0.05921630
[3/200] Training loss: 0.05312336
[4/200] Training loss: 0.04849912
[5/200] Training loss: 0.04521874
[6/200] Training loss: 0.04521952
[7/200] Training loss: 0.04095487
[8/200] Training loss: 0.03845939
[9/200] Training loss: 0.03451540
[10/200] Training loss: 0.03240390
[50/200] Training loss: 0.01683529
[100/200] Training loss: 0.01528231
[150/200] Training loss: 0.01462314
[200/200] Training loss: 0.01283253
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8542.451638727609 ----------
[1/200] Training loss: 0.15969292
[2/200] Training loss: 0.05754414
[3/200] Training loss: 0.05294512
[4/200] Training loss: 0.04828621
[5/200] Training loss: 0.04517506
[6/200] Training loss: 0.03921017
[7/200] Training loss: 0.03650234
[8/200] Training loss: 0.03430847
[9/200] Training loss: 0.03304647
[10/200] Training loss: 0.03147388
[50/200] Training loss: 0.01633087
[100/200] Training loss: 0.01407554
[150/200] Training loss: 0.01304121
[200/200] Training loss: 0.01207962
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7492.398814798903 ----------
[1/200] Training loss: 0.15737280
[2/200] Training loss: 0.06099459
[3/200] Training loss: 0.05487311
[4/200] Training loss: 0.04951470
[5/200] Training loss: 0.04349464
[6/200] Training loss: 0.04017609
[7/200] Training loss: 0.03840223
[8/200] Training loss: 0.03369805
[9/200] Training loss: 0.03149164
[10/200] Training loss: 0.03075856
[50/200] Training loss: 0.01908979
[100/200] Training loss: 0.01652076
[150/200] Training loss: 0.01480706
[200/200] Training loss: 0.01379655
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12547.52310219033 ----------
[1/200] Training loss: 0.15290651
[2/200] Training loss: 0.05469780
[3/200] Training loss: 0.04867613
[4/200] Training loss: 0.04431079
[5/200] Training loss: 0.04107486
[6/200] Training loss: 0.03909072
[7/200] Training loss: 0.03789354
[8/200] Training loss: 0.03434147
[9/200] Training loss: 0.03438065
[10/200] Training loss: 0.03279235
[50/200] Training loss: 0.01935888
[100/200] Training loss: 0.01546905
[150/200] Training loss: 0.01363279
[200/200] Training loss: 0.01144215
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20697.533766127788 ----------
[1/200] Training loss: 0.13014015
[2/200] Training loss: 0.05620566
[3/200] Training loss: 0.04948365
[4/200] Training loss: 0.04370901
[5/200] Training loss: 0.04577013
[6/200] Training loss: 0.03728754
[7/200] Training loss: 0.03563667
[8/200] Training loss: 0.03427733
[9/200] Training loss: 0.02854398
[10/200] Training loss: 0.02860040
[50/200] Training loss: 0.01761101
[100/200] Training loss: 0.01504967
[150/200] Training loss: 0.01360165
[200/200] Training loss: 0.01272321
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13401.099059405538 ----------
[1/200] Training loss: 0.15617158
[2/200] Training loss: 0.06076898
[3/200] Training loss: 0.05335979
[4/200] Training loss: 0.05160039
[5/200] Training loss: 0.04829331
[6/200] Training loss: 0.04442106
[7/200] Training loss: 0.04277448
[8/200] Training loss: 0.04003039
[9/200] Training loss: 0.03768678
[10/200] Training loss: 0.03682694
[50/200] Training loss: 0.01810481
[100/200] Training loss: 0.01522985
[150/200] Training loss: 0.01335384
[200/200] Training loss: 0.01207460
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6368.573152598626 ----------
[1/200] Training loss: 0.16582433
[2/200] Training loss: 0.05702863
[3/200] Training loss: 0.04983835
[4/200] Training loss: 0.04651128
[5/200] Training loss: 0.04351652
[6/200] Training loss: 0.04297090
[7/200] Training loss: 0.03944172
[8/200] Training loss: 0.03948718
[9/200] Training loss: 0.03709573
[10/200] Training loss: 0.03404816
[50/200] Training loss: 0.01922050
[100/200] Training loss: 0.01546707
[150/200] Training loss: 0.01461355
[200/200] Training loss: 0.01288808
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23176.813240823252 ----------
[1/200] Training loss: 0.16101320
[2/200] Training loss: 0.05995374
[3/200] Training loss: 0.05286585
[4/200] Training loss: 0.05202544
[5/200] Training loss: 0.04350847
[6/200] Training loss: 0.03839467
[7/200] Training loss: 0.03961363
[8/200] Training loss: 0.03731438
[9/200] Training loss: 0.03158832
[10/200] Training loss: 0.03096490
[50/200] Training loss: 0.01941473
[100/200] Training loss: 0.01735452
[150/200] Training loss: 0.01574798
[200/200] Training loss: 0.01491677
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5204202895800791 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14106.965655306602 ----------
[1/200] Training loss: 0.16675258
[2/200] Training loss: 0.05731774
[3/200] Training loss: 0.05510352
[4/200] Training loss: 0.05108203
[5/200] Training loss: 0.04742594
[6/200] Training loss: 0.04501176
[7/200] Training loss: 0.04118511
[8/200] Training loss: 0.04252527
[9/200] Training loss: 0.03905010
[10/200] Training loss: 0.03287984
[50/200] Training loss: 0.01780673
[100/200] Training loss: 0.01545017
[150/200] Training loss: 0.01394180
[200/200] Training loss: 0.01249806
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13374.464624799006 ----------
[1/200] Training loss: 0.15148001
[2/200] Training loss: 0.05417063
[3/200] Training loss: 0.04849840
[4/200] Training loss: 0.04612805
[5/200] Training loss: 0.04027726
[6/200] Training loss: 0.04085677
[7/200] Training loss: 0.03891376
[8/200] Training loss: 0.03628318
[9/200] Training loss: 0.03516039
[10/200] Training loss: 0.03284490
[50/200] Training loss: 0.01775211
[100/200] Training loss: 0.01567398
[150/200] Training loss: 0.01337160
[200/200] Training loss: 0.01216826
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8505.848340994566 ----------
[1/200] Training loss: 0.13757762
[2/200] Training loss: 0.05871549
[3/200] Training loss: 0.04952009
[4/200] Training loss: 0.04867973
[5/200] Training loss: 0.04638088
[6/200] Training loss: 0.04378497
[7/200] Training loss: 0.03925909
[8/200] Training loss: 0.03865254
[9/200] Training loss: 0.03850399
[10/200] Training loss: 0.03330885
[50/200] Training loss: 0.01745151
[100/200] Training loss: 0.01388309
[150/200] Training loss: 0.01305347
[200/200] Training loss: 0.01148814
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7262.569793124194 ----------
[1/200] Training loss: 0.18121819
[2/200] Training loss: 0.05480239
[3/200] Training loss: 0.05137913
[4/200] Training loss: 0.04667247
[5/200] Training loss: 0.04731045
[6/200] Training loss: 0.04058274
[7/200] Training loss: 0.04010113
[8/200] Training loss: 0.03739270
[9/200] Training loss: 0.03584843
[10/200] Training loss: 0.03286781
[50/200] Training loss: 0.01841642
[100/200] Training loss: 0.01615086
[150/200] Training loss: 0.01457637
[200/200] Training loss: 0.01426430
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9668.647475215963 ----------
[1/200] Training loss: 0.14362100
[2/200] Training loss: 0.05335254
[3/200] Training loss: 0.04882460
[4/200] Training loss: 0.04367836
[5/200] Training loss: 0.04162104
[6/200] Training loss: 0.03838365
[7/200] Training loss: 0.03901001
[8/200] Training loss: 0.03583898
[9/200] Training loss: 0.03402004
[10/200] Training loss: 0.02990590
[50/200] Training loss: 0.01803082
[100/200] Training loss: 0.01486324
[150/200] Training loss: 0.01336482
[200/200] Training loss: 0.01222526
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17822.484675263437 ----------
[1/200] Training loss: 0.16593665
[2/200] Training loss: 0.06392646
[3/200] Training loss: 0.05575738
[4/200] Training loss: 0.04754508
[5/200] Training loss: 0.04355972
[6/200] Training loss: 0.04424497
[7/200] Training loss: 0.03987194
[8/200] Training loss: 0.03820467
[9/200] Training loss: 0.03586575
[10/200] Training loss: 0.03587770
[50/200] Training loss: 0.01938354
[100/200] Training loss: 0.01513870
[150/200] Training loss: 0.01301946
[200/200] Training loss: 0.01171754
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7079.429638042884 ----------
[1/200] Training loss: 0.14756064
[2/200] Training loss: 0.05525410
[3/200] Training loss: 0.04896867
[4/200] Training loss: 0.04635639
[5/200] Training loss: 0.04320709
[6/200] Training loss: 0.04173995
[7/200] Training loss: 0.03972906
[8/200] Training loss: 0.03905040
[9/200] Training loss: 0.03529512
[10/200] Training loss: 0.03505396
[50/200] Training loss: 0.01779924
[100/200] Training loss: 0.01570490
[150/200] Training loss: 0.01353613
[200/200] Training loss: 0.01119405
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14941.058061596575 ----------
[1/200] Training loss: 0.17440975
[2/200] Training loss: 0.06147429
[3/200] Training loss: 0.05425881
[4/200] Training loss: 0.04812832
[5/200] Training loss: 0.04432676
[6/200] Training loss: 0.04253916
[7/200] Training loss: 0.03945447
[8/200] Training loss: 0.03738957
[9/200] Training loss: 0.03242501
[10/200] Training loss: 0.02975618
[50/200] Training loss: 0.01761503
[100/200] Training loss: 0.01572773
[150/200] Training loss: 0.01426002
[200/200] Training loss: 0.01287822
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 32178.327116243938 ----------
[1/200] Training loss: 0.16773249
[2/200] Training loss: 0.06062140
[3/200] Training loss: 0.05205412
[4/200] Training loss: 0.05170391
[5/200] Training loss: 0.04633953
[6/200] Training loss: 0.04399009
[7/200] Training loss: 0.04295184
[8/200] Training loss: 0.04159186
[9/200] Training loss: 0.03990562
[10/200] Training loss: 0.03971571
[50/200] Training loss: 0.01947609
[100/200] Training loss: 0.01530531
[150/200] Training loss: 0.01371774
[200/200] Training loss: 0.01222944
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15870.115815582443 ----------
[1/200] Training loss: 0.15364818
[2/200] Training loss: 0.05747595
[3/200] Training loss: 0.05311561
[4/200] Training loss: 0.04920561
[5/200] Training loss: 0.04502532
[6/200] Training loss: 0.04371831
[7/200] Training loss: 0.04257191
[8/200] Training loss: 0.03832919
[9/200] Training loss: 0.03571635
[10/200] Training loss: 0.03351709
[50/200] Training loss: 0.01696215
[100/200] Training loss: 0.01525825
[150/200] Training loss: 0.01405181
[200/200] Training loss: 0.01291066
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 27203.64446172608 ----------
[1/200] Training loss: 0.16780253
[2/200] Training loss: 0.06184600
[3/200] Training loss: 0.05572709
[4/200] Training loss: 0.05198079
[5/200] Training loss: 0.04754337
[6/200] Training loss: 0.04657562
[7/200] Training loss: 0.04234522
[8/200] Training loss: 0.04092151
[9/200] Training loss: 0.03915080
[10/200] Training loss: 0.03795306
[50/200] Training loss: 0.01769691
[100/200] Training loss: 0.01476453
[150/200] Training loss: 0.01274815
[200/200] Training loss: 0.01189053
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7162.0544538561 ----------
[1/200] Training loss: 0.19114215
[2/200] Training loss: 0.06040890
[3/200] Training loss: 0.05165237
[4/200] Training loss: 0.05014510
[5/200] Training loss: 0.04474318
[6/200] Training loss: 0.03917915
[7/200] Training loss: 0.03737983
[8/200] Training loss: 0.03562348
[9/200] Training loss: 0.03058050
[10/200] Training loss: 0.02955267
[50/200] Training loss: 0.01815781
[100/200] Training loss: 0.01529491
[150/200] Training loss: 0.01385779
[200/200] Training loss: 0.01295324
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8630.11981376852 ----------
[1/200] Training loss: 0.14608017
[2/200] Training loss: 0.05209505
[3/200] Training loss: 0.04987910
[4/200] Training loss: 0.04471040
[5/200] Training loss: 0.04399165
[6/200] Training loss: 0.03736838
[7/200] Training loss: 0.03817696
[8/200] Training loss: 0.03590737
[9/200] Training loss: 0.03212740
[10/200] Training loss: 0.02932328
[50/200] Training loss: 0.01733829
[100/200] Training loss: 0.01466327
[150/200] Training loss: 0.01407105
[200/200] Training loss: 0.01293654
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11041.567280055853 ----------
[1/200] Training loss: 0.16130082
[2/200] Training loss: 0.05811374
[3/200] Training loss: 0.05207022
[4/200] Training loss: 0.05105399
[5/200] Training loss: 0.04792327
[6/200] Training loss: 0.04278086
[7/200] Training loss: 0.04064323
[8/200] Training loss: 0.03611986
[9/200] Training loss: 0.03295802
[10/200] Training loss: 0.03143506
[50/200] Training loss: 0.01844345
[100/200] Training loss: 0.01704025
[150/200] Training loss: 0.01447800
[200/200] Training loss: 0.01309255
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9175.805141784562 ----------
[1/200] Training loss: 0.14484436
[2/200] Training loss: 0.05510608
[3/200] Training loss: 0.04966503
[4/200] Training loss: 0.04726524
[5/200] Training loss: 0.04292670
[6/200] Training loss: 0.04093497
[7/200] Training loss: 0.03969766
[8/200] Training loss: 0.03510919
[9/200] Training loss: 0.03555271
[10/200] Training loss: 0.03533740
[50/200] Training loss: 0.01591066
[100/200] Training loss: 0.01368827
[150/200] Training loss: 0.01198849
[200/200] Training loss: 0.01122396
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12520.48944730197 ----------
[1/200] Training loss: 0.16096986
[2/200] Training loss: 0.05567835
[3/200] Training loss: 0.05522043
[4/200] Training loss: 0.04559881
[5/200] Training loss: 0.04671425
[6/200] Training loss: 0.04319570
[7/200] Training loss: 0.03984185
[8/200] Training loss: 0.03809952
[9/200] Training loss: 0.03610447
[10/200] Training loss: 0.03555323
[50/200] Training loss: 0.01804593
[100/200] Training loss: 0.01414025
[150/200] Training loss: 0.01256819
[200/200] Training loss: 0.01168615
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6907.890850324721 ----------
[1/200] Training loss: 0.14404453
[2/200] Training loss: 0.05554313
[3/200] Training loss: 0.05070795
[4/200] Training loss: 0.04986494
[5/200] Training loss: 0.04719026
[6/200] Training loss: 0.04278977
[7/200] Training loss: 0.04174395
[8/200] Training loss: 0.03999707
[9/200] Training loss: 0.03760804
[10/200] Training loss: 0.03471464
[50/200] Training loss: 0.01868988
[100/200] Training loss: 0.01503119
[150/200] Training loss: 0.01338299
[200/200] Training loss: 0.01168087
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23784.596023477043 ----------
[1/200] Training loss: 0.18328431
[2/200] Training loss: 0.05824846
[3/200] Training loss: 0.05066638
[4/200] Training loss: 0.04906336
[5/200] Training loss: 0.04675335
[6/200] Training loss: 0.04306649
[7/200] Training loss: 0.04086109
[8/200] Training loss: 0.04071695
[9/200] Training loss: 0.04022100
[10/200] Training loss: 0.03581394
[50/200] Training loss: 0.01801901
[100/200] Training loss: 0.01611902
[150/200] Training loss: 0.01397677
[200/200] Training loss: 0.01294183
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10111.092522571436 ----------
[1/200] Training loss: 0.17052564
[2/200] Training loss: 0.06503449
[3/200] Training loss: 0.05855023
[4/200] Training loss: 0.05604730
[5/200] Training loss: 0.05237050
[6/200] Training loss: 0.05087313
[7/200] Training loss: 0.04852332
[8/200] Training loss: 0.04620787
[9/200] Training loss: 0.04563692
[10/200] Training loss: 0.04468424
[50/200] Training loss: 0.02128327
[100/200] Training loss: 0.01571895
[150/200] Training loss: 0.01487585
[200/200] Training loss: 0.01334539
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7791.312084623488 ----------
[1/200] Training loss: 0.15727444
[2/200] Training loss: 0.05753225
[3/200] Training loss: 0.05407619
[4/200] Training loss: 0.05131746
[5/200] Training loss: 0.04746525
[6/200] Training loss: 0.04403931
[7/200] Training loss: 0.04186555
[8/200] Training loss: 0.04104512
[9/200] Training loss: 0.03738270
[10/200] Training loss: 0.03477946
[50/200] Training loss: 0.01822086
[100/200] Training loss: 0.01578060
[150/200] Training loss: 0.01412546
[200/200] Training loss: 0.01225773
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12148.750717666406 ----------
[1/200] Training loss: 0.13963192
[2/200] Training loss: 0.05301438
[3/200] Training loss: 0.04977566
[4/200] Training loss: 0.04266578
[5/200] Training loss: 0.04128929
[6/200] Training loss: 0.04082972
[7/200] Training loss: 0.03740907
[8/200] Training loss: 0.03591958
[9/200] Training loss: 0.03512891
[10/200] Training loss: 0.03214469
[50/200] Training loss: 0.01957380
[100/200] Training loss: 0.01419623
[150/200] Training loss: 0.01280286
[200/200] Training loss: 0.01211404
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6574.142377527277 ----------
[1/200] Training loss: 0.15438962
[2/200] Training loss: 0.06018503
[3/200] Training loss: 0.05306318
[4/200] Training loss: 0.05026232
[5/200] Training loss: 0.04636315
[6/200] Training loss: 0.04347527
[7/200] Training loss: 0.04289034
[8/200] Training loss: 0.04202206
[9/200] Training loss: 0.03901171
[10/200] Training loss: 0.03717951
[50/200] Training loss: 0.01833062
[100/200] Training loss: 0.01612122
[150/200] Training loss: 0.01463663
[200/200] Training loss: 0.01353722
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 7622.9970484055675 ----------
[1/200] Training loss: 0.17039894
[2/200] Training loss: 0.05903376
[3/200] Training loss: 0.05093650
[4/200] Training loss: 0.04613114
[5/200] Training loss: 0.04469629
[6/200] Training loss: 0.04260275
[7/200] Training loss: 0.03810498
[8/200] Training loss: 0.03685892
[9/200] Training loss: 0.03550234
[10/200] Training loss: 0.03216850
[50/200] Training loss: 0.01985973
[100/200] Training loss: 0.01698239
[150/200] Training loss: 0.01571636
[200/200] Training loss: 0.01392512
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.42489771061029435 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14280.876723786954 ----------
[1/200] Training loss: 0.14364802
[2/200] Training loss: 0.05601248
[3/200] Training loss: 0.05326055
[4/200] Training loss: 0.05052014
[5/200] Training loss: 0.04967932
[6/200] Training loss: 0.04598130
[7/200] Training loss: 0.04100403
[8/200] Training loss: 0.03969707
[9/200] Training loss: 0.03838527
[10/200] Training loss: 0.03260081
[50/200] Training loss: 0.01750496
[100/200] Training loss: 0.01418459
[150/200] Training loss: 0.01292613
[200/200] Training loss: 0.01216548
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10715.351977420061 ----------
[1/200] Training loss: 0.17316355
[2/200] Training loss: 0.05447507
[3/200] Training loss: 0.04648789
[4/200] Training loss: 0.04537232
[5/200] Training loss: 0.04088844
[6/200] Training loss: 0.04201469
[7/200] Training loss: 0.03457679
[8/200] Training loss: 0.03289729
[9/200] Training loss: 0.03170637
[10/200] Training loss: 0.03076742
[50/200] Training loss: 0.01802467
[100/200] Training loss: 0.01556139
[150/200] Training loss: 0.01472126
[200/200] Training loss: 0.01399107
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13670.657628658542 ----------
[1/200] Training loss: 0.16705659
[2/200] Training loss: 0.06150316
[3/200] Training loss: 0.05197154
[4/200] Training loss: 0.04708240
[5/200] Training loss: 0.04419972
[6/200] Training loss: 0.04179265
[7/200] Training loss: 0.03781580
[8/200] Training loss: 0.03689620
[9/200] Training loss: 0.03335468
[10/200] Training loss: 0.03067922
[50/200] Training loss: 0.01848126
[100/200] Training loss: 0.01678946
[150/200] Training loss: 0.01574934
[200/200] Training loss: 0.01425824
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9708.892006815197 ----------
[1/200] Training loss: 0.14642707
[2/200] Training loss: 0.05688585
[3/200] Training loss: 0.04474908
[4/200] Training loss: 0.04203349
[5/200] Training loss: 0.04541409
[6/200] Training loss: 0.03719828
[7/200] Training loss: 0.03589871
[8/200] Training loss: 0.03288931
[9/200] Training loss: 0.03316506
[10/200] Training loss: 0.03043859
[50/200] Training loss: 0.01731634
[100/200] Training loss: 0.01611983
[150/200] Training loss: 0.01432353
[200/200] Training loss: 0.01322557
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12790.662844434608 ----------
[1/200] Training loss: 0.14926074
[2/200] Training loss: 0.06017905
[3/200] Training loss: 0.05555306
[4/200] Training loss: 0.05325906
[5/200] Training loss: 0.05015836
[6/200] Training loss: 0.04707605
[7/200] Training loss: 0.04439278
[8/200] Training loss: 0.04456290
[9/200] Training loss: 0.03954035
[10/200] Training loss: 0.03774141
[50/200] Training loss: 0.01725253
[100/200] Training loss: 0.01480680
[150/200] Training loss: 0.01347209
[200/200] Training loss: 0.01253140
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 9990.243640672634 ----------
[1/200] Training loss: 0.13619163
[2/200] Training loss: 0.05418264
[3/200] Training loss: 0.05300596
[4/200] Training loss: 0.04844332
[5/200] Training loss: 0.04378856
[6/200] Training loss: 0.04092522
[7/200] Training loss: 0.03948230
[8/200] Training loss: 0.03706726
[9/200] Training loss: 0.03470619
[10/200] Training loss: 0.03236511
[50/200] Training loss: 0.01690866
[100/200] Training loss: 0.01401127
[150/200] Training loss: 0.01270956
[200/200] Training loss: 0.01147002
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16606.28218476369 ----------
[1/200] Training loss: 0.14995525
[2/200] Training loss: 0.05957859
[3/200] Training loss: 0.05569630
[4/200] Training loss: 0.04755356
[5/200] Training loss: 0.04380466
[6/200] Training loss: 0.04313002
[7/200] Training loss: 0.04201840
[8/200] Training loss: 0.03707868
[9/200] Training loss: 0.03572701
[10/200] Training loss: 0.03651216
[50/200] Training loss: 0.01738788
[100/200] Training loss: 0.01384999
[150/200] Training loss: 0.01363440
[200/200] Training loss: 0.01257515
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12806.3671663747 ----------
[1/200] Training loss: 0.15674928
[2/200] Training loss: 0.05743722
[3/200] Training loss: 0.05357097
[4/200] Training loss: 0.04947506
[5/200] Training loss: 0.04877944
[6/200] Training loss: 0.04470588
[7/200] Training loss: 0.04010004
[8/200] Training loss: 0.03749192
[9/200] Training loss: 0.03616602
[10/200] Training loss: 0.03299737
[50/200] Training loss: 0.01906706
[100/200] Training loss: 0.01642717
[150/200] Training loss: 0.01482304
[200/200] Training loss: 0.01325976
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9406.053795295878 ----------
[1/200] Training loss: 0.16484487
[2/200] Training loss: 0.06198372
[3/200] Training loss: 0.05550425
[4/200] Training loss: 0.04830541
[5/200] Training loss: 0.04744182
[6/200] Training loss: 0.04322495
[7/200] Training loss: 0.04040240
[8/200] Training loss: 0.03798117
[9/200] Training loss: 0.03839731
[10/200] Training loss: 0.03597024
[50/200] Training loss: 0.01811185
[100/200] Training loss: 0.01604574
[150/200] Training loss: 0.01490863
[200/200] Training loss: 0.01346293
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9622.380162932663 ----------
[1/200] Training loss: 0.15167731
[2/200] Training loss: 0.05870299
[3/200] Training loss: 0.04944010
[4/200] Training loss: 0.04747332
[5/200] Training loss: 0.04407100
[6/200] Training loss: 0.04333019
[7/200] Training loss: 0.04033082
[8/200] Training loss: 0.04095357
[9/200] Training loss: 0.03939369
[10/200] Training loss: 0.03534776
[50/200] Training loss: 0.02031741
[100/200] Training loss: 0.01546464
[150/200] Training loss: 0.01379071
[200/200] Training loss: 0.01305264
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.39883944333435906 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8059.271182929632 ----------
[1/200] Training loss: 0.15491235
[2/200] Training loss: 0.05888657
[3/200] Training loss: 0.05080696
[4/200] Training loss: 0.04837857
[5/200] Training loss: 0.04359265
[6/200] Training loss: 0.04225148
[7/200] Training loss: 0.04162513
[8/200] Training loss: 0.03883219
[9/200] Training loss: 0.03661053
[10/200] Training loss: 0.03288239
[50/200] Training loss: 0.01901661
[100/200] Training loss: 0.01565308
[150/200] Training loss: 0.01428053
[200/200] Training loss: 0.01366238
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20689.935717638178 ----------
[1/200] Training loss: 0.18568601
[2/200] Training loss: 0.05938137
[3/200] Training loss: 0.05262093
[4/200] Training loss: 0.05108234
[5/200] Training loss: 0.04864131
[6/200] Training loss: 0.04306556
[7/200] Training loss: 0.04269994
[8/200] Training loss: 0.03952628
[9/200] Training loss: 0.03812348
[10/200] Training loss: 0.03801813
[50/200] Training loss: 0.02088551
[100/200] Training loss: 0.01662942
[150/200] Training loss: 0.01559548
[200/200] Training loss: 0.01488125
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.2054638886556247 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10933.319349584553 ----------
[1/200] Training loss: 0.12127536
[2/200] Training loss: 0.05001440
[3/200] Training loss: 0.04768964
[4/200] Training loss: 0.04252806
[5/200] Training loss: 0.04250386
[6/200] Training loss: 0.03668707
[7/200] Training loss: 0.03965416
[8/200] Training loss: 0.03647975
[9/200] Training loss: 0.03555303
[10/200] Training loss: 0.03329129
[50/200] Training loss: 0.02147787
[100/200] Training loss: 0.01522487
[150/200] Training loss: 0.01361408
[200/200] Training loss: 0.01284576
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 7192.634844060972 ----------
[1/200] Training loss: 0.17304600
[2/200] Training loss: 0.05636010
[3/200] Training loss: 0.05491633
[4/200] Training loss: 0.04901956
[5/200] Training loss: 0.04567433
[6/200] Training loss: 0.04380505
[7/200] Training loss: 0.04224932
[8/200] Training loss: 0.04232698
[9/200] Training loss: 0.03978479
[10/200] Training loss: 0.03916486
[50/200] Training loss: 0.01918380
[100/200] Training loss: 0.01585201
[150/200] Training loss: 0.01457955
[200/200] Training loss: 0.01288454
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 27748.535961380017 ----------
[1/200] Training loss: 0.15343624
[2/200] Training loss: 0.05689865
[3/200] Training loss: 0.05054348
[4/200] Training loss: 0.04834828
[5/200] Training loss: 0.04624677
[6/200] Training loss: 0.04196164
[7/200] Training loss: 0.04077541
[8/200] Training loss: 0.03867147
[9/200] Training loss: 0.03600030
[10/200] Training loss: 0.03309223
[50/200] Training loss: 0.01893259
[100/200] Training loss: 0.01588145
[150/200] Training loss: 0.01456586
[200/200] Training loss: 0.01312784
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21736.63745844789 ----------
[1/200] Training loss: 0.14920611
[2/200] Training loss: 0.06179564
[3/200] Training loss: 0.05325672
[4/200] Training loss: 0.05049528
[5/200] Training loss: 0.04662428
[6/200] Training loss: 0.04479899
[7/200] Training loss: 0.04134312
[8/200] Training loss: 0.03752699
[9/200] Training loss: 0.03823264
[10/200] Training loss: 0.03500203
[50/200] Training loss: 0.01750116
[100/200] Training loss: 0.01442151
[150/200] Training loss: 0.01271638
[200/200] Training loss: 0.01214914
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13209.027216263883 ----------
[1/200] Training loss: 0.16205762
[2/200] Training loss: 0.05560250
[3/200] Training loss: 0.05336223
[4/200] Training loss: 0.04514022
[5/200] Training loss: 0.04456729
[6/200] Training loss: 0.04271530
[7/200] Training loss: 0.03911409
[8/200] Training loss: 0.03662948
[9/200] Training loss: 0.03401312
[10/200] Training loss: 0.03435212
[50/200] Training loss: 0.01667663
[100/200] Training loss: 0.01430038
[150/200] Training loss: 0.01264553
[200/200] Training loss: 0.01205388
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8085.335119832696 ----------
[1/200] Training loss: 0.15985955
[2/200] Training loss: 0.05853320
[3/200] Training loss: 0.05561375
[4/200] Training loss: 0.05269234
[5/200] Training loss: 0.04881378
[6/200] Training loss: 0.04628293
[7/200] Training loss: 0.04348568
[8/200] Training loss: 0.03923550
[9/200] Training loss: 0.03775299
[10/200] Training loss: 0.03643312
[50/200] Training loss: 0.01950939
[100/200] Training loss: 0.01632393
[150/200] Training loss: 0.01451691
[200/200] Training loss: 0.01262202
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8127.152268783943 ----------
[1/200] Training loss: 0.15398599
[2/200] Training loss: 0.05404668
[3/200] Training loss: 0.05133674
[4/200] Training loss: 0.04550734
[5/200] Training loss: 0.04376205
[6/200] Training loss: 0.04058695
[7/200] Training loss: 0.03824899
[8/200] Training loss: 0.03472291
[9/200] Training loss: 0.03372361
[10/200] Training loss: 0.03038602
[50/200] Training loss: 0.01734297
[100/200] Training loss: 0.01460239
[150/200] Training loss: 0.01264177
[200/200] Training loss: 0.01167052
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20738.62406236248 ----------
[1/200] Training loss: 0.22513703
[2/200] Training loss: 0.06587445
[3/200] Training loss: 0.05426913
[4/200] Training loss: 0.05156059
[5/200] Training loss: 0.04867789
[6/200] Training loss: 0.04659794
[7/200] Training loss: 0.04283593
[8/200] Training loss: 0.04018356
[9/200] Training loss: 0.03743667
[10/200] Training loss: 0.03307276
[50/200] Training loss: 0.02011009
[100/200] Training loss: 0.01698673
[150/200] Training loss: 0.01470964
[200/200] Training loss: 0.01455474
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10234.607955363996 ----------
[1/200] Training loss: 0.19357421
[2/200] Training loss: 0.06296609
[3/200] Training loss: 0.05699965
[4/200] Training loss: 0.04952369
[5/200] Training loss: 0.04745129
[6/200] Training loss: 0.04603872
[7/200] Training loss: 0.04482239
[8/200] Training loss: 0.04261226
[9/200] Training loss: 0.04202760
[10/200] Training loss: 0.03809947
[50/200] Training loss: 0.01797503
[100/200] Training loss: 0.01565527
[150/200] Training loss: 0.01387611
[200/200] Training loss: 0.01237601
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15589.999871712636 ----------
[1/200] Training loss: 0.16766895
[2/200] Training loss: 0.05637313
[3/200] Training loss: 0.05247679
[4/200] Training loss: 0.04661769
[5/200] Training loss: 0.04321236
[6/200] Training loss: 0.04098873
[7/200] Training loss: 0.03777867
[8/200] Training loss: 0.03951891
[9/200] Training loss: 0.03799827
[10/200] Training loss: 0.03498889
[50/200] Training loss: 0.01759767
[100/200] Training loss: 0.01539965
[150/200] Training loss: 0.01359425
[200/200] Training loss: 0.01274266
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10042.62157008816 ----------
[1/200] Training loss: 0.13137686
[2/200] Training loss: 0.05292827
[3/200] Training loss: 0.04810818
[4/200] Training loss: 0.04791695
[5/200] Training loss: 0.04363574
[6/200] Training loss: 0.04162148
[7/200] Training loss: 0.04219114
[8/200] Training loss: 0.03685431
[9/200] Training loss: 0.03792125
[10/200] Training loss: 0.03645831
[50/200] Training loss: 0.01674406
[100/200] Training loss: 0.01394620
[150/200] Training loss: 0.01247394
[200/200] Training loss: 0.01187035
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 6036.797495361261 ----------
[1/200] Training loss: 0.16768523
[2/200] Training loss: 0.05506546
[3/200] Training loss: 0.05152386
[4/200] Training loss: 0.04746281
[5/200] Training loss: 0.04280410
[6/200] Training loss: 0.03902394
[7/200] Training loss: 0.03986765
[8/200] Training loss: 0.03472571
[9/200] Training loss: 0.03215509
[10/200] Training loss: 0.03130403
[50/200] Training loss: 0.01768120
[100/200] Training loss: 0.01537690
[150/200] Training loss: 0.01396521
[200/200] Training loss: 0.01298432
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4494.815235357289 ----------
[1/200] Training loss: 0.15477079
[2/200] Training loss: 0.05335085
[3/200] Training loss: 0.05123178
[4/200] Training loss: 0.05279779
[5/200] Training loss: 0.04881094
[6/200] Training loss: 0.04481137
[7/200] Training loss: 0.04226802
[8/200] Training loss: 0.04035767
[9/200] Training loss: 0.04141537
[10/200] Training loss: 0.03685334
[50/200] Training loss: 0.01796136
[100/200] Training loss: 0.01520248
[150/200] Training loss: 0.01375183
[200/200] Training loss: 0.01327336
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 13390.025541424482 ----------
[1/200] Training loss: 0.17648749
[2/200] Training loss: 0.05891466
[3/200] Training loss: 0.05591032
[4/200] Training loss: 0.05102086
[5/200] Training loss: 0.04925456
[6/200] Training loss: 0.04557284
[7/200] Training loss: 0.04303596
[8/200] Training loss: 0.03986190
[9/200] Training loss: 0.04206797
[10/200] Training loss: 0.03730555
[50/200] Training loss: 0.01853210
[100/200] Training loss: 0.01549833
[150/200] Training loss: 0.01374967
[200/200] Training loss: 0.01334642
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16691.129140953886 ----------
[1/200] Training loss: 0.17172434
[2/200] Training loss: 0.05642349
[3/200] Training loss: 0.05257922
[4/200] Training loss: 0.05175781
[5/200] Training loss: 0.04895153
[6/200] Training loss: 0.04594494
[7/200] Training loss: 0.04362773
[8/200] Training loss: 0.04216898
[9/200] Training loss: 0.04062225
[10/200] Training loss: 0.03650045
[50/200] Training loss: 0.01948412
[100/200] Training loss: 0.01543098
[150/200] Training loss: 0.01332877
[200/200] Training loss: 0.01196945
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14731.843061884687 ----------
[1/200] Training loss: 0.13835681
[2/200] Training loss: 0.05076897
[3/200] Training loss: 0.05020708
[4/200] Training loss: 0.04485515
[5/200] Training loss: 0.04295353
[6/200] Training loss: 0.03886673
[7/200] Training loss: 0.03803243
[8/200] Training loss: 0.03414961
[9/200] Training loss: 0.03186002
[10/200] Training loss: 0.03122849
[50/200] Training loss: 0.02018916
[100/200] Training loss: 0.01652261
[150/200] Training loss: 0.01546942
[200/200] Training loss: 0.01450703
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 8777.306192676657 ----------
[1/200] Training loss: 0.14388519
[2/200] Training loss: 0.05811187
[3/200] Training loss: 0.05475940
[4/200] Training loss: 0.05173760
[5/200] Training loss: 0.04728385
[6/200] Training loss: 0.04603899
[7/200] Training loss: 0.04385163
[8/200] Training loss: 0.04357328
[9/200] Training loss: 0.03956447
[10/200] Training loss: 0.03887656
[50/200] Training loss: 0.01833209
[100/200] Training loss: 0.01501503
[150/200] Training loss: 0.01342732
[200/200] Training loss: 0.01217347
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 13501.88372042953 ----------
[1/200] Training loss: 0.17681119
[2/200] Training loss: 0.05494508
[3/200] Training loss: 0.05150850
[4/200] Training loss: 0.04776021
[5/200] Training loss: 0.04513307
[6/200] Training loss: 0.04102062
[7/200] Training loss: 0.03895445
[8/200] Training loss: 0.03759486
[9/200] Training loss: 0.03630545
[10/200] Training loss: 0.03363272
[50/200] Training loss: 0.01971987
[100/200] Training loss: 0.01696123
[150/200] Training loss: 0.01596595
[200/200] Training loss: 0.01479171
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 5136.081775049926 ----------
[1/200] Training loss: 0.15460959
[2/200] Training loss: 0.06016239
[3/200] Training loss: 0.05058321
[4/200] Training loss: 0.04808248
[5/200] Training loss: 0.04603145
[6/200] Training loss: 0.04047740
[7/200] Training loss: 0.03961936
[8/200] Training loss: 0.03601701
[9/200] Training loss: 0.03448985
[10/200] Training loss: 0.03077713
[50/200] Training loss: 0.01742661
[100/200] Training loss: 0.01622186
[150/200] Training loss: 0.01434097
[200/200] Training loss: 0.01349020
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14792.240668674911 ----------
[1/200] Training loss: 0.14573303
[2/200] Training loss: 0.05759699
[3/200] Training loss: 0.04963355
[4/200] Training loss: 0.04775943
[5/200] Training loss: 0.04494090
[6/200] Training loss: 0.04141876
[7/200] Training loss: 0.03968281
[8/200] Training loss: 0.03644895
[9/200] Training loss: 0.03451024
[10/200] Training loss: 0.03210568
[50/200] Training loss: 0.01665056
[100/200] Training loss: 0.01386161
[150/200] Training loss: 0.01271024
[200/200] Training loss: 0.01109885
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15973.994365843504 ----------
[1/200] Training loss: 0.16956622
[2/200] Training loss: 0.06001683
[3/200] Training loss: 0.05137346
[4/200] Training loss: 0.05155538
[5/200] Training loss: 0.04877774
[6/200] Training loss: 0.04544478
[7/200] Training loss: 0.03855001
[8/200] Training loss: 0.03749249
[9/200] Training loss: 0.03557022
[10/200] Training loss: 0.03424063
[50/200] Training loss: 0.01743345
[100/200] Training loss: 0.01541087
[150/200] Training loss: 0.01450046
[200/200] Training loss: 0.01323482
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11209.956645768083 ----------
[1/200] Training loss: 0.17674012
[2/200] Training loss: 0.05664879
[3/200] Training loss: 0.05212937
[4/200] Training loss: 0.04813647
[5/200] Training loss: 0.04436123
[6/200] Training loss: 0.03953395
[7/200] Training loss: 0.03612790
[8/200] Training loss: 0.03671759
[9/200] Training loss: 0.03458124
[10/200] Training loss: 0.03410115
[50/200] Training loss: 0.01890192
[100/200] Training loss: 0.01618882
[150/200] Training loss: 0.01471304
[200/200] Training loss: 0.01405592
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21527.87439576885 ----------
[1/200] Training loss: 0.14990394
[2/200] Training loss: 0.05739499
[3/200] Training loss: 0.05192719
[4/200] Training loss: 0.04875371
[5/200] Training loss: 0.04490502
[6/200] Training loss: 0.04255865
[7/200] Training loss: 0.03768698
[8/200] Training loss: 0.03707859
[9/200] Training loss: 0.03442766
[10/200] Training loss: 0.03018194
[50/200] Training loss: 0.01724086
[100/200] Training loss: 0.01453190
[150/200] Training loss: 0.01422767
[200/200] Training loss: 0.01284036
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12113.77926165076 ----------
[1/200] Training loss: 0.14635995
[2/200] Training loss: 0.06245339
[3/200] Training loss: 0.05452174
[4/200] Training loss: 0.05486781
[5/200] Training loss: 0.05228093
[6/200] Training loss: 0.05057509
[7/200] Training loss: 0.04614399
[8/200] Training loss: 0.04388477
[9/200] Training loss: 0.03769533
[10/200] Training loss: 0.03311460
[50/200] Training loss: 0.01947829
[100/200] Training loss: 0.01507266
[150/200] Training loss: 0.01328068
[200/200] Training loss: 0.01260141
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 13226.096627501252 ----------
[1/200] Training loss: 0.13827451
[2/200] Training loss: 0.05628016
[3/200] Training loss: 0.05375154
[4/200] Training loss: 0.05173140
[5/200] Training loss: 0.04766494
[6/200] Training loss: 0.04731504
[7/200] Training loss: 0.04430387
[8/200] Training loss: 0.04075586
[9/200] Training loss: 0.03885937
[10/200] Training loss: 0.03798727
[50/200] Training loss: 0.01712518
[100/200] Training loss: 0.01493092
[150/200] Training loss: 0.01382655
[200/200] Training loss: 0.01257531
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 7492.336084293069 ----------
[1/200] Training loss: 0.14621670
[2/200] Training loss: 0.05970215
[3/200] Training loss: 0.04971538
[4/200] Training loss: 0.04602396
[5/200] Training loss: 0.04371689
[6/200] Training loss: 0.04186984
[7/200] Training loss: 0.03990035
[8/200] Training loss: 0.03738379
[9/200] Training loss: 0.03643073
[10/200] Training loss: 0.03246589
[50/200] Training loss: 0.01704812
[100/200] Training loss: 0.01431390
[150/200] Training loss: 0.01338346
[200/200] Training loss: 0.01129417
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21437.87377516716 ----------
[1/200] Training loss: 0.17757167
[2/200] Training loss: 0.05933718
[3/200] Training loss: 0.05135756
[4/200] Training loss: 0.04995220
[5/200] Training loss: 0.04778081
[6/200] Training loss: 0.04590789
[7/200] Training loss: 0.04084729
[8/200] Training loss: 0.04037135
[9/200] Training loss: 0.03767048
[10/200] Training loss: 0.03654255
[50/200] Training loss: 0.01916572
[100/200] Training loss: 0.01635056
[150/200] Training loss: 0.01406313
[200/200] Training loss: 0.01352097
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 4573.841711296971 ----------
[1/200] Training loss: 0.12450522
[2/200] Training loss: 0.05975046
[3/200] Training loss: 0.05314417
[4/200] Training loss: 0.05272702
[5/200] Training loss: 0.04842890
[6/200] Training loss: 0.04541051
[7/200] Training loss: 0.04312921
[8/200] Training loss: 0.04204293
[9/200] Training loss: 0.04081092
[10/200] Training loss: 0.04020981
[50/200] Training loss: 0.01637856
[100/200] Training loss: 0.01406940
[150/200] Training loss: 0.01329853
[200/200] Training loss: 0.01226157
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 20051.36923005509 ----------
[1/200] Training loss: 0.15130557
[2/200] Training loss: 0.05535092
[3/200] Training loss: 0.04855709
[4/200] Training loss: 0.04636782
[5/200] Training loss: 0.04431425
[6/200] Training loss: 0.04127219
[7/200] Training loss: 0.03526490
[8/200] Training loss: 0.03328811
[9/200] Training loss: 0.03566471
[10/200] Training loss: 0.03184841
[50/200] Training loss: 0.01883847
[100/200] Training loss: 0.01592648
[150/200] Training loss: 0.01457625
[200/200] Training loss: 0.01412082
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 7891.403423979793 ----------
[1/200] Training loss: 0.08310155
[2/200] Training loss: 0.04620640
[3/200] Training loss: 0.03955298
[4/200] Training loss: 0.03450417
[5/200] Training loss: 0.03161466
[6/200] Training loss: 0.02873823
[7/200] Training loss: 0.02742004
[8/200] Training loss: 0.02682842
[9/200] Training loss: 0.02695165
[10/200] Training loss: 0.02549404
[50/200] Training loss: 0.01576628
[100/200] Training loss: 0.01238792
[150/200] Training loss: 0.01139136
[200/200] Training loss: 0.01051087
---batch_size---: 4 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 5969.75744900913 ----------
[1/200] Training loss: 0.13413134
[2/200] Training loss: 0.06097404
[3/200] Training loss: 0.04869706
[4/200] Training loss: 0.04755130
[5/200] Training loss: 0.04570813
[6/200] Training loss: 0.03970711
[7/200] Training loss: 0.03674214
[8/200] Training loss: 0.03479477
[9/200] Training loss: 0.03395391
[10/200] Training loss: 0.03083607
[50/200] Training loss: 0.01702895
[100/200] Training loss: 0.01441954
[150/200] Training loss: 0.01303650
[200/200] Training loss: 0.01236270
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17255.588312196138 ----------
[1/200] Training loss: 0.12752140
[2/200] Training loss: 0.05367475
[3/200] Training loss: 0.04787546
[4/200] Training loss: 0.04285392
[5/200] Training loss: 0.04314169
[6/200] Training loss: 0.03886422
[7/200] Training loss: 0.03858949
[8/200] Training loss: 0.03616600
[9/200] Training loss: 0.03324442
[10/200] Training loss: 0.03305439
[50/200] Training loss: 0.01764706
[100/200] Training loss: 0.01403567
[150/200] Training loss: 0.01267300
[200/200] Training loss: 0.01140779
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.39883944333435906 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18834.430599304033 ----------
[1/200] Training loss: 0.19518199
[2/200] Training loss: 0.06567796
[3/200] Training loss: 0.05768783
[4/200] Training loss: 0.05440467
[5/200] Training loss: 0.05220390
[6/200] Training loss: 0.05214104
[7/200] Training loss: 0.04710267
[8/200] Training loss: 0.04569857
[9/200] Training loss: 0.04417215
[10/200] Training loss: 0.04313323
[50/200] Training loss: 0.02190791
[100/200] Training loss: 0.01737245
[150/200] Training loss: 0.01540076
[200/200] Training loss: 0.01452938
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15415.232466622097 ----------
[1/200] Training loss: 0.14265418
[2/200] Training loss: 0.05883731
[3/200] Training loss: 0.05357666
[4/200] Training loss: 0.04800899
[5/200] Training loss: 0.04623856
[6/200] Training loss: 0.04553545
[7/200] Training loss: 0.04263134
[8/200] Training loss: 0.04033123
[9/200] Training loss: 0.03969708
[10/200] Training loss: 0.03672116
[50/200] Training loss: 0.02043822
[100/200] Training loss: 0.01872567
[150/200] Training loss: 0.01647087
[200/200] Training loss: 0.01532678
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 7855.833755878493 ----------
[1/200] Training loss: 0.12574730
[2/200] Training loss: 0.06085604
[3/200] Training loss: 0.05297616
[4/200] Training loss: 0.04957634
[5/200] Training loss: 0.04776616
[6/200] Training loss: 0.04403511
[7/200] Training loss: 0.03979699
[8/200] Training loss: 0.04125175
[9/200] Training loss: 0.03808688
[10/200] Training loss: 0.03257246
[50/200] Training loss: 0.01832649
[100/200] Training loss: 0.01515911
[150/200] Training loss: 0.01328607
[200/200] Training loss: 0.01239492
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 16761.42141943815 ----------
[1/200] Training loss: 0.15007944
[2/200] Training loss: 0.06057147
[3/200] Training loss: 0.05358744
[4/200] Training loss: 0.05152642
[5/200] Training loss: 0.04940731
[6/200] Training loss: 0.04842817
[7/200] Training loss: 0.04519053
[8/200] Training loss: 0.04182799
[9/200] Training loss: 0.03956084
[10/200] Training loss: 0.03876893
[50/200] Training loss: 0.01825723
[100/200] Training loss: 0.01537487
[150/200] Training loss: 0.01420884
[200/200] Training loss: 0.01402255
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 17799.039075186054 ----------
[1/200] Training loss: 0.17241992
[2/200] Training loss: 0.05928087
[3/200] Training loss: 0.05411835
[4/200] Training loss: 0.04905397
[5/200] Training loss: 0.04636504
[6/200] Training loss: 0.04449059
[7/200] Training loss: 0.04276874
[8/200] Training loss: 0.03966859
[9/200] Training loss: 0.03779784
[10/200] Training loss: 0.03744866
[50/200] Training loss: 0.01784315
[100/200] Training loss: 0.01417693
[150/200] Training loss: 0.01273338
[200/200] Training loss: 0.01215621
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13869.37287695446 ----------
[1/200] Training loss: 0.14624081
[2/200] Training loss: 0.05750608
[3/200] Training loss: 0.05006200
[4/200] Training loss: 0.04721931
[5/200] Training loss: 0.04523524
[6/200] Training loss: 0.03840893
[7/200] Training loss: 0.04038773
[8/200] Training loss: 0.03637003
[9/200] Training loss: 0.03532467
[10/200] Training loss: 0.03266232
[50/200] Training loss: 0.01950735
[100/200] Training loss: 0.01670725
[150/200] Training loss: 0.01554392
[200/200] Training loss: 0.01420148
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 14099.89900673051 ----------
[1/200] Training loss: 0.15922158
[2/200] Training loss: 0.06147810
[3/200] Training loss: 0.05162654
[4/200] Training loss: 0.04794720
[5/200] Training loss: 0.04384742
[6/200] Training loss: 0.04017616
[7/200] Training loss: 0.03494415
[8/200] Training loss: 0.03408331
[9/200] Training loss: 0.03173984
[10/200] Training loss: 0.02901858
[50/200] Training loss: 0.01852603
[100/200] Training loss: 0.01622271
[150/200] Training loss: 0.01432293
[200/200] Training loss: 0.01280317
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20812.6169426144 ----------
[1/200] Training loss: 0.16371638
[2/200] Training loss: 0.05919068
[3/200] Training loss: 0.05390093
[4/200] Training loss: 0.04938776
[5/200] Training loss: 0.04524132
[6/200] Training loss: 0.04279254
[7/200] Training loss: 0.03955842
[8/200] Training loss: 0.03828960
[9/200] Training loss: 0.03319697
[10/200] Training loss: 0.03132406
[50/200] Training loss: 0.01858320
[100/200] Training loss: 0.01640844
[150/200] Training loss: 0.01477003
[200/200] Training loss: 0.01394494
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 8234.416069157545 ----------
[1/200] Training loss: 0.14831980
[2/200] Training loss: 0.05903021
[3/200] Training loss: 0.05134428
[4/200] Training loss: 0.04648518
[5/200] Training loss: 0.04225308
[6/200] Training loss: 0.04024780
[7/200] Training loss: 0.03485313
[8/200] Training loss: 0.03418334
[9/200] Training loss: 0.03167553
[10/200] Training loss: 0.02990813
[50/200] Training loss: 0.01775828
[100/200] Training loss: 0.01649173
[150/200] Training loss: 0.01454598
[200/200] Training loss: 0.01372100
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 27349.223608724253 ----------
[1/200] Training loss: 0.14129883
[2/200] Training loss: 0.05403831
[3/200] Training loss: 0.05054043
[4/200] Training loss: 0.04976199
[5/200] Training loss: 0.04482209
[6/200] Training loss: 0.04237836
[7/200] Training loss: 0.03938600
[8/200] Training loss: 0.03962276
[9/200] Training loss: 0.03361626
[10/200] Training loss: 0.03338314
[50/200] Training loss: 0.02158230
[100/200] Training loss: 0.01739752
[150/200] Training loss: 0.01621703
[200/200] Training loss: 0.01525160
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 12694.488882975951 ----------
[1/200] Training loss: 0.15520289
[2/200] Training loss: 0.05581226
[3/200] Training loss: 0.05135516
[4/200] Training loss: 0.04823766
[5/200] Training loss: 0.04362499
[6/200] Training loss: 0.04109310
[7/200] Training loss: 0.03791922
[8/200] Training loss: 0.03669943
[9/200] Training loss: 0.03442175
[10/200] Training loss: 0.03214754
[50/200] Training loss: 0.01712385
[100/200] Training loss: 0.01375145
[150/200] Training loss: 0.01211518
[200/200] Training loss: 0.01113890
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 9169.565311398354 ----------
[1/200] Training loss: 0.15663166
[2/200] Training loss: 0.06380069
[3/200] Training loss: 0.05287398
[4/200] Training loss: 0.05285999
[5/200] Training loss: 0.04795323
[6/200] Training loss: 0.04730525
[7/200] Training loss: 0.04614122
[8/200] Training loss: 0.04358692
[9/200] Training loss: 0.03918150
[10/200] Training loss: 0.04078489
[50/200] Training loss: 0.01856447
[100/200] Training loss: 0.01664830
[150/200] Training loss: 0.01501347
[200/200] Training loss: 0.01407934
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11560.846681796278 ----------
[1/200] Training loss: 0.13222190
[2/200] Training loss: 0.04937781
[3/200] Training loss: 0.04787146
[4/200] Training loss: 0.04452319
[5/200] Training loss: 0.03887426
[6/200] Training loss: 0.04003658
[7/200] Training loss: 0.03539257
[8/200] Training loss: 0.03375944
[9/200] Training loss: 0.03273532
[10/200] Training loss: 0.03133944
[50/200] Training loss: 0.01756906
[100/200] Training loss: 0.01494616
[150/200] Training loss: 0.01463151
[200/200] Training loss: 0.01306351
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 5355.460017589526 ----------
[1/200] Training loss: 0.19110355
[2/200] Training loss: 0.05944820
[3/200] Training loss: 0.05074360
[4/200] Training loss: 0.04968381
[5/200] Training loss: 0.04631377
[6/200] Training loss: 0.04391983
[7/200] Training loss: 0.04250405
[8/200] Training loss: 0.04034099
[9/200] Training loss: 0.03981581
[10/200] Training loss: 0.03683867
[50/200] Training loss: 0.01901338
[100/200] Training loss: 0.01675894
[150/200] Training loss: 0.01515259
[200/200] Training loss: 0.01416642
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14797.520873443633 ----------
[1/200] Training loss: 0.16435275
[2/200] Training loss: 0.06283222
[3/200] Training loss: 0.05653607
[4/200] Training loss: 0.05035170
[5/200] Training loss: 0.04657610
[6/200] Training loss: 0.04806212
[7/200] Training loss: 0.04382648
[8/200] Training loss: 0.04233061
[9/200] Training loss: 0.03888395
[10/200] Training loss: 0.03597136
[50/200] Training loss: 0.01871185
[100/200] Training loss: 0.01629266
[150/200] Training loss: 0.01494443
[200/200] Training loss: 0.01400209
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12281.508050724064 ----------
[1/200] Training loss: 0.16716045
[2/200] Training loss: 0.06071817
[3/200] Training loss: 0.05215457
[4/200] Training loss: 0.04999051
[5/200] Training loss: 0.04867703
[6/200] Training loss: 0.04351172
[7/200] Training loss: 0.04315812
[8/200] Training loss: 0.03917507
[9/200] Training loss: 0.03681090
[10/200] Training loss: 0.03610003
[50/200] Training loss: 0.01897198
[100/200] Training loss: 0.01519152
[150/200] Training loss: 0.01410123
[200/200] Training loss: 0.01231739
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18361.904040703404 ----------
[1/200] Training loss: 0.16255682
[2/200] Training loss: 0.06456201
[3/200] Training loss: 0.05485184
[4/200] Training loss: 0.04943654
[5/200] Training loss: 0.04455941
[6/200] Training loss: 0.04456910
[7/200] Training loss: 0.04184472
[8/200] Training loss: 0.04152663
[9/200] Training loss: 0.03641393
[10/200] Training loss: 0.03649955
[50/200] Training loss: 0.01796285
[100/200] Training loss: 0.01521908
[150/200] Training loss: 0.01420159
[200/200] Training loss: 0.01354510
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 32 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16634.210531311666 ----------
[1/200] Training loss: 0.17057005
[2/200] Training loss: 0.06165392
[3/200] Training loss: 0.05606068
[4/200] Training loss: 0.05537151
[5/200] Training loss: 0.05225012
[6/200] Training loss: 0.04981251
[7/200] Training loss: 0.04918401
[8/200] Training loss: 0.04607100
[9/200] Training loss: 0.04356786
[10/200] Training loss: 0.04281274
[50/200] Training loss: 0.01855131
[100/200] Training loss: 0.01581364
[150/200] Training loss: 0.01403482
[200/200] Training loss: 0.01304512
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16402.585162101735 ----------
[1/200] Training loss: 0.13939091
[2/200] Training loss: 0.05621175
[3/200] Training loss: 0.04867756
[4/200] Training loss: 0.04534093
[5/200] Training loss: 0.04324277
[6/200] Training loss: 0.03734323
[7/200] Training loss: 0.03882122
[8/200] Training loss: 0.03051049
[9/200] Training loss: 0.03348979
[10/200] Training loss: 0.03167330
[50/200] Training loss: 0.01757974
[100/200] Training loss: 0.01393556
[150/200] Training loss: 0.01250680
[200/200] Training loss: 0.01155671
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15507.594526553756 ----------
[1/200] Training loss: 0.13440959
[2/200] Training loss: 0.06043121
[3/200] Training loss: 0.05510162
[4/200] Training loss: 0.04822155
[5/200] Training loss: 0.04562735
[6/200] Training loss: 0.04217116
[7/200] Training loss: 0.03754957
[8/200] Training loss: 0.03721585
[9/200] Training loss: 0.03431807
[10/200] Training loss: 0.03184208
[50/200] Training loss: 0.01793378
[100/200] Training loss: 0.01599093
[150/200] Training loss: 0.01391171
[200/200] Training loss: 0.01285395
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17283.090464381654 ----------
[1/200] Training loss: 0.14583545
[2/200] Training loss: 0.05723574
[3/200] Training loss: 0.05438975
[4/200] Training loss: 0.05077196
[5/200] Training loss: 0.04768209
[6/200] Training loss: 0.04470781
[7/200] Training loss: 0.03980360
[8/200] Training loss: 0.03785945
[9/200] Training loss: 0.03661791
[10/200] Training loss: 0.03161543
[50/200] Training loss: 0.01782034
[100/200] Training loss: 0.01587467
[150/200] Training loss: 0.01516151
[200/200] Training loss: 0.01395305
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 15527.472427925932 ----------
[1/200] Training loss: 0.17021740
[2/200] Training loss: 0.06666620
[3/200] Training loss: 0.05526735
[4/200] Training loss: 0.05467876
[5/200] Training loss: 0.05509204
[6/200] Training loss: 0.05011718
[7/200] Training loss: 0.04720012
[8/200] Training loss: 0.04631557
[9/200] Training loss: 0.04697393
[10/200] Training loss: 0.04165477
[50/200] Training loss: 0.01849448
[100/200] Training loss: 0.01590827
[150/200] Training loss: 0.01349568
[200/200] Training loss: 0.01261672
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11369.97167982401 ----------
[1/200] Training loss: 0.12590151
[2/200] Training loss: 0.05696967
[3/200] Training loss: 0.05502986
[4/200] Training loss: 0.04940839
[5/200] Training loss: 0.04689429
[6/200] Training loss: 0.04485202
[7/200] Training loss: 0.04417800
[8/200] Training loss: 0.04192360
[9/200] Training loss: 0.04083086
[10/200] Training loss: 0.03767189
[50/200] Training loss: 0.01785797
[100/200] Training loss: 0.01462731
[150/200] Training loss: 0.01256293
[200/200] Training loss: 0.01182110
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 2823.2062446799737 ----------
[1/200] Training loss: 0.16894641
[2/200] Training loss: 0.05615385
[3/200] Training loss: 0.05057414
[4/200] Training loss: 0.04639878
[5/200] Training loss: 0.04338792
[6/200] Training loss: 0.04255730
[7/200] Training loss: 0.03985712
[8/200] Training loss: 0.03894622
[9/200] Training loss: 0.03704995
[10/200] Training loss: 0.03289537
[50/200] Training loss: 0.01817602
[100/200] Training loss: 0.01555604
[150/200] Training loss: 0.01438528
[200/200] Training loss: 0.01324009
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22226.850069229335 ----------
[1/200] Training loss: 0.15912587
[2/200] Training loss: 0.06059082
[3/200] Training loss: 0.05684349
[4/200] Training loss: 0.05147156
[5/200] Training loss: 0.04619701
[6/200] Training loss: 0.04468860
[7/200] Training loss: 0.04222298
[8/200] Training loss: 0.03687934
[9/200] Training loss: 0.03796073
[10/200] Training loss: 0.03440501
[50/200] Training loss: 0.01850029
[100/200] Training loss: 0.01495657
[150/200] Training loss: 0.01387076
[200/200] Training loss: 0.01333151
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19033.06512362105 ----------
[1/200] Training loss: 0.14528901
[2/200] Training loss: 0.05888757
[3/200] Training loss: 0.05327661
[4/200] Training loss: 0.04804305
[5/200] Training loss: 0.04491097
[6/200] Training loss: 0.04397883
[7/200] Training loss: 0.04165430
[8/200] Training loss: 0.03920893
[9/200] Training loss: 0.03570489
[10/200] Training loss: 0.03796031
[50/200] Training loss: 0.02032192
[100/200] Training loss: 0.01622486
[150/200] Training loss: 0.01455654
[200/200] Training loss: 0.01298855
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 11361.153814644003 ----------
[1/200] Training loss: 0.19787114
[2/200] Training loss: 0.06942388
[3/200] Training loss: 0.05896261
[4/200] Training loss: 0.05274224
[5/200] Training loss: 0.05217046
[6/200] Training loss: 0.05121217
[7/200] Training loss: 0.04610689
[8/200] Training loss: 0.04387018
[9/200] Training loss: 0.04219994
[10/200] Training loss: 0.04170661
[50/200] Training loss: 0.02044350
[100/200] Training loss: 0.01668676
[150/200] Training loss: 0.01483874
[200/200] Training loss: 0.01348149
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15701.923958547246 ----------
[1/200] Training loss: 0.14592190
[2/200] Training loss: 0.05698486
[3/200] Training loss: 0.04951663
[4/200] Training loss: 0.04777277
[5/200] Training loss: 0.03923019
[6/200] Training loss: 0.03859398
[7/200] Training loss: 0.03547456
[8/200] Training loss: 0.03397258
[9/200] Training loss: 0.03041388
[10/200] Training loss: 0.02957227
[50/200] Training loss: 0.01668836
[100/200] Training loss: 0.01395562
[150/200] Training loss: 0.01324135
[200/200] Training loss: 0.01212422
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16574.275006768774 ----------
[1/200] Training loss: 0.14335334
[2/200] Training loss: 0.06079894
[3/200] Training loss: 0.05073755
[4/200] Training loss: 0.05162619
[5/200] Training loss: 0.04503451
[6/200] Training loss: 0.04374058
[7/200] Training loss: 0.04105261
[8/200] Training loss: 0.04023139
[9/200] Training loss: 0.03896284
[10/200] Training loss: 0.03556383
[50/200] Training loss: 0.01836443
[100/200] Training loss: 0.01499433
[150/200] Training loss: 0.01416517
[200/200] Training loss: 0.01235633
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15582.01784108849 ----------
[1/200] Training loss: 0.19396614
[2/200] Training loss: 0.06107154
[3/200] Training loss: 0.05632562
[4/200] Training loss: 0.05004111
[5/200] Training loss: 0.04958351
[6/200] Training loss: 0.04677649
[7/200] Training loss: 0.04098866
[8/200] Training loss: 0.04206707
[9/200] Training loss: 0.03922212
[10/200] Training loss: 0.03783616
[50/200] Training loss: 0.01903588
[100/200] Training loss: 0.01631862
[150/200] Training loss: 0.01519007
[200/200] Training loss: 0.01343841
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5118.900467873936 ----------
[1/200] Training loss: 0.17144247
[2/200] Training loss: 0.05707134
[3/200] Training loss: 0.04829395
[4/200] Training loss: 0.04744562
[5/200] Training loss: 0.04309484
[6/200] Training loss: 0.04097333
[7/200] Training loss: 0.03577140
[8/200] Training loss: 0.03475549
[9/200] Training loss: 0.03414303
[10/200] Training loss: 0.03073114
[50/200] Training loss: 0.01907096
[100/200] Training loss: 0.01706642
[150/200] Training loss: 0.01559026
[200/200] Training loss: 0.01440133
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11167.069087276213 ----------
[1/200] Training loss: 0.06285216
[2/200] Training loss: 0.04260265
[3/200] Training loss: 0.03702219
[4/200] Training loss: 0.03231594
[5/200] Training loss: 0.02811124
[6/200] Training loss: 0.02572447
[7/200] Training loss: 0.02486425
[8/200] Training loss: 0.02320043
[9/200] Training loss: 0.02225328
[10/200] Training loss: 0.02166709
[50/200] Training loss: 0.01294959
[100/200] Training loss: 0.01131322
[150/200] Training loss: 0.01021987
[200/200] Training loss: 0.00972073
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 16986.76378831471 ----------
[1/200] Training loss: 0.14598412
[2/200] Training loss: 0.05748259
[3/200] Training loss: 0.04894015
[4/200] Training loss: 0.04608535
[5/200] Training loss: 0.04293653
[6/200] Training loss: 0.03763826
[7/200] Training loss: 0.03962314
[8/200] Training loss: 0.03637203
[9/200] Training loss: 0.03177008
[10/200] Training loss: 0.03168745
[50/200] Training loss: 0.01784772
[100/200] Training loss: 0.01471170
[150/200] Training loss: 0.01295743
[200/200] Training loss: 0.01191293
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13637.57456441577 ----------
[1/200] Training loss: 0.15103437
[2/200] Training loss: 0.05262213
[3/200] Training loss: 0.04968044
[4/200] Training loss: 0.04241909
[5/200] Training loss: 0.04004823
[6/200] Training loss: 0.03942697
[7/200] Training loss: 0.03431140
[8/200] Training loss: 0.03481345
[9/200] Training loss: 0.03350204
[10/200] Training loss: 0.03045127
[50/200] Training loss: 0.01856164
[100/200] Training loss: 0.01490387
[150/200] Training loss: 0.01259875
[200/200] Training loss: 0.01143204
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 4553.2948509842845 ----------
[1/200] Training loss: 0.13779630
[2/200] Training loss: 0.05536552
[3/200] Training loss: 0.05464484
[4/200] Training loss: 0.05014532
[5/200] Training loss: 0.04846518
[6/200] Training loss: 0.04581526
[7/200] Training loss: 0.04295436
[8/200] Training loss: 0.04404449
[9/200] Training loss: 0.04111678
[10/200] Training loss: 0.03932255
[50/200] Training loss: 0.01992514
[100/200] Training loss: 0.01580957
[150/200] Training loss: 0.01383331
[200/200] Training loss: 0.01193920
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 13203.591026686641 ----------
[1/200] Training loss: 0.17807435
[2/200] Training loss: 0.07690921
[3/200] Training loss: 0.05893025
[4/200] Training loss: 0.05846015
[5/200] Training loss: 0.05415013
[6/200] Training loss: 0.05361934
[7/200] Training loss: 0.05146502
[8/200] Training loss: 0.05078861
[9/200] Training loss: 0.04795573
[10/200] Training loss: 0.04659531
[50/200] Training loss: 0.01886124
[100/200] Training loss: 0.01691691
[150/200] Training loss: 0.01485593
[200/200] Training loss: 0.01424237
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.02816541882984358
----FITNESS-----------RMSE---- 17629.01880423298 ----------
[1/200] Training loss: 0.14704688
[2/200] Training loss: 0.05790206
[3/200] Training loss: 0.05089332
[4/200] Training loss: 0.04661630
[5/200] Training loss: 0.04305584
[6/200] Training loss: 0.04009383
[7/200] Training loss: 0.04008692
[8/200] Training loss: 0.03617180
[9/200] Training loss: 0.03494201
[10/200] Training loss: 0.03218683
[50/200] Training loss: 0.01772305
[100/200] Training loss: 0.01614095
[150/200] Training loss: 0.01375584
[200/200] Training loss: 0.01338574
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22248.90037732202 ----------
[1/200] Training loss: 0.15486093
[2/200] Training loss: 0.06202816
[3/200] Training loss: 0.05280617
[4/200] Training loss: 0.04767449
[5/200] Training loss: 0.04020296
[6/200] Training loss: 0.03644214
[7/200] Training loss: 0.03654568
[8/200] Training loss: 0.03442801
[9/200] Training loss: 0.03169579
[10/200] Training loss: 0.02803775
[50/200] Training loss: 0.01642271
[100/200] Training loss: 0.01285678
[150/200] Training loss: 0.01191723
[200/200] Training loss: 0.01127290
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19542.620499820387 ----------
[1/200] Training loss: 0.13577973
[2/200] Training loss: 0.05708969
[3/200] Training loss: 0.05412633
[4/200] Training loss: 0.05089530
[5/200] Training loss: 0.04793493
[6/200] Training loss: 0.04827199
[7/200] Training loss: 0.04380774
[8/200] Training loss: 0.04036947
[9/200] Training loss: 0.03860257
[10/200] Training loss: 0.03555291
[50/200] Training loss: 0.01949619
[100/200] Training loss: 0.01698773
[150/200] Training loss: 0.01569578
[200/200] Training loss: 0.01398825
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 21099.60757928924 ----------
[1/200] Training loss: 0.15705454
[2/200] Training loss: 0.05571900
[3/200] Training loss: 0.05068783
[4/200] Training loss: 0.04729015
[5/200] Training loss: 0.04301407
[6/200] Training loss: 0.04115355
[7/200] Training loss: 0.03909850
[8/200] Training loss: 0.03550853
[9/200] Training loss: 0.03506650
[10/200] Training loss: 0.03039085
[50/200] Training loss: 0.01717578
[100/200] Training loss: 0.01384765
[150/200] Training loss: 0.01223924
[200/200] Training loss: 0.01254068
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8627.064390625585 ----------
[1/200] Training loss: 0.14620809
[2/200] Training loss: 0.06066976
[3/200] Training loss: 0.05503139
[4/200] Training loss: 0.04762625
[5/200] Training loss: 0.04417975
[6/200] Training loss: 0.04275193
[7/200] Training loss: 0.03944077
[8/200] Training loss: 0.03760692
[9/200] Training loss: 0.03456085
[10/200] Training loss: 0.03445373
[50/200] Training loss: 0.01766956
[100/200] Training loss: 0.01527858
[150/200] Training loss: 0.01457590
[200/200] Training loss: 0.01277804
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5704.423020779577 ----------
[1/200] Training loss: 0.15407242
[2/200] Training loss: 0.05151411
[3/200] Training loss: 0.04577728
[4/200] Training loss: 0.03926443
[5/200] Training loss: 0.03906024
[6/200] Training loss: 0.03797896
[7/200] Training loss: 0.03557395
[8/200] Training loss: 0.03187793
[9/200] Training loss: 0.03198464
[10/200] Training loss: 0.02851868
[50/200] Training loss: 0.01891799
[100/200] Training loss: 0.01659488
[150/200] Training loss: 0.01462301
[200/200] Training loss: 0.01409788
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10166.186699052894 ----------
[1/200] Training loss: 0.15930560
[2/200] Training loss: 0.06210910
[3/200] Training loss: 0.05329921
[4/200] Training loss: 0.05117892
[5/200] Training loss: 0.04757886
[6/200] Training loss: 0.04443052
[7/200] Training loss: 0.03935764
[8/200] Training loss: 0.03741319
[9/200] Training loss: 0.03369473
[10/200] Training loss: 0.03172452
[50/200] Training loss: 0.01630511
[100/200] Training loss: 0.01342816
[150/200] Training loss: 0.01246576
[200/200] Training loss: 0.01159001
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19535.52988787353 ----------
[1/200] Training loss: 0.18819470
[2/200] Training loss: 0.06927742
[3/200] Training loss: 0.05598020
[4/200] Training loss: 0.05235172
[5/200] Training loss: 0.05130980
[6/200] Training loss: 0.04901255
[7/200] Training loss: 0.04664031
[8/200] Training loss: 0.04431100
[9/200] Training loss: 0.03885886
[10/200] Training loss: 0.03853501
[50/200] Training loss: 0.01944547
[100/200] Training loss: 0.01694680
[150/200] Training loss: 0.01606387
[200/200] Training loss: 0.01469911
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9693.623471127812 ----------
[1/200] Training loss: 0.16885821
[2/200] Training loss: 0.06145134
[3/200] Training loss: 0.05222651
[4/200] Training loss: 0.05249120
[5/200] Training loss: 0.04650866
[6/200] Training loss: 0.04184178
[7/200] Training loss: 0.03936749
[8/200] Training loss: 0.03924340
[9/200] Training loss: 0.03735387
[10/200] Training loss: 0.03456588
[50/200] Training loss: 0.01876024
[100/200] Training loss: 0.01534856
[150/200] Training loss: 0.01383045
[200/200] Training loss: 0.01216190
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22298.594753930123 ----------
[1/200] Training loss: 0.12679000
[2/200] Training loss: 0.05097614
[3/200] Training loss: 0.04798145
[4/200] Training loss: 0.04475746
[5/200] Training loss: 0.04224080
[6/200] Training loss: 0.04170227
[7/200] Training loss: 0.03768764
[8/200] Training loss: 0.03794842
[9/200] Training loss: 0.03375187
[10/200] Training loss: 0.03512446
[50/200] Training loss: 0.01897539
[100/200] Training loss: 0.01419823
[150/200] Training loss: 0.01312314
[200/200] Training loss: 0.01223772
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5609.100284359338 ----------
[1/200] Training loss: 0.15235375
[2/200] Training loss: 0.05745034
[3/200] Training loss: 0.04948997
[4/200] Training loss: 0.04965574
[5/200] Training loss: 0.04444926
[6/200] Training loss: 0.04268758
[7/200] Training loss: 0.04035991
[8/200] Training loss: 0.03682833
[9/200] Training loss: 0.03672562
[10/200] Training loss: 0.03489876
[50/200] Training loss: 0.01830789
[100/200] Training loss: 0.01508524
[150/200] Training loss: 0.01407818
[200/200] Training loss: 0.01316220
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 11974.306493488464 ----------
[1/200] Training loss: 0.15086197
[2/200] Training loss: 0.06061369
[3/200] Training loss: 0.05275252
[4/200] Training loss: 0.04894962
[5/200] Training loss: 0.04597978
[6/200] Training loss: 0.04317296
[7/200] Training loss: 0.04527649
[8/200] Training loss: 0.03953302
[9/200] Training loss: 0.03978591
[10/200] Training loss: 0.03584733
[50/200] Training loss: 0.01945555
[100/200] Training loss: 0.01577999
[150/200] Training loss: 0.01446101
[200/200] Training loss: 0.01224604
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20040.828725379597 ----------
[1/200] Training loss: 0.09492554
[2/200] Training loss: 0.04983091
[3/200] Training loss: 0.04024528
[4/200] Training loss: 0.03609124
[5/200] Training loss: 0.03188776
[6/200] Training loss: 0.02982944
[7/200] Training loss: 0.02894466
[8/200] Training loss: 0.02774100
[9/200] Training loss: 0.02608068
[10/200] Training loss: 0.02452758
[50/200] Training loss: 0.01437192
[100/200] Training loss: 0.01180933
[150/200] Training loss: 0.01087916
[200/200] Training loss: 0.01010200
---batch_size---: 4 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15768.473864011063 ----------
[1/200] Training loss: 0.13951844
[2/200] Training loss: 0.05705560
[3/200] Training loss: 0.04887263
[4/200] Training loss: 0.04707030
[5/200] Training loss: 0.04044052
[6/200] Training loss: 0.04241202
[7/200] Training loss: 0.03734786
[8/200] Training loss: 0.03364985
[9/200] Training loss: 0.03029042
[10/200] Training loss: 0.03004857
[50/200] Training loss: 0.01692125
[100/200] Training loss: 0.01501067
[150/200] Training loss: 0.01340451
[200/200] Training loss: 0.01239882
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 13390.059596581339 ----------
[1/200] Training loss: 0.16367081
[2/200] Training loss: 0.07482997
[3/200] Training loss: 0.05814413
[4/200] Training loss: 0.05629306
[5/200] Training loss: 0.04958507
[6/200] Training loss: 0.04672374
[7/200] Training loss: 0.04692506
[8/200] Training loss: 0.04380846
[9/200] Training loss: 0.04425310
[10/200] Training loss: 0.04083637
[50/200] Training loss: 0.02060964
[100/200] Training loss: 0.01764300
[150/200] Training loss: 0.01560993
[200/200] Training loss: 0.01417328
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11416.35984015921 ----------
[1/200] Training loss: 0.15633510
[2/200] Training loss: 0.05554545
[3/200] Training loss: 0.05053161
[4/200] Training loss: 0.04760456
[5/200] Training loss: 0.04693446
[6/200] Training loss: 0.04336170
[7/200] Training loss: 0.04431242
[8/200] Training loss: 0.04144349
[9/200] Training loss: 0.03688265
[10/200] Training loss: 0.03793187
[50/200] Training loss: 0.01901039
[100/200] Training loss: 0.01595710
[150/200] Training loss: 0.01505497
[200/200] Training loss: 0.01399337
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 8470.49845050455 ----------
[1/200] Training loss: 0.17370145
[2/200] Training loss: 0.05992939
[3/200] Training loss: 0.05746054
[4/200] Training loss: 0.05217193
[5/200] Training loss: 0.05108031
[6/200] Training loss: 0.04648729
[7/200] Training loss: 0.04317631
[8/200] Training loss: 0.04091858
[9/200] Training loss: 0.04294249
[10/200] Training loss: 0.03972622
[50/200] Training loss: 0.02195641
[100/200] Training loss: 0.01616795
[150/200] Training loss: 0.01363993
[200/200] Training loss: 0.01331561
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9385.08902461772 ----------
[1/200] Training loss: 0.14023239
[2/200] Training loss: 0.05293535
[3/200] Training loss: 0.04695181
[4/200] Training loss: 0.04231018
[5/200] Training loss: 0.04090125
[6/200] Training loss: 0.03811127
[7/200] Training loss: 0.03598528
[8/200] Training loss: 0.03334153
[9/200] Training loss: 0.03174994
[10/200] Training loss: 0.02943341
[50/200] Training loss: 0.01793923
[100/200] Training loss: 0.01443511
[150/200] Training loss: 0.01359577
[200/200] Training loss: 0.01277936
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 9478.097699433152 ----------
[1/200] Training loss: 0.16447853
[2/200] Training loss: 0.06197533
[3/200] Training loss: 0.05174758
[4/200] Training loss: 0.05203693
[5/200] Training loss: 0.04667091
[6/200] Training loss: 0.04514522
[7/200] Training loss: 0.04358205
[8/200] Training loss: 0.03848567
[9/200] Training loss: 0.03902400
[10/200] Training loss: 0.03573971
[50/200] Training loss: 0.01871483
[100/200] Training loss: 0.01482245
[150/200] Training loss: 0.01339355
[200/200] Training loss: 0.01211617
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8460.796652798128 ----------
[1/200] Training loss: 0.16019158
[2/200] Training loss: 0.05820618
[3/200] Training loss: 0.04996742
[4/200] Training loss: 0.04332638
[5/200] Training loss: 0.04003117
[6/200] Training loss: 0.03916523
[7/200] Training loss: 0.03586943
[8/200] Training loss: 0.03392774
[9/200] Training loss: 0.03218804
[10/200] Training loss: 0.02867257
[50/200] Training loss: 0.01889422
[100/200] Training loss: 0.01677002
[150/200] Training loss: 0.01492154
[200/200] Training loss: 0.01482176
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10744.090841015819 ----------
[1/200] Training loss: 0.21901773
[2/200] Training loss: 0.06946589
[3/200] Training loss: 0.06232565
[4/200] Training loss: 0.05787795
[5/200] Training loss: 0.05309903
[6/200] Training loss: 0.05191846
[7/200] Training loss: 0.04756874
[8/200] Training loss: 0.04628157
[9/200] Training loss: 0.04675183
[10/200] Training loss: 0.04440572
[50/200] Training loss: 0.02138296
[100/200] Training loss: 0.01791839
[150/200] Training loss: 0.01522157
[200/200] Training loss: 0.01439254
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5771.078408755161 ----------
[1/200] Training loss: 0.14417244
[2/200] Training loss: 0.05932269
[3/200] Training loss: 0.05162412
[4/200] Training loss: 0.04772534
[5/200] Training loss: 0.04538090
[6/200] Training loss: 0.04247563
[7/200] Training loss: 0.03920598
[8/200] Training loss: 0.03847024
[9/200] Training loss: 0.03560942
[10/200] Training loss: 0.03461056
[50/200] Training loss: 0.01768415
[100/200] Training loss: 0.01442017
[150/200] Training loss: 0.01251893
[200/200] Training loss: 0.01179354
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 26398.34419049801 ----------
[1/200] Training loss: 0.16879810
[2/200] Training loss: 0.05804405
[3/200] Training loss: 0.05448672
[4/200] Training loss: 0.04952289
[5/200] Training loss: 0.04834561
[6/200] Training loss: 0.04183074
[7/200] Training loss: 0.03799964
[8/200] Training loss: 0.03622590
[9/200] Training loss: 0.03443390
[10/200] Training loss: 0.03362210
[50/200] Training loss: 0.01856738
[100/200] Training loss: 0.01404290
[150/200] Training loss: 0.01280602
[200/200] Training loss: 0.01224582
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11011.068976262024 ----------
[1/200] Training loss: 0.16263139
[2/200] Training loss: 0.06014451
[3/200] Training loss: 0.05243137
[4/200] Training loss: 0.05037965
[5/200] Training loss: 0.04991073
[6/200] Training loss: 0.04739123
[7/200] Training loss: 0.04590722
[8/200] Training loss: 0.04300095
[9/200] Training loss: 0.04134573
[10/200] Training loss: 0.04007536
[50/200] Training loss: 0.01870296
[100/200] Training loss: 0.01686816
[150/200] Training loss: 0.01475882
[200/200] Training loss: 0.01319741
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20048.687937119477 ----------
[1/200] Training loss: 0.15813994
[2/200] Training loss: 0.06257716
[3/200] Training loss: 0.05442186
[4/200] Training loss: 0.05096817
[5/200] Training loss: 0.04684233
[6/200] Training loss: 0.04421361
[7/200] Training loss: 0.04099425
[8/200] Training loss: 0.03426868
[9/200] Training loss: 0.03687544
[10/200] Training loss: 0.03119038
[50/200] Training loss: 0.01971233
[100/200] Training loss: 0.01531610
[150/200] Training loss: 0.01436516
[200/200] Training loss: 0.01350746
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17032.979774543266 ----------
[1/200] Training loss: 0.18399874
[2/200] Training loss: 0.06232358
[3/200] Training loss: 0.05221371
[4/200] Training loss: 0.05085529
[5/200] Training loss: 0.04561616
[6/200] Training loss: 0.04473331
[7/200] Training loss: 0.04421709
[8/200] Training loss: 0.03737019
[9/200] Training loss: 0.04012674
[10/200] Training loss: 0.03624253
[50/200] Training loss: 0.02193759
[100/200] Training loss: 0.01500717
[150/200] Training loss: 0.01344567
[200/200] Training loss: 0.01194777
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11950.463087261514 ----------
[1/200] Training loss: 0.14785430
[2/200] Training loss: 0.06282366
[3/200] Training loss: 0.05180224
[4/200] Training loss: 0.04403083
[5/200] Training loss: 0.03906695
[6/200] Training loss: 0.03763951
[7/200] Training loss: 0.03546274
[8/200] Training loss: 0.03471957
[9/200] Training loss: 0.03443827
[10/200] Training loss: 0.03346009
[50/200] Training loss: 0.01707464
[100/200] Training loss: 0.01363207
[150/200] Training loss: 0.01176331
[200/200] Training loss: 0.01134867
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7851.781708631487 ----------
[1/200] Training loss: 0.16471526
[2/200] Training loss: 0.05650701
[3/200] Training loss: 0.05102498
[4/200] Training loss: 0.05042871
[5/200] Training loss: 0.04709117
[6/200] Training loss: 0.04136949
[7/200] Training loss: 0.03783580
[8/200] Training loss: 0.03814284
[9/200] Training loss: 0.03468069
[10/200] Training loss: 0.03204342
[50/200] Training loss: 0.01653796
[100/200] Training loss: 0.01312532
[150/200] Training loss: 0.01265615
[200/200] Training loss: 0.01177632
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15946.570289563835 ----------
[1/200] Training loss: 0.16377839
[2/200] Training loss: 0.06105134
[3/200] Training loss: 0.04906614
[4/200] Training loss: 0.04246683
[5/200] Training loss: 0.03651981
[6/200] Training loss: 0.03611069
[7/200] Training loss: 0.03117588
[8/200] Training loss: 0.03008003
[9/200] Training loss: 0.02817082
[10/200] Training loss: 0.02606371
[50/200] Training loss: 0.01540308
[100/200] Training loss: 0.01313460
[150/200] Training loss: 0.01192035
[200/200] Training loss: 0.01097544
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17956.069057563796 ----------
[1/200] Training loss: 0.15934314
[2/200] Training loss: 0.05413856
[3/200] Training loss: 0.05132416
[4/200] Training loss: 0.04702580
[5/200] Training loss: 0.04267031
[6/200] Training loss: 0.04173906
[7/200] Training loss: 0.04087429
[8/200] Training loss: 0.03912370
[9/200] Training loss: 0.03733820
[10/200] Training loss: 0.03467137
[50/200] Training loss: 0.01848730
[100/200] Training loss: 0.01629833
[150/200] Training loss: 0.01381423
[200/200] Training loss: 0.01290717
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14801.312374245737 ----------
[1/200] Training loss: 0.15133216
[2/200] Training loss: 0.06100617
[3/200] Training loss: 0.05707314
[4/200] Training loss: 0.05236152
[5/200] Training loss: 0.04725666
[6/200] Training loss: 0.04659897
[7/200] Training loss: 0.04184600
[8/200] Training loss: 0.03956852
[9/200] Training loss: 0.03860402
[10/200] Training loss: 0.03672795
[50/200] Training loss: 0.01799591
[100/200] Training loss: 0.01546564
[150/200] Training loss: 0.01425156
[200/200] Training loss: 0.01236404
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21236.288941338127 ----------
[1/200] Training loss: 0.15451782
[2/200] Training loss: 0.05534515
[3/200] Training loss: 0.04902167
[4/200] Training loss: 0.04720998
[5/200] Training loss: 0.04285905
[6/200] Training loss: 0.04072576
[7/200] Training loss: 0.03701764
[8/200] Training loss: 0.03701475
[9/200] Training loss: 0.03565800
[10/200] Training loss: 0.03103705
[50/200] Training loss: 0.01884575
[100/200] Training loss: 0.01538406
[150/200] Training loss: 0.01430283
[200/200] Training loss: 0.01360688
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 8799.625446574417 ----------
[1/200] Training loss: 0.13466329
[2/200] Training loss: 0.05610927
[3/200] Training loss: 0.05208656
[4/200] Training loss: 0.04798526
[5/200] Training loss: 0.04280315
[6/200] Training loss: 0.04123463
[7/200] Training loss: 0.04033535
[8/200] Training loss: 0.03728742
[9/200] Training loss: 0.03443384
[10/200] Training loss: 0.03530448
[50/200] Training loss: 0.01815037
[100/200] Training loss: 0.01496566
[150/200] Training loss: 0.01273554
[200/200] Training loss: 0.01247539
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6794.118338680892 ----------
[1/200] Training loss: 0.13990747
[2/200] Training loss: 0.05180193
[3/200] Training loss: 0.04911997
[4/200] Training loss: 0.04251456
[5/200] Training loss: 0.04007950
[6/200] Training loss: 0.03654833
[7/200] Training loss: 0.03402675
[8/200] Training loss: 0.03293596
[9/200] Training loss: 0.03081869
[10/200] Training loss: 0.02744102
[50/200] Training loss: 0.01745005
[100/200] Training loss: 0.01537063
[150/200] Training loss: 0.01411196
[200/200] Training loss: 0.01356081
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 6516.935476126797 ----------
[1/200] Training loss: 0.15537798
[2/200] Training loss: 0.05754056
[3/200] Training loss: 0.04995359
[4/200] Training loss: 0.04585478
[5/200] Training loss: 0.04334074
[6/200] Training loss: 0.04007251
[7/200] Training loss: 0.03847015
[8/200] Training loss: 0.03489788
[9/200] Training loss: 0.03285861
[10/200] Training loss: 0.03084416
[50/200] Training loss: 0.01680484
[100/200] Training loss: 0.01500782
[150/200] Training loss: 0.01311659
[200/200] Training loss: 0.01258853
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8718.571442616043 ----------
[1/200] Training loss: 0.03947350
[2/200] Training loss: 0.00244056
[3/200] Training loss: 0.00219376
[4/200] Training loss: 0.00212656
[5/200] Training loss: 0.00178915
[6/200] Training loss: 0.00166056
[7/200] Training loss: 0.00142631
[8/200] Training loss: 0.00131951
[9/200] Training loss: 0.00101652
[10/200] Training loss: 0.00098027
[50/200] Training loss: 0.00028122
[100/200] Training loss: 0.00023143
[150/200] Training loss: 0.00019929
[200/200] Training loss: 0.00018741
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10845.765256541375 ----------
[1/200] Training loss: 0.18405867
[2/200] Training loss: 0.06203782
[3/200] Training loss: 0.05238353
[4/200] Training loss: 0.05162420
[5/200] Training loss: 0.04738156
[6/200] Training loss: 0.04521142
[7/200] Training loss: 0.04453000
[8/200] Training loss: 0.04027729
[9/200] Training loss: 0.03706549
[10/200] Training loss: 0.03931565
[50/200] Training loss: 0.02013184
[100/200] Training loss: 0.01604991
[150/200] Training loss: 0.01426531
[200/200] Training loss: 0.01349950
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4983.522047708829 ----------
[1/200] Training loss: 0.12458253
[2/200] Training loss: 0.05377721
[3/200] Training loss: 0.04934503
[4/200] Training loss: 0.04306525
[5/200] Training loss: 0.03771877
[6/200] Training loss: 0.03686606
[7/200] Training loss: 0.03372446
[8/200] Training loss: 0.03191437
[9/200] Training loss: 0.03257163
[10/200] Training loss: 0.03149760
[50/200] Training loss: 0.01628383
[100/200] Training loss: 0.01333667
[150/200] Training loss: 0.01211700
[200/200] Training loss: 0.01117108
---batch_size---: 16 ---n_steps---: 8 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 27009.57696817927 ----------
[1/200] Training loss: 0.17053679
[2/200] Training loss: 0.05834610
[3/200] Training loss: 0.05426141
[4/200] Training loss: 0.05193074
[5/200] Training loss: 0.04641629
[6/200] Training loss: 0.04660595
[7/200] Training loss: 0.04192179
[8/200] Training loss: 0.04186777
[9/200] Training loss: 0.03584175
[10/200] Training loss: 0.03933741
[50/200] Training loss: 0.01838461
[100/200] Training loss: 0.01559165
[150/200] Training loss: 0.01412870
[200/200] Training loss: 0.01353887
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16071.680932621826 ----------
[1/200] Training loss: 0.17575664
[2/200] Training loss: 0.06034234
[3/200] Training loss: 0.05541693
[4/200] Training loss: 0.04934091
[5/200] Training loss: 0.05051759
[6/200] Training loss: 0.04647250
[7/200] Training loss: 0.04237831
[8/200] Training loss: 0.04078556
[9/200] Training loss: 0.03828303
[10/200] Training loss: 0.03235543
[50/200] Training loss: 0.01769727
[100/200] Training loss: 0.01498729
[150/200] Training loss: 0.01389589
[200/200] Training loss: 0.01342415
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18714.855703424484 ----------
[1/200] Training loss: 0.20524966
[2/200] Training loss: 0.08497175
[3/200] Training loss: 0.08005067
[4/200] Training loss: 0.07581505
[5/200] Training loss: 0.06664906
[6/200] Training loss: 0.05932023
[7/200] Training loss: 0.05479221
[8/200] Training loss: 0.05257355
[9/200] Training loss: 0.05132807
[10/200] Training loss: 0.05223443
[50/200] Training loss: 0.03105963
[100/200] Training loss: 0.02581956
[150/200] Training loss: 0.02310186
[200/200] Training loss: 0.02161923
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: SGD ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11825.995095551156 ----------
[1/200] Training loss: 0.18363198
[2/200] Training loss: 0.06159446
[3/200] Training loss: 0.05318340
[4/200] Training loss: 0.05088768
[5/200] Training loss: 0.04669921
[6/200] Training loss: 0.04615658
[7/200] Training loss: 0.04417181
[8/200] Training loss: 0.04184632
[9/200] Training loss: 0.03969398
[10/200] Training loss: 0.03681640
[50/200] Training loss: 0.01715343
[100/200] Training loss: 0.01458031
[150/200] Training loss: 0.01349601
[200/200] Training loss: 0.01291260
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8881.68362417847 ----------
[1/200] Training loss: 0.16280737
[2/200] Training loss: 0.06036552
[3/200] Training loss: 0.05357772
[4/200] Training loss: 0.04795403
[5/200] Training loss: 0.04011850
[6/200] Training loss: 0.03734245
[7/200] Training loss: 0.03821416
[8/200] Training loss: 0.03298148
[9/200] Training loss: 0.03209136
[10/200] Training loss: 0.03086059
[50/200] Training loss: 0.01642805
[100/200] Training loss: 0.01403805
[150/200] Training loss: 0.01283075
[200/200] Training loss: 0.01185392
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.4299046447407143 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19851.993149303675 ----------
[1/200] Training loss: 0.16411673
[2/200] Training loss: 0.05944363
[3/200] Training loss: 0.04768270
[4/200] Training loss: 0.05086809
[5/200] Training loss: 0.04004850
[6/200] Training loss: 0.03917950
[7/200] Training loss: 0.03816608
[8/200] Training loss: 0.03630867
[9/200] Training loss: 0.03230434
[10/200] Training loss: 0.03180986
[50/200] Training loss: 0.01936898
[100/200] Training loss: 0.01579573
[150/200] Training loss: 0.01405152
[200/200] Training loss: 0.01298850
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13485.159250079325 ----------
[1/200] Training loss: 0.14076149
[2/200] Training loss: 0.05219439
[3/200] Training loss: 0.05327097
[4/200] Training loss: 0.04704182
[5/200] Training loss: 0.04362837
[6/200] Training loss: 0.04049176
[7/200] Training loss: 0.03656302
[8/200] Training loss: 0.03532480
[9/200] Training loss: 0.03373886
[10/200] Training loss: 0.03015279
[50/200] Training loss: 0.01825214
[100/200] Training loss: 0.01491039
[150/200] Training loss: 0.01465341
[200/200] Training loss: 0.01312733
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 10488.703637723778 ----------
[1/200] Training loss: 0.12615097
[2/200] Training loss: 0.05792761
[3/200] Training loss: 0.05364740
[4/200] Training loss: 0.05036145
[5/200] Training loss: 0.04511321
[6/200] Training loss: 0.04458258
[7/200] Training loss: 0.04143645
[8/200] Training loss: 0.03737291
[9/200] Training loss: 0.03649519
[10/200] Training loss: 0.03560983
[50/200] Training loss: 0.01823181
[100/200] Training loss: 0.01583934
[150/200] Training loss: 0.01416477
[200/200] Training loss: 0.01338255
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 7951.578711174278 ----------
[1/200] Training loss: 0.17221879
[2/200] Training loss: 0.06567771
[3/200] Training loss: 0.05521433
[4/200] Training loss: 0.05385429
[5/200] Training loss: 0.04968100
[6/200] Training loss: 0.04841956
[7/200] Training loss: 0.04753824
[8/200] Training loss: 0.04565595
[9/200] Training loss: 0.04421469
[10/200] Training loss: 0.03955225
[50/200] Training loss: 0.01772703
[100/200] Training loss: 0.01558353
[150/200] Training loss: 0.01432337
[200/200] Training loss: 0.01300331
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17216.452596281266 ----------
[1/200] Training loss: 0.16477631
[2/200] Training loss: 0.06179188
[3/200] Training loss: 0.06220069
[4/200] Training loss: 0.05517532
[5/200] Training loss: 0.05289319
[6/200] Training loss: 0.04571357
[7/200] Training loss: 0.04503862
[8/200] Training loss: 0.04020807
[9/200] Training loss: 0.03837714
[10/200] Training loss: 0.03833376
[50/200] Training loss: 0.01712855
[100/200] Training loss: 0.01256025
[150/200] Training loss: 0.01207580
[200/200] Training loss: 0.01076756
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17912.09825788146 ----------
[1/200] Training loss: 0.15553743
[2/200] Training loss: 0.05713037
[3/200] Training loss: 0.05357766
[4/200] Training loss: 0.04948403
[5/200] Training loss: 0.04032353
[6/200] Training loss: 0.03808332
[7/200] Training loss: 0.03869388
[8/200] Training loss: 0.03153758
[9/200] Training loss: 0.03192293
[10/200] Training loss: 0.03060170
[50/200] Training loss: 0.01603401
[100/200] Training loss: 0.01271360
[150/200] Training loss: 0.01149327
[200/200] Training loss: 0.01068404
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22470.474850345287 ----------
[1/200] Training loss: 0.14311740
[2/200] Training loss: 0.05912710
[3/200] Training loss: 0.05604329
[4/200] Training loss: 0.05327224
[5/200] Training loss: 0.04923190
[6/200] Training loss: 0.05018926
[7/200] Training loss: 0.04799023
[8/200] Training loss: 0.04597175
[9/200] Training loss: 0.04314350
[10/200] Training loss: 0.04248473
[50/200] Training loss: 0.02000353
[100/200] Training loss: 0.01740795
[150/200] Training loss: 0.01534708
[200/200] Training loss: 0.01446873
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 9923.39296813343 ----------
[1/200] Training loss: 0.03645982
[2/200] Training loss: 0.00350079
[3/200] Training loss: 0.00295080
[4/200] Training loss: 0.00257371
[5/200] Training loss: 0.00206631
[6/200] Training loss: 0.00194581
[7/200] Training loss: 0.00229073
[8/200] Training loss: 0.00180267
[9/200] Training loss: 0.00161376
[10/200] Training loss: 0.00145263
[50/200] Training loss: 0.00051402
[100/200] Training loss: 0.00039244
[150/200] Training loss: 0.00033633
[200/200] Training loss: 0.00027474
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12783.028123257807 ----------
[1/200] Training loss: 0.14654211
[2/200] Training loss: 0.06127515
[3/200] Training loss: 0.05083754
[4/200] Training loss: 0.05009932
[5/200] Training loss: 0.04528295
[6/200] Training loss: 0.04224973
[7/200] Training loss: 0.04312213
[8/200] Training loss: 0.03475566
[9/200] Training loss: 0.03423413
[10/200] Training loss: 0.03409023
[50/200] Training loss: 0.01876330
[100/200] Training loss: 0.01629762
[150/200] Training loss: 0.01448984
[200/200] Training loss: 0.01439910
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17195.843218638627 ----------
[1/200] Training loss: 0.17798029
[2/200] Training loss: 0.06331932
[3/200] Training loss: 0.05335653
[4/200] Training loss: 0.04755031
[5/200] Training loss: 0.04785363
[6/200] Training loss: 0.04519373
[7/200] Training loss: 0.03946332
[8/200] Training loss: 0.03800162
[9/200] Training loss: 0.03461792
[10/200] Training loss: 0.03210299
[50/200] Training loss: 0.01920220
[100/200] Training loss: 0.01646727
[150/200] Training loss: 0.01524555
[200/200] Training loss: 0.01397571
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19311.846726815125 ----------
[1/200] Training loss: 0.14324361
[2/200] Training loss: 0.05437020
[3/200] Training loss: 0.04845041
[4/200] Training loss: 0.04800233
[5/200] Training loss: 0.04787769
[6/200] Training loss: 0.04402040
[7/200] Training loss: 0.04108750
[8/200] Training loss: 0.04018785
[9/200] Training loss: 0.03863413
[10/200] Training loss: 0.03552464
[50/200] Training loss: 0.01722856
[100/200] Training loss: 0.01464626
[150/200] Training loss: 0.01398351
[200/200] Training loss: 0.01302424
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 5569.649360597128 ----------
[1/200] Training loss: 0.16111816
[2/200] Training loss: 0.05222445
[3/200] Training loss: 0.04711931
[4/200] Training loss: 0.04640869
[5/200] Training loss: 0.04142827
[6/200] Training loss: 0.04048447
[7/200] Training loss: 0.03532770
[8/200] Training loss: 0.03506230
[9/200] Training loss: 0.03065120
[10/200] Training loss: 0.03246206
[50/200] Training loss: 0.01868084
[100/200] Training loss: 0.01617543
[150/200] Training loss: 0.01454985
[200/200] Training loss: 0.01420654
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12532.43663458946 ----------
[1/200] Training loss: 0.10090572
[2/200] Training loss: 0.05560364
[3/200] Training loss: 0.05078204
[4/200] Training loss: 0.04949539
[5/200] Training loss: 0.04660664
[6/200] Training loss: 0.04370233
[7/200] Training loss: 0.04207982
[8/200] Training loss: 0.04204001
[9/200] Training loss: 0.03938551
[10/200] Training loss: 0.03742141
[50/200] Training loss: 0.01911921
[100/200] Training loss: 0.01540895
[150/200] Training loss: 0.01366403
[200/200] Training loss: 0.01191468
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 11459.99476439671 ----------
[1/200] Training loss: 0.18280219
[2/200] Training loss: 0.06002494
[3/200] Training loss: 0.05476674
[4/200] Training loss: 0.05063173
[5/200] Training loss: 0.05000293
[6/200] Training loss: 0.04721919
[7/200] Training loss: 0.04373302
[8/200] Training loss: 0.04309162
[9/200] Training loss: 0.04301642
[10/200] Training loss: 0.03796970
[50/200] Training loss: 0.01775396
[100/200] Training loss: 0.01477367
[150/200] Training loss: 0.01272786
[200/200] Training loss: 0.01244167
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4442.475436060396 ----------
[1/200] Training loss: 0.17220443
[2/200] Training loss: 0.06333244
[3/200] Training loss: 0.05457893
[4/200] Training loss: 0.05307466
[5/200] Training loss: 0.04892257
[6/200] Training loss: 0.04586434
[7/200] Training loss: 0.04524064
[8/200] Training loss: 0.04262945
[9/200] Training loss: 0.04028494
[10/200] Training loss: 0.03853951
[50/200] Training loss: 0.01914819
[100/200] Training loss: 0.01645851
[150/200] Training loss: 0.01317281
[200/200] Training loss: 0.01284908
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8185.232800598893 ----------
[1/200] Training loss: 0.13837830
[2/200] Training loss: 0.05736865
[3/200] Training loss: 0.05180739
[4/200] Training loss: 0.04489981
[5/200] Training loss: 0.04251324
[6/200] Training loss: 0.03882333
[7/200] Training loss: 0.04047327
[8/200] Training loss: 0.03611647
[9/200] Training loss: 0.03433008
[10/200] Training loss: 0.03551886
[50/200] Training loss: 0.01812902
[100/200] Training loss: 0.01488364
[150/200] Training loss: 0.01371192
[200/200] Training loss: 0.01176919
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8931.681140748364 ----------
[1/200] Training loss: 0.19322352
[2/200] Training loss: 0.06586464
[3/200] Training loss: 0.05807815
[4/200] Training loss: 0.05543710
[5/200] Training loss: 0.05062280
[6/200] Training loss: 0.05065827
[7/200] Training loss: 0.04514937
[8/200] Training loss: 0.04816772
[9/200] Training loss: 0.04185314
[10/200] Training loss: 0.04180320
[50/200] Training loss: 0.01832891
[100/200] Training loss: 0.01486671
[150/200] Training loss: 0.01354151
[200/200] Training loss: 0.01271698
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4746.644709687043 ----------
[1/200] Training loss: 0.15490070
[2/200] Training loss: 0.05532438
[3/200] Training loss: 0.05201258
[4/200] Training loss: 0.04536976
[5/200] Training loss: 0.04235642
[6/200] Training loss: 0.04270797
[7/200] Training loss: 0.03892738
[8/200] Training loss: 0.03612386
[9/200] Training loss: 0.03264569
[10/200] Training loss: 0.03424767
[50/200] Training loss: 0.01873381
[100/200] Training loss: 0.01416463
[150/200] Training loss: 0.01298927
[200/200] Training loss: 0.01184088
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6824.532218401493 ----------
[1/200] Training loss: 0.15285952
[2/200] Training loss: 0.05880732
[3/200] Training loss: 0.05364418
[4/200] Training loss: 0.05161902
[5/200] Training loss: 0.04449819
[6/200] Training loss: 0.04024967
[7/200] Training loss: 0.03576675
[8/200] Training loss: 0.03513333
[9/200] Training loss: 0.03019872
[10/200] Training loss: 0.03145442
[50/200] Training loss: 0.01825700
[100/200] Training loss: 0.01566835
[150/200] Training loss: 0.01513506
[200/200] Training loss: 0.01441280
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 16805.21252469007 ----------
[1/200] Training loss: 0.16488515
[2/200] Training loss: 0.05905103
[3/200] Training loss: 0.05370461
[4/200] Training loss: 0.05172719
[5/200] Training loss: 0.04826581
[6/200] Training loss: 0.04756864
[7/200] Training loss: 0.04281047
[8/200] Training loss: 0.04115468
[9/200] Training loss: 0.03821963
[10/200] Training loss: 0.03386426
[50/200] Training loss: 0.02004584
[100/200] Training loss: 0.01711832
[150/200] Training loss: 0.01637380
[200/200] Training loss: 0.01480877
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10762.81636004257 ----------
[1/200] Training loss: 0.14020251
[2/200] Training loss: 0.05353661
[3/200] Training loss: 0.04997470
[4/200] Training loss: 0.04811239
[5/200] Training loss: 0.04582584
[6/200] Training loss: 0.04390482
[7/200] Training loss: 0.04160864
[8/200] Training loss: 0.04024811
[9/200] Training loss: 0.03592350
[10/200] Training loss: 0.03547036
[50/200] Training loss: 0.01961395
[100/200] Training loss: 0.01562650
[150/200] Training loss: 0.01423997
[200/200] Training loss: 0.01241852
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 7592.262903772497 ----------
[1/200] Training loss: 0.16347835
[2/200] Training loss: 0.05834200
[3/200] Training loss: 0.05245498
[4/200] Training loss: 0.04556842
[5/200] Training loss: 0.04250944
[6/200] Training loss: 0.04228504
[7/200] Training loss: 0.03964060
[8/200] Training loss: 0.03609189
[9/200] Training loss: 0.03705206
[10/200] Training loss: 0.03415538
[50/200] Training loss: 0.01791740
[100/200] Training loss: 0.01564983
[150/200] Training loss: 0.01409419
[200/200] Training loss: 0.01279967
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15345.820017190348 ----------
[1/200] Training loss: 0.12771656
[2/200] Training loss: 0.05580304
[3/200] Training loss: 0.05021619
[4/200] Training loss: 0.04208548
[5/200] Training loss: 0.04009495
[6/200] Training loss: 0.03888118
[7/200] Training loss: 0.03693801
[8/200] Training loss: 0.03486170
[9/200] Training loss: 0.03632916
[10/200] Training loss: 0.03077034
[50/200] Training loss: 0.01790482
[100/200] Training loss: 0.01605198
[150/200] Training loss: 0.01436874
[200/200] Training loss: 0.01200553
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12452.921263703549 ----------
[1/200] Training loss: 0.18103881
[2/200] Training loss: 0.06469953
[3/200] Training loss: 0.05215291
[4/200] Training loss: 0.05053832
[5/200] Training loss: 0.05021445
[6/200] Training loss: 0.04512408
[7/200] Training loss: 0.04708882
[8/200] Training loss: 0.04651463
[9/200] Training loss: 0.04181132
[10/200] Training loss: 0.04051216
[50/200] Training loss: 0.01970529
[100/200] Training loss: 0.01709436
[150/200] Training loss: 0.01560470
[200/200] Training loss: 0.01452428
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16234.150670731131 ----------
[1/200] Training loss: 0.13909479
[2/200] Training loss: 0.05840237
[3/200] Training loss: 0.05337307
[4/200] Training loss: 0.05078581
[5/200] Training loss: 0.04816888
[6/200] Training loss: 0.04766333
[7/200] Training loss: 0.04546364
[8/200] Training loss: 0.04372023
[9/200] Training loss: 0.04128078
[10/200] Training loss: 0.04022921
[50/200] Training loss: 0.01741417
[100/200] Training loss: 0.01561393
[150/200] Training loss: 0.01329900
[200/200] Training loss: 0.01216618
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 14723.88644346322 ----------
[1/200] Training loss: 0.13871022
[2/200] Training loss: 0.05357942
[3/200] Training loss: 0.04858398
[4/200] Training loss: 0.04267600
[5/200] Training loss: 0.03823015
[6/200] Training loss: 0.03844834
[7/200] Training loss: 0.03579273
[8/200] Training loss: 0.02834759
[9/200] Training loss: 0.02910606
[10/200] Training loss: 0.02869022
[50/200] Training loss: 0.01734516
[100/200] Training loss: 0.01435349
[150/200] Training loss: 0.01301307
[200/200] Training loss: 0.01203671
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19280.799153562075 ----------
[1/200] Training loss: 0.14076690
[2/200] Training loss: 0.05688935
[3/200] Training loss: 0.05283837
[4/200] Training loss: 0.04653859
[5/200] Training loss: 0.04491235
[6/200] Training loss: 0.04358828
[7/200] Training loss: 0.04112721
[8/200] Training loss: 0.03961409
[9/200] Training loss: 0.03850659
[10/200] Training loss: 0.03618463
[50/200] Training loss: 0.01748477
[100/200] Training loss: 0.01488150
[150/200] Training loss: 0.01326382
[200/200] Training loss: 0.01311965
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 10376.235926384867 ----------
[1/200] Training loss: 0.13948987
[2/200] Training loss: 0.05834847
[3/200] Training loss: 0.05452638
[4/200] Training loss: 0.05009076
[5/200] Training loss: 0.04923783
[6/200] Training loss: 0.04815834
[7/200] Training loss: 0.04474621
[8/200] Training loss: 0.04388176
[9/200] Training loss: 0.04190808
[10/200] Training loss: 0.03965949
[50/200] Training loss: 0.01847251
[100/200] Training loss: 0.01604208
[150/200] Training loss: 0.01417227
[200/200] Training loss: 0.01296002
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 23000.2525203529 ----------
[1/200] Training loss: 0.15634845
[2/200] Training loss: 0.05721092
[3/200] Training loss: 0.04906081
[4/200] Training loss: 0.04381730
[5/200] Training loss: 0.04147894
[6/200] Training loss: 0.03947673
[7/200] Training loss: 0.03739374
[8/200] Training loss: 0.03506468
[9/200] Training loss: 0.03461703
[10/200] Training loss: 0.03292269
[50/200] Training loss: 0.01676243
[100/200] Training loss: 0.01377876
[150/200] Training loss: 0.01253172
[200/200] Training loss: 0.01210728
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11534.439214803639 ----------
[1/200] Training loss: 0.15378071
[2/200] Training loss: 0.05685817
[3/200] Training loss: 0.05447600
[4/200] Training loss: 0.04919541
[5/200] Training loss: 0.04664577
[6/200] Training loss: 0.04020459
[7/200] Training loss: 0.04002332
[8/200] Training loss: 0.03687961
[9/200] Training loss: 0.03574608
[10/200] Training loss: 0.03136484
[50/200] Training loss: 0.01867076
[100/200] Training loss: 0.01683485
[150/200] Training loss: 0.01498718
[200/200] Training loss: 0.01326679
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8506.331289104604 ----------
[1/200] Training loss: 0.14560982
[2/200] Training loss: 0.05476893
[3/200] Training loss: 0.04812002
[4/200] Training loss: 0.04367551
[5/200] Training loss: 0.04018849
[6/200] Training loss: 0.03841381
[7/200] Training loss: 0.03646585
[8/200] Training loss: 0.03303043
[9/200] Training loss: 0.03059056
[10/200] Training loss: 0.02767514
[50/200] Training loss: 0.01897444
[100/200] Training loss: 0.01649518
[150/200] Training loss: 0.01424988
[200/200] Training loss: 0.01267357
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17728.513530468368 ----------
[1/200] Training loss: 0.14254425
[2/200] Training loss: 0.05380013
[3/200] Training loss: 0.04734773
[4/200] Training loss: 0.04578707
[5/200] Training loss: 0.04450059
[6/200] Training loss: 0.03971649
[7/200] Training loss: 0.03899316
[8/200] Training loss: 0.03629421
[9/200] Training loss: 0.03794770
[10/200] Training loss: 0.03535960
[50/200] Training loss: 0.01879102
[100/200] Training loss: 0.01614844
[150/200] Training loss: 0.01443807
[200/200] Training loss: 0.01326508
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 6068.276196746487 ----------
[1/200] Training loss: 0.16525000
[2/200] Training loss: 0.06349870
[3/200] Training loss: 0.05532481
[4/200] Training loss: 0.05281067
[5/200] Training loss: 0.04791025
[6/200] Training loss: 0.04771366
[7/200] Training loss: 0.04433524
[8/200] Training loss: 0.03640425
[9/200] Training loss: 0.03328140
[10/200] Training loss: 0.02979739
[50/200] Training loss: 0.01563456
[100/200] Training loss: 0.01406380
[150/200] Training loss: 0.01177594
[200/200] Training loss: 0.01056537
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18130.082845922134 ----------
[1/200] Training loss: 0.15327050
[2/200] Training loss: 0.05607617
[3/200] Training loss: 0.05446469
[4/200] Training loss: 0.04628486
[5/200] Training loss: 0.04487876
[6/200] Training loss: 0.03923955
[7/200] Training loss: 0.04086805
[8/200] Training loss: 0.03581058
[9/200] Training loss: 0.03478786
[10/200] Training loss: 0.03373227
[50/200] Training loss: 0.01775681
[100/200] Training loss: 0.01524670
[150/200] Training loss: 0.01324116
[200/200] Training loss: 0.01206408
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17606.992247399896 ----------
[1/200] Training loss: 0.13134759
[2/200] Training loss: 0.05672638
[3/200] Training loss: 0.05100445
[4/200] Training loss: 0.04869599
[5/200] Training loss: 0.04634739
[6/200] Training loss: 0.04474415
[7/200] Training loss: 0.03962237
[8/200] Training loss: 0.04175563
[9/200] Training loss: 0.03746455
[10/200] Training loss: 0.03747005
[50/200] Training loss: 0.01902281
[100/200] Training loss: 0.01581774
[150/200] Training loss: 0.01394581
[200/200] Training loss: 0.01292516
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 16859.19476131645 ----------
[1/200] Training loss: 0.14578839
[2/200] Training loss: 0.05616993
[3/200] Training loss: 0.05152090
[4/200] Training loss: 0.04962017
[5/200] Training loss: 0.04756648
[6/200] Training loss: 0.04525236
[7/200] Training loss: 0.04246930
[8/200] Training loss: 0.04119841
[9/200] Training loss: 0.04073111
[10/200] Training loss: 0.03856665
[50/200] Training loss: 0.02305971
[100/200] Training loss: 0.01759687
[150/200] Training loss: 0.01503640
[200/200] Training loss: 0.01327794
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 6293.780739746182 ----------
[1/200] Training loss: 0.14495037
[2/200] Training loss: 0.05820007
[3/200] Training loss: 0.05493634
[4/200] Training loss: 0.05036027
[5/200] Training loss: 0.04729066
[6/200] Training loss: 0.04709800
[7/200] Training loss: 0.04577747
[8/200] Training loss: 0.04304388
[9/200] Training loss: 0.04083092
[10/200] Training loss: 0.03739598
[50/200] Training loss: 0.01837418
[100/200] Training loss: 0.01593233
[150/200] Training loss: 0.01335156
[200/200] Training loss: 0.01169946
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 15478.5978693162 ----------
[1/200] Training loss: 0.15902317
[2/200] Training loss: 0.05727469
[3/200] Training loss: 0.04919882
[4/200] Training loss: 0.04460264
[5/200] Training loss: 0.04421377
[6/200] Training loss: 0.04238899
[7/200] Training loss: 0.04183686
[8/200] Training loss: 0.03778252
[9/200] Training loss: 0.03416460
[10/200] Training loss: 0.03395631
[50/200] Training loss: 0.01630400
[100/200] Training loss: 0.01422130
[150/200] Training loss: 0.01279949
[200/200] Training loss: 0.01131428
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12180.874845428796 ----------
[1/200] Training loss: 0.13943146
[2/200] Training loss: 0.05957536
[3/200] Training loss: 0.05091436
[4/200] Training loss: 0.04654791
[5/200] Training loss: 0.04316102
[6/200] Training loss: 0.04108994
[7/200] Training loss: 0.03717982
[8/200] Training loss: 0.03486584
[9/200] Training loss: 0.03379314
[10/200] Training loss: 0.03109418
[50/200] Training loss: 0.01690037
[100/200] Training loss: 0.01382029
[150/200] Training loss: 0.01226229
[200/200] Training loss: 0.01108192
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6998.60586116978 ----------
[1/200] Training loss: 0.16228978
[2/200] Training loss: 0.05549489
[3/200] Training loss: 0.05076093
[4/200] Training loss: 0.04792745
[5/200] Training loss: 0.04325951
[6/200] Training loss: 0.04183189
[7/200] Training loss: 0.04005784
[8/200] Training loss: 0.03609013
[9/200] Training loss: 0.03679800
[10/200] Training loss: 0.03347578
[50/200] Training loss: 0.01954377
[100/200] Training loss: 0.01634574
[150/200] Training loss: 0.01421450
[200/200] Training loss: 0.01335763
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4857.151222681871 ----------
[1/200] Training loss: 0.15167615
[2/200] Training loss: 0.05950081
[3/200] Training loss: 0.05278855
[4/200] Training loss: 0.04790950
[5/200] Training loss: 0.04380453
[6/200] Training loss: 0.04348634
[7/200] Training loss: 0.03974324
[8/200] Training loss: 0.03594685
[9/200] Training loss: 0.03505058
[10/200] Training loss: 0.03201615
[50/200] Training loss: 0.01885234
[100/200] Training loss: 0.01415066
[150/200] Training loss: 0.01340397
[200/200] Training loss: 0.01169636
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15472.237330134256 ----------
[1/200] Training loss: 0.17066427
[2/200] Training loss: 0.05594459
[3/200] Training loss: 0.05290720
[4/200] Training loss: 0.04683393
[5/200] Training loss: 0.04060271
[6/200] Training loss: 0.04013312
[7/200] Training loss: 0.03953785
[8/200] Training loss: 0.03601620
[9/200] Training loss: 0.03426583
[10/200] Training loss: 0.03439452
[50/200] Training loss: 0.01973950
[100/200] Training loss: 0.01633470
[150/200] Training loss: 0.01515959
[200/200] Training loss: 0.01412323
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12962.136243690698 ----------
[1/200] Training loss: 0.14936456
[2/200] Training loss: 0.05290415
[3/200] Training loss: 0.04928334
[4/200] Training loss: 0.04719279
[5/200] Training loss: 0.04151850
[6/200] Training loss: 0.03620729
[7/200] Training loss: 0.03473045
[8/200] Training loss: 0.03140553
[9/200] Training loss: 0.02909254
[10/200] Training loss: 0.03001235
[50/200] Training loss: 0.01687155
[100/200] Training loss: 0.01407456
[150/200] Training loss: 0.01278558
[200/200] Training loss: 0.01209913
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11804.860016112008 ----------
[1/200] Training loss: 0.14478896
[2/200] Training loss: 0.05830205
[3/200] Training loss: 0.05129377
[4/200] Training loss: 0.05009631
[5/200] Training loss: 0.04665669
[6/200] Training loss: 0.04420968
[7/200] Training loss: 0.04092057
[8/200] Training loss: 0.03774322
[9/200] Training loss: 0.03428835
[10/200] Training loss: 0.03450749
[50/200] Training loss: 0.02031239
[100/200] Training loss: 0.01560170
[150/200] Training loss: 0.01461175
[200/200] Training loss: 0.01352136
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13236.238740669496 ----------
[1/200] Training loss: 0.16174272
[2/200] Training loss: 0.05725330
[3/200] Training loss: 0.05142988
[4/200] Training loss: 0.04715746
[5/200] Training loss: 0.04221948
[6/200] Training loss: 0.04343688
[7/200] Training loss: 0.04000589
[8/200] Training loss: 0.03875357
[9/200] Training loss: 0.03625830
[10/200] Training loss: 0.03506242
[50/200] Training loss: 0.01722680
[100/200] Training loss: 0.01504045
[150/200] Training loss: 0.01342458
[200/200] Training loss: 0.01212806
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10118.95686323447 ----------
[1/200] Training loss: 0.14777469
[2/200] Training loss: 0.05741133
[3/200] Training loss: 0.05023410
[4/200] Training loss: 0.04230765
[5/200] Training loss: 0.04181760
[6/200] Training loss: 0.03954135
[7/200] Training loss: 0.03710793
[8/200] Training loss: 0.03733719
[9/200] Training loss: 0.03646860
[10/200] Training loss: 0.03269843
[50/200] Training loss: 0.01916478
[100/200] Training loss: 0.01520350
[150/200] Training loss: 0.01416721
[200/200] Training loss: 0.01269367
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5558.63292545928 ----------
[1/200] Training loss: 0.16030637
[2/200] Training loss: 0.05748708
[3/200] Training loss: 0.04822913
[4/200] Training loss: 0.04718796
[5/200] Training loss: 0.03973319
[6/200] Training loss: 0.03813556
[7/200] Training loss: 0.03331064
[8/200] Training loss: 0.02970519
[9/200] Training loss: 0.02894403
[10/200] Training loss: 0.02736740
[50/200] Training loss: 0.01829531
[100/200] Training loss: 0.01578678
[150/200] Training loss: 0.01401038
[200/200] Training loss: 0.01249305
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17898.214436082722 ----------
[1/200] Training loss: 0.13900534
[2/200] Training loss: 0.05270057
[3/200] Training loss: 0.04906818
[4/200] Training loss: 0.04541840
[5/200] Training loss: 0.04542282
[6/200] Training loss: 0.04329253
[7/200] Training loss: 0.03928526
[8/200] Training loss: 0.03941110
[9/200] Training loss: 0.03706648
[10/200] Training loss: 0.03567890
[50/200] Training loss: 0.01870498
[100/200] Training loss: 0.01648152
[150/200] Training loss: 0.01466407
[200/200] Training loss: 0.01371935
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 7715.634776218999 ----------
[1/200] Training loss: 0.14148391
[2/200] Training loss: 0.05539081
[3/200] Training loss: 0.04992136
[4/200] Training loss: 0.04522546
[5/200] Training loss: 0.04222998
[6/200] Training loss: 0.03899325
[7/200] Training loss: 0.03550211
[8/200] Training loss: 0.03521902
[9/200] Training loss: 0.03415003
[10/200] Training loss: 0.03112059
[50/200] Training loss: 0.01819723
[100/200] Training loss: 0.01498008
[150/200] Training loss: 0.01419113
[200/200] Training loss: 0.01218175
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15278.038617571301 ----------
[1/200] Training loss: 0.16371992
[2/200] Training loss: 0.06042550
[3/200] Training loss: 0.04212865
[4/200] Training loss: 0.05014780
[5/200] Training loss: 0.03837579
[6/200] Training loss: 0.02862495
[7/200] Training loss: 0.02667167
[8/200] Training loss: 0.03706977
[9/200] Training loss: 0.02649188
[10/200] Training loss: 0.02420088
[50/200] Training loss: 0.01683572
[100/200] Training loss: 0.01230782
[150/200] Training loss: 0.01151558
[200/200] Training loss: 0.01133612
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adam ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 29270.27051463652 ----------
[1/200] Training loss: 0.14927170
[2/200] Training loss: 0.05849944
[3/200] Training loss: 0.04840286
[4/200] Training loss: 0.04704642
[5/200] Training loss: 0.04146461
[6/200] Training loss: 0.04068589
[7/200] Training loss: 0.03602332
[8/200] Training loss: 0.03502155
[9/200] Training loss: 0.03207684
[10/200] Training loss: 0.03100714
[50/200] Training loss: 0.01627989
[100/200] Training loss: 0.01388507
[150/200] Training loss: 0.01257264
[200/200] Training loss: 0.01233539
---batch_size---: 16 ---n_steps---: 8 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13871.670122952031 ----------
[1/200] Training loss: 0.16188677
[2/200] Training loss: 0.06219218
[3/200] Training loss: 0.05366881
[4/200] Training loss: 0.04960320
[5/200] Training loss: 0.04594233
[6/200] Training loss: 0.04416575
[7/200] Training loss: 0.04126768
[8/200] Training loss: 0.04110796
[9/200] Training loss: 0.03619285
[10/200] Training loss: 0.03438817
[50/200] Training loss: 0.01954096
[100/200] Training loss: 0.01635098
[150/200] Training loss: 0.01483709
[200/200] Training loss: 0.01329449
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 13912.228290248835 ----------
[1/200] Training loss: 0.14464180
[2/200] Training loss: 0.05797195
[3/200] Training loss: 0.04824489
[4/200] Training loss: 0.04979935
[5/200] Training loss: 0.04372236
[6/200] Training loss: 0.04196466
[7/200] Training loss: 0.03827100
[8/200] Training loss: 0.03628680
[9/200] Training loss: 0.03397651
[10/200] Training loss: 0.03463246
[50/200] Training loss: 0.01766606
[100/200] Training loss: 0.01537013
[150/200] Training loss: 0.01439502
[200/200] Training loss: 0.01250627
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14327.140119367856 ----------
[1/200] Training loss: 0.16534930
[2/200] Training loss: 0.05496312
[3/200] Training loss: 0.05189928
[4/200] Training loss: 0.04803432
[5/200] Training loss: 0.04505409
[6/200] Training loss: 0.04262394
[7/200] Training loss: 0.04136656
[8/200] Training loss: 0.03825789
[9/200] Training loss: 0.03681451
[10/200] Training loss: 0.03518851
[50/200] Training loss: 0.01875234
[100/200] Training loss: 0.01625161
[150/200] Training loss: 0.01456061
[200/200] Training loss: 0.01301644
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18362.6080936233 ----------
[1/200] Training loss: 0.16232416
[2/200] Training loss: 0.05761343
[3/200] Training loss: 0.05133105
[4/200] Training loss: 0.04521038
[5/200] Training loss: 0.04391131
[6/200] Training loss: 0.04290487
[7/200] Training loss: 0.03855769
[8/200] Training loss: 0.03586205
[9/200] Training loss: 0.03600128
[10/200] Training loss: 0.03389221
[50/200] Training loss: 0.01824674
[100/200] Training loss: 0.01482737
[150/200] Training loss: 0.01394924
[200/200] Training loss: 0.01297377
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14589.07262302851 ----------
[1/200] Training loss: 0.17113068
[2/200] Training loss: 0.06386623
[3/200] Training loss: 0.05501063
[4/200] Training loss: 0.05182746
[5/200] Training loss: 0.04823920
[6/200] Training loss: 0.04587984
[7/200] Training loss: 0.04425350
[8/200] Training loss: 0.04263074
[9/200] Training loss: 0.04057395
[10/200] Training loss: 0.03640673
[50/200] Training loss: 0.01997269
[100/200] Training loss: 0.01757999
[150/200] Training loss: 0.01518980
[200/200] Training loss: 0.01338073
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18269.60930069387 ----------
[1/200] Training loss: 0.18142288
[2/200] Training loss: 0.06313020
[3/200] Training loss: 0.05397296
[4/200] Training loss: 0.04844490
[5/200] Training loss: 0.04635323
[6/200] Training loss: 0.04356205
[7/200] Training loss: 0.04140407
[8/200] Training loss: 0.03732796
[9/200] Training loss: 0.03663865
[10/200] Training loss: 0.03326712
[50/200] Training loss: 0.01790810
[100/200] Training loss: 0.01437102
[150/200] Training loss: 0.01356196
[200/200] Training loss: 0.01180996
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12754.824028578363 ----------
[1/200] Training loss: 0.16880257
[2/200] Training loss: 0.06164623
[3/200] Training loss: 0.05384267
[4/200] Training loss: 0.05099534
[5/200] Training loss: 0.04723785
[6/200] Training loss: 0.04324384
[7/200] Training loss: 0.04124820
[8/200] Training loss: 0.03845604
[9/200] Training loss: 0.03662369
[10/200] Training loss: 0.03586308
[50/200] Training loss: 0.01670449
[100/200] Training loss: 0.01476816
[150/200] Training loss: 0.01380756
[200/200] Training loss: 0.01345104
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15981.159407252027 ----------
[1/200] Training loss: 0.16532787
[2/200] Training loss: 0.06242652
[3/200] Training loss: 0.05187770
[4/200] Training loss: 0.05081307
[5/200] Training loss: 0.04673591
[6/200] Training loss: 0.04227117
[7/200] Training loss: 0.04144651
[8/200] Training loss: 0.04027282
[9/200] Training loss: 0.03688361
[10/200] Training loss: 0.03544141
[50/200] Training loss: 0.01725964
[100/200] Training loss: 0.01429004
[150/200] Training loss: 0.01260345
[200/200] Training loss: 0.01180519
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9054.643007871708 ----------
[1/200] Training loss: 0.14546075
[2/200] Training loss: 0.05284349
[3/200] Training loss: 0.04574265
[4/200] Training loss: 0.04236746
[5/200] Training loss: 0.04082644
[6/200] Training loss: 0.03967537
[7/200] Training loss: 0.03506315
[8/200] Training loss: 0.03226146
[9/200] Training loss: 0.03315083
[10/200] Training loss: 0.03301439
[50/200] Training loss: 0.01871775
[100/200] Training loss: 0.01581798
[150/200] Training loss: 0.01319950
[200/200] Training loss: 0.01256537
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8646.996704058583 ----------
[1/200] Training loss: 0.14888492
[2/200] Training loss: 0.05641784
[3/200] Training loss: 0.05071888
[4/200] Training loss: 0.04793575
[5/200] Training loss: 0.04485825
[6/200] Training loss: 0.04301759
[7/200] Training loss: 0.04247024
[8/200] Training loss: 0.04074116
[9/200] Training loss: 0.03695586
[10/200] Training loss: 0.03485756
[50/200] Training loss: 0.01876406
[100/200] Training loss: 0.01555933
[150/200] Training loss: 0.01270224
[200/200] Training loss: 0.01078386
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9917.379492587746 ----------
[1/200] Training loss: 0.16542918
[2/200] Training loss: 0.05755485
[3/200] Training loss: 0.05396977
[4/200] Training loss: 0.04751875
[5/200] Training loss: 0.04679583
[6/200] Training loss: 0.04217303
[7/200] Training loss: 0.04115615
[8/200] Training loss: 0.03824760
[9/200] Training loss: 0.03477752
[10/200] Training loss: 0.03398782
[50/200] Training loss: 0.01716365
[100/200] Training loss: 0.01343130
[150/200] Training loss: 0.01180071
[200/200] Training loss: 0.01064453
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9891.947836498128 ----------
[1/200] Training loss: 0.15286142
[2/200] Training loss: 0.06391671
[3/200] Training loss: 0.05479468
[4/200] Training loss: 0.05231650
[5/200] Training loss: 0.05186309
[6/200] Training loss: 0.04857554
[7/200] Training loss: 0.04519179
[8/200] Training loss: 0.04254968
[9/200] Training loss: 0.03963308
[10/200] Training loss: 0.03934147
[50/200] Training loss: 0.01717342
[100/200] Training loss: 0.01482494
[150/200] Training loss: 0.01339678
[200/200] Training loss: 0.01284208
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10151.981875476335 ----------
[1/200] Training loss: 0.17563349
[2/200] Training loss: 0.05703525
[3/200] Training loss: 0.05404420
[4/200] Training loss: 0.04884169
[5/200] Training loss: 0.04621943
[6/200] Training loss: 0.04372157
[7/200] Training loss: 0.04401027
[8/200] Training loss: 0.04004498
[9/200] Training loss: 0.03902632
[10/200] Training loss: 0.03637243
[50/200] Training loss: 0.01776757
[100/200] Training loss: 0.01570408
[150/200] Training loss: 0.01410483
[200/200] Training loss: 0.01266489
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5736.218092088201 ----------
[1/200] Training loss: 0.17420808
[2/200] Training loss: 0.06374971
[3/200] Training loss: 0.05684267
[4/200] Training loss: 0.05282669
[5/200] Training loss: 0.05104975
[6/200] Training loss: 0.04951272
[7/200] Training loss: 0.04596541
[8/200] Training loss: 0.04637677
[9/200] Training loss: 0.04387978
[10/200] Training loss: 0.04432200
[50/200] Training loss: 0.01959194
[100/200] Training loss: 0.01636262
[150/200] Training loss: 0.01435692
[200/200] Training loss: 0.01308827
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19341.730222500777 ----------
[1/200] Training loss: 0.16627919
[2/200] Training loss: 0.05899908
[3/200] Training loss: 0.05340387
[4/200] Training loss: 0.04785666
[5/200] Training loss: 0.04089149
[6/200] Training loss: 0.04248726
[7/200] Training loss: 0.03826307
[8/200] Training loss: 0.03926175
[9/200] Training loss: 0.03239168
[10/200] Training loss: 0.03351364
[50/200] Training loss: 0.01814912
[100/200] Training loss: 0.01491358
[150/200] Training loss: 0.01326031
[200/200] Training loss: 0.01244098
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 29455.70667969112 ----------
[1/200] Training loss: 0.15113215
[2/200] Training loss: 0.05551801
[3/200] Training loss: 0.04481018
[4/200] Training loss: 0.03689998
[5/200] Training loss: 0.03643022
[6/200] Training loss: 0.03525114
[7/200] Training loss: 0.03363368
[8/200] Training loss: 0.02937876
[9/200] Training loss: 0.02922840
[10/200] Training loss: 0.02998027
[50/200] Training loss: 0.01674829
[100/200] Training loss: 0.01256859
[150/200] Training loss: 0.01123891
[200/200] Training loss: 0.01028586
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23285.49900689268 ----------
[1/200] Training loss: 0.17239840
[2/200] Training loss: 0.06124306
[3/200] Training loss: 0.05567504
[4/200] Training loss: 0.05058859
[5/200] Training loss: 0.04995558
[6/200] Training loss: 0.04541419
[7/200] Training loss: 0.03979439
[8/200] Training loss: 0.03838221
[9/200] Training loss: 0.03981612
[10/200] Training loss: 0.03466291
[50/200] Training loss: 0.02010259
[100/200] Training loss: 0.01622355
[150/200] Training loss: 0.01544680
[200/200] Training loss: 0.01388233
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10262.425054537549 ----------
[1/200] Training loss: 0.16024753
[2/200] Training loss: 0.05534822
[3/200] Training loss: 0.05075102
[4/200] Training loss: 0.04558796
[5/200] Training loss: 0.04439194
[6/200] Training loss: 0.03676253
[7/200] Training loss: 0.03881140
[8/200] Training loss: 0.03316107
[9/200] Training loss: 0.03015625
[10/200] Training loss: 0.02822878
[50/200] Training loss: 0.01817671
[100/200] Training loss: 0.01446665
[150/200] Training loss: 0.01365267
[200/200] Training loss: 0.01275166
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13288.867521350343 ----------
[1/200] Training loss: 0.15917558
[2/200] Training loss: 0.05863665
[3/200] Training loss: 0.05029379
[4/200] Training loss: 0.05287774
[5/200] Training loss: 0.04748531
[6/200] Training loss: 0.04302470
[7/200] Training loss: 0.04558472
[8/200] Training loss: 0.03913403
[9/200] Training loss: 0.03904636
[10/200] Training loss: 0.03774821
[50/200] Training loss: 0.01825205
[100/200] Training loss: 0.01489798
[150/200] Training loss: 0.01401147
[200/200] Training loss: 0.01289354
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7880.423592675714 ----------
[1/200] Training loss: 0.17766278
[2/200] Training loss: 0.05884055
[3/200] Training loss: 0.05492280
[4/200] Training loss: 0.05518030
[5/200] Training loss: 0.04271644
[6/200] Training loss: 0.03822829
[7/200] Training loss: 0.03817766
[8/200] Training loss: 0.03505219
[9/200] Training loss: 0.03135121
[10/200] Training loss: 0.03093735
[50/200] Training loss: 0.01686886
[100/200] Training loss: 0.01526435
[150/200] Training loss: 0.01353418
[200/200] Training loss: 0.01310654
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19981.195159449297 ----------
[1/200] Training loss: 0.13410011
[2/200] Training loss: 0.05532676
[3/200] Training loss: 0.04852404
[4/200] Training loss: 0.04487831
[5/200] Training loss: 0.04180407
[6/200] Training loss: 0.03862510
[7/200] Training loss: 0.03589691
[8/200] Training loss: 0.03526881
[9/200] Training loss: 0.03259602
[10/200] Training loss: 0.03068024
[50/200] Training loss: 0.01562910
[100/200] Training loss: 0.01268649
[150/200] Training loss: 0.01093849
[200/200] Training loss: 0.01037774
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15597.792664348375 ----------
[1/200] Training loss: 0.14000896
[2/200] Training loss: 0.05769968
[3/200] Training loss: 0.04784465
[4/200] Training loss: 0.04760748
[5/200] Training loss: 0.04175005
[6/200] Training loss: 0.04117299
[7/200] Training loss: 0.04046107
[8/200] Training loss: 0.03997748
[9/200] Training loss: 0.03716158
[10/200] Training loss: 0.03834503
[50/200] Training loss: 0.01826795
[100/200] Training loss: 0.01417312
[150/200] Training loss: 0.01316603
[200/200] Training loss: 0.01250646
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 7245.98178302982 ----------
[1/200] Training loss: 0.14869651
[2/200] Training loss: 0.05915736
[3/200] Training loss: 0.05278821
[4/200] Training loss: 0.04986523
[5/200] Training loss: 0.04658663
[6/200] Training loss: 0.04478538
[7/200] Training loss: 0.04294841
[8/200] Training loss: 0.03656225
[9/200] Training loss: 0.03572658
[10/200] Training loss: 0.03451793
[50/200] Training loss: 0.01656073
[100/200] Training loss: 0.01365902
[150/200] Training loss: 0.01270025
[200/200] Training loss: 0.01226874
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12309.006133721763 ----------
[1/200] Training loss: 0.13945814
[2/200] Training loss: 0.05753038
[3/200] Training loss: 0.04970363
[4/200] Training loss: 0.05104465
[5/200] Training loss: 0.04721594
[6/200] Training loss: 0.04732534
[7/200] Training loss: 0.04276916
[8/200] Training loss: 0.04399181
[9/200] Training loss: 0.03973314
[10/200] Training loss: 0.03889647
[50/200] Training loss: 0.01791179
[100/200] Training loss: 0.01444338
[150/200] Training loss: 0.01372640
[200/200] Training loss: 0.01240362
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 12424.135221414808 ----------
[1/200] Training loss: 0.17820254
[2/200] Training loss: 0.05851022
[3/200] Training loss: 0.04992144
[4/200] Training loss: 0.05071220
[5/200] Training loss: 0.04092867
[6/200] Training loss: 0.04420860
[7/200] Training loss: 0.04007309
[8/200] Training loss: 0.03808140
[9/200] Training loss: 0.03442514
[10/200] Training loss: 0.03388498
[50/200] Training loss: 0.01792749
[100/200] Training loss: 0.01543281
[150/200] Training loss: 0.01387609
[200/200] Training loss: 0.01192822
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14353.239634312527 ----------
[1/200] Training loss: 0.16258686
[2/200] Training loss: 0.05892664
[3/200] Training loss: 0.05486498
[4/200] Training loss: 0.04976853
[5/200] Training loss: 0.04599766
[6/200] Training loss: 0.04526034
[7/200] Training loss: 0.04380594
[8/200] Training loss: 0.03766162
[9/200] Training loss: 0.03572101
[10/200] Training loss: 0.03192718
[50/200] Training loss: 0.01643088
[100/200] Training loss: 0.01317430
[150/200] Training loss: 0.01173631
[200/200] Training loss: 0.01097429
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11219.358626944768 ----------
[1/200] Training loss: 0.14628564
[2/200] Training loss: 0.05877118
[3/200] Training loss: 0.05111414
[4/200] Training loss: 0.04987027
[5/200] Training loss: 0.04750914
[6/200] Training loss: 0.04543525
[7/200] Training loss: 0.04503248
[8/200] Training loss: 0.03994399
[9/200] Training loss: 0.03978430
[10/200] Training loss: 0.03831826
[50/200] Training loss: 0.01723196
[100/200] Training loss: 0.01443418
[150/200] Training loss: 0.01301537
[200/200] Training loss: 0.01214583
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 12217.759532745764 ----------
[1/200] Training loss: 0.16102466
[2/200] Training loss: 0.05776031
[3/200] Training loss: 0.05295253
[4/200] Training loss: 0.04712890
[5/200] Training loss: 0.04509877
[6/200] Training loss: 0.04129470
[7/200] Training loss: 0.03692614
[8/200] Training loss: 0.03726525
[9/200] Training loss: 0.03488290
[10/200] Training loss: 0.03335333
[50/200] Training loss: 0.01755123
[100/200] Training loss: 0.01513733
[150/200] Training loss: 0.01335042
[200/200] Training loss: 0.01203804
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11351.921070902494 ----------
[1/200] Training loss: 0.16453465
[2/200] Training loss: 0.05740486
[3/200] Training loss: 0.05109983
[4/200] Training loss: 0.04633076
[5/200] Training loss: 0.04001966
[6/200] Training loss: 0.03927015
[7/200] Training loss: 0.03890130
[8/200] Training loss: 0.03342204
[9/200] Training loss: 0.03339623
[10/200] Training loss: 0.03108694
[50/200] Training loss: 0.01899993
[100/200] Training loss: 0.01661657
[150/200] Training loss: 0.01470544
[200/200] Training loss: 0.01370403
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16331.628210316325 ----------
[1/200] Training loss: 0.17027916
[2/200] Training loss: 0.06355330
[3/200] Training loss: 0.05575144
[4/200] Training loss: 0.05616043
[5/200] Training loss: 0.05040040
[6/200] Training loss: 0.04869471
[7/200] Training loss: 0.04850159
[8/200] Training loss: 0.04406800
[9/200] Training loss: 0.04041264
[10/200] Training loss: 0.03575109
[50/200] Training loss: 0.01755849
[100/200] Training loss: 0.01360777
[150/200] Training loss: 0.01267142
[200/200] Training loss: 0.01160001
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14694.207838464787 ----------
[1/200] Training loss: 0.14254673
[2/200] Training loss: 0.05663059
[3/200] Training loss: 0.05161455
[4/200] Training loss: 0.04519143
[5/200] Training loss: 0.04468395
[6/200] Training loss: 0.03823591
[7/200] Training loss: 0.03973888
[8/200] Training loss: 0.03636897
[9/200] Training loss: 0.03233390
[10/200] Training loss: 0.03261609
[50/200] Training loss: 0.01859479
[100/200] Training loss: 0.01598565
[150/200] Training loss: 0.01316914
[200/200] Training loss: 0.01244753
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14050.129679116844 ----------
[1/200] Training loss: 0.16313722
[2/200] Training loss: 0.05419072
[3/200] Training loss: 0.05284816
[4/200] Training loss: 0.04725845
[5/200] Training loss: 0.04655756
[6/200] Training loss: 0.04178021
[7/200] Training loss: 0.04042333
[8/200] Training loss: 0.03745686
[9/200] Training loss: 0.03660786
[10/200] Training loss: 0.03393776
[50/200] Training loss: 0.01805634
[100/200] Training loss: 0.01538893
[150/200] Training loss: 0.01358619
[200/200] Training loss: 0.01321598
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12154.913409810866 ----------
[1/200] Training loss: 0.14734732
[2/200] Training loss: 0.06084702
[3/200] Training loss: 0.05250816
[4/200] Training loss: 0.04669765
[5/200] Training loss: 0.04558141
[6/200] Training loss: 0.04266353
[7/200] Training loss: 0.04008622
[8/200] Training loss: 0.03664709
[9/200] Training loss: 0.03411769
[10/200] Training loss: 0.03092727
[50/200] Training loss: 0.01583992
[100/200] Training loss: 0.01442978
[150/200] Training loss: 0.01259241
[200/200] Training loss: 0.01185943
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 35816.54589711297 ----------
[1/200] Training loss: 0.16395183
[2/200] Training loss: 0.05980900
[3/200] Training loss: 0.05564251
[4/200] Training loss: 0.05008658
[5/200] Training loss: 0.04558894
[6/200] Training loss: 0.04461686
[7/200] Training loss: 0.04198649
[8/200] Training loss: 0.03984592
[9/200] Training loss: 0.03691323
[10/200] Training loss: 0.03072763
[50/200] Training loss: 0.01911791
[100/200] Training loss: 0.01658461
[150/200] Training loss: 0.01518890
[200/200] Training loss: 0.01366669
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14559.554938252748 ----------
[1/200] Training loss: 0.14179200
[2/200] Training loss: 0.05754112
[3/200] Training loss: 0.04980738
[4/200] Training loss: 0.04965402
[5/200] Training loss: 0.04732467
[6/200] Training loss: 0.04318071
[7/200] Training loss: 0.04002929
[8/200] Training loss: 0.03682520
[9/200] Training loss: 0.03730758
[10/200] Training loss: 0.03157503
[50/200] Training loss: 0.01839729
[100/200] Training loss: 0.01525684
[150/200] Training loss: 0.01374760
[200/200] Training loss: 0.01243903
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7836.9680361731735 ----------
[1/200] Training loss: 0.18576665
[2/200] Training loss: 0.05975403
[3/200] Training loss: 0.05078961
[4/200] Training loss: 0.04646782
[5/200] Training loss: 0.04476949
[6/200] Training loss: 0.04079584
[7/200] Training loss: 0.03902511
[8/200] Training loss: 0.03922993
[9/200] Training loss: 0.03559233
[10/200] Training loss: 0.03423691
[50/200] Training loss: 0.01909757
[100/200] Training loss: 0.01662989
[150/200] Training loss: 0.01532528
[200/200] Training loss: 0.01337215
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17991.38638348918 ----------
[1/200] Training loss: 0.13959046
[2/200] Training loss: 0.05517274
[3/200] Training loss: 0.04853594
[4/200] Training loss: 0.04696687
[5/200] Training loss: 0.04396041
[6/200] Training loss: 0.04058518
[7/200] Training loss: 0.03626400
[8/200] Training loss: 0.03607671
[9/200] Training loss: 0.03416772
[10/200] Training loss: 0.03543867
[50/200] Training loss: 0.01645719
[100/200] Training loss: 0.01442922
[150/200] Training loss: 0.01291655
[200/200] Training loss: 0.01237773
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18028.37496836584 ----------
[1/200] Training loss: 0.18353632
[2/200] Training loss: 0.05848565
[3/200] Training loss: 0.05488161
[4/200] Training loss: 0.04827027
[5/200] Training loss: 0.04573233
[6/200] Training loss: 0.04197871
[7/200] Training loss: 0.04226156
[8/200] Training loss: 0.04030857
[9/200] Training loss: 0.03625803
[10/200] Training loss: 0.03418616
[50/200] Training loss: 0.01835288
[100/200] Training loss: 0.01484005
[150/200] Training loss: 0.01443157
[200/200] Training loss: 0.01336262
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6810.888047824601 ----------
[1/200] Training loss: 0.16081215
[2/200] Training loss: 0.05156665
[3/200] Training loss: 0.04815633
[4/200] Training loss: 0.04257039
[5/200] Training loss: 0.03815450
[6/200] Training loss: 0.03818628
[7/200] Training loss: 0.03594866
[8/200] Training loss: 0.03310933
[9/200] Training loss: 0.03036307
[10/200] Training loss: 0.02819359
[50/200] Training loss: 0.01855194
[100/200] Training loss: 0.01590893
[150/200] Training loss: 0.01446410
[200/200] Training loss: 0.01361493
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12663.240027733818 ----------
[1/200] Training loss: 0.18106958
[2/200] Training loss: 0.06942657
[3/200] Training loss: 0.05674216
[4/200] Training loss: 0.05193088
[5/200] Training loss: 0.05331022
[6/200] Training loss: 0.05026275
[7/200] Training loss: 0.04582629
[8/200] Training loss: 0.04682775
[9/200] Training loss: 0.04251286
[10/200] Training loss: 0.04157429
[50/200] Training loss: 0.01859597
[100/200] Training loss: 0.01518977
[150/200] Training loss: 0.01413544
[200/200] Training loss: 0.01249897
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 3748.4840936037062 ----------
[1/200] Training loss: 0.14741629
[2/200] Training loss: 0.05923101
[3/200] Training loss: 0.05350015
[4/200] Training loss: 0.04806634
[5/200] Training loss: 0.04688702
[6/200] Training loss: 0.04395949
[7/200] Training loss: 0.04287388
[8/200] Training loss: 0.04157396
[9/200] Training loss: 0.03889070
[10/200] Training loss: 0.03561074
[50/200] Training loss: 0.01847624
[100/200] Training loss: 0.01584868
[150/200] Training loss: 0.01349258
[200/200] Training loss: 0.01175836
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01529367424404272
----FITNESS-----------RMSE---- 17984.938587607132 ----------
[1/200] Training loss: 0.16836436
[2/200] Training loss: 0.05851711
[3/200] Training loss: 0.05256601
[4/200] Training loss: 0.04869712
[5/200] Training loss: 0.04726170
[6/200] Training loss: 0.04198762
[7/200] Training loss: 0.04183826
[8/200] Training loss: 0.03940094
[9/200] Training loss: 0.03717491
[10/200] Training loss: 0.03729961
[50/200] Training loss: 0.01823805
[100/200] Training loss: 0.01485180
[150/200] Training loss: 0.01293810
[200/200] Training loss: 0.01191015
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7773.235105154095 ----------
[1/200] Training loss: 0.15889407
[2/200] Training loss: 0.05607791
[3/200] Training loss: 0.05349819
[4/200] Training loss: 0.04634598
[5/200] Training loss: 0.04267092
[6/200] Training loss: 0.03815165
[7/200] Training loss: 0.03793867
[8/200] Training loss: 0.03451224
[9/200] Training loss: 0.03182197
[10/200] Training loss: 0.02947398
[50/200] Training loss: 0.01750992
[100/200] Training loss: 0.01543324
[150/200] Training loss: 0.01456105
[200/200] Training loss: 0.01337390
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12944.72247674704 ----------
[1/200] Training loss: 0.13901992
[2/200] Training loss: 0.05963301
[3/200] Training loss: 0.04925655
[4/200] Training loss: 0.04539836
[5/200] Training loss: 0.04100666
[6/200] Training loss: 0.04228961
[7/200] Training loss: 0.03674644
[8/200] Training loss: 0.03711135
[9/200] Training loss: 0.03517641
[10/200] Training loss: 0.03464446
[50/200] Training loss: 0.01775934
[100/200] Training loss: 0.01449890
[150/200] Training loss: 0.01285564
[200/200] Training loss: 0.01232479
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9405.610665980174 ----------
[1/200] Training loss: 0.18129147
[2/200] Training loss: 0.06804482
[3/200] Training loss: 0.05540072
[4/200] Training loss: 0.05317246
[5/200] Training loss: 0.04873796
[6/200] Training loss: 0.04633772
[7/200] Training loss: 0.04189336
[8/200] Training loss: 0.03942824
[9/200] Training loss: 0.03760594
[10/200] Training loss: 0.03722834
[50/200] Training loss: 0.01865764
[100/200] Training loss: 0.01635461
[150/200] Training loss: 0.01460179
[200/200] Training loss: 0.01405925
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11837.540960858383 ----------
[1/200] Training loss: 0.16563237
[2/200] Training loss: 0.06025140
[3/200] Training loss: 0.05593660
[4/200] Training loss: 0.05098832
[5/200] Training loss: 0.04743027
[6/200] Training loss: 0.04564655
[7/200] Training loss: 0.04591419
[8/200] Training loss: 0.04143098
[9/200] Training loss: 0.03828392
[10/200] Training loss: 0.03870877
[50/200] Training loss: 0.01913037
[100/200] Training loss: 0.01552007
[150/200] Training loss: 0.01383391
[200/200] Training loss: 0.01254577
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18647.604457409536 ----------
[1/200] Training loss: 0.17667672
[2/200] Training loss: 0.06813973
[3/200] Training loss: 0.05625096
[4/200] Training loss: 0.04886668
[5/200] Training loss: 0.04711069
[6/200] Training loss: 0.04472674
[7/200] Training loss: 0.04021431
[8/200] Training loss: 0.03592903
[9/200] Training loss: 0.03524281
[10/200] Training loss: 0.03425710
[50/200] Training loss: 0.01826200
[100/200] Training loss: 0.01607891
[150/200] Training loss: 0.01448918
[200/200] Training loss: 0.01371018
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18466.37159812398 ----------
[1/200] Training loss: 0.16095464
[2/200] Training loss: 0.05942264
[3/200] Training loss: 0.05199079
[4/200] Training loss: 0.04658347
[5/200] Training loss: 0.04425696
[6/200] Training loss: 0.03783772
[7/200] Training loss: 0.03368526
[8/200] Training loss: 0.03243781
[9/200] Training loss: 0.03104056
[10/200] Training loss: 0.02951589
[50/200] Training loss: 0.01763983
[100/200] Training loss: 0.01461762
[150/200] Training loss: 0.01324052
[200/200] Training loss: 0.01252831
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16415.32649690526 ----------
[1/200] Training loss: 0.18634035
[2/200] Training loss: 0.06019657
[3/200] Training loss: 0.05672336
[4/200] Training loss: 0.05502589
[5/200] Training loss: 0.05139683
[6/200] Training loss: 0.05001289
[7/200] Training loss: 0.04779219
[8/200] Training loss: 0.04491968
[9/200] Training loss: 0.04079570
[10/200] Training loss: 0.04150572
[50/200] Training loss: 0.01779218
[100/200] Training loss: 0.01488422
[150/200] Training loss: 0.01387551
[200/200] Training loss: 0.01283396
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7317.368926055321 ----------
[1/200] Training loss: 0.16549246
[2/200] Training loss: 0.06485028
[3/200] Training loss: 0.05545056
[4/200] Training loss: 0.05305181
[5/200] Training loss: 0.05184430
[6/200] Training loss: 0.05095216
[7/200] Training loss: 0.04969560
[8/200] Training loss: 0.04823901
[9/200] Training loss: 0.04780609
[10/200] Training loss: 0.04420950
[50/200] Training loss: 0.01885746
[100/200] Training loss: 0.01549440
[150/200] Training loss: 0.01365011
[200/200] Training loss: 0.01311954
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15373.011676311184 ----------
[1/200] Training loss: 0.16184704
[2/200] Training loss: 0.05894231
[3/200] Training loss: 0.05271653
[4/200] Training loss: 0.04787185
[5/200] Training loss: 0.04219865
[6/200] Training loss: 0.03893383
[7/200] Training loss: 0.03691730
[8/200] Training loss: 0.03538389
[9/200] Training loss: 0.03234610
[10/200] Training loss: 0.03132015
[50/200] Training loss: 0.01966088
[100/200] Training loss: 0.01725690
[150/200] Training loss: 0.01590048
[200/200] Training loss: 0.01495901
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10553.18226887037 ----------
[1/200] Training loss: 0.15410351
[2/200] Training loss: 0.05655073
[3/200] Training loss: 0.05006474
[4/200] Training loss: 0.04661269
[5/200] Training loss: 0.04402396
[6/200] Training loss: 0.03991216
[7/200] Training loss: 0.03962112
[8/200] Training loss: 0.03592211
[9/200] Training loss: 0.03338565
[10/200] Training loss: 0.03262893
[50/200] Training loss: 0.01848062
[100/200] Training loss: 0.01395543
[150/200] Training loss: 0.01242223
[200/200] Training loss: 0.01121765
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17455.99633363848 ----------
[1/200] Training loss: 0.18446377
[2/200] Training loss: 0.06132852
[3/200] Training loss: 0.05432866
[4/200] Training loss: 0.04728825
[5/200] Training loss: 0.04572815
[6/200] Training loss: 0.04069792
[7/200] Training loss: 0.04069898
[8/200] Training loss: 0.03653748
[9/200] Training loss: 0.03675680
[10/200] Training loss: 0.03472114
[50/200] Training loss: 0.01999548
[100/200] Training loss: 0.01682235
[150/200] Training loss: 0.01606209
[200/200] Training loss: 0.01463133
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14461.917438569479 ----------
[1/200] Training loss: 0.14877290
[2/200] Training loss: 0.05199677
[3/200] Training loss: 0.04875450
[4/200] Training loss: 0.04658321
[5/200] Training loss: 0.04096721
[6/200] Training loss: 0.03894440
[7/200] Training loss: 0.03815037
[8/200] Training loss: 0.03363402
[9/200] Training loss: 0.03511804
[10/200] Training loss: 0.03002082
[50/200] Training loss: 0.01794639
[100/200] Training loss: 0.01491225
[150/200] Training loss: 0.01352868
[200/200] Training loss: 0.01240871
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 31590.201012339254 ----------
[1/200] Training loss: 0.16426943
[2/200] Training loss: 0.05914544
[3/200] Training loss: 0.04889328
[4/200] Training loss: 0.04750185
[5/200] Training loss: 0.04309955
[6/200] Training loss: 0.04341017
[7/200] Training loss: 0.03766775
[8/200] Training loss: 0.03387916
[9/200] Training loss: 0.03224292
[10/200] Training loss: 0.03279215
[50/200] Training loss: 0.01794890
[100/200] Training loss: 0.01601332
[150/200] Training loss: 0.01385036
[200/200] Training loss: 0.01252963
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 24736.66881372672 ----------
[1/200] Training loss: 0.17844230
[2/200] Training loss: 0.06254959
[3/200] Training loss: 0.05401471
[4/200] Training loss: 0.05323003
[5/200] Training loss: 0.05131309
[6/200] Training loss: 0.04921625
[7/200] Training loss: 0.04759306
[8/200] Training loss: 0.04357681
[9/200] Training loss: 0.04399102
[10/200] Training loss: 0.04100746
[50/200] Training loss: 0.01839544
[100/200] Training loss: 0.01642648
[150/200] Training loss: 0.01463083
[200/200] Training loss: 0.01345099
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 24330.262308491456 ----------
[1/200] Training loss: 0.16662492
[2/200] Training loss: 0.06241967
[3/200] Training loss: 0.05556177
[4/200] Training loss: 0.04917067
[5/200] Training loss: 0.04587685
[6/200] Training loss: 0.04636788
[7/200] Training loss: 0.04326799
[8/200] Training loss: 0.04249622
[9/200] Training loss: 0.04020236
[10/200] Training loss: 0.03686508
[50/200] Training loss: 0.01918701
[100/200] Training loss: 0.01553947
[150/200] Training loss: 0.01486624
[200/200] Training loss: 0.01333577
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12110.062592736671 ----------
[1/200] Training loss: 0.16133792
[2/200] Training loss: 0.05755059
[3/200] Training loss: 0.05090328
[4/200] Training loss: 0.04594245
[5/200] Training loss: 0.04631756
[6/200] Training loss: 0.04302712
[7/200] Training loss: 0.04069898
[8/200] Training loss: 0.03696344
[9/200] Training loss: 0.03536472
[10/200] Training loss: 0.03435354
[50/200] Training loss: 0.01736704
[100/200] Training loss: 0.01518696
[150/200] Training loss: 0.01360455
[200/200] Training loss: 0.01277294
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12105.820087875089 ----------
[1/200] Training loss: 0.15616578
[2/200] Training loss: 0.05742414
[3/200] Training loss: 0.05317120
[4/200] Training loss: 0.04781485
[5/200] Training loss: 0.04856221
[6/200] Training loss: 0.04131053
[7/200] Training loss: 0.04035599
[8/200] Training loss: 0.03956610
[9/200] Training loss: 0.03774101
[10/200] Training loss: 0.03459172
[50/200] Training loss: 0.01888424
[100/200] Training loss: 0.01535985
[150/200] Training loss: 0.01401278
[200/200] Training loss: 0.01306607
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10555.57217776469 ----------
[1/200] Training loss: 0.17334331
[2/200] Training loss: 0.05902952
[3/200] Training loss: 0.04965728
[4/200] Training loss: 0.04701550
[5/200] Training loss: 0.04188580
[6/200] Training loss: 0.03809901
[7/200] Training loss: 0.03480174
[8/200] Training loss: 0.03325800
[9/200] Training loss: 0.02984878
[10/200] Training loss: 0.03025175
[50/200] Training loss: 0.01823386
[100/200] Training loss: 0.01676196
[150/200] Training loss: 0.01542872
[200/200] Training loss: 0.01437165
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13963.594379671733 ----------
[1/200] Training loss: 0.19494993
[2/200] Training loss: 0.06011564
[3/200] Training loss: 0.05866022
[4/200] Training loss: 0.05138696
[5/200] Training loss: 0.05074866
[6/200] Training loss: 0.05073083
[7/200] Training loss: 0.04617560
[8/200] Training loss: 0.04333364
[9/200] Training loss: 0.03900207
[10/200] Training loss: 0.04222322
[50/200] Training loss: 0.02305398
[100/200] Training loss: 0.01623025
[150/200] Training loss: 0.01539220
[200/200] Training loss: 0.01458816
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14906.10291122398 ----------
[1/200] Training loss: 0.18055086
[2/200] Training loss: 0.06347575
[3/200] Training loss: 0.05306233
[4/200] Training loss: 0.04862262
[5/200] Training loss: 0.04502861
[6/200] Training loss: 0.04126343
[7/200] Training loss: 0.03733285
[8/200] Training loss: 0.03863947
[9/200] Training loss: 0.03568446
[10/200] Training loss: 0.03349031
[50/200] Training loss: 0.01935128
[100/200] Training loss: 0.01728170
[150/200] Training loss: 0.01516993
[200/200] Training loss: 0.01430714
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8720.083944550075 ----------
[1/200] Training loss: 0.16052588
[2/200] Training loss: 0.06243616
[3/200] Training loss: 0.05476270
[4/200] Training loss: 0.05012865
[5/200] Training loss: 0.05150045
[6/200] Training loss: 0.04750662
[7/200] Training loss: 0.04419082
[8/200] Training loss: 0.04361077
[9/200] Training loss: 0.04332895
[10/200] Training loss: 0.04043404
[50/200] Training loss: 0.01886613
[100/200] Training loss: 0.01622689
[150/200] Training loss: 0.01449128
[200/200] Training loss: 0.01325976
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10049.953233722035 ----------
[1/200] Training loss: 0.14596025
[2/200] Training loss: 0.05680554
[3/200] Training loss: 0.05066708
[4/200] Training loss: 0.04741830
[5/200] Training loss: 0.04324376
[6/200] Training loss: 0.03902403
[7/200] Training loss: 0.03739393
[8/200] Training loss: 0.03650943
[9/200] Training loss: 0.03135942
[10/200] Training loss: 0.03284705
[50/200] Training loss: 0.01835817
[100/200] Training loss: 0.01425865
[150/200] Training loss: 0.01290668
[200/200] Training loss: 0.01183551
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 28621.739430020672 ----------
[1/200] Training loss: 0.13953628
[2/200] Training loss: 0.06003123
[3/200] Training loss: 0.05267378
[4/200] Training loss: 0.04672300
[5/200] Training loss: 0.04403252
[6/200] Training loss: 0.04297301
[7/200] Training loss: 0.04171600
[8/200] Training loss: 0.04022064
[9/200] Training loss: 0.03558387
[10/200] Training loss: 0.03629020
[50/200] Training loss: 0.01791741
[100/200] Training loss: 0.01492836
[150/200] Training loss: 0.01332929
[200/200] Training loss: 0.01209939
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9790.566071479217 ----------
[1/200] Training loss: 0.16275409
[2/200] Training loss: 0.05791473
[3/200] Training loss: 0.05579711
[4/200] Training loss: 0.04826690
[5/200] Training loss: 0.04603143
[6/200] Training loss: 0.04270051
[7/200] Training loss: 0.03907103
[8/200] Training loss: 0.03761024
[9/200] Training loss: 0.03562185
[10/200] Training loss: 0.03540554
[50/200] Training loss: 0.01694667
[100/200] Training loss: 0.01470040
[150/200] Training loss: 0.01377160
[200/200] Training loss: 0.01256909
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17763.69150824231 ----------
[1/200] Training loss: 0.17974091
[2/200] Training loss: 0.05851735
[3/200] Training loss: 0.05348823
[4/200] Training loss: 0.05072848
[5/200] Training loss: 0.04545942
[6/200] Training loss: 0.04442928
[7/200] Training loss: 0.04251729
[8/200] Training loss: 0.03965321
[9/200] Training loss: 0.03753546
[10/200] Training loss: 0.03636248
[50/200] Training loss: 0.01954459
[100/200] Training loss: 0.01589243
[150/200] Training loss: 0.01423275
[200/200] Training loss: 0.01218918
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9776.651369461837 ----------
[1/200] Training loss: 0.17471481
[2/200] Training loss: 0.06693838
[3/200] Training loss: 0.05068975
[4/200] Training loss: 0.05135360
[5/200] Training loss: 0.04519547
[6/200] Training loss: 0.04185047
[7/200] Training loss: 0.03917177
[8/200] Training loss: 0.03688220
[9/200] Training loss: 0.03381340
[10/200] Training loss: 0.03156540
[50/200] Training loss: 0.01724251
[100/200] Training loss: 0.01352859
[150/200] Training loss: 0.01246735
[200/200] Training loss: 0.01152765
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13943.899024304501 ----------
[1/200] Training loss: 0.15918989
[2/200] Training loss: 0.05805593
[3/200] Training loss: 0.05415357
[4/200] Training loss: 0.05143474
[5/200] Training loss: 0.05115912
[6/200] Training loss: 0.04706181
[7/200] Training loss: 0.04667428
[8/200] Training loss: 0.03908898
[9/200] Training loss: 0.04342645
[10/200] Training loss: 0.03764120
[50/200] Training loss: 0.01670012
[100/200] Training loss: 0.01429863
[150/200] Training loss: 0.01233151
[200/200] Training loss: 0.01106125
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18712.690880789967 ----------
[1/200] Training loss: 0.15319324
[2/200] Training loss: 0.06175781
[3/200] Training loss: 0.05251168
[4/200] Training loss: 0.05054158
[5/200] Training loss: 0.04852745
[6/200] Training loss: 0.04580726
[7/200] Training loss: 0.04136075
[8/200] Training loss: 0.03892848
[9/200] Training loss: 0.03862043
[10/200] Training loss: 0.03481631
[50/200] Training loss: 0.01717217
[100/200] Training loss: 0.01463524
[150/200] Training loss: 0.01260225
[200/200] Training loss: 0.01197418
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6941.07887867585 ----------
[1/200] Training loss: 0.14156139
[2/200] Training loss: 0.05824825
[3/200] Training loss: 0.05496704
[4/200] Training loss: 0.05049865
[5/200] Training loss: 0.04821787
[6/200] Training loss: 0.04310796
[7/200] Training loss: 0.04193752
[8/200] Training loss: 0.03977564
[9/200] Training loss: 0.03750872
[10/200] Training loss: 0.03277666
[50/200] Training loss: 0.01681743
[100/200] Training loss: 0.01439097
[150/200] Training loss: 0.01341124
[200/200] Training loss: 0.01285060
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14902.966147717038 ----------
[1/200] Training loss: 0.16687540
[2/200] Training loss: 0.06301981
[3/200] Training loss: 0.05421118
[4/200] Training loss: 0.05047359
[5/200] Training loss: 0.04824140
[6/200] Training loss: 0.04519158
[7/200] Training loss: 0.04457235
[8/200] Training loss: 0.04068734
[9/200] Training loss: 0.03955487
[10/200] Training loss: 0.03675029
[50/200] Training loss: 0.01999569
[100/200] Training loss: 0.01684882
[150/200] Training loss: 0.01476810
[200/200] Training loss: 0.01374763
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7990.143928616055 ----------
[1/200] Training loss: 0.13561321
[2/200] Training loss: 0.05610353
[3/200] Training loss: 0.04897620
[4/200] Training loss: 0.04536392
[5/200] Training loss: 0.04429296
[6/200] Training loss: 0.03996103
[7/200] Training loss: 0.04004006
[8/200] Training loss: 0.03754779
[9/200] Training loss: 0.03422497
[10/200] Training loss: 0.03110158
[50/200] Training loss: 0.01716059
[100/200] Training loss: 0.01498579
[150/200] Training loss: 0.01340549
[200/200] Training loss: 0.01198489
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19196.26713712851 ----------
[1/200] Training loss: 0.18629343
[2/200] Training loss: 0.05449497
[3/200] Training loss: 0.05222225
[4/200] Training loss: 0.04717157
[5/200] Training loss: 0.04228507
[6/200] Training loss: 0.04377787
[7/200] Training loss: 0.03909888
[8/200] Training loss: 0.03401201
[9/200] Training loss: 0.03281515
[10/200] Training loss: 0.03347573
[50/200] Training loss: 0.01748086
[100/200] Training loss: 0.01356820
[150/200] Training loss: 0.01225509
[200/200] Training loss: 0.01187075
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9327.63807188079 ----------
[1/200] Training loss: 0.16498552
[2/200] Training loss: 0.06381238
[3/200] Training loss: 0.05362580
[4/200] Training loss: 0.04881795
[5/200] Training loss: 0.04792494
[6/200] Training loss: 0.04329532
[7/200] Training loss: 0.03997676
[8/200] Training loss: 0.03731032
[9/200] Training loss: 0.03696201
[10/200] Training loss: 0.03324711
[50/200] Training loss: 0.01731325
[100/200] Training loss: 0.01364984
[150/200] Training loss: 0.01236412
[200/200] Training loss: 0.01147955
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16652.050444314656 ----------
[1/200] Training loss: 0.16516420
[2/200] Training loss: 0.06759933
[3/200] Training loss: 0.05302305
[4/200] Training loss: 0.05028084
[5/200] Training loss: 0.04756653
[6/200] Training loss: 0.04425251
[7/200] Training loss: 0.04433778
[8/200] Training loss: 0.04321917
[9/200] Training loss: 0.03894364
[10/200] Training loss: 0.03724302
[50/200] Training loss: 0.01785893
[100/200] Training loss: 0.01450788
[150/200] Training loss: 0.01363713
[200/200] Training loss: 0.01213906
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9417.965385368541 ----------
[1/200] Training loss: 0.14436058
[2/200] Training loss: 0.05466709
[3/200] Training loss: 0.05053370
[4/200] Training loss: 0.04614702
[5/200] Training loss: 0.04232937
[6/200] Training loss: 0.04467639
[7/200] Training loss: 0.04047616
[8/200] Training loss: 0.03821093
[9/200] Training loss: 0.03531122
[10/200] Training loss: 0.03494146
[50/200] Training loss: 0.01814694
[100/200] Training loss: 0.01578126
[150/200] Training loss: 0.01278178
[200/200] Training loss: 0.01237842
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6911.205683525849 ----------
[1/200] Training loss: 0.14793135
[2/200] Training loss: 0.05195062
[3/200] Training loss: 0.04667114
[4/200] Training loss: 0.04441241
[5/200] Training loss: 0.04282725
[6/200] Training loss: 0.03845194
[7/200] Training loss: 0.03740911
[8/200] Training loss: 0.03485698
[9/200] Training loss: 0.03546104
[10/200] Training loss: 0.03158609
[50/200] Training loss: 0.01884995
[100/200] Training loss: 0.01546522
[150/200] Training loss: 0.01365819
[200/200] Training loss: 0.01233775
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9872.745109644024 ----------
[1/200] Training loss: 0.14590075
[2/200] Training loss: 0.06315810
[3/200] Training loss: 0.05622108
[4/200] Training loss: 0.04899053
[5/200] Training loss: 0.04766745
[6/200] Training loss: 0.04237344
[7/200] Training loss: 0.03963976
[8/200] Training loss: 0.03928860
[9/200] Training loss: 0.03358196
[10/200] Training loss: 0.03321787
[50/200] Training loss: 0.01644097
[100/200] Training loss: 0.01437854
[150/200] Training loss: 0.01335296
[200/200] Training loss: 0.01288884
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23219.972093006487 ----------
[1/200] Training loss: 0.14700515
[2/200] Training loss: 0.05857990
[3/200] Training loss: 0.04818756
[4/200] Training loss: 0.03974875
[5/200] Training loss: 0.03758289
[6/200] Training loss: 0.03274863
[7/200] Training loss: 0.03035596
[8/200] Training loss: 0.02700178
[9/200] Training loss: 0.02699504
[10/200] Training loss: 0.02740237
[50/200] Training loss: 0.01688245
[100/200] Training loss: 0.01474006
[150/200] Training loss: 0.01337511
[200/200] Training loss: 0.01266340
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14690.723059128166 ----------
[1/200] Training loss: 0.16205312
[2/200] Training loss: 0.05981205
[3/200] Training loss: 0.05248373
[4/200] Training loss: 0.05289210
[5/200] Training loss: 0.04574507
[6/200] Training loss: 0.04419817
[7/200] Training loss: 0.04063719
[8/200] Training loss: 0.03744987
[9/200] Training loss: 0.03337176
[10/200] Training loss: 0.02988191
[50/200] Training loss: 0.01737841
[100/200] Training loss: 0.01508927
[150/200] Training loss: 0.01386358
[200/200] Training loss: 0.01304305
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13251.493500734172 ----------
[1/200] Training loss: 0.17313856
[2/200] Training loss: 0.07024974
[3/200] Training loss: 0.05505767
[4/200] Training loss: 0.04995010
[5/200] Training loss: 0.04764598
[6/200] Training loss: 0.04252047
[7/200] Training loss: 0.04156849
[8/200] Training loss: 0.04269507
[9/200] Training loss: 0.03725656
[10/200] Training loss: 0.03677224
[50/200] Training loss: 0.01883741
[100/200] Training loss: 0.01610828
[150/200] Training loss: 0.01465752
[200/200] Training loss: 0.01316821
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6584.343854933459 ----------
[1/200] Training loss: 0.14241879
[2/200] Training loss: 0.05315985
[3/200] Training loss: 0.04629368
[4/200] Training loss: 0.04750274
[5/200] Training loss: 0.04388452
[6/200] Training loss: 0.04275094
[7/200] Training loss: 0.03838534
[8/200] Training loss: 0.03707616
[9/200] Training loss: 0.03369174
[10/200] Training loss: 0.03111424
[50/200] Training loss: 0.01578502
[100/200] Training loss: 0.01206496
[150/200] Training loss: 0.01115598
[200/200] Training loss: 0.01024953
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17459.785107497744 ----------
[1/200] Training loss: 0.16995727
[2/200] Training loss: 0.05894820
[3/200] Training loss: 0.04708175
[4/200] Training loss: 0.04289657
[5/200] Training loss: 0.04294968
[6/200] Training loss: 0.03466313
[7/200] Training loss: 0.03348730
[8/200] Training loss: 0.02928448
[9/200] Training loss: 0.02804944
[10/200] Training loss: 0.02807143
[50/200] Training loss: 0.01743368
[100/200] Training loss: 0.01534961
[150/200] Training loss: 0.01369711
[200/200] Training loss: 0.01269987
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10980.53987743772 ----------
[1/200] Training loss: 0.15535303
[2/200] Training loss: 0.05747398
[3/200] Training loss: 0.04865916
[4/200] Training loss: 0.04802396
[5/200] Training loss: 0.04473574
[6/200] Training loss: 0.03941717
[7/200] Training loss: 0.03578483
[8/200] Training loss: 0.03951550
[9/200] Training loss: 0.03105181
[10/200] Training loss: 0.03163309
[50/200] Training loss: 0.01724515
[100/200] Training loss: 0.01518421
[150/200] Training loss: 0.01262319
[200/200] Training loss: 0.01129061
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.028486980267549446
----FITNESS-----------RMSE---- 11961.848017760467 ----------
[1/200] Training loss: 0.17794458
[2/200] Training loss: 0.05927041
[3/200] Training loss: 0.05711159
[4/200] Training loss: 0.04856191
[5/200] Training loss: 0.04458654
[6/200] Training loss: 0.04167616
[7/200] Training loss: 0.04165546
[8/200] Training loss: 0.03630947
[9/200] Training loss: 0.03457259
[10/200] Training loss: 0.03418489
[50/200] Training loss: 0.01929818
[100/200] Training loss: 0.01625938
[150/200] Training loss: 0.01357958
[200/200] Training loss: 0.01300930
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14940.202408267432 ----------
[1/200] Training loss: 0.15304525
[2/200] Training loss: 0.06602573
[3/200] Training loss: 0.05459592
[4/200] Training loss: 0.04882675
[5/200] Training loss: 0.04518762
[6/200] Training loss: 0.04312131
[7/200] Training loss: 0.03987917
[8/200] Training loss: 0.03846315
[9/200] Training loss: 0.03742002
[10/200] Training loss: 0.03363958
[50/200] Training loss: 0.01646712
[100/200] Training loss: 0.01401974
[150/200] Training loss: 0.01195454
[200/200] Training loss: 0.01175028
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17506.207356249382 ----------
[1/200] Training loss: 0.12321275
[2/200] Training loss: 0.05502390
[3/200] Training loss: 0.05153344
[4/200] Training loss: 0.04982908
[5/200] Training loss: 0.04508715
[6/200] Training loss: 0.04223377
[7/200] Training loss: 0.03990384
[8/200] Training loss: 0.03893938
[9/200] Training loss: 0.03686369
[10/200] Training loss: 0.03232007
[50/200] Training loss: 0.01872733
[100/200] Training loss: 0.01424778
[150/200] Training loss: 0.01315950
[200/200] Training loss: 0.01223908
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 16 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7701.710459371996 ----------
[1/200] Training loss: 0.19400659
[2/200] Training loss: 0.06412344
[3/200] Training loss: 0.05892595
[4/200] Training loss: 0.05514334
[5/200] Training loss: 0.04942262
[6/200] Training loss: 0.04870293
[7/200] Training loss: 0.04540362
[8/200] Training loss: 0.04436876
[9/200] Training loss: 0.04253651
[10/200] Training loss: 0.04090660
[50/200] Training loss: 0.02340255
[100/200] Training loss: 0.01694393
[150/200] Training loss: 0.01475489
[200/200] Training loss: 0.01352895
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11214.34509902384 ----------
[1/200] Training loss: 0.16263203
[2/200] Training loss: 0.06009950
[3/200] Training loss: 0.05192404
[4/200] Training loss: 0.04787450
[5/200] Training loss: 0.04364342
[6/200] Training loss: 0.03810520
[7/200] Training loss: 0.03726191
[8/200] Training loss: 0.03118205
[9/200] Training loss: 0.03199270
[10/200] Training loss: 0.02909967
[50/200] Training loss: 0.01689275
[100/200] Training loss: 0.01485082
[150/200] Training loss: 0.01445339
[200/200] Training loss: 0.01259021
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9413.11850557508 ----------
[1/200] Training loss: 0.15116992
[2/200] Training loss: 0.05515438
[3/200] Training loss: 0.04988794
[4/200] Training loss: 0.04481819
[5/200] Training loss: 0.04549170
[6/200] Training loss: 0.04099178
[7/200] Training loss: 0.03519097
[8/200] Training loss: 0.03389200
[9/200] Training loss: 0.03389297
[10/200] Training loss: 0.02990664
[50/200] Training loss: 0.01794954
[100/200] Training loss: 0.01460724
[150/200] Training loss: 0.01261643
[200/200] Training loss: 0.01129114
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 16 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18675.109424043545 ----------
[1/200] Training loss: 0.16597606
[2/200] Training loss: 0.06091650
[3/200] Training loss: 0.05119422
[4/200] Training loss: 0.04920305
[5/200] Training loss: 0.04809284
[6/200] Training loss: 0.04585945
[7/200] Training loss: 0.04544171
[8/200] Training loss: 0.04058818
[9/200] Training loss: 0.03693080
[10/200] Training loss: 0.03683931
[50/200] Training loss: 0.01864302
[100/200] Training loss: 0.01471280
[150/200] Training loss: 0.01343354
[200/200] Training loss: 0.01297736
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12810.338324962382 ----------
[1/200] Training loss: 0.14039152
[2/200] Training loss: 0.05602925
[3/200] Training loss: 0.04984601
[4/200] Training loss: 0.04832595
[5/200] Training loss: 0.04565306
[6/200] Training loss: 0.04336009
[7/200] Training loss: 0.03969364
[8/200] Training loss: 0.03972103
[9/200] Training loss: 0.03596364
[10/200] Training loss: 0.03373887
[50/200] Training loss: 0.02177191
[100/200] Training loss: 0.01621442
[150/200] Training loss: 0.01420378
[200/200] Training loss: 0.01256863
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16705.827486239643 ----------
[1/200] Training loss: 0.15700968
[2/200] Training loss: 0.05145643
[3/200] Training loss: 0.04853803
[4/200] Training loss: 0.04850902
[5/200] Training loss: 0.04598557
[6/200] Training loss: 0.04596471
[7/200] Training loss: 0.04521202
[8/200] Training loss: 0.04387321
[9/200] Training loss: 0.04405827
[10/200] Training loss: 0.04197511
[50/200] Training loss: 0.02482301
[100/200] Training loss: 0.01794234
[150/200] Training loss: 0.01520885
[200/200] Training loss: 0.01375431
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.002942038122550084
----FITNESS-----------RMSE---- 14873.777731296108 ----------
[1/200] Training loss: 0.16238802
[2/200] Training loss: 0.06661065
[3/200] Training loss: 0.05017708
[4/200] Training loss: 0.04888098
[5/200] Training loss: 0.04526488
[6/200] Training loss: 0.04598442
[7/200] Training loss: 0.04200444
[8/200] Training loss: 0.03648256
[9/200] Training loss: 0.03485743
[10/200] Training loss: 0.03121832
[50/200] Training loss: 0.01895730
[100/200] Training loss: 0.01492575
[150/200] Training loss: 0.01338192
[200/200] Training loss: 0.01226063
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13016.869669778522 ----------
[1/200] Training loss: 0.15054492
[2/200] Training loss: 0.05657349
[3/200] Training loss: 0.05345452
[4/200] Training loss: 0.04574910
[5/200] Training loss: 0.04690027
[6/200] Training loss: 0.04327332
[7/200] Training loss: 0.04077684
[8/200] Training loss: 0.03762909
[9/200] Training loss: 0.04062736
[10/200] Training loss: 0.03394876
[50/200] Training loss: 0.01924933
[100/200] Training loss: 0.01594250
[150/200] Training loss: 0.01423813
[200/200] Training loss: 0.01271120
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 29779.04122029452 ----------
[1/200] Training loss: 0.17261137
[2/200] Training loss: 0.06225180
[3/200] Training loss: 0.05750815
[4/200] Training loss: 0.05329354
[5/200] Training loss: 0.05110266
[6/200] Training loss: 0.04686826
[7/200] Training loss: 0.04720203
[8/200] Training loss: 0.04458495
[9/200] Training loss: 0.04246512
[10/200] Training loss: 0.04113858
[50/200] Training loss: 0.01801127
[100/200] Training loss: 0.01507014
[150/200] Training loss: 0.01393803
[200/200] Training loss: 0.01332078
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19006.183414878433 ----------
[1/200] Training loss: 0.16861522
[2/200] Training loss: 0.06948225
[3/200] Training loss: 0.05603263
[4/200] Training loss: 0.05625859
[5/200] Training loss: 0.04946645
[6/200] Training loss: 0.04628264
[7/200] Training loss: 0.04353570
[8/200] Training loss: 0.04161748
[9/200] Training loss: 0.03830645
[10/200] Training loss: 0.03614110
[50/200] Training loss: 0.01945050
[100/200] Training loss: 0.01682470
[150/200] Training loss: 0.01399663
[200/200] Training loss: 0.01351532
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16596.245358514076 ----------
[1/200] Training loss: 0.16669797
[2/200] Training loss: 0.06203086
[3/200] Training loss: 0.05286991
[4/200] Training loss: 0.04575830
[5/200] Training loss: 0.04553109
[6/200] Training loss: 0.04266343
[7/200] Training loss: 0.04070138
[8/200] Training loss: 0.03981257
[9/200] Training loss: 0.03811562
[10/200] Training loss: 0.03677586
[50/200] Training loss: 0.01822063
[100/200] Training loss: 0.01635642
[150/200] Training loss: 0.01521389
[200/200] Training loss: 0.01339739
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8665.732052169626 ----------
[1/200] Training loss: 0.17373330
[2/200] Training loss: 0.06293401
[3/200] Training loss: 0.05389297
[4/200] Training loss: 0.05054522
[5/200] Training loss: 0.04915310
[6/200] Training loss: 0.04712338
[7/200] Training loss: 0.04544177
[8/200] Training loss: 0.04152715
[9/200] Training loss: 0.04105036
[10/200] Training loss: 0.03609882
[50/200] Training loss: 0.01933624
[100/200] Training loss: 0.01660855
[150/200] Training loss: 0.01484729
[200/200] Training loss: 0.01300206
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21537.266678945125 ----------
[1/200] Training loss: 0.16668782
[2/200] Training loss: 0.05672596
[3/200] Training loss: 0.05185956
[4/200] Training loss: 0.04513256
[5/200] Training loss: 0.04250292
[6/200] Training loss: 0.04004918
[7/200] Training loss: 0.03766684
[8/200] Training loss: 0.03190484
[9/200] Training loss: 0.03117169
[10/200] Training loss: 0.02982005
[50/200] Training loss: 0.01565047
[100/200] Training loss: 0.01444401
[150/200] Training loss: 0.01270380
[200/200] Training loss: 0.01214410
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7330.433820723028 ----------
[1/200] Training loss: 0.16025993
[2/200] Training loss: 0.05791951
[3/200] Training loss: 0.05134064
[4/200] Training loss: 0.04551697
[5/200] Training loss: 0.04430710
[6/200] Training loss: 0.03995651
[7/200] Training loss: 0.03831625
[8/200] Training loss: 0.03607972
[9/200] Training loss: 0.03441096
[10/200] Training loss: 0.03065066
[50/200] Training loss: 0.01777374
[100/200] Training loss: 0.01577213
[150/200] Training loss: 0.01386588
[200/200] Training loss: 0.01284746
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15683.321076863791 ----------
[1/200] Training loss: 0.13828316
[2/200] Training loss: 0.05516390
[3/200] Training loss: 0.05355496
[4/200] Training loss: 0.04647528
[5/200] Training loss: 0.04482985
[6/200] Training loss: 0.04005003
[7/200] Training loss: 0.03654926
[8/200] Training loss: 0.03714803
[9/200] Training loss: 0.03502029
[10/200] Training loss: 0.03237057
[50/200] Training loss: 0.01740353
[100/200] Training loss: 0.01404687
[150/200] Training loss: 0.01265710
[200/200] Training loss: 0.01148306
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17889.12429382724 ----------
[1/200] Training loss: 0.15670236
[2/200] Training loss: 0.06110862
[3/200] Training loss: 0.05716302
[4/200] Training loss: 0.05084532
[5/200] Training loss: 0.04565383
[6/200] Training loss: 0.04416738
[7/200] Training loss: 0.04271278
[8/200] Training loss: 0.04033134
[9/200] Training loss: 0.03660673
[10/200] Training loss: 0.03633586
[50/200] Training loss: 0.01833862
[100/200] Training loss: 0.01620989
[150/200] Training loss: 0.01482680
[200/200] Training loss: 0.01370342
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11804.74819722979 ----------
[1/200] Training loss: 0.18904264
[2/200] Training loss: 0.06163057
[3/200] Training loss: 0.05297352
[4/200] Training loss: 0.05242915
[5/200] Training loss: 0.04985666
[6/200] Training loss: 0.04799747
[7/200] Training loss: 0.04659181
[8/200] Training loss: 0.04273626
[9/200] Training loss: 0.03963111
[10/200] Training loss: 0.03812338
[50/200] Training loss: 0.01792209
[100/200] Training loss: 0.01550338
[150/200] Training loss: 0.01438395
[200/200] Training loss: 0.01361549
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16615.778525245212 ----------
[1/200] Training loss: 0.19764282
[2/200] Training loss: 0.05981714
[3/200] Training loss: 0.05523516
[4/200] Training loss: 0.05357582
[5/200] Training loss: 0.04919950
[6/200] Training loss: 0.04998801
[7/200] Training loss: 0.04239941
[8/200] Training loss: 0.04176144
[9/200] Training loss: 0.04043219
[10/200] Training loss: 0.03975638
[50/200] Training loss: 0.02115495
[100/200] Training loss: 0.01798891
[150/200] Training loss: 0.01553130
[200/200] Training loss: 0.01377431
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7399.424301930522 ----------
[1/200] Training loss: 0.16790776
[2/200] Training loss: 0.06554009
[3/200] Training loss: 0.05990477
[4/200] Training loss: 0.04807887
[5/200] Training loss: 0.04821526
[6/200] Training loss: 0.04617934
[7/200] Training loss: 0.04120933
[8/200] Training loss: 0.03973096
[9/200] Training loss: 0.03978431
[10/200] Training loss: 0.03390806
[50/200] Training loss: 0.01723689
[100/200] Training loss: 0.01428123
[150/200] Training loss: 0.01378924
[200/200] Training loss: 0.01272054
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 26300.037414422055 ----------
[1/200] Training loss: 0.16105689
[2/200] Training loss: 0.05765155
[3/200] Training loss: 0.04892188
[4/200] Training loss: 0.04482006
[5/200] Training loss: 0.04217729
[6/200] Training loss: 0.04266808
[7/200] Training loss: 0.03908324
[8/200] Training loss: 0.03683518
[9/200] Training loss: 0.03563206
[10/200] Training loss: 0.03281854
[50/200] Training loss: 0.01755614
[100/200] Training loss: 0.01560588
[150/200] Training loss: 0.01436766
[200/200] Training loss: 0.01333587
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9930.478739718443 ----------
[1/200] Training loss: 0.18814915
[2/200] Training loss: 0.06485552
[3/200] Training loss: 0.05451719
[4/200] Training loss: 0.04734560
[5/200] Training loss: 0.04523103
[6/200] Training loss: 0.04176187
[7/200] Training loss: 0.03919622
[8/200] Training loss: 0.03773585
[9/200] Training loss: 0.03720884
[10/200] Training loss: 0.03444338
[50/200] Training loss: 0.01875844
[100/200] Training loss: 0.01622699
[150/200] Training loss: 0.01549421
[200/200] Training loss: 0.01406248
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15561.186587146882 ----------
[1/200] Training loss: 0.15615532
[2/200] Training loss: 0.06313135
[3/200] Training loss: 0.05624968
[4/200] Training loss: 0.05397449
[5/200] Training loss: 0.04925942
[6/200] Training loss: 0.04580655
[7/200] Training loss: 0.04686991
[8/200] Training loss: 0.04372408
[9/200] Training loss: 0.03700460
[10/200] Training loss: 0.03745386
[50/200] Training loss: 0.01887908
[100/200] Training loss: 0.01477609
[150/200] Training loss: 0.01389893
[200/200] Training loss: 0.01176524
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20391.257734627357 ----------
[1/200] Training loss: 0.20099885
[2/200] Training loss: 0.06204566
[3/200] Training loss: 0.05757301
[4/200] Training loss: 0.05390610
[5/200] Training loss: 0.05385443
[6/200] Training loss: 0.05398695
[7/200] Training loss: 0.04826417
[8/200] Training loss: 0.04747658
[9/200] Training loss: 0.04703257
[10/200] Training loss: 0.04409281
[50/200] Training loss: 0.01986224
[100/200] Training loss: 0.01714623
[150/200] Training loss: 0.01561134
[200/200] Training loss: 0.01367393
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8867.371200079537 ----------
[1/200] Training loss: 0.15252506
[2/200] Training loss: 0.06101281
[3/200] Training loss: 0.05483614
[4/200] Training loss: 0.04855408
[5/200] Training loss: 0.04993805
[6/200] Training loss: 0.04229668
[7/200] Training loss: 0.04055765
[8/200] Training loss: 0.03944722
[9/200] Training loss: 0.03570634
[10/200] Training loss: 0.03502189
[50/200] Training loss: 0.01796467
[100/200] Training loss: 0.01540150
[150/200] Training loss: 0.01328452
[200/200] Training loss: 0.01284591
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14517.751341030746 ----------
[1/200] Training loss: 0.17568626
[2/200] Training loss: 0.06127629
[3/200] Training loss: 0.05042618
[4/200] Training loss: 0.04731014
[5/200] Training loss: 0.04570410
[6/200] Training loss: 0.03912449
[7/200] Training loss: 0.04146095
[8/200] Training loss: 0.03875638
[9/200] Training loss: 0.03696662
[10/200] Training loss: 0.03804346
[50/200] Training loss: 0.01826261
[100/200] Training loss: 0.01479212
[150/200] Training loss: 0.01429285
[200/200] Training loss: 0.01198698
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12127.736804531998 ----------
[1/200] Training loss: 0.17017077
[2/200] Training loss: 0.06542266
[3/200] Training loss: 0.05377006
[4/200] Training loss: 0.05122093
[5/200] Training loss: 0.04973025
[6/200] Training loss: 0.04543323
[7/200] Training loss: 0.04396468
[8/200] Training loss: 0.04120248
[9/200] Training loss: 0.03949229
[10/200] Training loss: 0.03787613
[50/200] Training loss: 0.01999509
[100/200] Training loss: 0.01644753
[150/200] Training loss: 0.01394201
[200/200] Training loss: 0.01278062
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11243.005292180556 ----------
[1/200] Training loss: 0.16823526
[2/200] Training loss: 0.05812006
[3/200] Training loss: 0.05172348
[4/200] Training loss: 0.04227500
[5/200] Training loss: 0.04012701
[6/200] Training loss: 0.03741535
[7/200] Training loss: 0.03599621
[8/200] Training loss: 0.03284944
[9/200] Training loss: 0.03412375
[10/200] Training loss: 0.02976084
[50/200] Training loss: 0.01873082
[100/200] Training loss: 0.01543350
[150/200] Training loss: 0.01477160
[200/200] Training loss: 0.01364011
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9748.121460055778 ----------
[1/200] Training loss: 0.15052776
[2/200] Training loss: 0.05825085
[3/200] Training loss: 0.05485393
[4/200] Training loss: 0.05104688
[5/200] Training loss: 0.04898405
[6/200] Training loss: 0.04594855
[7/200] Training loss: 0.04629445
[8/200] Training loss: 0.04150536
[9/200] Training loss: 0.04076634
[10/200] Training loss: 0.04053577
[50/200] Training loss: 0.01886801
[100/200] Training loss: 0.01508605
[150/200] Training loss: 0.01493965
[200/200] Training loss: 0.01312330
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11834.830628276857 ----------
[1/200] Training loss: 0.18110726
[2/200] Training loss: 0.06569168
[3/200] Training loss: 0.06127016
[4/200] Training loss: 0.05345334
[5/200] Training loss: 0.05068291
[6/200] Training loss: 0.04871136
[7/200] Training loss: 0.04951060
[8/200] Training loss: 0.04688692
[9/200] Training loss: 0.04453637
[10/200] Training loss: 0.03977208
[50/200] Training loss: 0.01834309
[100/200] Training loss: 0.01605844
[150/200] Training loss: 0.01518547
[200/200] Training loss: 0.01425460
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16037.28805004138 ----------
[1/200] Training loss: 0.13405484
[2/200] Training loss: 0.06433075
[3/200] Training loss: 0.05692778
[4/200] Training loss: 0.05345699
[5/200] Training loss: 0.05090937
[6/200] Training loss: 0.04639474
[7/200] Training loss: 0.04453685
[8/200] Training loss: 0.04360248
[9/200] Training loss: 0.04005318
[10/200] Training loss: 0.03546569
[50/200] Training loss: 0.01783153
[100/200] Training loss: 0.01487096
[150/200] Training loss: 0.01379621
[200/200] Training loss: 0.01253065
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12373.682394501646 ----------
[1/200] Training loss: 0.14700087
[2/200] Training loss: 0.05791184
[3/200] Training loss: 0.05101415
[4/200] Training loss: 0.04198258
[5/200] Training loss: 0.04117563
[6/200] Training loss: 0.04247743
[7/200] Training loss: 0.03451196
[8/200] Training loss: 0.03392910
[9/200] Training loss: 0.03450934
[10/200] Training loss: 0.03384293
[50/200] Training loss: 0.01844653
[100/200] Training loss: 0.01555618
[150/200] Training loss: 0.01415916
[200/200] Training loss: 0.01362172
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17743.678085447787 ----------
[1/200] Training loss: 0.16458019
[2/200] Training loss: 0.06114856
[3/200] Training loss: 0.04979121
[4/200] Training loss: 0.04683739
[5/200] Training loss: 0.04857231
[6/200] Training loss: 0.04331561
[7/200] Training loss: 0.04294094
[8/200] Training loss: 0.03713429
[9/200] Training loss: 0.03887850
[10/200] Training loss: 0.03673578
[50/200] Training loss: 0.01770743
[100/200] Training loss: 0.01501678
[150/200] Training loss: 0.01280171
[200/200] Training loss: 0.01174688
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5658.371497171249 ----------
[1/200] Training loss: 0.14463537
[2/200] Training loss: 0.05823307
[3/200] Training loss: 0.04948127
[4/200] Training loss: 0.04759303
[5/200] Training loss: 0.04512571
[6/200] Training loss: 0.04114664
[7/200] Training loss: 0.03699504
[8/200] Training loss: 0.03415208
[9/200] Training loss: 0.03232369
[10/200] Training loss: 0.03181213
[50/200] Training loss: 0.01583516
[100/200] Training loss: 0.01359702
[150/200] Training loss: 0.01163004
[200/200] Training loss: 0.01119472
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19392.301977846775 ----------
[1/200] Training loss: 0.17391348
[2/200] Training loss: 0.06031952
[3/200] Training loss: 0.05737952
[4/200] Training loss: 0.05601449
[5/200] Training loss: 0.05292190
[6/200] Training loss: 0.05300724
[7/200] Training loss: 0.04965572
[8/200] Training loss: 0.04797893
[9/200] Training loss: 0.04414806
[10/200] Training loss: 0.04475655
[50/200] Training loss: 0.01961011
[100/200] Training loss: 0.01618730
[150/200] Training loss: 0.01534520
[200/200] Training loss: 0.01449255
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18356.76311335961 ----------
[1/200] Training loss: 0.15000608
[2/200] Training loss: 0.06143546
[3/200] Training loss: 0.05501704
[4/200] Training loss: 0.05160098
[5/200] Training loss: 0.05017022
[6/200] Training loss: 0.04728956
[7/200] Training loss: 0.04289511
[8/200] Training loss: 0.04060140
[9/200] Training loss: 0.03692512
[10/200] Training loss: 0.03490255
[50/200] Training loss: 0.01750239
[100/200] Training loss: 0.01596936
[150/200] Training loss: 0.01482153
[200/200] Training loss: 0.01427366
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10759.205546879379 ----------
[1/200] Training loss: 0.14351496
[2/200] Training loss: 0.05510124
[3/200] Training loss: 0.05068416
[4/200] Training loss: 0.05026227
[5/200] Training loss: 0.04195740
[6/200] Training loss: 0.04363007
[7/200] Training loss: 0.03982046
[8/200] Training loss: 0.03677060
[9/200] Training loss: 0.03600862
[10/200] Training loss: 0.03266523
[50/200] Training loss: 0.01751114
[100/200] Training loss: 0.01511553
[150/200] Training loss: 0.01367001
[200/200] Training loss: 0.01271138
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19619.431999933127 ----------
[1/200] Training loss: 0.16710399
[2/200] Training loss: 0.05581537
[3/200] Training loss: 0.05199576
[4/200] Training loss: 0.04782991
[5/200] Training loss: 0.04370412
[6/200] Training loss: 0.04465048
[7/200] Training loss: 0.04024687
[8/200] Training loss: 0.03817067
[9/200] Training loss: 0.03515627
[10/200] Training loss: 0.03374777
[50/200] Training loss: 0.01742194
[100/200] Training loss: 0.01498101
[150/200] Training loss: 0.01414832
[200/200] Training loss: 0.01291892
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15109.964394398818 ----------
[1/200] Training loss: 0.15996378
[2/200] Training loss: 0.06003790
[3/200] Training loss: 0.05847417
[4/200] Training loss: 0.05136535
[5/200] Training loss: 0.04771356
[6/200] Training loss: 0.04778763
[7/200] Training loss: 0.04454461
[8/200] Training loss: 0.04020815
[9/200] Training loss: 0.04137068
[10/200] Training loss: 0.03638312
[50/200] Training loss: 0.01811111
[100/200] Training loss: 0.01521084
[150/200] Training loss: 0.01357557
[200/200] Training loss: 0.01261316
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15690.931393642635 ----------
[1/200] Training loss: 0.17532339
[2/200] Training loss: 0.05871135
[3/200] Training loss: 0.05462391
[4/200] Training loss: 0.05168709
[5/200] Training loss: 0.04920034
[6/200] Training loss: 0.04819559
[7/200] Training loss: 0.04589387
[8/200] Training loss: 0.04290775
[9/200] Training loss: 0.04199378
[10/200] Training loss: 0.03865025
[50/200] Training loss: 0.01787588
[100/200] Training loss: 0.01482322
[150/200] Training loss: 0.01415773
[200/200] Training loss: 0.01262080
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11408.041725028885 ----------
[1/200] Training loss: 0.15350785
[2/200] Training loss: 0.05860512
[3/200] Training loss: 0.04959410
[4/200] Training loss: 0.04833925
[5/200] Training loss: 0.04686683
[6/200] Training loss: 0.04202106
[7/200] Training loss: 0.04114846
[8/200] Training loss: 0.04128460
[9/200] Training loss: 0.03799232
[10/200] Training loss: 0.03530351
[50/200] Training loss: 0.01749044
[100/200] Training loss: 0.01511464
[150/200] Training loss: 0.01382602
[200/200] Training loss: 0.01329289
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6013.1585710007685 ----------
[1/200] Training loss: 0.15312129
[2/200] Training loss: 0.05780862
[3/200] Training loss: 0.04957848
[4/200] Training loss: 0.04684880
[5/200] Training loss: 0.04262396
[6/200] Training loss: 0.04236793
[7/200] Training loss: 0.03939390
[8/200] Training loss: 0.03667879
[9/200] Training loss: 0.03597482
[10/200] Training loss: 0.03329597
[50/200] Training loss: 0.01776857
[100/200] Training loss: 0.01548700
[150/200] Training loss: 0.01412316
[200/200] Training loss: 0.01331303
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11990.105921133474 ----------
[1/200] Training loss: 0.15288668
[2/200] Training loss: 0.06040673
[3/200] Training loss: 0.05358893
[4/200] Training loss: 0.04905860
[5/200] Training loss: 0.04669435
[6/200] Training loss: 0.04771842
[7/200] Training loss: 0.04459104
[8/200] Training loss: 0.04313215
[9/200] Training loss: 0.04210441
[10/200] Training loss: 0.04049476
[50/200] Training loss: 0.02011830
[100/200] Training loss: 0.01756966
[150/200] Training loss: 0.01596173
[200/200] Training loss: 0.01435138
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11248.125532727665 ----------
[1/200] Training loss: 0.13310101
[2/200] Training loss: 0.05722309
[3/200] Training loss: 0.04718285
[4/200] Training loss: 0.04611804
[5/200] Training loss: 0.04441144
[6/200] Training loss: 0.04219000
[7/200] Training loss: 0.03998269
[8/200] Training loss: 0.03800283
[9/200] Training loss: 0.03690250
[10/200] Training loss: 0.03763060
[50/200] Training loss: 0.01995506
[100/200] Training loss: 0.01641602
[150/200] Training loss: 0.01498207
[200/200] Training loss: 0.01248859
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17284.857650556456 ----------
[1/200] Training loss: 0.16315955
[2/200] Training loss: 0.06504426
[3/200] Training loss: 0.05599096
[4/200] Training loss: 0.05012058
[5/200] Training loss: 0.04956429
[6/200] Training loss: 0.04887251
[7/200] Training loss: 0.04799379
[8/200] Training loss: 0.04450838
[9/200] Training loss: 0.04131894
[10/200] Training loss: 0.04154364
[50/200] Training loss: 0.01862077
[100/200] Training loss: 0.01659491
[150/200] Training loss: 0.01508748
[200/200] Training loss: 0.01396132
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18078.495512624937 ----------
[1/200] Training loss: 0.17066537
[2/200] Training loss: 0.06504661
[3/200] Training loss: 0.05130044
[4/200] Training loss: 0.04749341
[5/200] Training loss: 0.04789881
[6/200] Training loss: 0.03997339
[7/200] Training loss: 0.03747270
[8/200] Training loss: 0.03360223
[9/200] Training loss: 0.03064409
[10/200] Training loss: 0.02782897
[50/200] Training loss: 0.01604117
[100/200] Training loss: 0.01296241
[150/200] Training loss: 0.01098052
[200/200] Training loss: 0.00973651
---batch_size---: 16 ---n_steps---: 8 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7208.424238347796 ----------
[1/200] Training loss: 0.14507288
[2/200] Training loss: 0.05419419
[3/200] Training loss: 0.04943968
[4/200] Training loss: 0.04728396
[5/200] Training loss: 0.04115296
[6/200] Training loss: 0.04097381
[7/200] Training loss: 0.03743829
[8/200] Training loss: 0.03631758
[9/200] Training loss: 0.03440378
[10/200] Training loss: 0.03382195
[50/200] Training loss: 0.01726204
[100/200] Training loss: 0.01458894
[150/200] Training loss: 0.01275982
[200/200] Training loss: 0.01128223
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23026.499516861004 ----------
[1/200] Training loss: 0.16219116
[2/200] Training loss: 0.06277399
[3/200] Training loss: 0.05261988
[4/200] Training loss: 0.05015272
[5/200] Training loss: 0.05059168
[6/200] Training loss: 0.04519752
[7/200] Training loss: 0.04565229
[8/200] Training loss: 0.04378866
[9/200] Training loss: 0.04087650
[10/200] Training loss: 0.03707477
[50/200] Training loss: 0.01774655
[100/200] Training loss: 0.01542334
[150/200] Training loss: 0.01340960
[200/200] Training loss: 0.01175741
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12878.68875313011 ----------
[1/200] Training loss: 0.15088587
[2/200] Training loss: 0.05841125
[3/200] Training loss: 0.05045005
[4/200] Training loss: 0.05087854
[5/200] Training loss: 0.04769896
[6/200] Training loss: 0.04412435
[7/200] Training loss: 0.04108667
[8/200] Training loss: 0.03953969
[9/200] Training loss: 0.03612800
[10/200] Training loss: 0.03194304
[50/200] Training loss: 0.01598680
[100/200] Training loss: 0.01365207
[150/200] Training loss: 0.01220340
[200/200] Training loss: 0.01093310
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10904.61150156208 ----------
[1/200] Training loss: 0.13987999
[2/200] Training loss: 0.05773514
[3/200] Training loss: 0.05566136
[4/200] Training loss: 0.05206552
[5/200] Training loss: 0.05162361
[6/200] Training loss: 0.04999183
[7/200] Training loss: 0.04844703
[8/200] Training loss: 0.04623892
[9/200] Training loss: 0.04247809
[10/200] Training loss: 0.04225100
[50/200] Training loss: 0.02063214
[100/200] Training loss: 0.01533330
[150/200] Training loss: 0.01411685
[200/200] Training loss: 0.01323716
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01743059261096011
----FITNESS-----------RMSE---- 9403.092257337477 ----------
[1/200] Training loss: 0.16925008
[2/200] Training loss: 0.05687058
[3/200] Training loss: 0.04839074
[4/200] Training loss: 0.04461494
[5/200] Training loss: 0.04353891
[6/200] Training loss: 0.03968485
[7/200] Training loss: 0.03960932
[8/200] Training loss: 0.03444970
[9/200] Training loss: 0.03407577
[10/200] Training loss: 0.03188485
[50/200] Training loss: 0.01822088
[100/200] Training loss: 0.01535169
[150/200] Training loss: 0.01427658
[200/200] Training loss: 0.01278846
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 33256.337741850046 ----------
[1/200] Training loss: 0.17979680
[2/200] Training loss: 0.06429071
[3/200] Training loss: 0.05287101
[4/200] Training loss: 0.05224838
[5/200] Training loss: 0.04830711
[6/200] Training loss: 0.04794096
[7/200] Training loss: 0.04412564
[8/200] Training loss: 0.04218819
[9/200] Training loss: 0.03788396
[10/200] Training loss: 0.03824417
[50/200] Training loss: 0.01936492
[100/200] Training loss: 0.01653217
[150/200] Training loss: 0.01390325
[200/200] Training loss: 0.01342589
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17703.095322570007 ----------
[1/200] Training loss: 0.16464376
[2/200] Training loss: 0.05846846
[3/200] Training loss: 0.05125845
[4/200] Training loss: 0.04746880
[5/200] Training loss: 0.04580599
[6/200] Training loss: 0.04147376
[7/200] Training loss: 0.03908891
[8/200] Training loss: 0.03649821
[9/200] Training loss: 0.03433973
[10/200] Training loss: 0.03615503
[50/200] Training loss: 0.01984551
[100/200] Training loss: 0.01645102
[150/200] Training loss: 0.01595761
[200/200] Training loss: 0.01431814
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7775.89531822542 ----------
[1/200] Training loss: 0.19389192
[2/200] Training loss: 0.05474684
[3/200] Training loss: 0.05800635
[4/200] Training loss: 0.05004919
[5/200] Training loss: 0.04700046
[6/200] Training loss: 0.04380681
[7/200] Training loss: 0.04167246
[8/200] Training loss: 0.03988599
[9/200] Training loss: 0.04038603
[10/200] Training loss: 0.03623422
[50/200] Training loss: 0.01984988
[100/200] Training loss: 0.01680248
[150/200] Training loss: 0.01481186
[200/200] Training loss: 0.01402903
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6813.367449360118 ----------
[1/200] Training loss: 0.17467974
[2/200] Training loss: 0.05974518
[3/200] Training loss: 0.05176636
[4/200] Training loss: 0.04740901
[5/200] Training loss: 0.04647224
[6/200] Training loss: 0.04299848
[7/200] Training loss: 0.03950865
[8/200] Training loss: 0.03650054
[9/200] Training loss: 0.03659321
[10/200] Training loss: 0.03430381
[50/200] Training loss: 0.01743907
[100/200] Training loss: 0.01611440
[150/200] Training loss: 0.01419356
[200/200] Training loss: 0.01280538
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12070.36569454298 ----------
[1/200] Training loss: 0.17048708
[2/200] Training loss: 0.05534859
[3/200] Training loss: 0.05076890
[4/200] Training loss: 0.04559007
[5/200] Training loss: 0.04420021
[6/200] Training loss: 0.03984845
[7/200] Training loss: 0.03929946
[8/200] Training loss: 0.03713808
[9/200] Training loss: 0.03475640
[10/200] Training loss: 0.03348926
[50/200] Training loss: 0.01914614
[100/200] Training loss: 0.01736914
[150/200] Training loss: 0.01594026
[200/200] Training loss: 0.01457347
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17070.804081823444 ----------
[1/200] Training loss: 0.14942603
[2/200] Training loss: 0.05245071
[3/200] Training loss: 0.04529605
[4/200] Training loss: 0.04145202
[5/200] Training loss: 0.03785764
[6/200] Training loss: 0.03673230
[7/200] Training loss: 0.03731305
[8/200] Training loss: 0.03639538
[9/200] Training loss: 0.03619526
[10/200] Training loss: 0.03578054
[50/200] Training loss: 0.02985964
[100/200] Training loss: 0.02843987
[150/200] Training loss: 0.02812671
[200/200] Training loss: 0.02738984
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Rprop ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22544.79061779018 ----------
[1/200] Training loss: 0.19053059
[2/200] Training loss: 0.06203390
[3/200] Training loss: 0.05589554
[4/200] Training loss: 0.05290788
[5/200] Training loss: 0.04660380
[6/200] Training loss: 0.04836559
[7/200] Training loss: 0.04618971
[8/200] Training loss: 0.04200672
[9/200] Training loss: 0.04214385
[10/200] Training loss: 0.03733615
[50/200] Training loss: 0.01780705
[100/200] Training loss: 0.01489271
[150/200] Training loss: 0.01441892
[200/200] Training loss: 0.01293253
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16714.19803640007 ----------
[1/200] Training loss: 0.16796691
[2/200] Training loss: 0.06211630
[3/200] Training loss: 0.05659449
[4/200] Training loss: 0.04979795
[5/200] Training loss: 0.05072329
[6/200] Training loss: 0.04730848
[7/200] Training loss: 0.04563966
[8/200] Training loss: 0.04324052
[9/200] Training loss: 0.04048400
[10/200] Training loss: 0.03929130
[50/200] Training loss: 0.01859030
[100/200] Training loss: 0.01656972
[150/200] Training loss: 0.01523443
[200/200] Training loss: 0.01357393
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21346.23526526399 ----------
[1/200] Training loss: 0.18144974
[2/200] Training loss: 0.06206319
[3/200] Training loss: 0.06020432
[4/200] Training loss: 0.05169054
[5/200] Training loss: 0.05206973
[6/200] Training loss: 0.04975460
[7/200] Training loss: 0.04617887
[8/200] Training loss: 0.04360716
[9/200] Training loss: 0.04604683
[10/200] Training loss: 0.04056642
[50/200] Training loss: 0.01930754
[100/200] Training loss: 0.01660278
[150/200] Training loss: 0.01499642
[200/200] Training loss: 0.01400512
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13744.931865964269 ----------
[1/200] Training loss: 0.16248961
[2/200] Training loss: 0.05948737
[3/200] Training loss: 0.05447072
[4/200] Training loss: 0.04825805
[5/200] Training loss: 0.04604713
[6/200] Training loss: 0.04228766
[7/200] Training loss: 0.03819166
[8/200] Training loss: 0.03779201
[9/200] Training loss: 0.03707713
[10/200] Training loss: 0.03584170
[50/200] Training loss: 0.01918311
[100/200] Training loss: 0.01439214
[150/200] Training loss: 0.01330906
[200/200] Training loss: 0.01238821
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19958.013528405074 ----------
[1/200] Training loss: 0.15890284
[2/200] Training loss: 0.06001513
[3/200] Training loss: 0.05308126
[4/200] Training loss: 0.05098254
[5/200] Training loss: 0.04750724
[6/200] Training loss: 0.04488453
[7/200] Training loss: 0.03872403
[8/200] Training loss: 0.03724714
[9/200] Training loss: 0.03684089
[10/200] Training loss: 0.03458258
[50/200] Training loss: 0.01693108
[100/200] Training loss: 0.01315247
[150/200] Training loss: 0.01177881
[200/200] Training loss: 0.01130884
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16814.17592390421 ----------
[1/200] Training loss: 0.17541705
[2/200] Training loss: 0.05902037
[3/200] Training loss: 0.05017415
[4/200] Training loss: 0.04661063
[5/200] Training loss: 0.04352268
[6/200] Training loss: 0.03885052
[7/200] Training loss: 0.03692235
[8/200] Training loss: 0.03389424
[9/200] Training loss: 0.03553226
[10/200] Training loss: 0.03141769
[50/200] Training loss: 0.01703066
[100/200] Training loss: 0.01465367
[150/200] Training loss: 0.01314972
[200/200] Training loss: 0.01203057
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10709.862370731007 ----------
[1/200] Training loss: 0.17124809
[2/200] Training loss: 0.05626578
[3/200] Training loss: 0.05177055
[4/200] Training loss: 0.04873289
[5/200] Training loss: 0.04092550
[6/200] Training loss: 0.04067861
[7/200] Training loss: 0.03737350
[8/200] Training loss: 0.03740418
[9/200] Training loss: 0.03202582
[10/200] Training loss: 0.03078363
[50/200] Training loss: 0.01660852
[100/200] Training loss: 0.01360183
[150/200] Training loss: 0.01230441
[200/200] Training loss: 0.01091576
---batch_size---: 16 ---n_steps---: 8 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15435.37702811305 ----------
[1/200] Training loss: 0.15708735
[2/200] Training loss: 0.06165275
[3/200] Training loss: 0.05694346
[4/200] Training loss: 0.05155004
[5/200] Training loss: 0.04816529
[6/200] Training loss: 0.04609325
[7/200] Training loss: 0.04279683
[8/200] Training loss: 0.03853857
[9/200] Training loss: 0.03770739
[10/200] Training loss: 0.03250411
[50/200] Training loss: 0.01800109
[100/200] Training loss: 0.01593486
[150/200] Training loss: 0.01475114
[200/200] Training loss: 0.01362380
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9387.416257948722 ----------
[1/200] Training loss: 0.16234576
[2/200] Training loss: 0.06169899
[3/200] Training loss: 0.05700848
[4/200] Training loss: 0.05384755
[5/200] Training loss: 0.05127869
[6/200] Training loss: 0.04781108
[7/200] Training loss: 0.04738340
[8/200] Training loss: 0.04280626
[9/200] Training loss: 0.03914814
[10/200] Training loss: 0.03994409
[50/200] Training loss: 0.01713888
[100/200] Training loss: 0.01495738
[150/200] Training loss: 0.01382945
[200/200] Training loss: 0.01236368
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18314.73024644371 ----------
[1/200] Training loss: 0.18308437
[2/200] Training loss: 0.06687981
[3/200] Training loss: 0.05721449
[4/200] Training loss: 0.05358999
[5/200] Training loss: 0.05050892
[6/200] Training loss: 0.04897814
[7/200] Training loss: 0.04619209
[8/200] Training loss: 0.04440007
[9/200] Training loss: 0.04299680
[10/200] Training loss: 0.04061771
[50/200] Training loss: 0.01855837
[100/200] Training loss: 0.01547283
[150/200] Training loss: 0.01406119
[200/200] Training loss: 0.01262502
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19197.633187453084 ----------
[1/200] Training loss: 0.18018070
[2/200] Training loss: 0.06443987
[3/200] Training loss: 0.05563387
[4/200] Training loss: 0.05079150
[5/200] Training loss: 0.04998201
[6/200] Training loss: 0.04649782
[7/200] Training loss: 0.04068583
[8/200] Training loss: 0.04273703
[9/200] Training loss: 0.03731467
[10/200] Training loss: 0.03730456
[50/200] Training loss: 0.01914986
[100/200] Training loss: 0.01593723
[150/200] Training loss: 0.01500612
[200/200] Training loss: 0.01402155
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14667.487310374603 ----------
[1/200] Training loss: 0.16049685
[2/200] Training loss: 0.05693998
[3/200] Training loss: 0.04907473
[4/200] Training loss: 0.05031914
[5/200] Training loss: 0.04457828
[6/200] Training loss: 0.04109057
[7/200] Training loss: 0.03810170
[8/200] Training loss: 0.03295407
[9/200] Training loss: 0.03150837
[10/200] Training loss: 0.02821288
[50/200] Training loss: 0.01799457
[100/200] Training loss: 0.01428153
[150/200] Training loss: 0.01206814
[200/200] Training loss: 0.01129508
---batch_size---: 16 ---n_steps---: 8 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20040.046307331726 ----------
[1/200] Training loss: 0.13920095
[2/200] Training loss: 0.05278670
[3/200] Training loss: 0.05262422
[4/200] Training loss: 0.04638147
[5/200] Training loss: 0.04395914
[6/200] Training loss: 0.04076168
[7/200] Training loss: 0.03973418
[8/200] Training loss: 0.03492188
[9/200] Training loss: 0.03187845
[10/200] Training loss: 0.02879026
[50/200] Training loss: 0.01656057
[100/200] Training loss: 0.01438501
[150/200] Training loss: 0.01272237
[200/200] Training loss: 0.01169784
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15862.060900147875 ----------
[1/200] Training loss: 0.16215727
[2/200] Training loss: 0.05726812
[3/200] Training loss: 0.04884175
[4/200] Training loss: 0.04554953
[5/200] Training loss: 0.04177595
[6/200] Training loss: 0.03796475
[7/200] Training loss: 0.03220689
[8/200] Training loss: 0.03282327
[9/200] Training loss: 0.03315522
[10/200] Training loss: 0.02955450
[50/200] Training loss: 0.01854562
[100/200] Training loss: 0.01641471
[150/200] Training loss: 0.01542441
[200/200] Training loss: 0.01450263
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16049.80199254807 ----------
[1/200] Training loss: 0.17528549
[2/200] Training loss: 0.06365483
[3/200] Training loss: 0.05088789
[4/200] Training loss: 0.04831293
[5/200] Training loss: 0.04690787
[6/200] Training loss: 0.04464792
[7/200] Training loss: 0.04030541
[8/200] Training loss: 0.03834899
[9/200] Training loss: 0.03481330
[10/200] Training loss: 0.03357177
[50/200] Training loss: 0.01928096
[100/200] Training loss: 0.01520707
[150/200] Training loss: 0.01340132
[200/200] Training loss: 0.01339947
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17361.455238544953 ----------
[1/200] Training loss: 0.17356063
[2/200] Training loss: 0.05659099
[3/200] Training loss: 0.05533062
[4/200] Training loss: 0.04896058
[5/200] Training loss: 0.04353785
[6/200] Training loss: 0.04326275
[7/200] Training loss: 0.03722740
[8/200] Training loss: 0.03424291
[9/200] Training loss: 0.03414833
[10/200] Training loss: 0.03155066
[50/200] Training loss: 0.01675799
[100/200] Training loss: 0.01436146
[150/200] Training loss: 0.01371704
[200/200] Training loss: 0.01235766
---batch_size---: 16 ---n_steps---: 8 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14606.771032640992 ----------
[1/200] Training loss: 0.11849820
[2/200] Training loss: 0.05194190
[3/200] Training loss: 0.04490914
[4/200] Training loss: 0.03754735
[5/200] Training loss: 0.03371120
[6/200] Training loss: 0.03377170
[7/200] Training loss: 0.03137180
[8/200] Training loss: 0.02992375
[9/200] Training loss: 0.02844250
[10/200] Training loss: 0.02730214
[50/200] Training loss: 0.01856990
[100/200] Training loss: 0.01552993
[150/200] Training loss: 0.01416489
[200/200] Training loss: 0.01336472
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11809.117833267648 ----------
[1/200] Training loss: 0.15218645
[2/200] Training loss: 0.05474409
[3/200] Training loss: 0.04865863
[4/200] Training loss: 0.04544506
[5/200] Training loss: 0.03873631
[6/200] Training loss: 0.03750196
[7/200] Training loss: 0.03286415
[8/200] Training loss: 0.03256077
[9/200] Training loss: 0.02967321
[10/200] Training loss: 0.02905487
[50/200] Training loss: 0.01729678
[100/200] Training loss: 0.01521467
[150/200] Training loss: 0.01350963
[200/200] Training loss: 0.01183069
---batch_size---: 16 ---n_steps---: 8 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9498.198566043984 ----------
[1/200] Training loss: 0.15984986
[2/200] Training loss: 0.06462865
[3/200] Training loss: 0.05491731
[4/200] Training loss: 0.05392923
[5/200] Training loss: 0.05067160
[6/200] Training loss: 0.05121524
[7/200] Training loss: 0.04860961
[8/200] Training loss: 0.04745595
[9/200] Training loss: 0.04570896
[10/200] Training loss: 0.04051886
[50/200] Training loss: 0.01830150
[100/200] Training loss: 0.01531123
[150/200] Training loss: 0.01351305
[200/200] Training loss: 0.01212946
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8066.557630117075 ----------
[1/200] Training loss: 0.16236929
[2/200] Training loss: 0.05697010
[3/200] Training loss: 0.05036908
[4/200] Training loss: 0.04752161
[5/200] Training loss: 0.04435576
[6/200] Training loss: 0.04192255
[7/200] Training loss: 0.03661684
[8/200] Training loss: 0.03524953
[9/200] Training loss: 0.03151593
[10/200] Training loss: 0.03024663
[50/200] Training loss: 0.01707920
[100/200] Training loss: 0.01435443
[150/200] Training loss: 0.01310705
[200/200] Training loss: 0.01228166
---batch_size---: 16 ---n_steps---: 8 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6816.372935806843 ----------
[1/200] Training loss: 0.15028581
[2/200] Training loss: 0.05688044
[3/200] Training loss: 0.04823160
[4/200] Training loss: 0.04834414
[5/200] Training loss: 0.04232125
[6/200] Training loss: 0.03789325
[7/200] Training loss: 0.03717014
[8/200] Training loss: 0.03706358
[9/200] Training loss: 0.03503203
[10/200] Training loss: 0.03364325
[50/200] Training loss: 0.01909168
[100/200] Training loss: 0.01599974
[150/200] Training loss: 0.01364203
[200/200] Training loss: 0.01324362
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15306.686643424828 ----------
[1/200] Training loss: 0.16082189
[2/200] Training loss: 0.06055915
[3/200] Training loss: 0.05259926
[4/200] Training loss: 0.04709022
[5/200] Training loss: 0.04910524
[6/200] Training loss: 0.04476251
[7/200] Training loss: 0.04030620
[8/200] Training loss: 0.03824844
[9/200] Training loss: 0.03864465
[10/200] Training loss: 0.03622224
[50/200] Training loss: 0.01985638
[100/200] Training loss: 0.01604225
[150/200] Training loss: 0.01445095
[200/200] Training loss: 0.01278889
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 24096.531202644084 ----------
[1/200] Training loss: 0.16084465
[2/200] Training loss: 0.05531540
[3/200] Training loss: 0.04593076
[4/200] Training loss: 0.04265624
[5/200] Training loss: 0.03976955
[6/200] Training loss: 0.03494836
[7/200] Training loss: 0.03189523
[8/200] Training loss: 0.03026588
[9/200] Training loss: 0.03044639
[10/200] Training loss: 0.02701408
[50/200] Training loss: 0.01801825
[100/200] Training loss: 0.01579545
[150/200] Training loss: 0.01405587
[200/200] Training loss: 0.01275780
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14924.779931375873 ----------
[1/200] Training loss: 0.14398353
[2/200] Training loss: 0.05985942
[3/200] Training loss: 0.05459729
[4/200] Training loss: 0.04666314
[5/200] Training loss: 0.04479347
[6/200] Training loss: 0.04148887
[7/200] Training loss: 0.03720269
[8/200] Training loss: 0.03730513
[9/200] Training loss: 0.03597043
[10/200] Training loss: 0.03239948
[50/200] Training loss: 0.01831676
[100/200] Training loss: 0.01552779
[150/200] Training loss: 0.01397121
[200/200] Training loss: 0.01219860
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19105.532601840754 ----------
[1/200] Training loss: 0.15839519
[2/200] Training loss: 0.05526322
[3/200] Training loss: 0.04887173
[4/200] Training loss: 0.04621964
[5/200] Training loss: 0.04234832
[6/200] Training loss: 0.03931948
[7/200] Training loss: 0.03700101
[8/200] Training loss: 0.03657411
[9/200] Training loss: 0.03604564
[10/200] Training loss: 0.03326773
[50/200] Training loss: 0.01695397
[100/200] Training loss: 0.01421096
[150/200] Training loss: 0.01395463
[200/200] Training loss: 0.01264072
---batch_size---: 16 ---n_steps---: 8 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19617.652458946253 ----------
[1/200] Training loss: 0.16318506
[2/200] Training loss: 0.06592230
[3/200] Training loss: 0.05655473
[4/200] Training loss: 0.05390419
[5/200] Training loss: 0.05166847
[6/200] Training loss: 0.04914320
[7/200] Training loss: 0.04798372
[8/200] Training loss: 0.04511057
[9/200] Training loss: 0.04606791
[10/200] Training loss: 0.04554169
[50/200] Training loss: 0.01927497
[100/200] Training loss: 0.01598955
[150/200] Training loss: 0.01442282
[200/200] Training loss: 0.01227541
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8199.910243411205 ----------
[1/200] Training loss: 0.16136473
[2/200] Training loss: 0.06181489
[3/200] Training loss: 0.05305757
[4/200] Training loss: 0.05053886
[5/200] Training loss: 0.04586931
[6/200] Training loss: 0.04089393
[7/200] Training loss: 0.03521808
[8/200] Training loss: 0.03539802
[9/200] Training loss: 0.03202391
[10/200] Training loss: 0.02858451
[50/200] Training loss: 0.01766352
[100/200] Training loss: 0.01470305
[150/200] Training loss: 0.01349891
[200/200] Training loss: 0.01262754
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13332.281425172512 ----------
[1/200] Training loss: 0.13635799
[2/200] Training loss: 0.05478382
[3/200] Training loss: 0.04745307
[4/200] Training loss: 0.04534636
[5/200] Training loss: 0.04160236
[6/200] Training loss: 0.03492364
[7/200] Training loss: 0.03714786
[8/200] Training loss: 0.03113437
[9/200] Training loss: 0.03033050
[10/200] Training loss: 0.02874364
[50/200] Training loss: 0.01742925
[100/200] Training loss: 0.01484551
[150/200] Training loss: 0.01297963
[200/200] Training loss: 0.01214377
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20721.69722778518 ----------
[1/200] Training loss: 0.16418320
[2/200] Training loss: 0.06059981
[3/200] Training loss: 0.04986576
[4/200] Training loss: 0.04834221
[5/200] Training loss: 0.04500438
[6/200] Training loss: 0.04267287
[7/200] Training loss: 0.04046454
[8/200] Training loss: 0.03599102
[9/200] Training loss: 0.03354244
[10/200] Training loss: 0.03321642
[50/200] Training loss: 0.01826580
[100/200] Training loss: 0.01505924
[150/200] Training loss: 0.01403488
[200/200] Training loss: 0.01284033
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10811.809839245232 ----------
[1/200] Training loss: 0.15984793
[2/200] Training loss: 0.06149241
[3/200] Training loss: 0.05557073
[4/200] Training loss: 0.04921672
[5/200] Training loss: 0.04801573
[6/200] Training loss: 0.04725548
[7/200] Training loss: 0.04395570
[8/200] Training loss: 0.04221529
[9/200] Training loss: 0.03948818
[10/200] Training loss: 0.03911837
[50/200] Training loss: 0.01864776
[100/200] Training loss: 0.01632057
[150/200] Training loss: 0.01477217
[200/200] Training loss: 0.01366766
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14632.993952024992 ----------
[1/200] Training loss: 0.19155676
[2/200] Training loss: 0.06090938
[3/200] Training loss: 0.05235672
[4/200] Training loss: 0.04932403
[5/200] Training loss: 0.04454530
[6/200] Training loss: 0.04324315
[7/200] Training loss: 0.04348684
[8/200] Training loss: 0.04279736
[9/200] Training loss: 0.03757274
[10/200] Training loss: 0.03550890
[50/200] Training loss: 0.01873196
[100/200] Training loss: 0.01652359
[150/200] Training loss: 0.01506660
[200/200] Training loss: 0.01327047
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9897.406933131526 ----------
[1/200] Training loss: 0.14564492
[2/200] Training loss: 0.05495285
[3/200] Training loss: 0.04791763
[4/200] Training loss: 0.04648895
[5/200] Training loss: 0.04330707
[6/200] Training loss: 0.04160492
[7/200] Training loss: 0.03936692
[8/200] Training loss: 0.03361234
[9/200] Training loss: 0.03224730
[10/200] Training loss: 0.02895169
[50/200] Training loss: 0.01646844
[100/200] Training loss: 0.01450439
[150/200] Training loss: 0.01274456
[200/200] Training loss: 0.01203670
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10337.521172892464 ----------
[1/200] Training loss: 0.17367014
[2/200] Training loss: 0.06027008
[3/200] Training loss: 0.05435321
[4/200] Training loss: 0.04927671
[5/200] Training loss: 0.04889946
[6/200] Training loss: 0.04437510
[7/200] Training loss: 0.04351370
[8/200] Training loss: 0.03807287
[9/200] Training loss: 0.03859939
[10/200] Training loss: 0.03428890
[50/200] Training loss: 0.01689599
[100/200] Training loss: 0.01547930
[150/200] Training loss: 0.01460020
[200/200] Training loss: 0.01369177
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11290.15925485553 ----------
[1/200] Training loss: 0.17057819
[2/200] Training loss: 0.06210539
[3/200] Training loss: 0.05305492
[4/200] Training loss: 0.05002275
[5/200] Training loss: 0.04788951
[6/200] Training loss: 0.04705914
[7/200] Training loss: 0.04378781
[8/200] Training loss: 0.03929969
[9/200] Training loss: 0.03679089
[10/200] Training loss: 0.03765164
[50/200] Training loss: 0.01770616
[100/200] Training loss: 0.01550289
[150/200] Training loss: 0.01408729
[200/200] Training loss: 0.01278566
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.47586781421453683 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11419.244458369389 ----------
[1/200] Training loss: 0.12438448
[2/200] Training loss: 0.05412160
[3/200] Training loss: 0.04807183
[4/200] Training loss: 0.04549257
[5/200] Training loss: 0.04311186
[6/200] Training loss: 0.04279024
[7/200] Training loss: 0.03779109
[8/200] Training loss: 0.03906053
[9/200] Training loss: 0.03556999
[10/200] Training loss: 0.03334770
[50/200] Training loss: 0.01818033
[100/200] Training loss: 0.01520533
[150/200] Training loss: 0.01275639
[200/200] Training loss: 0.01135899
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 24819.264775573025 ----------
[1/200] Training loss: 0.15799661
[2/200] Training loss: 0.06019246
[3/200] Training loss: 0.05439035
[4/200] Training loss: 0.05158603
[5/200] Training loss: 0.04990844
[6/200] Training loss: 0.04574135
[7/200] Training loss: 0.04394558
[8/200] Training loss: 0.04181343
[9/200] Training loss: 0.03807999
[10/200] Training loss: 0.03681994
[50/200] Training loss: 0.01545222
[100/200] Training loss: 0.01478355
[150/200] Training loss: 0.01299813
[200/200] Training loss: 0.01257537
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5179.264426537807 ----------
[1/200] Training loss: 0.14565006
[2/200] Training loss: 0.05617764
[3/200] Training loss: 0.05454506
[4/200] Training loss: 0.05057889
[5/200] Training loss: 0.04580428
[6/200] Training loss: 0.04377130
[7/200] Training loss: 0.03899095
[8/200] Training loss: 0.03833208
[9/200] Training loss: 0.03796414
[10/200] Training loss: 0.03620191
[50/200] Training loss: 0.01775331
[100/200] Training loss: 0.01611200
[150/200] Training loss: 0.01537169
[200/200] Training loss: 0.01491734
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01743059261096011
----FITNESS-----------RMSE---- 7880.627893765826 ----------
[1/200] Training loss: 0.14053449
[2/200] Training loss: 0.05912025
[3/200] Training loss: 0.05309424
[4/200] Training loss: 0.04896872
[5/200] Training loss: 0.04589212
[6/200] Training loss: 0.04430144
[7/200] Training loss: 0.04060951
[8/200] Training loss: 0.03864063
[9/200] Training loss: 0.03818624
[10/200] Training loss: 0.03317378
[50/200] Training loss: 0.01782188
[100/200] Training loss: 0.01491751
[150/200] Training loss: 0.01368726
[200/200] Training loss: 0.01175002
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11387.577442107693 ----------
[1/200] Training loss: 0.13082785
[2/200] Training loss: 0.05348652
[3/200] Training loss: 0.04971351
[4/200] Training loss: 0.04712712
[5/200] Training loss: 0.04268960
[6/200] Training loss: 0.03905927
[7/200] Training loss: 0.03775499
[8/200] Training loss: 0.03565765
[9/200] Training loss: 0.03390915
[10/200] Training loss: 0.03476867
[50/200] Training loss: 0.01631567
[100/200] Training loss: 0.01403948
[150/200] Training loss: 0.01222583
[200/200] Training loss: 0.01106535
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23207.798689233754 ----------
[1/200] Training loss: 0.17764335
[2/200] Training loss: 0.06073272
[3/200] Training loss: 0.05670907
[4/200] Training loss: 0.05480397
[5/200] Training loss: 0.05301503
[6/200] Training loss: 0.04937567
[7/200] Training loss: 0.04681560
[8/200] Training loss: 0.04660203
[9/200] Training loss: 0.04567333
[10/200] Training loss: 0.04178828
[50/200] Training loss: 0.01723266
[100/200] Training loss: 0.01451350
[150/200] Training loss: 0.01354421
[200/200] Training loss: 0.01269059
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10448.769114111 ----------
[1/200] Training loss: 0.16156366
[2/200] Training loss: 0.05769913
[3/200] Training loss: 0.05359808
[4/200] Training loss: 0.05142828
[5/200] Training loss: 0.04663273
[6/200] Training loss: 0.04670349
[7/200] Training loss: 0.04552061
[8/200] Training loss: 0.04210716
[9/200] Training loss: 0.04080678
[10/200] Training loss: 0.03894167
[50/200] Training loss: 0.02175302
[100/200] Training loss: 0.01651024
[150/200] Training loss: 0.01420739
[200/200] Training loss: 0.01324123
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01743059261096011
----FITNESS-----------RMSE---- 10524.202962695084 ----------
[1/200] Training loss: 0.13173183
[2/200] Training loss: 0.05560603
[3/200] Training loss: 0.04983492
[4/200] Training loss: 0.04365506
[5/200] Training loss: 0.04084975
[6/200] Training loss: 0.03822069
[7/200] Training loss: 0.03709125
[8/200] Training loss: 0.03218662
[9/200] Training loss: 0.03146448
[10/200] Training loss: 0.03220485
[50/200] Training loss: 0.01510402
[100/200] Training loss: 0.01288826
[150/200] Training loss: 0.01172443
[200/200] Training loss: 0.01074794
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13845.129685199774 ----------
[1/200] Training loss: 0.16042615
[2/200] Training loss: 0.05454544
[3/200] Training loss: 0.04821615
[4/200] Training loss: 0.04679563
[5/200] Training loss: 0.04359802
[6/200] Training loss: 0.04152669
[7/200] Training loss: 0.03628889
[8/200] Training loss: 0.03476172
[9/200] Training loss: 0.03793059
[10/200] Training loss: 0.03238257
[50/200] Training loss: 0.01889139
[100/200] Training loss: 0.01614725
[150/200] Training loss: 0.01457409
[200/200] Training loss: 0.01306975
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15637.285186374264 ----------
[1/100] Training loss: 0.17243799
[2/100] Training loss: 0.06120285
[3/100] Training loss: 0.05642371
[4/100] Training loss: 0.05312908
[5/100] Training loss: 0.04896207
[6/100] Training loss: 0.04740812
[7/100] Training loss: 0.03939511
[8/100] Training loss: 0.04063575
[9/100] Training loss: 0.03747524
[10/100] Training loss: 0.03623937
[50/100] Training loss: 0.01834486
[100/100] Training loss: 0.01520721
---batch_size---: 16 ---n_steps---: 8 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17189.31295892887 ----------
[1/200] Training loss: 0.14706137
[2/200] Training loss: 0.05864159
[3/200] Training loss: 0.05849286
[4/200] Training loss: 0.05195478
[5/200] Training loss: 0.04683371
[6/200] Training loss: 0.04433104
[7/200] Training loss: 0.04226235
[8/200] Training loss: 0.04262848
[9/200] Training loss: 0.04074272
[10/200] Training loss: 0.03957133
[50/200] Training loss: 0.02000571
[100/200] Training loss: 0.01564855
[150/200] Training loss: 0.01314287
[200/200] Training loss: 0.01153904
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20688.69604397532 ----------
[1/200] Training loss: 0.14968485
[2/200] Training loss: 0.05412465
[3/200] Training loss: 0.05004204
[4/200] Training loss: 0.04683538
[5/200] Training loss: 0.04066207
[6/200] Training loss: 0.03765596
[7/200] Training loss: 0.03286234
[8/200] Training loss: 0.03064972
[9/200] Training loss: 0.02871482
[10/200] Training loss: 0.02807860
[50/200] Training loss: 0.01773630
[100/200] Training loss: 0.01613813
[150/200] Training loss: 0.01463312
[200/200] Training loss: 0.01340945
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14861.743639290782 ----------
[1/200] Training loss: 0.17400944
[2/200] Training loss: 0.06486098
[3/200] Training loss: 0.05613061
[4/200] Training loss: 0.05165528
[5/200] Training loss: 0.05020499
[6/200] Training loss: 0.04784304
[7/200] Training loss: 0.04653435
[8/200] Training loss: 0.04580865
[9/200] Training loss: 0.04365341
[10/200] Training loss: 0.04185194
[50/200] Training loss: 0.01786419
[100/200] Training loss: 0.01557577
[150/200] Training loss: 0.01411760
[200/200] Training loss: 0.01276286
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8828.781116326307 ----------
[1/200] Training loss: 0.18488577
[2/200] Training loss: 0.06240055
[3/200] Training loss: 0.05377964
[4/200] Training loss: 0.05254075
[5/200] Training loss: 0.04964598
[6/200] Training loss: 0.04600828
[7/200] Training loss: 0.04332060
[8/200] Training loss: 0.04269312
[9/200] Training loss: 0.03949038
[10/200] Training loss: 0.03703823
[50/200] Training loss: 0.01799827
[100/200] Training loss: 0.01491228
[150/200] Training loss: 0.01398332
[200/200] Training loss: 0.01302968
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10689.141780330168 ----------
[1/200] Training loss: 0.15690475
[2/200] Training loss: 0.05830174
[3/200] Training loss: 0.05152452
[4/200] Training loss: 0.04645471
[5/200] Training loss: 0.04538169
[6/200] Training loss: 0.03916567
[7/200] Training loss: 0.03745541
[8/200] Training loss: 0.03461331
[9/200] Training loss: 0.03278980
[10/200] Training loss: 0.03389427
[50/200] Training loss: 0.01844674
[100/200] Training loss: 0.01645728
[150/200] Training loss: 0.01537052
[200/200] Training loss: 0.01452488
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7222.31735663838 ----------
[1/200] Training loss: 0.16326008
[2/200] Training loss: 0.05957244
[3/200] Training loss: 0.05132318
[4/200] Training loss: 0.05032491
[5/200] Training loss: 0.04758625
[6/200] Training loss: 0.04366322
[7/200] Training loss: 0.04178037
[8/200] Training loss: 0.03882138
[9/200] Training loss: 0.03851147
[10/200] Training loss: 0.03617232
[50/200] Training loss: 0.01723948
[100/200] Training loss: 0.01513820
[150/200] Training loss: 0.01291150
[200/200] Training loss: 0.01256566
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14308.445617885962 ----------
[1/200] Training loss: 0.23721963
[2/200] Training loss: 0.07853520
[3/200] Training loss: 0.06231024
[4/200] Training loss: 0.05834196
[5/200] Training loss: 0.05133115
[6/200] Training loss: 0.05315647
[7/200] Training loss: 0.04941413
[8/200] Training loss: 0.04805715
[9/200] Training loss: 0.04141755
[10/200] Training loss: 0.04084427
[50/200] Training loss: 0.02036981
[100/200] Training loss: 0.01672441
[150/200] Training loss: 0.01489598
[200/200] Training loss: 0.01329103
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17166.490147959776 ----------
[1/200] Training loss: 0.14660696
[2/200] Training loss: 0.05468173
[3/200] Training loss: 0.04637927
[4/200] Training loss: 0.04352914
[5/200] Training loss: 0.03802860
[6/200] Training loss: 0.03569691
[7/200] Training loss: 0.03601118
[8/200] Training loss: 0.03150391
[9/200] Training loss: 0.03152983
[10/200] Training loss: 0.03072871
[50/200] Training loss: 0.01745878
[100/200] Training loss: 0.01471191
[150/200] Training loss: 0.01303420
[200/200] Training loss: 0.01199784
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6093.067864384902 ----------
[1/200] Training loss: 0.13600436
[2/200] Training loss: 0.06109507
[3/200] Training loss: 0.05267658
[4/200] Training loss: 0.04934136
[5/200] Training loss: 0.04882199
[6/200] Training loss: 0.04637004
[7/200] Training loss: 0.04446177
[8/200] Training loss: 0.04085379
[9/200] Training loss: 0.04027850
[10/200] Training loss: 0.03749964
[50/200] Training loss: 0.01723191
[100/200] Training loss: 0.01510288
[150/200] Training loss: 0.01347248
[200/200] Training loss: 0.01272957
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01743059261096011
----FITNESS-----------RMSE---- 2946.5681733162055 ----------
[1/200] Training loss: 0.16377224
[2/200] Training loss: 0.05585354
[3/200] Training loss: 0.04728572
[4/200] Training loss: 0.04648667
[5/200] Training loss: 0.04115334
[6/200] Training loss: 0.04100675
[7/200] Training loss: 0.04228308
[8/200] Training loss: 0.03587759
[9/200] Training loss: 0.03630451
[10/200] Training loss: 0.03270756
[50/200] Training loss: 0.01863771
[100/200] Training loss: 0.01509060
[150/200] Training loss: 0.01344175
[200/200] Training loss: 0.01273457
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14811.510118823131 ----------
[1/200] Training loss: 0.16541913
[2/200] Training loss: 0.06066214
[3/200] Training loss: 0.05177807
[4/200] Training loss: 0.04967979
[5/200] Training loss: 0.04771886
[6/200] Training loss: 0.04765095
[7/200] Training loss: 0.04523675
[8/200] Training loss: 0.04278833
[9/200] Training loss: 0.04171507
[10/200] Training loss: 0.03999343
[50/200] Training loss: 0.01966949
[100/200] Training loss: 0.01548852
[150/200] Training loss: 0.01329482
[200/200] Training loss: 0.01247473
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01743059261096011
----FITNESS-----------RMSE---- 6948.376501025257 ----------
[1/200] Training loss: 0.14508934
[2/200] Training loss: 0.05296881
[3/200] Training loss: 0.04747087
[4/200] Training loss: 0.04536833
[5/200] Training loss: 0.04097772
[6/200] Training loss: 0.03517850
[7/200] Training loss: 0.03426020
[8/200] Training loss: 0.03086002
[9/200] Training loss: 0.02742425
[10/200] Training loss: 0.02746685
[50/200] Training loss: 0.01594814
[100/200] Training loss: 0.01409387
[150/200] Training loss: 0.01317404
[200/200] Training loss: 0.01175162
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 27310.214352875373 ----------
[1/200] Training loss: 0.16083083
[2/200] Training loss: 0.05591094
[3/200] Training loss: 0.05028245
[4/200] Training loss: 0.04648435
[5/200] Training loss: 0.04166038
[6/200] Training loss: 0.04172904
[7/200] Training loss: 0.03608773
[8/200] Training loss: 0.03131941
[9/200] Training loss: 0.03222502
[10/200] Training loss: 0.02825266
[50/200] Training loss: 0.01624586
[100/200] Training loss: 0.01467808
[150/200] Training loss: 0.01291058
[200/200] Training loss: 0.01191953
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20955.48348285002 ----------
[1/200] Training loss: 0.16678576
[2/200] Training loss: 0.06126174
[3/200] Training loss: 0.05485023
[4/200] Training loss: 0.05502210
[5/200] Training loss: 0.04763422
[6/200] Training loss: 0.04424581
[7/200] Training loss: 0.04570384
[8/200] Training loss: 0.03981245
[9/200] Training loss: 0.03719752
[10/200] Training loss: 0.03676533
[50/200] Training loss: 0.01732432
[100/200] Training loss: 0.01488745
[150/200] Training loss: 0.01326312
[200/200] Training loss: 0.01215483
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14991.175537628795 ----------
[1/200] Training loss: 0.18038274
[2/200] Training loss: 0.05852931
[3/200] Training loss: 0.05502617
[4/200] Training loss: 0.04841744
[5/200] Training loss: 0.04933262
[6/200] Training loss: 0.04232544
[7/200] Training loss: 0.04310594
[8/200] Training loss: 0.04163931
[9/200] Training loss: 0.03795334
[10/200] Training loss: 0.03547929
[50/200] Training loss: 0.01714710
[100/200] Training loss: 0.01498049
[150/200] Training loss: 0.01371964
[200/200] Training loss: 0.01274654
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7263.113106650619 ----------
[1/200] Training loss: 0.39326647
[2/200] Training loss: 0.10537787
[3/200] Training loss: 0.09014880
[4/200] Training loss: 0.08154493
[5/200] Training loss: 0.07970880
[6/200] Training loss: 0.07611428
[7/200] Training loss: 0.06321793
[8/200] Training loss: 0.06207201
[9/200] Training loss: 0.06427122
[10/200] Training loss: 0.06146278
[50/200] Training loss: 0.04562276
[100/200] Training loss: 0.03814833
[150/200] Training loss: 0.03859693
[200/200] Training loss: 0.03858399
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: RMSprop ---learning_rate---: 0.01743059261096011
----FITNESS-----------RMSE---- 72229.30950798298 ----------
[1/200] Training loss: 0.15670553
[2/200] Training loss: 0.05812656
[3/200] Training loss: 0.04742070
[4/200] Training loss: 0.04515890
[5/200] Training loss: 0.04410207
[6/200] Training loss: 0.03910807
[7/200] Training loss: 0.03433165
[8/200] Training loss: 0.03109675
[9/200] Training loss: 0.03163013
[10/200] Training loss: 0.02748891
[50/200] Training loss: 0.01802048
[100/200] Training loss: 0.01543803
[150/200] Training loss: 0.01456194
[200/200] Training loss: 0.01310232
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14434.886075061348 ----------
[1/200] Training loss: 0.15341384
[2/200] Training loss: 0.06118622
[3/200] Training loss: 0.05404333
[4/200] Training loss: 0.05225160
[5/200] Training loss: 0.05087096
[6/200] Training loss: 0.04614118
[7/200] Training loss: 0.04181643
[8/200] Training loss: 0.04145752
[9/200] Training loss: 0.03759788
[10/200] Training loss: 0.03464158
[50/200] Training loss: 0.01864228
[100/200] Training loss: 0.01548895
[150/200] Training loss: 0.01413873
[200/200] Training loss: 0.01279742
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01743059261096011
----FITNESS-----------RMSE---- 4931.353161151613 ----------
[1/150] Training loss: 0.12992355
[2/150] Training loss: 0.05537308
[3/150] Training loss: 0.05037005
[4/150] Training loss: 0.04256211
[5/150] Training loss: 0.04405597
[6/150] Training loss: 0.03784720
[7/150] Training loss: 0.03769391
[8/150] Training loss: 0.03204613
[9/150] Training loss: 0.03495777
[10/150] Training loss: 0.03308373
[50/150] Training loss: 0.01736139
[100/150] Training loss: 0.01404666
[150/150] Training loss: 0.01330333
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 150
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18347.29582254562 ----------
[1/200] Training loss: 0.13832113
[2/200] Training loss: 0.05604709
[3/200] Training loss: 0.04592781
[4/200] Training loss: 0.04191153
[5/200] Training loss: 0.03838478
[6/200] Training loss: 0.03573042
[7/200] Training loss: 0.03189928
[8/200] Training loss: 0.02937545
[9/200] Training loss: 0.02785357
[10/200] Training loss: 0.02671020
[50/200] Training loss: 0.01864448
[100/200] Training loss: 0.01615924
[150/200] Training loss: 0.01546996
[200/200] Training loss: 0.01418432
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01743059261096011
----FITNESS-----------RMSE---- 13835.980919327692 ----------
[1/200] Training loss: 0.16906136
[2/200] Training loss: 0.06080154
[3/200] Training loss: 0.05669610
[4/200] Training loss: 0.05300484
[5/200] Training loss: 0.04849711
[6/200] Training loss: 0.04830162
[7/200] Training loss: 0.04571578
[8/200] Training loss: 0.04377997
[9/200] Training loss: 0.04175781
[10/200] Training loss: 0.03864478
[50/200] Training loss: 0.01802026
[100/200] Training loss: 0.01634774
[150/200] Training loss: 0.01356404
[200/200] Training loss: 0.01291804
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01743059261096011
----FITNESS-----------RMSE---- 5840.749267003335 ----------
[1/200] Training loss: 0.18128906
[2/200] Training loss: 0.06387019
[3/200] Training loss: 0.05331615
[4/200] Training loss: 0.04864689
[5/200] Training loss: 0.04541678
[6/200] Training loss: 0.04213518
[7/200] Training loss: 0.04044163
[8/200] Training loss: 0.03858576
[9/200] Training loss: 0.03651841
[10/200] Training loss: 0.03392542
[50/200] Training loss: 0.01830708
[100/200] Training loss: 0.01642068
[150/200] Training loss: 0.01441362
[200/200] Training loss: 0.01327009
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11588.021401430013 ----------
[1/200] Training loss: 0.15951115
[2/200] Training loss: 0.05523668
[3/200] Training loss: 0.05093825
[4/200] Training loss: 0.04763584
[5/200] Training loss: 0.04369014
[6/200] Training loss: 0.03855002
[7/200] Training loss: 0.03685939
[8/200] Training loss: 0.03421089
[9/200] Training loss: 0.03288734
[10/200] Training loss: 0.03005617
[50/200] Training loss: 0.01692737
[100/200] Training loss: 0.01443027
[150/200] Training loss: 0.01277447
[200/200] Training loss: 0.01180581
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20650.868456314372 ----------
[1/200] Training loss: 0.14903570
[2/200] Training loss: 0.05652726
[3/200] Training loss: 0.05269885
[4/200] Training loss: 0.04792309
[5/200] Training loss: 0.04331681
[6/200] Training loss: 0.04065806
[7/200] Training loss: 0.03842251
[8/200] Training loss: 0.03689225
[9/200] Training loss: 0.03530878
[10/200] Training loss: 0.03422377
[50/200] Training loss: 0.01703155
[100/200] Training loss: 0.01480224
[150/200] Training loss: 0.01417436
[200/200] Training loss: 0.01237362
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15904.811347513683 ----------
[1/200] Training loss: 0.15001421
[2/200] Training loss: 0.06124646
[3/200] Training loss: 0.05186382
[4/200] Training loss: 0.04807188
[5/200] Training loss: 0.04598438
[6/200] Training loss: 0.04565653
[7/200] Training loss: 0.04055146
[8/200] Training loss: 0.03862200
[9/200] Training loss: 0.03452976
[10/200] Training loss: 0.03278486
[50/200] Training loss: 0.01750826
[100/200] Training loss: 0.01461077
[150/200] Training loss: 0.01318963
[200/200] Training loss: 0.01193278
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19339.195433109413 ----------
[1/200] Training loss: 0.17024494
[2/200] Training loss: 0.06371315
[3/200] Training loss: 0.05435619
[4/200] Training loss: 0.04887407
[5/200] Training loss: 0.04844879
[6/200] Training loss: 0.04616837
[7/200] Training loss: 0.04127065
[8/200] Training loss: 0.04013657
[9/200] Training loss: 0.03652007
[10/200] Training loss: 0.03286311
[50/200] Training loss: 0.01661529
[100/200] Training loss: 0.01431707
[150/200] Training loss: 0.01372019
[200/200] Training loss: 0.01241246
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8992.294479163813 ----------
[1/200] Training loss: 0.14997697
[2/200] Training loss: 0.05628658
[3/200] Training loss: 0.05115662
[4/200] Training loss: 0.04501421
[5/200] Training loss: 0.03915962
[6/200] Training loss: 0.03753920
[7/200] Training loss: 0.03740534
[8/200] Training loss: 0.03352808
[9/200] Training loss: 0.03371582
[10/200] Training loss: 0.02843180
[50/200] Training loss: 0.01768830
[100/200] Training loss: 0.01503404
[150/200] Training loss: 0.01429135
[200/200] Training loss: 0.01296656
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8836.794441425012 ----------
[1/200] Training loss: 0.14171711
[2/200] Training loss: 0.05524090
[3/200] Training loss: 0.05011148
[4/200] Training loss: 0.04748464
[5/200] Training loss: 0.04217329
[6/200] Training loss: 0.03974565
[7/200] Training loss: 0.03776305
[8/200] Training loss: 0.03669304
[9/200] Training loss: 0.03137387
[10/200] Training loss: 0.03239178
[50/200] Training loss: 0.01853760
[100/200] Training loss: 0.01622420
[150/200] Training loss: 0.01490394
[200/200] Training loss: 0.01355800
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01743059261096011
----FITNESS-----------RMSE---- 12817.298935423172 ----------
[1/200] Training loss: 0.18799828
[2/200] Training loss: 0.06351876
[3/200] Training loss: 0.05135072
[4/200] Training loss: 0.05165386
[5/200] Training loss: 0.04370261
[6/200] Training loss: 0.04466449
[7/200] Training loss: 0.04143327
[8/200] Training loss: 0.04094577
[9/200] Training loss: 0.03708963
[10/200] Training loss: 0.03518954
[50/200] Training loss: 0.01787253
[100/200] Training loss: 0.01623769
[150/200] Training loss: 0.01428255
[200/200] Training loss: 0.01297181
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19978.95292551639 ----------
[1/200] Training loss: 0.14383448
[2/200] Training loss: 0.06047204
[3/200] Training loss: 0.05283009
[4/200] Training loss: 0.04873799
[5/200] Training loss: 0.04846323
[6/200] Training loss: 0.04367752
[7/200] Training loss: 0.04116641
[8/200] Training loss: 0.03550559
[9/200] Training loss: 0.03186470
[10/200] Training loss: 0.02964920
[50/200] Training loss: 0.01728776
[100/200] Training loss: 0.01444686
[150/200] Training loss: 0.01375204
[200/200] Training loss: 0.01200554
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17119.01772883012 ----------
[1/200] Training loss: 0.16262900
[2/200] Training loss: 0.05986631
[3/200] Training loss: 0.05304614
[4/200] Training loss: 0.05016746
[5/200] Training loss: 0.04614742
[6/200] Training loss: 0.04300573
[7/200] Training loss: 0.04200645
[8/200] Training loss: 0.04026315
[9/200] Training loss: 0.03847344
[10/200] Training loss: 0.03476728
[50/200] Training loss: 0.01803505
[100/200] Training loss: 0.01475588
[150/200] Training loss: 0.01363956
[200/200] Training loss: 0.01239987
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15590.7767606364 ----------
[1/200] Training loss: 0.16605507
[2/200] Training loss: 0.05616682
[3/200] Training loss: 0.05423018
[4/200] Training loss: 0.04755900
[5/200] Training loss: 0.04397997
[6/200] Training loss: 0.04038489
[7/200] Training loss: 0.04242357
[8/200] Training loss: 0.03780968
[9/200] Training loss: 0.03475492
[10/200] Training loss: 0.03695015
[50/200] Training loss: 0.01972398
[100/200] Training loss: 0.01665627
[150/200] Training loss: 0.01438712
[200/200] Training loss: 0.01270696
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4527.759269219157 ----------
[1/200] Training loss: 0.17159412
[2/200] Training loss: 0.05850162
[3/200] Training loss: 0.05018114
[4/200] Training loss: 0.04668427
[5/200] Training loss: 0.04288618
[6/200] Training loss: 0.03917703
[7/200] Training loss: 0.03744490
[8/200] Training loss: 0.03340869
[9/200] Training loss: 0.03391183
[10/200] Training loss: 0.03170987
[50/200] Training loss: 0.01738058
[100/200] Training loss: 0.01572517
[150/200] Training loss: 0.01289859
[200/200] Training loss: 0.01192445
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4817.09850428658 ----------
[1/200] Training loss: 0.17373540
[2/200] Training loss: 0.06480539
[3/200] Training loss: 0.05640500
[4/200] Training loss: 0.05334406
[5/200] Training loss: 0.05144958
[6/200] Training loss: 0.04704062
[7/200] Training loss: 0.04569984
[8/200] Training loss: 0.03893497
[9/200] Training loss: 0.03722891
[10/200] Training loss: 0.03516949
[50/200] Training loss: 0.01819075
[100/200] Training loss: 0.01640044
[150/200] Training loss: 0.01515373
[200/200] Training loss: 0.01451292
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 26039.352372898986 ----------
[1/200] Training loss: 0.18203150
[2/200] Training loss: 0.05965544
[3/200] Training loss: 0.05474106
[4/200] Training loss: 0.04978552
[5/200] Training loss: 0.05042849
[6/200] Training loss: 0.04402727
[7/200] Training loss: 0.04148642
[8/200] Training loss: 0.03831621
[9/200] Training loss: 0.03959943
[10/200] Training loss: 0.03594339
[50/200] Training loss: 0.01779221
[100/200] Training loss: 0.01505010
[150/200] Training loss: 0.01382798
[200/200] Training loss: 0.01263950
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9213.598645480495 ----------
[1/200] Training loss: 0.14344155
[2/200] Training loss: 0.06314659
[3/200] Training loss: 0.05528424
[4/200] Training loss: 0.05040060
[5/200] Training loss: 0.04551228
[6/200] Training loss: 0.04354907
[7/200] Training loss: 0.04373382
[8/200] Training loss: 0.03679224
[9/200] Training loss: 0.03685296
[10/200] Training loss: 0.03295775
[50/200] Training loss: 0.01755668
[100/200] Training loss: 0.01505112
[150/200] Training loss: 0.01360335
[200/200] Training loss: 0.01321122
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15862.176899782702 ----------
[1/200] Training loss: 0.17206192
[2/200] Training loss: 0.05978242
[3/200] Training loss: 0.05103017
[4/200] Training loss: 0.04408761
[5/200] Training loss: 0.04197864
[6/200] Training loss: 0.03932750
[7/200] Training loss: 0.03627297
[8/200] Training loss: 0.03693306
[9/200] Training loss: 0.03511343
[10/200] Training loss: 0.03181502
[50/200] Training loss: 0.01914283
[100/200] Training loss: 0.01681114
[150/200] Training loss: 0.01547663
[200/200] Training loss: 0.01380150
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12836.80832605987 ----------
[1/200] Training loss: 0.16827578
[2/200] Training loss: 0.05941179
[3/200] Training loss: 0.05102626
[4/200] Training loss: 0.05006847
[5/200] Training loss: 0.04382842
[6/200] Training loss: 0.03877807
[7/200] Training loss: 0.03578182
[8/200] Training loss: 0.03147930
[9/200] Training loss: 0.03122578
[10/200] Training loss: 0.02851728
[50/200] Training loss: 0.01635880
[100/200] Training loss: 0.01417774
[150/200] Training loss: 0.01185324
[200/200] Training loss: 0.01110554
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 24805.789001763278 ----------
[1/200] Training loss: 0.17206127
[2/200] Training loss: 0.05918595
[3/200] Training loss: 0.05564193
[4/200] Training loss: 0.05314303
[5/200] Training loss: 0.05229990
[6/200] Training loss: 0.04868339
[7/200] Training loss: 0.04644197
[8/200] Training loss: 0.04575883
[9/200] Training loss: 0.04135327
[10/200] Training loss: 0.03915469
[50/200] Training loss: 0.01782744
[100/200] Training loss: 0.01531458
[150/200] Training loss: 0.01481373
[200/200] Training loss: 0.01330760
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7422.82749361724 ----------
[1/200] Training loss: 0.16327122
[2/200] Training loss: 0.05644744
[3/200] Training loss: 0.05322275
[4/200] Training loss: 0.04543970
[5/200] Training loss: 0.04147922
[6/200] Training loss: 0.03880461
[7/200] Training loss: 0.03848674
[8/200] Training loss: 0.03514734
[9/200] Training loss: 0.03089332
[10/200] Training loss: 0.03185358
[50/200] Training loss: 0.01805616
[100/200] Training loss: 0.01567535
[150/200] Training loss: 0.01466661
[200/200] Training loss: 0.01335756
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15152.486264636573 ----------
[1/200] Training loss: 0.13858064
[2/200] Training loss: 0.06038653
[3/200] Training loss: 0.05466380
[4/200] Training loss: 0.05131045
[5/200] Training loss: 0.04828746
[6/200] Training loss: 0.04294278
[7/200] Training loss: 0.04454473
[8/200] Training loss: 0.04105016
[9/200] Training loss: 0.03719909
[10/200] Training loss: 0.03475082
[50/200] Training loss: 0.01754845
[100/200] Training loss: 0.01460864
[150/200] Training loss: 0.01267438
[200/200] Training loss: 0.01152496
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20329.944810549783 ----------
[1/200] Training loss: 0.15214486
[2/200] Training loss: 0.06078071
[3/200] Training loss: 0.04985124
[4/200] Training loss: 0.04937473
[5/200] Training loss: 0.04713535
[6/200] Training loss: 0.04021251
[7/200] Training loss: 0.03953640
[8/200] Training loss: 0.03535491
[9/200] Training loss: 0.03271484
[10/200] Training loss: 0.03240254
[50/200] Training loss: 0.01657991
[100/200] Training loss: 0.01463623
[150/200] Training loss: 0.01352971
[200/200] Training loss: 0.01292972
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23131.0112187081 ----------
[1/200] Training loss: 0.13241719
[2/200] Training loss: 0.05414052
[3/200] Training loss: 0.04962835
[4/200] Training loss: 0.04568963
[5/200] Training loss: 0.04445907
[6/200] Training loss: 0.04211002
[7/200] Training loss: 0.03521925
[8/200] Training loss: 0.03629062
[9/200] Training loss: 0.03256204
[10/200] Training loss: 0.02994478
[50/200] Training loss: 0.01815066
[100/200] Training loss: 0.01574527
[150/200] Training loss: 0.01353678
[200/200] Training loss: 0.01237027
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14822.515845834 ----------
[1/200] Training loss: 0.17722861
[2/200] Training loss: 0.06529901
[3/200] Training loss: 0.05547020
[4/200] Training loss: 0.04992421
[5/200] Training loss: 0.04470870
[6/200] Training loss: 0.04575578
[7/200] Training loss: 0.04081474
[8/200] Training loss: 0.03632120
[9/200] Training loss: 0.03394405
[10/200] Training loss: 0.03257186
[50/200] Training loss: 0.02015812
[100/200] Training loss: 0.01678855
[150/200] Training loss: 0.01467720
[200/200] Training loss: 0.01298174
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14023.25668309612 ----------
[1/200] Training loss: 0.16946536
[2/200] Training loss: 0.06075602
[3/200] Training loss: 0.05792569
[4/200] Training loss: 0.05419767
[5/200] Training loss: 0.04868280
[6/200] Training loss: 0.04662717
[7/200] Training loss: 0.04633190
[8/200] Training loss: 0.04193102
[9/200] Training loss: 0.03592366
[10/200] Training loss: 0.03532083
[50/200] Training loss: 0.01774381
[100/200] Training loss: 0.01569120
[150/200] Training loss: 0.01433977
[200/200] Training loss: 0.01331488
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19999.363189861822 ----------
[1/200] Training loss: 0.19009274
[2/200] Training loss: 0.06504270
[3/200] Training loss: 0.05403613
[4/200] Training loss: 0.05094613
[5/200] Training loss: 0.05096381
[6/200] Training loss: 0.04764053
[7/200] Training loss: 0.04387227
[8/200] Training loss: 0.04368433
[9/200] Training loss: 0.04152644
[10/200] Training loss: 0.04183228
[50/200] Training loss: 0.01939665
[100/200] Training loss: 0.01657765
[150/200] Training loss: 0.01494773
[200/200] Training loss: 0.01332523
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9178.394195064842 ----------
[1/200] Training loss: 0.14836733
[2/200] Training loss: 0.06432901
[3/200] Training loss: 0.05416938
[4/200] Training loss: 0.05074177
[5/200] Training loss: 0.04557161
[6/200] Training loss: 0.04279992
[7/200] Training loss: 0.03925026
[8/200] Training loss: 0.03651819
[9/200] Training loss: 0.03655337
[10/200] Training loss: 0.03279328
[50/200] Training loss: 0.01902577
[100/200] Training loss: 0.01549613
[150/200] Training loss: 0.01434838
[200/200] Training loss: 0.01323419
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 27380.09671275834 ----------
[1/200] Training loss: 0.16988677
[2/200] Training loss: 0.06473671
[3/200] Training loss: 0.05566623
[4/200] Training loss: 0.05104148
[5/200] Training loss: 0.04779553
[6/200] Training loss: 0.04558612
[7/200] Training loss: 0.04230755
[8/200] Training loss: 0.04135671
[9/200] Training loss: 0.03991706
[10/200] Training loss: 0.03674738
[50/200] Training loss: 0.02004290
[100/200] Training loss: 0.01735143
[150/200] Training loss: 0.01615927
[200/200] Training loss: 0.01531066
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8514.39580945119 ----------
[1/200] Training loss: 0.15210318
[2/200] Training loss: 0.05421311
[3/200] Training loss: 0.05365845
[4/200] Training loss: 0.04702963
[5/200] Training loss: 0.04477667
[6/200] Training loss: 0.04166609
[7/200] Training loss: 0.03692461
[8/200] Training loss: 0.03350514
[9/200] Training loss: 0.03518588
[10/200] Training loss: 0.03007719
[50/200] Training loss: 0.01899199
[100/200] Training loss: 0.01641729
[150/200] Training loss: 0.01469051
[200/200] Training loss: 0.01275049
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22447.188866314642 ----------
[1/200] Training loss: 0.15546764
[2/200] Training loss: 0.05877316
[3/200] Training loss: 0.04710805
[4/200] Training loss: 0.04296459
[5/200] Training loss: 0.03744155
[6/200] Training loss: 0.03664592
[7/200] Training loss: 0.03557724
[8/200] Training loss: 0.03425991
[9/200] Training loss: 0.03182982
[10/200] Training loss: 0.02936382
[50/200] Training loss: 0.01846701
[100/200] Training loss: 0.01612629
[150/200] Training loss: 0.01459781
[200/200] Training loss: 0.01313940
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9826.957616678725 ----------
[1/200] Training loss: 0.18935485
[2/200] Training loss: 0.06226469
[3/200] Training loss: 0.05686565
[4/200] Training loss: 0.05172475
[5/200] Training loss: 0.05177844
[6/200] Training loss: 0.04887533
[7/200] Training loss: 0.04553330
[8/200] Training loss: 0.04360624
[9/200] Training loss: 0.04304344
[10/200] Training loss: 0.03883530
[50/200] Training loss: 0.01845544
[100/200] Training loss: 0.01599591
[150/200] Training loss: 0.01442818
[200/200] Training loss: 0.01347424
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9507.233035957413 ----------
[1/200] Training loss: 0.17630011
[2/200] Training loss: 0.05766391
[3/200] Training loss: 0.05110688
[4/200] Training loss: 0.04729524
[5/200] Training loss: 0.04662112
[6/200] Training loss: 0.04177258
[7/200] Training loss: 0.03753895
[8/200] Training loss: 0.03777857
[9/200] Training loss: 0.03429985
[10/200] Training loss: 0.03544314
[50/200] Training loss: 0.01712947
[100/200] Training loss: 0.01422774
[150/200] Training loss: 0.01341033
[200/200] Training loss: 0.01278420
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7697.851388536934 ----------
[1/200] Training loss: 0.14786063
[2/200] Training loss: 0.05798162
[3/200] Training loss: 0.05222421
[4/200] Training loss: 0.05211919
[5/200] Training loss: 0.04607105
[6/200] Training loss: 0.04154846
[7/200] Training loss: 0.03990799
[8/200] Training loss: 0.03803113
[9/200] Training loss: 0.03506997
[10/200] Training loss: 0.03658410
[50/200] Training loss: 0.01741620
[100/200] Training loss: 0.01397608
[150/200] Training loss: 0.01317252
[200/200] Training loss: 0.01178027
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23906.429595403828 ----------
[1/200] Training loss: 0.14988047
[2/200] Training loss: 0.06195258
[3/200] Training loss: 0.05013482
[4/200] Training loss: 0.04912825
[5/200] Training loss: 0.04747633
[6/200] Training loss: 0.04539871
[7/200] Training loss: 0.03985954
[8/200] Training loss: 0.03758523
[9/200] Training loss: 0.03464214
[10/200] Training loss: 0.03307718
[50/200] Training loss: 0.01796530
[100/200] Training loss: 0.01507263
[150/200] Training loss: 0.01410067
[200/200] Training loss: 0.01260186
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7904.342856936306 ----------
[1/200] Training loss: 0.16361091
[2/200] Training loss: 0.05875090
[3/200] Training loss: 0.05370041
[4/200] Training loss: 0.04928332
[5/200] Training loss: 0.04661175
[6/200] Training loss: 0.04518127
[7/200] Training loss: 0.04118903
[8/200] Training loss: 0.03962863
[9/200] Training loss: 0.03782331
[10/200] Training loss: 0.03477159
[50/200] Training loss: 0.01684476
[100/200] Training loss: 0.01432238
[150/200] Training loss: 0.01334215
[200/200] Training loss: 0.01228588
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17510.213705149345 ----------
[1/200] Training loss: 0.16178802
[2/200] Training loss: 0.06304827
[3/200] Training loss: 0.05629260
[4/200] Training loss: 0.05149013
[5/200] Training loss: 0.05006597
[6/200] Training loss: 0.04796629
[7/200] Training loss: 0.04697729
[8/200] Training loss: 0.04456242
[9/200] Training loss: 0.04046844
[10/200] Training loss: 0.03955459
[50/200] Training loss: 0.01882087
[100/200] Training loss: 0.01621896
[150/200] Training loss: 0.01450889
[200/200] Training loss: 0.01339684
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14031.123689854638 ----------
[1/200] Training loss: 0.16168704
[2/200] Training loss: 0.05858752
[3/200] Training loss: 0.05480630
[4/200] Training loss: 0.05068893
[5/200] Training loss: 0.04448251
[6/200] Training loss: 0.04384235
[7/200] Training loss: 0.03896751
[8/200] Training loss: 0.03685951
[9/200] Training loss: 0.03713698
[10/200] Training loss: 0.03238945
[50/200] Training loss: 0.01954055
[100/200] Training loss: 0.01495912
[150/200] Training loss: 0.01319293
[200/200] Training loss: 0.01249298
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10371.4060763235 ----------
[1/200] Training loss: 0.17422099
[2/200] Training loss: 0.06337019
[3/200] Training loss: 0.05779862
[4/200] Training loss: 0.05458236
[5/200] Training loss: 0.05199112
[6/200] Training loss: 0.04923741
[7/200] Training loss: 0.04856347
[8/200] Training loss: 0.04445818
[9/200] Training loss: 0.04291995
[10/200] Training loss: 0.04023831
[50/200] Training loss: 0.01813116
[100/200] Training loss: 0.01545365
[150/200] Training loss: 0.01430474
[200/200] Training loss: 0.01321592
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13119.634141240373 ----------
[1/200] Training loss: 0.17179579
[2/200] Training loss: 0.06330689
[3/200] Training loss: 0.05566243
[4/200] Training loss: 0.04561496
[5/200] Training loss: 0.04542142
[6/200] Training loss: 0.04012378
[7/200] Training loss: 0.03522750
[8/200] Training loss: 0.03432454
[9/200] Training loss: 0.03454375
[10/200] Training loss: 0.03046217
[50/200] Training loss: 0.01634576
[100/200] Training loss: 0.01511705
[150/200] Training loss: 0.01242295
[200/200] Training loss: 0.01119462
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 28460.47982729736 ----------
[1/200] Training loss: 0.18769129
[2/200] Training loss: 0.06468634
[3/200] Training loss: 0.05612847
[4/200] Training loss: 0.05190767
[5/200] Training loss: 0.05021523
[6/200] Training loss: 0.04813538
[7/200] Training loss: 0.04614257
[8/200] Training loss: 0.04296877
[9/200] Training loss: 0.03993267
[10/200] Training loss: 0.03860277
[50/200] Training loss: 0.01773225
[100/200] Training loss: 0.01632745
[150/200] Training loss: 0.01408775
[200/200] Training loss: 0.01360509
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18461.759829442046 ----------
[1/200] Training loss: 0.14253646
[2/200] Training loss: 0.05833338
[3/200] Training loss: 0.05171926
[4/200] Training loss: 0.04780151
[5/200] Training loss: 0.04542025
[6/200] Training loss: 0.04500437
[7/200] Training loss: 0.04122593
[8/200] Training loss: 0.03635629
[9/200] Training loss: 0.03515583
[10/200] Training loss: 0.03177264
[50/200] Training loss: 0.01821075
[100/200] Training loss: 0.01719372
[150/200] Training loss: 0.01600866
[200/200] Training loss: 0.01492341
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8906.511325990665 ----------
[1/200] Training loss: 0.18116466
[2/200] Training loss: 0.05991100
[3/200] Training loss: 0.05456376
[4/200] Training loss: 0.04890455
[5/200] Training loss: 0.04736382
[6/200] Training loss: 0.04422522
[7/200] Training loss: 0.04401622
[8/200] Training loss: 0.04251128
[9/200] Training loss: 0.04111513
[10/200] Training loss: 0.03616683
[50/200] Training loss: 0.01891993
[100/200] Training loss: 0.01579800
[150/200] Training loss: 0.01418156
[200/200] Training loss: 0.01317369
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14660.334511872503 ----------
[1/200] Training loss: 0.16577544
[2/200] Training loss: 0.06038275
[3/200] Training loss: 0.05339064
[4/200] Training loss: 0.05042540
[5/200] Training loss: 0.04695336
[6/200] Training loss: 0.04769145
[7/200] Training loss: 0.04370455
[8/200] Training loss: 0.04377270
[9/200] Training loss: 0.04122050
[10/200] Training loss: 0.03960463
[50/200] Training loss: 0.01869229
[100/200] Training loss: 0.01642868
[150/200] Training loss: 0.01571510
[200/200] Training loss: 0.01358039
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4810.124114822818 ----------
[1/200] Training loss: 0.16089465
[2/200] Training loss: 0.05841899
[3/200] Training loss: 0.05378097
[4/200] Training loss: 0.05063050
[5/200] Training loss: 0.04710446
[6/200] Training loss: 0.04251037
[7/200] Training loss: 0.04363743
[8/200] Training loss: 0.03570812
[9/200] Training loss: 0.03727805
[10/200] Training loss: 0.03343551
[50/200] Training loss: 0.01868450
[100/200] Training loss: 0.01558566
[150/200] Training loss: 0.01337220
[200/200] Training loss: 0.01226180
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21754.63095527019 ----------
[1/200] Training loss: 0.14267956
[2/200] Training loss: 0.05297298
[3/200] Training loss: 0.05136420
[4/200] Training loss: 0.04581768
[5/200] Training loss: 0.04193499
[6/200] Training loss: 0.03970081
[7/200] Training loss: 0.04054823
[8/200] Training loss: 0.03672445
[9/200] Training loss: 0.03593126
[10/200] Training loss: 0.03153053
[50/200] Training loss: 0.01732176
[100/200] Training loss: 0.01347816
[150/200] Training loss: 0.01218036
[200/200] Training loss: 0.01180346
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10523.848725632652 ----------
[1/200] Training loss: 0.75307249
[2/200] Training loss: 0.11024172
[3/200] Training loss: 0.08904682
[4/200] Training loss: 0.07453425
[5/200] Training loss: 0.07001238
[6/200] Training loss: 0.06488895
[7/200] Training loss: 0.06187880
[8/200] Training loss: 0.05539082
[9/200] Training loss: 0.06165384
[10/200] Training loss: 0.05840498
[50/200] Training loss: 0.04211429
[100/200] Training loss: 0.04230631
[150/200] Training loss: 0.03589810
[200/200] Training loss: 0.03474644
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: RMSprop ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22996.04905195673 ----------
[1/200] Training loss: 0.15018127
[2/200] Training loss: 0.05594963
[3/200] Training loss: 0.05001871
[4/200] Training loss: 0.04841770
[5/200] Training loss: 0.04706708
[6/200] Training loss: 0.04206678
[7/200] Training loss: 0.04238244
[8/200] Training loss: 0.03629496
[9/200] Training loss: 0.03389301
[10/200] Training loss: 0.03591511
[50/200] Training loss: 0.01855150
[100/200] Training loss: 0.01472930
[150/200] Training loss: 0.01303446
[200/200] Training loss: 0.01207549
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14881.871925265315 ----------
[1/200] Training loss: 0.17544441
[2/200] Training loss: 0.06087174
[3/200] Training loss: 0.05718059
[4/200] Training loss: 0.05105727
[5/200] Training loss: 0.04977425
[6/200] Training loss: 0.05105156
[7/200] Training loss: 0.04308347
[8/200] Training loss: 0.04187239
[9/200] Training loss: 0.03881205
[10/200] Training loss: 0.03792287
[50/200] Training loss: 0.02048533
[100/200] Training loss: 0.01588186
[150/200] Training loss: 0.01404156
[200/200] Training loss: 0.01237189
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 26043.843648739716 ----------
[1/200] Training loss: 0.17869805
[2/200] Training loss: 0.05683514
[3/200] Training loss: 0.05535078
[4/200] Training loss: 0.04965269
[5/200] Training loss: 0.04617997
[6/200] Training loss: 0.04398716
[7/200] Training loss: 0.04250710
[8/200] Training loss: 0.03648159
[9/200] Training loss: 0.03634355
[10/200] Training loss: 0.03591245
[50/200] Training loss: 0.01937812
[100/200] Training loss: 0.01553682
[150/200] Training loss: 0.01435866
[200/200] Training loss: 0.01303511
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12511.565209836857 ----------
[1/200] Training loss: 0.17249409
[2/200] Training loss: 0.06455727
[3/200] Training loss: 0.05868424
[4/200] Training loss: 0.05467801
[5/200] Training loss: 0.05031908
[6/200] Training loss: 0.04931517
[7/200] Training loss: 0.04488220
[8/200] Training loss: 0.04094868
[9/200] Training loss: 0.04104270
[10/200] Training loss: 0.03742996
[50/200] Training loss: 0.01769135
[100/200] Training loss: 0.01554198
[150/200] Training loss: 0.01403413
[200/200] Training loss: 0.01282892
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19072.995779373516 ----------
[1/200] Training loss: 0.17400305
[2/200] Training loss: 0.06167940
[3/200] Training loss: 0.05271265
[4/200] Training loss: 0.05000765
[5/200] Training loss: 0.04499100
[6/200] Training loss: 0.04275625
[7/200] Training loss: 0.04347817
[8/200] Training loss: 0.03888416
[9/200] Training loss: 0.03633308
[10/200] Training loss: 0.03619385
[50/200] Training loss: 0.01755652
[100/200] Training loss: 0.01569288
[150/200] Training loss: 0.01389435
[200/200] Training loss: 0.01269322
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12430.429759264158 ----------
[1/200] Training loss: 0.13916925
[2/200] Training loss: 0.05707975
[3/200] Training loss: 0.04880655
[4/200] Training loss: 0.04881858
[5/200] Training loss: 0.04191046
[6/200] Training loss: 0.03820150
[7/200] Training loss: 0.03307971
[8/200] Training loss: 0.03134044
[9/200] Training loss: 0.03162577
[10/200] Training loss: 0.02741089
[50/200] Training loss: 0.01643757
[100/200] Training loss: 0.01457782
[150/200] Training loss: 0.01299064
[200/200] Training loss: 0.01215643
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19321.735739834556 ----------
[1/200] Training loss: 0.17434768
[2/200] Training loss: 0.06603755
[3/200] Training loss: 0.05190705
[4/200] Training loss: 0.04952070
[5/200] Training loss: 0.04628680
[6/200] Training loss: 0.04289362
[7/200] Training loss: 0.04067263
[8/200] Training loss: 0.03579722
[9/200] Training loss: 0.03663170
[10/200] Training loss: 0.03262675
[50/200] Training loss: 0.01829115
[100/200] Training loss: 0.01527925
[150/200] Training loss: 0.01385647
[200/200] Training loss: 0.01355320
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14350.276373645213 ----------
[1/200] Training loss: 0.18604942
[2/200] Training loss: 0.06158809
[3/200] Training loss: 0.05553188
[4/200] Training loss: 0.05325039
[5/200] Training loss: 0.05360047
[6/200] Training loss: 0.05030414
[7/200] Training loss: 0.04761429
[8/200] Training loss: 0.04537070
[9/200] Training loss: 0.04181257
[10/200] Training loss: 0.04132363
[50/200] Training loss: 0.01789494
[100/200] Training loss: 0.01537358
[150/200] Training loss: 0.01436884
[200/200] Training loss: 0.01254193
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12695.67485405955 ----------
[1/200] Training loss: 0.15203258
[2/200] Training loss: 0.05449029
[3/200] Training loss: 0.04808730
[4/200] Training loss: 0.04428014
[5/200] Training loss: 0.04098519
[6/200] Training loss: 0.03808970
[7/200] Training loss: 0.03566181
[8/200] Training loss: 0.03302794
[9/200] Training loss: 0.03310265
[10/200] Training loss: 0.02969660
[50/200] Training loss: 0.01698350
[100/200] Training loss: 0.01431365
[150/200] Training loss: 0.01335892
[200/200] Training loss: 0.01230260
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17290.091497733607 ----------
[1/200] Training loss: 0.17896822
[2/200] Training loss: 0.05587632
[3/200] Training loss: 0.04894943
[4/200] Training loss: 0.04609799
[5/200] Training loss: 0.04187149
[6/200] Training loss: 0.04400352
[7/200] Training loss: 0.03739923
[8/200] Training loss: 0.03658433
[9/200] Training loss: 0.03497826
[10/200] Training loss: 0.03540565
[50/200] Training loss: 0.01883706
[100/200] Training loss: 0.01597962
[150/200] Training loss: 0.01409246
[200/200] Training loss: 0.01371829
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10055.770482663176 ----------
[1/200] Training loss: 0.16855561
[2/200] Training loss: 0.06139496
[3/200] Training loss: 0.05240022
[4/200] Training loss: 0.05490999
[5/200] Training loss: 0.04971188
[6/200] Training loss: 0.04743806
[7/200] Training loss: 0.04368861
[8/200] Training loss: 0.04209922
[9/200] Training loss: 0.04199915
[10/200] Training loss: 0.03897120
[50/200] Training loss: 0.01828282
[100/200] Training loss: 0.01473653
[150/200] Training loss: 0.01348451
[200/200] Training loss: 0.01254935
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 29104.883988774116 ----------
[1/200] Training loss: 0.17249609
[2/200] Training loss: 0.06145339
[3/200] Training loss: 0.05655069
[4/200] Training loss: 0.05124395
[5/200] Training loss: 0.04840129
[6/200] Training loss: 0.04437065
[7/200] Training loss: 0.04142205
[8/200] Training loss: 0.03732310
[9/200] Training loss: 0.03569931
[10/200] Training loss: 0.03369804
[50/200] Training loss: 0.01657193
[100/200] Training loss: 0.01462321
[150/200] Training loss: 0.01307737
[200/200] Training loss: 0.01206610
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23433.14438994477 ----------
[1/200] Training loss: 0.16678218
[2/200] Training loss: 0.06121125
[3/200] Training loss: 0.05650726
[4/200] Training loss: 0.05689666
[5/200] Training loss: 0.05505042
[6/200] Training loss: 0.05184773
[7/200] Training loss: 0.05149769
[8/200] Training loss: 0.04888713
[9/200] Training loss: 0.04804002
[10/200] Training loss: 0.04566015
[50/200] Training loss: 0.01880852
[100/200] Training loss: 0.01626099
[150/200] Training loss: 0.01458942
[200/200] Training loss: 0.01337738
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23338.912399681354 ----------
[1/200] Training loss: 0.16579737
[2/200] Training loss: 0.06571062
[3/200] Training loss: 0.05533315
[4/200] Training loss: 0.05249890
[5/200] Training loss: 0.04874023
[6/200] Training loss: 0.04727445
[7/200] Training loss: 0.04617022
[8/200] Training loss: 0.04240547
[9/200] Training loss: 0.03984758
[10/200] Training loss: 0.03977137
[50/200] Training loss: 0.01880149
[100/200] Training loss: 0.01445640
[150/200] Training loss: 0.01280923
[200/200] Training loss: 0.01224915
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23331.607402834466 ----------
[1/200] Training loss: 0.13859865
[2/200] Training loss: 0.05054997
[3/200] Training loss: 0.04704922
[4/200] Training loss: 0.04321227
[5/200] Training loss: 0.04103759
[6/200] Training loss: 0.03904132
[7/200] Training loss: 0.03668024
[8/200] Training loss: 0.03342042
[9/200] Training loss: 0.03278363
[10/200] Training loss: 0.02860248
[50/200] Training loss: 0.01752533
[100/200] Training loss: 0.01629835
[150/200] Training loss: 0.01472705
[200/200] Training loss: 0.01403875
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18863.042384514752 ----------
[1/200] Training loss: 0.15747103
[2/200] Training loss: 0.06330036
[3/200] Training loss: 0.05525153
[4/200] Training loss: 0.05141487
[5/200] Training loss: 0.04865443
[6/200] Training loss: 0.04567936
[7/200] Training loss: 0.04384774
[8/200] Training loss: 0.03758409
[9/200] Training loss: 0.03539274
[10/200] Training loss: 0.03298319
[50/200] Training loss: 0.01684993
[100/200] Training loss: 0.01428316
[150/200] Training loss: 0.01285936
[200/200] Training loss: 0.01270064
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6854.224099050161 ----------
[1/200] Training loss: 0.20145762
[2/200] Training loss: 0.06951040
[3/200] Training loss: 0.05458754
[4/200] Training loss: 0.05180528
[5/200] Training loss: 0.04781867
[6/200] Training loss: 0.04559999
[7/200] Training loss: 0.03807753
[8/200] Training loss: 0.03776013
[9/200] Training loss: 0.03442645
[10/200] Training loss: 0.03174385
[50/200] Training loss: 0.02189047
[100/200] Training loss: 0.01594823
[150/200] Training loss: 0.01463594
[200/200] Training loss: 0.01381330
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15345.553101794669 ----------
[1/200] Training loss: 0.20339417
[2/200] Training loss: 0.06858907
[3/200] Training loss: 0.05701729
[4/200] Training loss: 0.05208456
[5/200] Training loss: 0.05239106
[6/200] Training loss: 0.04719007
[7/200] Training loss: 0.04328364
[8/200] Training loss: 0.04213940
[9/200] Training loss: 0.03677571
[10/200] Training loss: 0.03453385
[50/200] Training loss: 0.01892942
[100/200] Training loss: 0.01665327
[150/200] Training loss: 0.01478266
[200/200] Training loss: 0.01358946
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17604.865691052575 ----------
[1/200] Training loss: 0.17724181
[2/200] Training loss: 0.06186295
[3/200] Training loss: 0.05594298
[4/200] Training loss: 0.05311044
[5/200] Training loss: 0.04981803
[6/200] Training loss: 0.04915082
[7/200] Training loss: 0.04672276
[8/200] Training loss: 0.04453498
[9/200] Training loss: 0.04299138
[10/200] Training loss: 0.04307556
[50/200] Training loss: 0.02373790
[100/200] Training loss: 0.01612155
[150/200] Training loss: 0.01479328
[200/200] Training loss: 0.01312127
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10610.762460822501 ----------
[1/200] Training loss: 0.16667339
[2/200] Training loss: 0.06117335
[3/200] Training loss: 0.05116893
[4/200] Training loss: 0.04720255
[5/200] Training loss: 0.04401110
[6/200] Training loss: 0.04710562
[7/200] Training loss: 0.04065146
[8/200] Training loss: 0.04033417
[9/200] Training loss: 0.03485909
[10/200] Training loss: 0.03475956
[50/200] Training loss: 0.01849341
[100/200] Training loss: 0.01479698
[150/200] Training loss: 0.01381461
[200/200] Training loss: 0.01216591
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6902.957337257706 ----------
[1/200] Training loss: 0.17451802
[2/200] Training loss: 0.05834367
[3/200] Training loss: 0.05242108
[4/200] Training loss: 0.04576004
[5/200] Training loss: 0.04174452
[6/200] Training loss: 0.03972474
[7/200] Training loss: 0.03566353
[8/200] Training loss: 0.03765123
[9/200] Training loss: 0.03457432
[10/200] Training loss: 0.03419768
[50/200] Training loss: 0.01833434
[100/200] Training loss: 0.01475795
[150/200] Training loss: 0.01329578
[200/200] Training loss: 0.01268222
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13701.836373274935 ----------
[1/200] Training loss: 0.14935438
[2/200] Training loss: 0.06009104
[3/200] Training loss: 0.04988041
[4/200] Training loss: 0.04632653
[5/200] Training loss: 0.04004875
[6/200] Training loss: 0.04030720
[7/200] Training loss: 0.03502924
[8/200] Training loss: 0.03689469
[9/200] Training loss: 0.03131340
[10/200] Training loss: 0.03104376
[50/200] Training loss: 0.01862524
[100/200] Training loss: 0.01597131
[150/200] Training loss: 0.01385399
[200/200] Training loss: 0.01335309
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9386.583617056847 ----------
[1/200] Training loss: 0.15882330
[2/200] Training loss: 0.06131195
[3/200] Training loss: 0.05131866
[4/200] Training loss: 0.05011696
[5/200] Training loss: 0.04605837
[6/200] Training loss: 0.04350915
[7/200] Training loss: 0.03513578
[8/200] Training loss: 0.03678704
[9/200] Training loss: 0.03348227
[10/200] Training loss: 0.03631706
[50/200] Training loss: 0.01968064
[100/200] Training loss: 0.01517467
[150/200] Training loss: 0.01323367
[200/200] Training loss: 0.01200956
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14639.213640083268 ----------
[1/200] Training loss: 0.17380126
[2/200] Training loss: 0.05943103
[3/200] Training loss: 0.05209251
[4/200] Training loss: 0.04687553
[5/200] Training loss: 0.04552581
[6/200] Training loss: 0.04405167
[7/200] Training loss: 0.03877432
[8/200] Training loss: 0.03652097
[9/200] Training loss: 0.03478686
[10/200] Training loss: 0.03122751
[50/200] Training loss: 0.01872256
[100/200] Training loss: 0.01671988
[150/200] Training loss: 0.01520916
[200/200] Training loss: 0.01379452
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19200.281664600654 ----------
[1/200] Training loss: 0.17554279
[2/200] Training loss: 0.05639463
[3/200] Training loss: 0.05018584
[4/200] Training loss: 0.04744304
[5/200] Training loss: 0.04367727
[6/200] Training loss: 0.04088523
[7/200] Training loss: 0.03875843
[8/200] Training loss: 0.03746878
[9/200] Training loss: 0.03266263
[10/200] Training loss: 0.03155717
[50/200] Training loss: 0.01577740
[100/200] Training loss: 0.01365073
[150/200] Training loss: 0.01246696
[200/200] Training loss: 0.01111216
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12418.549995873109 ----------
[1/200] Training loss: 0.13517428
[2/200] Training loss: 0.05306032
[3/200] Training loss: 0.05083120
[4/200] Training loss: 0.04062133
[5/200] Training loss: 0.04203705
[6/200] Training loss: 0.03788484
[7/200] Training loss: 0.03477447
[8/200] Training loss: 0.03435569
[9/200] Training loss: 0.03414197
[10/200] Training loss: 0.03021266
[50/200] Training loss: 0.01727795
[100/200] Training loss: 0.01467484
[150/200] Training loss: 0.01288649
[200/200] Training loss: 0.01269730
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9171.614906874362 ----------
[1/200] Training loss: 0.17388782
[2/200] Training loss: 0.06655136
[3/200] Training loss: 0.05402272
[4/200] Training loss: 0.05215732
[5/200] Training loss: 0.04783525
[6/200] Training loss: 0.04675477
[7/200] Training loss: 0.04145103
[8/200] Training loss: 0.03966566
[9/200] Training loss: 0.03614523
[10/200] Training loss: 0.03383969
[50/200] Training loss: 0.02048332
[100/200] Training loss: 0.01781290
[150/200] Training loss: 0.01583579
[200/200] Training loss: 0.01469350
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11275.432053806187 ----------
[1/200] Training loss: 0.17193546
[2/200] Training loss: 0.05587296
[3/200] Training loss: 0.05140177
[4/200] Training loss: 0.04736236
[5/200] Training loss: 0.04574378
[6/200] Training loss: 0.04433357
[7/200] Training loss: 0.03988275
[8/200] Training loss: 0.03815515
[9/200] Training loss: 0.03577908
[10/200] Training loss: 0.03529004
[50/200] Training loss: 0.01822779
[100/200] Training loss: 0.01599088
[150/200] Training loss: 0.01484496
[200/200] Training loss: 0.01368148
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10656.563048187722 ----------
[1/200] Training loss: 0.16875158
[2/200] Training loss: 0.06100397
[3/200] Training loss: 0.04973311
[4/200] Training loss: 0.04845262
[5/200] Training loss: 0.04472457
[6/200] Training loss: 0.04193767
[7/200] Training loss: 0.04091815
[8/200] Training loss: 0.04065573
[9/200] Training loss: 0.03818594
[10/200] Training loss: 0.03368654
[50/200] Training loss: 0.01801591
[100/200] Training loss: 0.01506603
[150/200] Training loss: 0.01356823
[200/200] Training loss: 0.01227667
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15113.064546940835 ----------
[1/200] Training loss: 0.15812771
[2/200] Training loss: 0.05761196
[3/200] Training loss: 0.05233012
[4/200] Training loss: 0.05047499
[5/200] Training loss: 0.04376906
[6/200] Training loss: 0.04224886
[7/200] Training loss: 0.04042536
[8/200] Training loss: 0.04205547
[9/200] Training loss: 0.03687825
[10/200] Training loss: 0.03231411
[50/200] Training loss: 0.01800938
[100/200] Training loss: 0.01492582
[150/200] Training loss: 0.01352009
[200/200] Training loss: 0.01281175
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23099.29730532944 ----------
[1/200] Training loss: 0.16833465
[2/200] Training loss: 0.05830595
[3/200] Training loss: 0.05102387
[4/200] Training loss: 0.04803676
[5/200] Training loss: 0.04986336
[6/200] Training loss: 0.04252161
[7/200] Training loss: 0.04056939
[8/200] Training loss: 0.03949577
[9/200] Training loss: 0.03775045
[10/200] Training loss: 0.03543421
[50/200] Training loss: 0.01786470
[100/200] Training loss: 0.01666537
[150/200] Training loss: 0.01499614
[200/200] Training loss: 0.01360038
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15112.04711480215 ----------
[1/200] Training loss: 0.15712512
[2/200] Training loss: 0.05798007
[3/200] Training loss: 0.05843590
[4/200] Training loss: 0.05267345
[5/200] Training loss: 0.04830698
[6/200] Training loss: 0.04875505
[7/200] Training loss: 0.04617867
[8/200] Training loss: 0.04089583
[9/200] Training loss: 0.04160834
[10/200] Training loss: 0.03779522
[50/200] Training loss: 0.01841766
[100/200] Training loss: 0.01522867
[150/200] Training loss: 0.01348227
[200/200] Training loss: 0.01268995
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11812.79035621982 ----------
[1/200] Training loss: 0.09316227
[2/200] Training loss: 0.05026241
[3/200] Training loss: 0.04548479
[4/200] Training loss: 0.03808233
[5/200] Training loss: 0.03201782
[6/200] Training loss: 0.03022951
[7/200] Training loss: 0.02775909
[8/200] Training loss: 0.02668423
[9/200] Training loss: 0.02469337
[10/200] Training loss: 0.02404718
[50/200] Training loss: 0.01531383
[100/200] Training loss: 0.01326352
[150/200] Training loss: 0.01189083
[200/200] Training loss: 0.01108236
---batch_size---: 4 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11077.664374767815 ----------
[1/200] Training loss: 0.17866622
[2/200] Training loss: 0.06179752
[3/200] Training loss: 0.05212740
[4/200] Training loss: 0.05340999
[5/200] Training loss: 0.04778066
[6/200] Training loss: 0.04739377
[7/200] Training loss: 0.04051322
[8/200] Training loss: 0.03976621
[9/200] Training loss: 0.03768357
[10/200] Training loss: 0.03598806
[50/200] Training loss: 0.01697490
[100/200] Training loss: 0.01450492
[150/200] Training loss: 0.01308990
[200/200] Training loss: 0.01123636
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6469.826891038121 ----------
[1/200] Training loss: 0.16309489
[2/200] Training loss: 0.05716383
[3/200] Training loss: 0.05013351
[4/200] Training loss: 0.04857103
[5/200] Training loss: 0.04707349
[6/200] Training loss: 0.04204433
[7/200] Training loss: 0.04012188
[8/200] Training loss: 0.03910208
[9/200] Training loss: 0.03523485
[10/200] Training loss: 0.03376900
[50/200] Training loss: 0.01714999
[100/200] Training loss: 0.01422221
[150/200] Training loss: 0.01315286
[200/200] Training loss: 0.01188428
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16952.49173425547 ----------
[1/200] Training loss: 0.17546147
[2/200] Training loss: 0.05736137
[3/200] Training loss: 0.05444129
[4/200] Training loss: 0.04707737
[5/200] Training loss: 0.04502238
[6/200] Training loss: 0.04090648
[7/200] Training loss: 0.03913052
[8/200] Training loss: 0.03804221
[9/200] Training loss: 0.03258794
[10/200] Training loss: 0.03165000
[50/200] Training loss: 0.01813404
[100/200] Training loss: 0.01541658
[150/200] Training loss: 0.01333969
[200/200] Training loss: 0.01236970
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16557.024853517614 ----------
[1/200] Training loss: 0.14064255
[2/200] Training loss: 0.05380513
[3/200] Training loss: 0.04814891
[4/200] Training loss: 0.04825524
[5/200] Training loss: 0.04674864
[6/200] Training loss: 0.04389999
[7/200] Training loss: 0.03866963
[8/200] Training loss: 0.03714822
[9/200] Training loss: 0.03454281
[10/200] Training loss: 0.03364561
[50/200] Training loss: 0.01792838
[100/200] Training loss: 0.01531868
[150/200] Training loss: 0.01468544
[200/200] Training loss: 0.01290206
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10941.467908831977 ----------
[1/200] Training loss: 0.15503817
[2/200] Training loss: 0.06121466
[3/200] Training loss: 0.05656853
[4/200] Training loss: 0.05109926
[5/200] Training loss: 0.05011582
[6/200] Training loss: 0.04835649
[7/200] Training loss: 0.04547587
[8/200] Training loss: 0.04699673
[9/200] Training loss: 0.04256290
[10/200] Training loss: 0.04069621
[50/200] Training loss: 0.01845900
[100/200] Training loss: 0.01598296
[150/200] Training loss: 0.01392942
[200/200] Training loss: 0.01323440
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19453.096000379992 ----------
[1/200] Training loss: 0.17494589
[2/200] Training loss: 0.05745229
[3/200] Training loss: 0.05364769
[4/200] Training loss: 0.04701431
[5/200] Training loss: 0.04462514
[6/200] Training loss: 0.04204856
[7/200] Training loss: 0.03757907
[8/200] Training loss: 0.03773680
[9/200] Training loss: 0.03692200
[10/200] Training loss: 0.03460649
[50/200] Training loss: 0.01762303
[100/200] Training loss: 0.01573074
[150/200] Training loss: 0.01485053
[200/200] Training loss: 0.01306045
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15331.942081810772 ----------
[1/200] Training loss: 0.18126246
[2/200] Training loss: 0.05732322
[3/200] Training loss: 0.05295071
[4/200] Training loss: 0.04812019
[5/200] Training loss: 0.04892607
[6/200] Training loss: 0.04543799
[7/200] Training loss: 0.04346841
[8/200] Training loss: 0.04101774
[9/200] Training loss: 0.04071233
[10/200] Training loss: 0.03968385
[50/200] Training loss: 0.01894994
[100/200] Training loss: 0.01458625
[150/200] Training loss: 0.01326555
[200/200] Training loss: 0.01285473
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6866.457601995369 ----------
[1/200] Training loss: 0.16284854
[2/200] Training loss: 0.05751477
[3/200] Training loss: 0.04638102
[4/200] Training loss: 0.04352487
[5/200] Training loss: 0.04006436
[6/200] Training loss: 0.04017139
[7/200] Training loss: 0.03811842
[8/200] Training loss: 0.03471579
[9/200] Training loss: 0.03304223
[10/200] Training loss: 0.03054976
[50/200] Training loss: 0.01821524
[100/200] Training loss: 0.01459432
[150/200] Training loss: 0.01429580
[200/200] Training loss: 0.01261160
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6482.585595269838 ----------
[1/200] Training loss: 0.17340425
[2/200] Training loss: 0.06060175
[3/200] Training loss: 0.05467288
[4/200] Training loss: 0.05092644
[5/200] Training loss: 0.04721940
[6/200] Training loss: 0.04700544
[7/200] Training loss: 0.04358683
[8/200] Training loss: 0.04639687
[9/200] Training loss: 0.03961139
[10/200] Training loss: 0.03781873
[50/200] Training loss: 0.01724742
[100/200] Training loss: 0.01487855
[150/200] Training loss: 0.01288436
[200/200] Training loss: 0.01156462
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9707.586723794951 ----------
[1/200] Training loss: 0.14323833
[2/200] Training loss: 0.05611109
[3/200] Training loss: 0.05082842
[4/200] Training loss: 0.04585903
[5/200] Training loss: 0.04088530
[6/200] Training loss: 0.03668360
[7/200] Training loss: 0.03365142
[8/200] Training loss: 0.03105126
[9/200] Training loss: 0.03044908
[10/200] Training loss: 0.03074959
[50/200] Training loss: 0.01758988
[100/200] Training loss: 0.01553360
[150/200] Training loss: 0.01388981
[200/200] Training loss: 0.01311164
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8531.540071991692 ----------
[1/200] Training loss: 0.18287494
[2/200] Training loss: 0.05897054
[3/200] Training loss: 0.05170878
[4/200] Training loss: 0.04565333
[5/200] Training loss: 0.04065731
[6/200] Training loss: 0.03902674
[7/200] Training loss: 0.03869895
[8/200] Training loss: 0.03410781
[9/200] Training loss: 0.03290886
[10/200] Training loss: 0.02919900
[50/200] Training loss: 0.01933642
[100/200] Training loss: 0.01662099
[150/200] Training loss: 0.01491414
[200/200] Training loss: 0.01398753
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19001.97968633795 ----------
[1/200] Training loss: 0.14531401
[2/200] Training loss: 0.06005174
[3/200] Training loss: 0.05307721
[4/200] Training loss: 0.05104807
[5/200] Training loss: 0.05002925
[6/200] Training loss: 0.04714672
[7/200] Training loss: 0.04543167
[8/200] Training loss: 0.04610166
[9/200] Training loss: 0.04051695
[10/200] Training loss: 0.03943791
[50/200] Training loss: 0.01738251
[100/200] Training loss: 0.01518249
[150/200] Training loss: 0.01432346
[200/200] Training loss: 0.01317153
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 3068.896870212487 ----------
[1/200] Training loss: 0.17518295
[2/200] Training loss: 0.06306226
[3/200] Training loss: 0.05211415
[4/200] Training loss: 0.04992336
[5/200] Training loss: 0.04774662
[6/200] Training loss: 0.04712979
[7/200] Training loss: 0.04433982
[8/200] Training loss: 0.04227099
[9/200] Training loss: 0.04001419
[10/200] Training loss: 0.03883402
[50/200] Training loss: 0.01822292
[100/200] Training loss: 0.01539588
[150/200] Training loss: 0.01389355
[200/200] Training loss: 0.01303600
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16288.208250142186 ----------
[1/200] Training loss: 0.14722260
[2/200] Training loss: 0.06151206
[3/200] Training loss: 0.05364745
[4/200] Training loss: 0.04821820
[5/200] Training loss: 0.04454005
[6/200] Training loss: 0.04185045
[7/200] Training loss: 0.04170059
[8/200] Training loss: 0.04014022
[9/200] Training loss: 0.03571567
[10/200] Training loss: 0.03319090
[50/200] Training loss: 0.01751242
[100/200] Training loss: 0.01339836
[150/200] Training loss: 0.01175103
[200/200] Training loss: 0.01146628
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10922.323562319512 ----------
[1/200] Training loss: 0.16078925
[2/200] Training loss: 0.05207378
[3/200] Training loss: 0.04953284
[4/200] Training loss: 0.04808510
[5/200] Training loss: 0.04497195
[6/200] Training loss: 0.04210101
[7/200] Training loss: 0.03770307
[8/200] Training loss: 0.03762489
[9/200] Training loss: 0.03452771
[10/200] Training loss: 0.03658493
[50/200] Training loss: 0.01856091
[100/200] Training loss: 0.01461628
[150/200] Training loss: 0.01339972
[200/200] Training loss: 0.01176505
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9222.688545104405 ----------
[1/200] Training loss: 0.15934249
[2/200] Training loss: 0.05717268
[3/200] Training loss: 0.05079414
[4/200] Training loss: 0.04692423
[5/200] Training loss: 0.04232106
[6/200] Training loss: 0.04270186
[7/200] Training loss: 0.03823768
[8/200] Training loss: 0.03475498
[9/200] Training loss: 0.03290895
[10/200] Training loss: 0.03517383
[50/200] Training loss: 0.01868023
[100/200] Training loss: 0.01521842
[150/200] Training loss: 0.01376804
[200/200] Training loss: 0.01289786
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19762.628165302307 ----------
[1/200] Training loss: 0.18719257
[2/200] Training loss: 0.06395241
[3/200] Training loss: 0.05609029
[4/200] Training loss: 0.05247560
[5/200] Training loss: 0.04865548
[6/200] Training loss: 0.04788704
[7/200] Training loss: 0.04628735
[8/200] Training loss: 0.04301949
[9/200] Training loss: 0.04062342
[10/200] Training loss: 0.03927286
[50/200] Training loss: 0.01848893
[100/200] Training loss: 0.01558615
[150/200] Training loss: 0.01462287
[200/200] Training loss: 0.01329375
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 24018.95118442935 ----------
[1/200] Training loss: 0.18196025
[2/200] Training loss: 0.06335818
[3/200] Training loss: 0.05711783
[4/200] Training loss: 0.05513070
[5/200] Training loss: 0.05266695
[6/200] Training loss: 0.05135441
[7/200] Training loss: 0.04825081
[8/200] Training loss: 0.04793889
[9/200] Training loss: 0.04574713
[10/200] Training loss: 0.04146297
[50/200] Training loss: 0.01905130
[100/200] Training loss: 0.01595030
[150/200] Training loss: 0.01488715
[200/200] Training loss: 0.01357732
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21198.72599945572 ----------
[1/200] Training loss: 0.20361970
[2/200] Training loss: 0.06589604
[3/200] Training loss: 0.05446251
[4/200] Training loss: 0.05302291
[5/200] Training loss: 0.04652187
[6/200] Training loss: 0.04422930
[7/200] Training loss: 0.04237470
[8/200] Training loss: 0.04083192
[9/200] Training loss: 0.04081983
[10/200] Training loss: 0.03590351
[50/200] Training loss: 0.01921712
[100/200] Training loss: 0.01535550
[150/200] Training loss: 0.01453169
[200/200] Training loss: 0.01333322
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 3389.0562108055983 ----------
[1/200] Training loss: 0.17521815
[2/200] Training loss: 0.06585580
[3/200] Training loss: 0.05655500
[4/200] Training loss: 0.05312747
[5/200] Training loss: 0.05351927
[6/200] Training loss: 0.04927089
[7/200] Training loss: 0.04734752
[8/200] Training loss: 0.04504969
[9/200] Training loss: 0.04325792
[10/200] Training loss: 0.03967996
[50/200] Training loss: 0.01930450
[100/200] Training loss: 0.01624761
[150/200] Training loss: 0.01464122
[200/200] Training loss: 0.01303036
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22318.331837303613 ----------
[1/200] Training loss: 0.16101599
[2/200] Training loss: 0.06618743
[3/200] Training loss: 0.05472572
[4/200] Training loss: 0.05021064
[5/200] Training loss: 0.04490395
[6/200] Training loss: 0.04436196
[7/200] Training loss: 0.03897737
[8/200] Training loss: 0.03734781
[9/200] Training loss: 0.03338490
[10/200] Training loss: 0.03411365
[50/200] Training loss: 0.01818216
[100/200] Training loss: 0.01623340
[150/200] Training loss: 0.01578957
[200/200] Training loss: 0.01371555
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8164.186181120565 ----------
[1/200] Training loss: 0.16135090
[2/200] Training loss: 0.06043742
[3/200] Training loss: 0.05751170
[4/200] Training loss: 0.05295701
[5/200] Training loss: 0.05029907
[6/200] Training loss: 0.04872895
[7/200] Training loss: 0.04472488
[8/200] Training loss: 0.04487694
[9/200] Training loss: 0.04067629
[10/200] Training loss: 0.04224208
[50/200] Training loss: 0.01653245
[100/200] Training loss: 0.01372345
[150/200] Training loss: 0.01317175
[200/200] Training loss: 0.01224381
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19245.601679344814 ----------
[1/200] Training loss: 0.16530995
[2/200] Training loss: 0.06270780
[3/200] Training loss: 0.05183246
[4/200] Training loss: 0.05093437
[5/200] Training loss: 0.04710846
[6/200] Training loss: 0.04263286
[7/200] Training loss: 0.04285799
[8/200] Training loss: 0.03960903
[9/200] Training loss: 0.03960435
[10/200] Training loss: 0.03487158
[50/200] Training loss: 0.02078462
[100/200] Training loss: 0.01576611
[150/200] Training loss: 0.01361970
[200/200] Training loss: 0.01214388
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8135.792278567589 ----------
[1/200] Training loss: 0.16968039
[2/200] Training loss: 0.05757161
[3/200] Training loss: 0.04579917
[4/200] Training loss: 0.04818383
[5/200] Training loss: 0.04090146
[6/200] Training loss: 0.03811276
[7/200] Training loss: 0.03531054
[8/200] Training loss: 0.03336812
[9/200] Training loss: 0.03054768
[10/200] Training loss: 0.03159839
[50/200] Training loss: 0.01846122
[100/200] Training loss: 0.01567082
[150/200] Training loss: 0.01440043
[200/200] Training loss: 0.01348916
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7745.216071873011 ----------
[1/200] Training loss: 0.16236285
[2/200] Training loss: 0.05434822
[3/200] Training loss: 0.04959692
[4/200] Training loss: 0.04749345
[5/200] Training loss: 0.04629142
[6/200] Training loss: 0.04386596
[7/200] Training loss: 0.03994124
[8/200] Training loss: 0.03952805
[9/200] Training loss: 0.03564840
[10/200] Training loss: 0.03438702
[50/200] Training loss: 0.01881003
[100/200] Training loss: 0.01456787
[150/200] Training loss: 0.01250469
[200/200] Training loss: 0.01133014
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10394.9680134188 ----------
[1/200] Training loss: 0.18269249
[2/200] Training loss: 0.06141289
[3/200] Training loss: 0.05301809
[4/200] Training loss: 0.04980255
[5/200] Training loss: 0.04965937
[6/200] Training loss: 0.04662895
[7/200] Training loss: 0.04203817
[8/200] Training loss: 0.04204561
[9/200] Training loss: 0.03995333
[10/200] Training loss: 0.03656197
[50/200] Training loss: 0.01809203
[100/200] Training loss: 0.01485715
[150/200] Training loss: 0.01323950
[200/200] Training loss: 0.01257681
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8650.966651189912 ----------
[1/200] Training loss: 0.17749463
[2/200] Training loss: 0.06235407
[3/200] Training loss: 0.05107012
[4/200] Training loss: 0.05008085
[5/200] Training loss: 0.04876354
[6/200] Training loss: 0.04670753
[7/200] Training loss: 0.04276119
[8/200] Training loss: 0.04055742
[9/200] Training loss: 0.03718354
[10/200] Training loss: 0.03796865
[50/200] Training loss: 0.01734792
[100/200] Training loss: 0.01432340
[150/200] Training loss: 0.01235603
[200/200] Training loss: 0.01192489
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13867.081019450345 ----------
[1/200] Training loss: 0.18739580
[2/200] Training loss: 0.05879150
[3/200] Training loss: 0.05207943
[4/200] Training loss: 0.05070694
[5/200] Training loss: 0.04478924
[6/200] Training loss: 0.04335465
[7/200] Training loss: 0.04339351
[8/200] Training loss: 0.04093656
[9/200] Training loss: 0.03764173
[10/200] Training loss: 0.03854417
[50/200] Training loss: 0.01868393
[100/200] Training loss: 0.01635686
[150/200] Training loss: 0.01478087
[200/200] Training loss: 0.01340163
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17317.84097397825 ----------
[1/200] Training loss: 0.17511120
[2/200] Training loss: 0.05381886
[3/200] Training loss: 0.05059634
[4/200] Training loss: 0.04526298
[5/200] Training loss: 0.03742694
[6/200] Training loss: 0.03672124
[7/200] Training loss: 0.03220640
[8/200] Training loss: 0.03291399
[9/200] Training loss: 0.02965535
[10/200] Training loss: 0.02965627
[50/200] Training loss: 0.01807135
[100/200] Training loss: 0.01572669
[150/200] Training loss: 0.01469246
[200/200] Training loss: 0.01339000
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14180.823107281185 ----------
[1/200] Training loss: 0.18156958
[2/200] Training loss: 0.06706659
[3/200] Training loss: 0.05682470
[4/200] Training loss: 0.05528538
[5/200] Training loss: 0.05185908
[6/200] Training loss: 0.05010280
[7/200] Training loss: 0.05111914
[8/200] Training loss: 0.04789632
[9/200] Training loss: 0.04814078
[10/200] Training loss: 0.04480118
[50/200] Training loss: 0.01902227
[100/200] Training loss: 0.01498350
[150/200] Training loss: 0.01323622
[200/200] Training loss: 0.01247551
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13332.950461169501 ----------
[1/200] Training loss: 0.13669885
[2/200] Training loss: 0.05360283
[3/200] Training loss: 0.05171583
[4/200] Training loss: 0.05064983
[5/200] Training loss: 0.04600174
[6/200] Training loss: 0.04265193
[7/200] Training loss: 0.04206413
[8/200] Training loss: 0.03949068
[9/200] Training loss: 0.03688796
[10/200] Training loss: 0.03453449
[50/200] Training loss: 0.01946538
[100/200] Training loss: 0.01652016
[150/200] Training loss: 0.01501160
[200/200] Training loss: 0.01282526
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18858.402053196343 ----------
[1/200] Training loss: 0.16508374
[2/200] Training loss: 0.05875442
[3/200] Training loss: 0.05438220
[4/200] Training loss: 0.04981906
[5/200] Training loss: 0.04684946
[6/200] Training loss: 0.04359619
[7/200] Training loss: 0.04074047
[8/200] Training loss: 0.03555111
[9/200] Training loss: 0.03486955
[10/200] Training loss: 0.03324671
[50/200] Training loss: 0.01940375
[100/200] Training loss: 0.01547754
[150/200] Training loss: 0.01319980
[200/200] Training loss: 0.01187549
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7917.813334500884 ----------
[1/200] Training loss: 0.15598611
[2/200] Training loss: 0.05903603
[3/200] Training loss: 0.05067065
[4/200] Training loss: 0.04718956
[5/200] Training loss: 0.04733878
[6/200] Training loss: 0.04321908
[7/200] Training loss: 0.03750165
[8/200] Training loss: 0.03522339
[9/200] Training loss: 0.02993094
[10/200] Training loss: 0.03070019
[50/200] Training loss: 0.01647204
[100/200] Training loss: 0.01311714
[150/200] Training loss: 0.01178584
[200/200] Training loss: 0.01160272
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12292.680098334944 ----------
[1/200] Training loss: 0.15719411
[2/200] Training loss: 0.05672368
[3/200] Training loss: 0.05268788
[4/200] Training loss: 0.04767503
[5/200] Training loss: 0.04660286
[6/200] Training loss: 0.04342634
[7/200] Training loss: 0.04273451
[8/200] Training loss: 0.03972232
[9/200] Training loss: 0.03697746
[10/200] Training loss: 0.03288042
[50/200] Training loss: 0.01763091
[100/200] Training loss: 0.01585946
[150/200] Training loss: 0.01330035
[200/200] Training loss: 0.01182851
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14159.803953445118 ----------
[1/200] Training loss: 0.12351891
[2/200] Training loss: 0.04935909
[3/200] Training loss: 0.04602004
[4/200] Training loss: 0.04064140
[5/200] Training loss: 0.03973194
[6/200] Training loss: 0.04007250
[7/200] Training loss: 0.03408094
[8/200] Training loss: 0.03416198
[9/200] Training loss: 0.03387239
[10/200] Training loss: 0.03305550
[50/200] Training loss: 0.01647619
[100/200] Training loss: 0.01355835
[150/200] Training loss: 0.01192180
[200/200] Training loss: 0.01111727
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11959.312354813717 ----------
[1/200] Training loss: 0.14216490
[2/200] Training loss: 0.05799654
[3/200] Training loss: 0.05011424
[4/200] Training loss: 0.04720378
[5/200] Training loss: 0.04579018
[6/200] Training loss: 0.04528500
[7/200] Training loss: 0.04330499
[8/200] Training loss: 0.03817099
[9/200] Training loss: 0.03746938
[10/200] Training loss: 0.03204143
[50/200] Training loss: 0.01670150
[100/200] Training loss: 0.01384640
[150/200] Training loss: 0.01282243
[200/200] Training loss: 0.01133049
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6711.721984707055 ----------
[1/200] Training loss: 0.13548415
[2/200] Training loss: 0.05997900
[3/200] Training loss: 0.04693748
[4/200] Training loss: 0.04908039
[5/200] Training loss: 0.04435383
[6/200] Training loss: 0.04446420
[7/200] Training loss: 0.03822298
[8/200] Training loss: 0.03575044
[9/200] Training loss: 0.03409567
[10/200] Training loss: 0.03297759
[50/200] Training loss: 0.01803537
[100/200] Training loss: 0.01401556
[150/200] Training loss: 0.01354477
[200/200] Training loss: 0.01270744
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10618.465049149054 ----------
[1/200] Training loss: 0.15283354
[2/200] Training loss: 0.05821661
[3/200] Training loss: 0.05013392
[4/200] Training loss: 0.04398668
[5/200] Training loss: 0.04042269
[6/200] Training loss: 0.03882324
[7/200] Training loss: 0.03394158
[8/200] Training loss: 0.03273731
[9/200] Training loss: 0.03132425
[10/200] Training loss: 0.02663702
[50/200] Training loss: 0.01937927
[100/200] Training loss: 0.01720068
[150/200] Training loss: 0.01559616
[200/200] Training loss: 0.01573945
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11652.522473696414 ----------
[1/200] Training loss: 0.15796913
[2/200] Training loss: 0.06208486
[3/200] Training loss: 0.05266322
[4/200] Training loss: 0.05412484
[5/200] Training loss: 0.04690062
[6/200] Training loss: 0.04338322
[7/200] Training loss: 0.03863287
[8/200] Training loss: 0.03714358
[9/200] Training loss: 0.03033960
[10/200] Training loss: 0.03038682
[50/200] Training loss: 0.01883383
[100/200] Training loss: 0.01643296
[150/200] Training loss: 0.01579936
[200/200] Training loss: 0.01534480
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13069.23838637891 ----------
[1/200] Training loss: 0.19757082
[2/200] Training loss: 0.06593036
[3/200] Training loss: 0.05617694
[4/200] Training loss: 0.05108064
[5/200] Training loss: 0.05035674
[6/200] Training loss: 0.04561110
[7/200] Training loss: 0.04574018
[8/200] Training loss: 0.04016957
[9/200] Training loss: 0.04168375
[10/200] Training loss: 0.03766631
[50/200] Training loss: 0.01846305
[100/200] Training loss: 0.01575286
[150/200] Training loss: 0.01474325
[200/200] Training loss: 0.01386872
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12074.509348209558 ----------
[1/200] Training loss: 0.17050528
[2/200] Training loss: 0.05963653
[3/200] Training loss: 0.05277161
[4/200] Training loss: 0.04595678
[5/200] Training loss: 0.04466381
[6/200] Training loss: 0.04290090
[7/200] Training loss: 0.03972657
[8/200] Training loss: 0.03779105
[9/200] Training loss: 0.03730801
[10/200] Training loss: 0.03395837
[50/200] Training loss: 0.01676800
[100/200] Training loss: 0.01394661
[150/200] Training loss: 0.01246652
[200/200] Training loss: 0.01195753
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10196.579034166312 ----------
[1/200] Training loss: 0.17198810
[2/200] Training loss: 0.05689328
[3/200] Training loss: 0.05163025
[4/200] Training loss: 0.04696031
[5/200] Training loss: 0.04615907
[6/200] Training loss: 0.03980632
[7/200] Training loss: 0.03712079
[8/200] Training loss: 0.03716318
[9/200] Training loss: 0.03530419
[10/200] Training loss: 0.03279887
[50/200] Training loss: 0.01668338
[100/200] Training loss: 0.01440809
[150/200] Training loss: 0.01296015
[200/200] Training loss: 0.01247411
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14356.628573589274 ----------
[1/200] Training loss: 0.16890764
[2/200] Training loss: 0.05674373
[3/200] Training loss: 0.05232294
[4/200] Training loss: 0.04986214
[5/200] Training loss: 0.04686637
[6/200] Training loss: 0.04215528
[7/200] Training loss: 0.03612606
[8/200] Training loss: 0.03653200
[9/200] Training loss: 0.03593167
[10/200] Training loss: 0.03210220
[50/200] Training loss: 0.01840211
[100/200] Training loss: 0.01499434
[150/200] Training loss: 0.01366978
[200/200] Training loss: 0.01263545
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14409.972657850534 ----------
[1/200] Training loss: 0.14802213
[2/200] Training loss: 0.05916524
[3/200] Training loss: 0.05258020
[4/200] Training loss: 0.04747774
[5/200] Training loss: 0.04467282
[6/200] Training loss: 0.04064437
[7/200] Training loss: 0.04012465
[8/200] Training loss: 0.03434348
[9/200] Training loss: 0.03487500
[10/200] Training loss: 0.03237787
[50/200] Training loss: 0.01888950
[100/200] Training loss: 0.01506266
[150/200] Training loss: 0.01382502
[200/200] Training loss: 0.01282685
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13870.512607686855 ----------
[1/200] Training loss: 0.17602470
[2/200] Training loss: 0.05884417
[3/200] Training loss: 0.05437550
[4/200] Training loss: 0.04920275
[5/200] Training loss: 0.04861724
[6/200] Training loss: 0.04412587
[7/200] Training loss: 0.04088658
[8/200] Training loss: 0.03730731
[9/200] Training loss: 0.03973404
[10/200] Training loss: 0.03527454
[50/200] Training loss: 0.01970405
[100/200] Training loss: 0.01559539
[150/200] Training loss: 0.01309781
[200/200] Training loss: 0.01283864
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11228.047025195432 ----------
[1/200] Training loss: 0.15517413
[2/200] Training loss: 0.05450019
[3/200] Training loss: 0.05472296
[4/200] Training loss: 0.04655965
[5/200] Training loss: 0.04396109
[6/200] Training loss: 0.04069861
[7/200] Training loss: 0.03944024
[8/200] Training loss: 0.03626019
[9/200] Training loss: 0.03409454
[10/200] Training loss: 0.03166269
[50/200] Training loss: 0.01769414
[100/200] Training loss: 0.01513286
[150/200] Training loss: 0.01453664
[200/200] Training loss: 0.01359179
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16992.16760745962 ----------
[1/200] Training loss: 0.18712424
[2/200] Training loss: 0.06267236
[3/200] Training loss: 0.05827469
[4/200] Training loss: 0.05298216
[5/200] Training loss: 0.05388118
[6/200] Training loss: 0.05012269
[7/200] Training loss: 0.04740466
[8/200] Training loss: 0.04776244
[9/200] Training loss: 0.04579464
[10/200] Training loss: 0.04019033
[50/200] Training loss: 0.01851124
[100/200] Training loss: 0.01597192
[150/200] Training loss: 0.01396242
[200/200] Training loss: 0.01346847
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15305.435374402128 ----------
[1/200] Training loss: 0.15093696
[2/200] Training loss: 0.05845450
[3/200] Training loss: 0.05069492
[4/200] Training loss: 0.04777003
[5/200] Training loss: 0.04908968
[6/200] Training loss: 0.04318723
[7/200] Training loss: 0.04311830
[8/200] Training loss: 0.03569439
[9/200] Training loss: 0.03530349
[10/200] Training loss: 0.03360089
[50/200] Training loss: 0.01732133
[100/200] Training loss: 0.01383771
[150/200] Training loss: 0.01187207
[200/200] Training loss: 0.01161744
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21010.76600221896 ----------
[1/200] Training loss: 0.18290549
[2/200] Training loss: 0.06097657
[3/200] Training loss: 0.05842450
[4/200] Training loss: 0.05185102
[5/200] Training loss: 0.05064012
[6/200] Training loss: 0.04465772
[7/200] Training loss: 0.04543820
[8/200] Training loss: 0.04460796
[9/200] Training loss: 0.03784336
[10/200] Training loss: 0.03807233
[50/200] Training loss: 0.01990406
[100/200] Training loss: 0.01652447
[150/200] Training loss: 0.01479297
[200/200] Training loss: 0.01311507
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20502.55632841915 ----------
[1/200] Training loss: 0.16000353
[2/200] Training loss: 0.05753428
[3/200] Training loss: 0.04802311
[4/200] Training loss: 0.04359439
[5/200] Training loss: 0.04052699
[6/200] Training loss: 0.04191493
[7/200] Training loss: 0.03682774
[8/200] Training loss: 0.03741688
[9/200] Training loss: 0.03592115
[10/200] Training loss: 0.03401765
[50/200] Training loss: 0.01748030
[100/200] Training loss: 0.01482513
[150/200] Training loss: 0.01300577
[200/200] Training loss: 0.01301950
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11546.171313470106 ----------
[1/200] Training loss: 0.17147674
[2/200] Training loss: 0.05867720
[3/200] Training loss: 0.05350389
[4/200] Training loss: 0.05013330
[5/200] Training loss: 0.04534352
[6/200] Training loss: 0.04481540
[7/200] Training loss: 0.04126946
[8/200] Training loss: 0.04013987
[9/200] Training loss: 0.03651134
[10/200] Training loss: 0.03621291
[50/200] Training loss: 0.01742588
[100/200] Training loss: 0.01512773
[150/200] Training loss: 0.01462662
[200/200] Training loss: 0.01336840
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10787.11527703306 ----------
[1/200] Training loss: 0.16713939
[2/200] Training loss: 0.06039267
[3/200] Training loss: 0.05070183
[4/200] Training loss: 0.04895153
[5/200] Training loss: 0.04427044
[6/200] Training loss: 0.04221904
[7/200] Training loss: 0.03893535
[8/200] Training loss: 0.03729658
[9/200] Training loss: 0.03758360
[10/200] Training loss: 0.03515210
[50/200] Training loss: 0.01907328
[100/200] Training loss: 0.01624307
[150/200] Training loss: 0.01366957
[200/200] Training loss: 0.01321279
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21966.12810670101 ----------
[1/200] Training loss: 0.19279229
[2/200] Training loss: 0.06218211
[3/200] Training loss: 0.05511024
[4/200] Training loss: 0.05119825
[5/200] Training loss: 0.04809963
[6/200] Training loss: 0.04665573
[7/200] Training loss: 0.04107588
[8/200] Training loss: 0.03824572
[9/200] Training loss: 0.03554991
[10/200] Training loss: 0.03224820
[50/200] Training loss: 0.01687213
[100/200] Training loss: 0.01522230
[150/200] Training loss: 0.01318066
[200/200] Training loss: 0.01204844
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11468.21694946516 ----------
[1/200] Training loss: 0.16250545
[2/200] Training loss: 0.05820206
[3/200] Training loss: 0.05515407
[4/200] Training loss: 0.04964170
[5/200] Training loss: 0.05086537
[6/200] Training loss: 0.04802018
[7/200] Training loss: 0.04576803
[8/200] Training loss: 0.04237861
[9/200] Training loss: 0.04223466
[10/200] Training loss: 0.03850328
[50/200] Training loss: 0.01852734
[100/200] Training loss: 0.01621169
[150/200] Training loss: 0.01453381
[200/200] Training loss: 0.01379869
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13605.360120187925 ----------
[1/200] Training loss: 0.15116016
[2/200] Training loss: 0.05690709
[3/200] Training loss: 0.05349799
[4/200] Training loss: 0.05123781
[5/200] Training loss: 0.05010928
[6/200] Training loss: 0.04660773
[7/200] Training loss: 0.04616259
[8/200] Training loss: 0.04422669
[9/200] Training loss: 0.04018546
[10/200] Training loss: 0.03954966
[50/200] Training loss: 0.01915042
[100/200] Training loss: 0.01543354
[150/200] Training loss: 0.01358286
[200/200] Training loss: 0.01237467
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18786.47683840693 ----------
[1/200] Training loss: 0.14177100
[2/200] Training loss: 0.05844314
[3/200] Training loss: 0.05288546
[4/200] Training loss: 0.05034702
[5/200] Training loss: 0.04642864
[6/200] Training loss: 0.04466944
[7/200] Training loss: 0.04334548
[8/200] Training loss: 0.04311062
[9/200] Training loss: 0.03934504
[10/200] Training loss: 0.03838720
[50/200] Training loss: 0.01873214
[100/200] Training loss: 0.01545332
[150/200] Training loss: 0.01478572
[200/200] Training loss: 0.01389649
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23694.27812785188 ----------
[1/200] Training loss: 0.17852882
[2/200] Training loss: 0.05813174
[3/200] Training loss: 0.05515715
[4/200] Training loss: 0.05193360
[5/200] Training loss: 0.04634860
[6/200] Training loss: 0.04400983
[7/200] Training loss: 0.04246750
[8/200] Training loss: 0.03876300
[9/200] Training loss: 0.03541634
[10/200] Training loss: 0.03489945
[50/200] Training loss: 0.01954549
[100/200] Training loss: 0.01568385
[150/200] Training loss: 0.01405757
[200/200] Training loss: 0.01364908
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11578.288992765727 ----------
[1/200] Training loss: 0.15607954
[2/200] Training loss: 0.05743545
[3/200] Training loss: 0.05310383
[4/200] Training loss: 0.04986517
[5/200] Training loss: 0.04273934
[6/200] Training loss: 0.04168950
[7/200] Training loss: 0.03965074
[8/200] Training loss: 0.03749969
[9/200] Training loss: 0.03494384
[10/200] Training loss: 0.03269295
[50/200] Training loss: 0.01643164
[100/200] Training loss: 0.01390785
[150/200] Training loss: 0.01315225
[200/200] Training loss: 0.01221374
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11373.436771706254 ----------
[1/200] Training loss: 0.14009112
[2/200] Training loss: 0.05519188
[3/200] Training loss: 0.04748393
[4/200] Training loss: 0.04738383
[5/200] Training loss: 0.04256550
[6/200] Training loss: 0.04087781
[7/200] Training loss: 0.03768571
[8/200] Training loss: 0.03494907
[9/200] Training loss: 0.03253703
[10/200] Training loss: 0.03230443
[50/200] Training loss: 0.01701346
[100/200] Training loss: 0.01489096
[150/200] Training loss: 0.01387361
[200/200] Training loss: 0.01317373
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13536.050827327741 ----------
[1/200] Training loss: 0.21813979
[2/200] Training loss: 0.06969596
[3/200] Training loss: 0.05708339
[4/200] Training loss: 0.05482792
[5/200] Training loss: 0.05096087
[6/200] Training loss: 0.04951834
[7/200] Training loss: 0.04927077
[8/200] Training loss: 0.04746504
[9/200] Training loss: 0.04524733
[10/200] Training loss: 0.04276633
[50/200] Training loss: 0.01932358
[100/200] Training loss: 0.01654610
[150/200] Training loss: 0.01497527
[200/200] Training loss: 0.01401127
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14131.147441025445 ----------
[1/200] Training loss: 0.17898385
[2/200] Training loss: 0.06089040
[3/200] Training loss: 0.05249108
[4/200] Training loss: 0.04909642
[5/200] Training loss: 0.04995910
[6/200] Training loss: 0.04493344
[7/200] Training loss: 0.04422353
[8/200] Training loss: 0.04097180
[9/200] Training loss: 0.03925198
[10/200] Training loss: 0.03728782
[50/200] Training loss: 0.01965739
[100/200] Training loss: 0.01636522
[150/200] Training loss: 0.01394367
[200/200] Training loss: 0.01373859
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9953.663044326948 ----------
[1/200] Training loss: 0.15857277
[2/200] Training loss: 0.05539554
[3/200] Training loss: 0.05238341
[4/200] Training loss: 0.04614115
[5/200] Training loss: 0.04284954
[6/200] Training loss: 0.04109937
[7/200] Training loss: 0.03679160
[8/200] Training loss: 0.03632604
[9/200] Training loss: 0.03257884
[10/200] Training loss: 0.03363591
[50/200] Training loss: 0.01804933
[100/200] Training loss: 0.01563675
[150/200] Training loss: 0.01452702
[200/200] Training loss: 0.01359814
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8163.346617656266 ----------
[1/200] Training loss: 0.19228846
[2/200] Training loss: 0.06198105
[3/200] Training loss: 0.05783342
[4/200] Training loss: 0.05267122
[5/200] Training loss: 0.04947388
[6/200] Training loss: 0.04770289
[7/200] Training loss: 0.04316558
[8/200] Training loss: 0.04353184
[9/200] Training loss: 0.03904803
[10/200] Training loss: 0.03525238
[50/200] Training loss: 0.01843858
[100/200] Training loss: 0.01593422
[150/200] Training loss: 0.01538568
[200/200] Training loss: 0.01359638
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15676.053074674122 ----------
[1/200] Training loss: 0.14699537
[2/200] Training loss: 0.05586174
[3/200] Training loss: 0.05271423
[4/200] Training loss: 0.05011508
[5/200] Training loss: 0.04540107
[6/200] Training loss: 0.04350007
[7/200] Training loss: 0.04321461
[8/200] Training loss: 0.03751954
[9/200] Training loss: 0.03590233
[10/200] Training loss: 0.03429431
[50/200] Training loss: 0.01556159
[100/200] Training loss: 0.01322743
[150/200] Training loss: 0.01219952
[200/200] Training loss: 0.01138326
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23974.734701347585 ----------
[1/200] Training loss: 0.17109850
[2/200] Training loss: 0.05565061
[3/200] Training loss: 0.04853104
[4/200] Training loss: 0.04622365
[5/200] Training loss: 0.04180841
[6/200] Training loss: 0.03735994
[7/200] Training loss: 0.03570976
[8/200] Training loss: 0.03308597
[9/200] Training loss: 0.03247371
[10/200] Training loss: 0.02992658
[50/200] Training loss: 0.01728210
[100/200] Training loss: 0.01376525
[150/200] Training loss: 0.01243209
[200/200] Training loss: 0.01140386
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13930.231297433651 ----------
[1/200] Training loss: 0.17937080
[2/200] Training loss: 0.06542110
[3/200] Training loss: 0.05246074
[4/200] Training loss: 0.05794849
[5/200] Training loss: 0.05297272
[6/200] Training loss: 0.04870862
[7/200] Training loss: 0.04611606
[8/200] Training loss: 0.04315835
[9/200] Training loss: 0.03941059
[10/200] Training loss: 0.03639343
[50/200] Training loss: 0.02021954
[100/200] Training loss: 0.01748118
[150/200] Training loss: 0.01602292
[200/200] Training loss: 0.01516845
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8730.54362568563 ----------
[1/200] Training loss: 0.15749162
[2/200] Training loss: 0.06149657
[3/200] Training loss: 0.05345370
[4/200] Training loss: 0.05061522
[5/200] Training loss: 0.04690287
[6/200] Training loss: 0.04361907
[7/200] Training loss: 0.04066176
[8/200] Training loss: 0.03889223
[9/200] Training loss: 0.03742403
[10/200] Training loss: 0.03478920
[50/200] Training loss: 0.01948809
[100/200] Training loss: 0.01738371
[150/200] Training loss: 0.01631795
[200/200] Training loss: 0.01514039
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16878.275741319077 ----------
[1/200] Training loss: 0.17998637
[2/200] Training loss: 0.05863933
[3/200] Training loss: 0.04867855
[4/200] Training loss: 0.04500597
[5/200] Training loss: 0.04525371
[6/200] Training loss: 0.04116239
[7/200] Training loss: 0.03758146
[8/200] Training loss: 0.03701835
[9/200] Training loss: 0.03448152
[10/200] Training loss: 0.03083332
[50/200] Training loss: 0.01924781
[100/200] Training loss: 0.01561737
[150/200] Training loss: 0.01363796
[200/200] Training loss: 0.01193926
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11307.716657221297 ----------
[1/200] Training loss: 0.16468385
[2/200] Training loss: 0.05952699
[3/200] Training loss: 0.05425810
[4/200] Training loss: 0.04624727
[5/200] Training loss: 0.04477000
[6/200] Training loss: 0.03946210
[7/200] Training loss: 0.03548354
[8/200] Training loss: 0.03476608
[9/200] Training loss: 0.03295924
[10/200] Training loss: 0.03185008
[50/200] Training loss: 0.01657203
[100/200] Training loss: 0.01340149
[150/200] Training loss: 0.01306410
[200/200] Training loss: 0.01180075
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13307.967538283223 ----------
[1/200] Training loss: 0.16261552
[2/200] Training loss: 0.06165421
[3/200] Training loss: 0.05024411
[4/200] Training loss: 0.04537109
[5/200] Training loss: 0.04277873
[6/200] Training loss: 0.04086258
[7/200] Training loss: 0.03588704
[8/200] Training loss: 0.03590552
[9/200] Training loss: 0.03311981
[10/200] Training loss: 0.03179411
[50/200] Training loss: 0.01704149
[100/200] Training loss: 0.01484969
[150/200] Training loss: 0.01341181
[200/200] Training loss: 0.01264891
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 25731.600494333812 ----------
[1/200] Training loss: 0.15984000
[2/200] Training loss: 0.06097408
[3/200] Training loss: 0.05316796
[4/200] Training loss: 0.04718587
[5/200] Training loss: 0.04529663
[6/200] Training loss: 0.04058671
[7/200] Training loss: 0.03566459
[8/200] Training loss: 0.03536851
[9/200] Training loss: 0.03436719
[10/200] Training loss: 0.02961350
[50/200] Training loss: 0.01775959
[100/200] Training loss: 0.01574667
[150/200] Training loss: 0.01356529
[200/200] Training loss: 0.01235287
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13278.207107889228 ----------
[1/200] Training loss: 0.17262134
[2/200] Training loss: 0.06119208
[3/200] Training loss: 0.05224171
[4/200] Training loss: 0.04833035
[5/200] Training loss: 0.04823788
[6/200] Training loss: 0.04575468
[7/200] Training loss: 0.04141950
[8/200] Training loss: 0.04021474
[9/200] Training loss: 0.03682745
[10/200] Training loss: 0.03598191
[50/200] Training loss: 0.01655081
[100/200] Training loss: 0.01454807
[150/200] Training loss: 0.01374540
[200/200] Training loss: 0.01274251
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12263.497054266372 ----------
[1/200] Training loss: 0.14696040
[2/200] Training loss: 0.05642827
[3/200] Training loss: 0.04913874
[4/200] Training loss: 0.04579144
[5/200] Training loss: 0.04335636
[6/200] Training loss: 0.04032596
[7/200] Training loss: 0.03564760
[8/200] Training loss: 0.03410984
[9/200] Training loss: 0.03116641
[10/200] Training loss: 0.03072397
[50/200] Training loss: 0.01800348
[100/200] Training loss: 0.01361882
[150/200] Training loss: 0.01270895
[200/200] Training loss: 0.01146509
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22581.41040767826 ----------
[1/200] Training loss: 0.13839533
[2/200] Training loss: 0.05532151
[3/200] Training loss: 0.05181931
[4/200] Training loss: 0.04610265
[5/200] Training loss: 0.04256904
[6/200] Training loss: 0.03755430
[7/200] Training loss: 0.03457880
[8/200] Training loss: 0.03285296
[9/200] Training loss: 0.03086274
[10/200] Training loss: 0.03140577
[50/200] Training loss: 0.01622327
[100/200] Training loss: 0.01432012
[150/200] Training loss: 0.01334851
[200/200] Training loss: 0.01178030
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16869.912625736983 ----------
[1/200] Training loss: 0.15929978
[2/200] Training loss: 0.05753557
[3/200] Training loss: 0.04976061
[4/200] Training loss: 0.04729797
[5/200] Training loss: 0.04680088
[6/200] Training loss: 0.04200113
[7/200] Training loss: 0.03879120
[8/200] Training loss: 0.03939950
[9/200] Training loss: 0.03839628
[10/200] Training loss: 0.03518012
[50/200] Training loss: 0.02045094
[100/200] Training loss: 0.01613292
[150/200] Training loss: 0.01437445
[200/200] Training loss: 0.01360617
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17015.82698548619 ----------
[1/200] Training loss: 0.16212982
[2/200] Training loss: 0.06050321
[3/200] Training loss: 0.05402829
[4/200] Training loss: 0.04603937
[5/200] Training loss: 0.04824531
[6/200] Training loss: 0.04324650
[7/200] Training loss: 0.03830791
[8/200] Training loss: 0.03559753
[9/200] Training loss: 0.03557715
[10/200] Training loss: 0.03123659
[50/200] Training loss: 0.01694596
[100/200] Training loss: 0.01380681
[150/200] Training loss: 0.01263938
[200/200] Training loss: 0.01189593
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11217.103726006995 ----------
[1/200] Training loss: 0.16145405
[2/200] Training loss: 0.06272953
[3/200] Training loss: 0.05775374
[4/200] Training loss: 0.05447624
[5/200] Training loss: 0.05152686
[6/200] Training loss: 0.04983827
[7/200] Training loss: 0.04886107
[8/200] Training loss: 0.04951163
[9/200] Training loss: 0.04393968
[10/200] Training loss: 0.04297790
[50/200] Training loss: 0.01828882
[100/200] Training loss: 0.01597800
[150/200] Training loss: 0.01447981
[200/200] Training loss: 0.01332066
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01787604307740652
----FITNESS-----------RMSE---- 20518.119212052552 ----------
[1/200] Training loss: 0.15364823
[2/200] Training loss: 0.05907421
[3/200] Training loss: 0.05165653
[4/200] Training loss: 0.04758290
[5/200] Training loss: 0.04337199
[6/200] Training loss: 0.04019790
[7/200] Training loss: 0.04052616
[8/200] Training loss: 0.03565662
[9/200] Training loss: 0.03566150
[10/200] Training loss: 0.03214799
[50/200] Training loss: 0.01902519
[100/200] Training loss: 0.01749243
[150/200] Training loss: 0.01571064
[200/200] Training loss: 0.01420893
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12150.899225983236 ----------
[1/200] Training loss: 0.14831477
[2/200] Training loss: 0.05440677
[3/200] Training loss: 0.05158590
[4/200] Training loss: 0.04715419
[5/200] Training loss: 0.04575307
[6/200] Training loss: 0.04402323
[7/200] Training loss: 0.04154033
[8/200] Training loss: 0.03906325
[9/200] Training loss: 0.03669627
[10/200] Training loss: 0.03516765
[50/200] Training loss: 0.01881168
[100/200] Training loss: 0.01535286
[150/200] Training loss: 0.01382649
[200/200] Training loss: 0.01258519
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 27661.220218927436 ----------
[1/200] Training loss: 0.16025877
[2/200] Training loss: 0.06057889
[3/200] Training loss: 0.05248177
[4/200] Training loss: 0.04568834
[5/200] Training loss: 0.04146526
[6/200] Training loss: 0.04055334
[7/200] Training loss: 0.03903451
[8/200] Training loss: 0.03613691
[9/200] Training loss: 0.03256190
[10/200] Training loss: 0.03148662
[50/200] Training loss: 0.01767315
[100/200] Training loss: 0.01456161
[150/200] Training loss: 0.01410568
[200/200] Training loss: 0.01274254
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12866.328147533002 ----------
[1/200] Training loss: 0.15616980
[2/200] Training loss: 0.06122224
[3/200] Training loss: 0.05451438
[4/200] Training loss: 0.05157084
[5/200] Training loss: 0.04826790
[6/200] Training loss: 0.04629561
[7/200] Training loss: 0.04122128
[8/200] Training loss: 0.04284478
[9/200] Training loss: 0.03902655
[10/200] Training loss: 0.03453648
[50/200] Training loss: 0.01718753
[100/200] Training loss: 0.01431361
[150/200] Training loss: 0.01377062
[200/200] Training loss: 0.01360202
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10562.774256794472 ----------
[1/200] Training loss: 0.15701787
[2/200] Training loss: 0.06000372
[3/200] Training loss: 0.05346506
[4/200] Training loss: 0.05232022
[5/200] Training loss: 0.05063252
[6/200] Training loss: 0.04835640
[7/200] Training loss: 0.04396021
[8/200] Training loss: 0.04410577
[9/200] Training loss: 0.03893959
[10/200] Training loss: 0.03862705
[50/200] Training loss: 0.01772386
[100/200] Training loss: 0.01587878
[150/200] Training loss: 0.01348009
[200/200] Training loss: 0.01248364
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14344.288340660194 ----------
[1/200] Training loss: 0.16628901
[2/200] Training loss: 0.05915039
[3/200] Training loss: 0.05049033
[4/200] Training loss: 0.05042627
[5/200] Training loss: 0.04595137
[6/200] Training loss: 0.04292510
[7/200] Training loss: 0.03787989
[8/200] Training loss: 0.03650435
[9/200] Training loss: 0.03445518
[10/200] Training loss: 0.03107613
[50/200] Training loss: 0.01651838
[100/200] Training loss: 0.01373761
[150/200] Training loss: 0.01260053
[200/200] Training loss: 0.01101704
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18238.90172132083 ----------
[1/200] Training loss: 0.16833549
[2/200] Training loss: 0.05923104
[3/200] Training loss: 0.05159446
[4/200] Training loss: 0.04647402
[5/200] Training loss: 0.04422832
[6/200] Training loss: 0.04205735
[7/200] Training loss: 0.03954419
[8/200] Training loss: 0.03862966
[9/200] Training loss: 0.03815789
[10/200] Training loss: 0.03513263
[50/200] Training loss: 0.01953272
[100/200] Training loss: 0.01586889
[150/200] Training loss: 0.01309724
[200/200] Training loss: 0.01191359
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7151.915547599818 ----------
[1/200] Training loss: 0.19605905
[2/200] Training loss: 0.06269607
[3/200] Training loss: 0.05864436
[4/200] Training loss: 0.05209659
[5/200] Training loss: 0.05005730
[6/200] Training loss: 0.04787078
[7/200] Training loss: 0.04423462
[8/200] Training loss: 0.04308038
[9/200] Training loss: 0.04132216
[10/200] Training loss: 0.03677477
[50/200] Training loss: 0.01855083
[100/200] Training loss: 0.01432734
[150/200] Training loss: 0.01383435
[200/200] Training loss: 0.01280549
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4885.157725191685 ----------
[1/200] Training loss: 0.15269500
[2/200] Training loss: 0.05932891
[3/200] Training loss: 0.05236736
[4/200] Training loss: 0.04792058
[5/200] Training loss: 0.04257072
[6/200] Training loss: 0.04552055
[7/200] Training loss: 0.04069087
[8/200] Training loss: 0.03861420
[9/200] Training loss: 0.03552573
[10/200] Training loss: 0.03357283
[50/200] Training loss: 0.01765777
[100/200] Training loss: 0.01441220
[150/200] Training loss: 0.01283578
[200/200] Training loss: 0.01133970
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11776.119564610406 ----------
[1/200] Training loss: 0.15823709
[2/200] Training loss: 0.05895560
[3/200] Training loss: 0.04905481
[4/200] Training loss: 0.04703154
[5/200] Training loss: 0.04200578
[6/200] Training loss: 0.04295998
[7/200] Training loss: 0.03827203
[8/200] Training loss: 0.03607723
[9/200] Training loss: 0.03509016
[10/200] Training loss: 0.03421639
[50/200] Training loss: 0.01963597
[100/200] Training loss: 0.01512358
[150/200] Training loss: 0.01331276
[200/200] Training loss: 0.01212685
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6898.040301418947 ----------
[1/200] Training loss: 0.14694096
[2/200] Training loss: 0.05776029
[3/200] Training loss: 0.04951762
[4/200] Training loss: 0.04657749
[5/200] Training loss: 0.04445583
[6/200] Training loss: 0.03854536
[7/200] Training loss: 0.03511257
[8/200] Training loss: 0.03603709
[9/200] Training loss: 0.03445333
[10/200] Training loss: 0.03122517
[50/200] Training loss: 0.01619360
[100/200] Training loss: 0.01356956
[150/200] Training loss: 0.01226507
[200/200] Training loss: 0.01207218
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14141.123010567442 ----------
[1/200] Training loss: 0.12958043
[2/200] Training loss: 0.05144058
[3/200] Training loss: 0.04444250
[4/200] Training loss: 0.04103187
[5/200] Training loss: 0.03956855
[6/200] Training loss: 0.03841203
[7/200] Training loss: 0.03547840
[8/200] Training loss: 0.03239891
[9/200] Training loss: 0.02932679
[10/200] Training loss: 0.02924053
[50/200] Training loss: 0.01774717
[100/200] Training loss: 0.01622208
[150/200] Training loss: 0.01438665
[200/200] Training loss: 0.01378652
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13132.602483894805 ----------
[1/200] Training loss: 0.14939060
[2/200] Training loss: 0.04921207
[3/200] Training loss: 0.04833254
[4/200] Training loss: 0.04508524
[5/200] Training loss: 0.04127727
[6/200] Training loss: 0.03902584
[7/200] Training loss: 0.03582002
[8/200] Training loss: 0.03486948
[9/200] Training loss: 0.03217193
[10/200] Training loss: 0.02990583
[50/200] Training loss: 0.01602823
[100/200] Training loss: 0.01425391
[150/200] Training loss: 0.01347348
[200/200] Training loss: 0.01223253
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5519.016941448903 ----------
[1/200] Training loss: 0.14275586
[2/200] Training loss: 0.05245931
[3/200] Training loss: 0.04817608
[4/200] Training loss: 0.04324478
[5/200] Training loss: 0.04059323
[6/200] Training loss: 0.03662070
[7/200] Training loss: 0.03412015
[8/200] Training loss: 0.03326199
[9/200] Training loss: 0.03143483
[10/200] Training loss: 0.03083449
[50/200] Training loss: 0.01799881
[100/200] Training loss: 0.01549288
[150/200] Training loss: 0.01418317
[200/200] Training loss: 0.01333987
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19743.457853172527 ----------
[1/200] Training loss: 0.16005622
[2/200] Training loss: 0.05821460
[3/200] Training loss: 0.05082537
[4/200] Training loss: 0.04713582
[5/200] Training loss: 0.04307933
[6/200] Training loss: 0.04188569
[7/200] Training loss: 0.03802449
[8/200] Training loss: 0.03460134
[9/200] Training loss: 0.03174491
[10/200] Training loss: 0.03290237
[50/200] Training loss: 0.01637018
[100/200] Training loss: 0.01515532
[150/200] Training loss: 0.01293499
[200/200] Training loss: 0.01274639
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10967.30085299022 ----------
[1/200] Training loss: 0.14859665
[2/200] Training loss: 0.05740378
[3/200] Training loss: 0.05181090
[4/200] Training loss: 0.04711244
[5/200] Training loss: 0.04505370
[6/200] Training loss: 0.04098191
[7/200] Training loss: 0.04070186
[8/200] Training loss: 0.03628071
[9/200] Training loss: 0.03222284
[10/200] Training loss: 0.03284428
[50/200] Training loss: 0.01851109
[100/200] Training loss: 0.01627474
[150/200] Training loss: 0.01475442
[200/200] Training loss: 0.01299594
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21148.651020809815 ----------
[1/50] Training loss: 0.19437408
[2/50] Training loss: 0.05538987
[3/50] Training loss: 0.05457928
[4/50] Training loss: 0.05268226
[5/50] Training loss: 0.04826837
[6/50] Training loss: 0.04423202
[7/50] Training loss: 0.04260695
[8/50] Training loss: 0.04020401
[9/50] Training loss: 0.03962308
[10/50] Training loss: 0.03769967
[50/50] Training loss: 0.02178855
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 50
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17725.349079778374 ----------
[1/200] Training loss: 0.15258406
[2/200] Training loss: 0.06253720
[3/200] Training loss: 0.05038110
[4/200] Training loss: 0.05019562
[5/200] Training loss: 0.04536368
[6/200] Training loss: 0.04421809
[7/200] Training loss: 0.03799432
[8/200] Training loss: 0.04147491
[9/200] Training loss: 0.03930618
[10/200] Training loss: 0.03615682
[50/200] Training loss: 0.01843420
[100/200] Training loss: 0.01639346
[150/200] Training loss: 0.01439165
[200/200] Training loss: 0.01310511
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16187.962935465352 ----------
[1/200] Training loss: 0.18649730
[2/200] Training loss: 0.05889195
[3/200] Training loss: 0.05503741
[4/200] Training loss: 0.05432438
[5/200] Training loss: 0.05006640
[6/200] Training loss: 0.04781939
[7/200] Training loss: 0.04666757
[8/200] Training loss: 0.04618889
[9/200] Training loss: 0.04347744
[10/200] Training loss: 0.04310053
[50/200] Training loss: 0.01910984
[100/200] Training loss: 0.01681073
[150/200] Training loss: 0.01468351
[200/200] Training loss: 0.01294304
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9926.02438038513 ----------
[1/200] Training loss: 0.14717870
[2/200] Training loss: 0.05653416
[3/200] Training loss: 0.05064803
[4/200] Training loss: 0.05007677
[5/200] Training loss: 0.04509148
[6/200] Training loss: 0.04142850
[7/200] Training loss: 0.04000878
[8/200] Training loss: 0.03532304
[9/200] Training loss: 0.03570218
[10/200] Training loss: 0.03358892
[50/200] Training loss: 0.01662037
[100/200] Training loss: 0.01431062
[150/200] Training loss: 0.01328326
[200/200] Training loss: 0.01168927
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12388.468831942066 ----------
[1/200] Training loss: 0.18494606
[2/200] Training loss: 0.06907389
[3/200] Training loss: 0.05547260
[4/200] Training loss: 0.05199500
[5/200] Training loss: 0.05053874
[6/200] Training loss: 0.04792507
[7/200] Training loss: 0.04581906
[8/200] Training loss: 0.04235422
[9/200] Training loss: 0.04388433
[10/200] Training loss: 0.03927767
[50/200] Training loss: 0.01947484
[100/200] Training loss: 0.01573014
[150/200] Training loss: 0.01534479
[200/200] Training loss: 0.01347925
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11932.23533123614 ----------
[1/200] Training loss: 0.15885936
[2/200] Training loss: 0.05633360
[3/200] Training loss: 0.05534227
[4/200] Training loss: 0.05378654
[5/200] Training loss: 0.04588500
[6/200] Training loss: 0.04632548
[7/200] Training loss: 0.04055023
[8/200] Training loss: 0.04012141
[9/200] Training loss: 0.03867338
[10/200] Training loss: 0.03345866
[50/200] Training loss: 0.01976887
[100/200] Training loss: 0.01570812
[150/200] Training loss: 0.01436032
[200/200] Training loss: 0.01324716
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20200.783351147547 ----------
[1/200] Training loss: 0.17419396
[2/200] Training loss: 0.05964346
[3/200] Training loss: 0.05366615
[4/200] Training loss: 0.04786773
[5/200] Training loss: 0.04679860
[6/200] Training loss: 0.04219439
[7/200] Training loss: 0.04321950
[8/200] Training loss: 0.03962575
[9/200] Training loss: 0.03535313
[10/200] Training loss: 0.03533736
[50/200] Training loss: 0.02125065
[100/200] Training loss: 0.01592188
[150/200] Training loss: 0.01308516
[200/200] Training loss: 0.01309502
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6616.539578964219 ----------
[1/200] Training loss: 0.12742392
[2/200] Training loss: 0.05535325
[3/200] Training loss: 0.04985014
[4/200] Training loss: 0.04545008
[5/200] Training loss: 0.04418663
[6/200] Training loss: 0.04015177
[7/200] Training loss: 0.03949410
[8/200] Training loss: 0.03734672
[9/200] Training loss: 0.03518883
[10/200] Training loss: 0.03454650
[50/200] Training loss: 0.01720084
[100/200] Training loss: 0.01448230
[150/200] Training loss: 0.01261239
[200/200] Training loss: 0.01144336
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10743.797466445465 ----------
[1/200] Training loss: 0.15794977
[2/200] Training loss: 0.05756058
[3/200] Training loss: 0.05228609
[4/200] Training loss: 0.04679759
[5/200] Training loss: 0.04608904
[6/200] Training loss: 0.03807271
[7/200] Training loss: 0.03810133
[8/200] Training loss: 0.03557531
[9/200] Training loss: 0.03308058
[10/200] Training loss: 0.03198178
[50/200] Training loss: 0.01768638
[100/200] Training loss: 0.01421472
[150/200] Training loss: 0.01292700
[200/200] Training loss: 0.01126828
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18321.44623112488 ----------
[1/200] Training loss: 0.16111861
[2/200] Training loss: 0.05942178
[3/200] Training loss: 0.05301213
[4/200] Training loss: 0.05281533
[5/200] Training loss: 0.04825261
[6/200] Training loss: 0.04532852
[7/200] Training loss: 0.04395213
[8/200] Training loss: 0.04099332
[9/200] Training loss: 0.04360617
[10/200] Training loss: 0.04028374
[50/200] Training loss: 0.01923689
[100/200] Training loss: 0.01660358
[150/200] Training loss: 0.01418321
[200/200] Training loss: 0.01415892
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11181.781968899233 ----------
[1/200] Training loss: 0.16419696
[2/200] Training loss: 0.05592337
[3/200] Training loss: 0.04937097
[4/200] Training loss: 0.04839992
[5/200] Training loss: 0.04173638
[6/200] Training loss: 0.04016942
[7/200] Training loss: 0.03823994
[8/200] Training loss: 0.03546215
[9/200] Training loss: 0.03418967
[10/200] Training loss: 0.03249729
[50/200] Training loss: 0.01760511
[100/200] Training loss: 0.01503731
[150/200] Training loss: 0.01405812
[200/200] Training loss: 0.01308397
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15726.492298030098 ----------
[1/200] Training loss: 0.14576736
[2/200] Training loss: 0.06037309
[3/200] Training loss: 0.05258477
[4/200] Training loss: 0.05008268
[5/200] Training loss: 0.04759582
[6/200] Training loss: 0.04125898
[7/200] Training loss: 0.08136141
[8/200] Training loss: 0.06849565
[9/200] Training loss: 0.05150438
[10/200] Training loss: 0.04718979
[50/200] Training loss: 0.01951441
[100/200] Training loss: 0.01609383
[150/200] Training loss: 0.01509440
[200/200] Training loss: 0.01363538
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13460.284098041913 ----------
[1/200] Training loss: 0.16138146
[2/200] Training loss: 0.05591373
[3/200] Training loss: 0.05272682
[4/200] Training loss: 0.04869937
[5/200] Training loss: 0.04801747
[6/200] Training loss: 0.04343341
[7/200] Training loss: 0.04361915
[8/200] Training loss: 0.04097081
[9/200] Training loss: 0.03980401
[10/200] Training loss: 0.03466648
[50/200] Training loss: 0.01984592
[100/200] Training loss: 0.01560941
[150/200] Training loss: 0.01425006
[200/200] Training loss: 0.01335317
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6977.094810879382 ----------
[1/200] Training loss: 0.17319532
[2/200] Training loss: 0.06005328
[3/200] Training loss: 0.05134339
[4/200] Training loss: 0.04692583
[5/200] Training loss: 0.04438681
[6/200] Training loss: 0.04204747
[7/200] Training loss: 0.03864970
[8/200] Training loss: 0.03604953
[9/200] Training loss: 0.03222483
[10/200] Training loss: 0.03682260
[50/200] Training loss: 0.01764074
[100/200] Training loss: 0.01515051
[150/200] Training loss: 0.01446944
[200/200] Training loss: 0.01272551
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7874.207769674356 ----------
[1/200] Training loss: 0.16788394
[2/200] Training loss: 0.05423321
[3/200] Training loss: 0.05143643
[4/200] Training loss: 0.04444100
[5/200] Training loss: 0.04275304
[6/200] Training loss: 0.04108360
[7/200] Training loss: 0.03691852
[8/200] Training loss: 0.03685318
[9/200] Training loss: 0.03373082
[10/200] Training loss: 0.03142368
[50/200] Training loss: 0.01739042
[100/200] Training loss: 0.01572815
[150/200] Training loss: 0.01481233
[200/200] Training loss: 0.01381757
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12415.684274336232 ----------
[1/200] Training loss: 0.17804281
[2/200] Training loss: 0.06140993
[3/200] Training loss: 0.05216926
[4/200] Training loss: 0.04903635
[5/200] Training loss: 0.05278770
[6/200] Training loss: 0.04823680
[7/200] Training loss: 0.04485226
[8/200] Training loss: 0.03936671
[9/200] Training loss: 0.03783398
[10/200] Training loss: 0.03435977
[50/200] Training loss: 0.01769260
[100/200] Training loss: 0.01544880
[150/200] Training loss: 0.01495346
[200/200] Training loss: 0.01359303
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10609.66842083201 ----------
[1/200] Training loss: 0.15553063
[2/200] Training loss: 0.06014312
[3/200] Training loss: 0.05163917
[4/200] Training loss: 0.04603996
[5/200] Training loss: 0.04189300
[6/200] Training loss: 0.03725809
[7/200] Training loss: 0.03634825
[8/200] Training loss: 0.03280552
[9/200] Training loss: 0.03047174
[10/200] Training loss: 0.02841897
[50/200] Training loss: 0.01628240
[100/200] Training loss: 0.01299142
[150/200] Training loss: 0.01176333
[200/200] Training loss: 0.01071760
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17091.26092481184 ----------
[1/100] Training loss: 0.17874266
[2/100] Training loss: 0.06572759
[3/100] Training loss: 0.06087961
[4/100] Training loss: 0.05617879
[5/100] Training loss: 0.05390901
[6/100] Training loss: 0.05362072
[7/100] Training loss: 0.05045985
[8/100] Training loss: 0.04743021
[9/100] Training loss: 0.04926566
[10/100] Training loss: 0.04519627
[50/100] Training loss: 0.01915152
[100/100] Training loss: 0.01528259
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15765.453878655064 ----------
[1/200] Training loss: 0.19310366
[2/200] Training loss: 0.05983416
[3/200] Training loss: 0.05456232
[4/200] Training loss: 0.05098399
[5/200] Training loss: 0.04933284
[6/200] Training loss: 0.04636120
[7/200] Training loss: 0.04386939
[8/200] Training loss: 0.04164009
[9/200] Training loss: 0.03925086
[10/200] Training loss: 0.03591015
[50/200] Training loss: 0.01918080
[100/200] Training loss: 0.01621236
[150/200] Training loss: 0.01414014
[200/200] Training loss: 0.01307790
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17898.60955493471 ----------
[1/200] Training loss: 0.18331560
[2/200] Training loss: 0.06506692
[3/200] Training loss: 0.05054889
[4/200] Training loss: 0.05036607
[5/200] Training loss: 0.04490636
[6/200] Training loss: 0.04167074
[7/200] Training loss: 0.03928770
[8/200] Training loss: 0.03591685
[9/200] Training loss: 0.03560880
[10/200] Training loss: 0.03164924
[50/200] Training loss: 0.01687281
[100/200] Training loss: 0.01451065
[150/200] Training loss: 0.01365010
[200/200] Training loss: 0.01264487
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14443.828855258567 ----------
[1/200] Training loss: 0.17066305
[2/200] Training loss: 0.05929633
[3/200] Training loss: 0.05176470
[4/200] Training loss: 0.04879634
[5/200] Training loss: 0.04341538
[6/200] Training loss: 0.04265640
[7/200] Training loss: 0.03702082
[8/200] Training loss: 0.03559600
[9/200] Training loss: 0.03653999
[10/200] Training loss: 0.03451264
[50/200] Training loss: 0.01731831
[100/200] Training loss: 0.01548549
[150/200] Training loss: 0.01317705
[200/200] Training loss: 0.01218097
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13222.403412390653 ----------
[1/200] Training loss: 0.16587570
[2/200] Training loss: 0.05979457
[3/200] Training loss: 0.05725077
[4/200] Training loss: 0.05193349
[5/200] Training loss: 0.04920950
[6/200] Training loss: 0.04706811
[7/200] Training loss: 0.04356296
[8/200] Training loss: 0.04083891
[9/200] Training loss: 0.03576295
[10/200] Training loss: 0.03473749
[50/200] Training loss: 0.01820700
[100/200] Training loss: 0.01517862
[150/200] Training loss: 0.01435640
[200/200] Training loss: 0.01365086
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15412.145859678334 ----------
[1/200] Training loss: 0.17918579
[2/200] Training loss: 0.06128759
[3/200] Training loss: 0.05568506
[4/200] Training loss: 0.05136645
[5/200] Training loss: 0.05180253
[6/200] Training loss: 0.04667726
[7/200] Training loss: 0.04635098
[8/200] Training loss: 0.04278278
[9/200] Training loss: 0.04084008
[10/200] Training loss: 0.04150450
[50/200] Training loss: 0.02151285
[100/200] Training loss: 0.01607778
[150/200] Training loss: 0.01442841
[200/200] Training loss: 0.01341905
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.02568525518847999
----FITNESS-----------RMSE---- 14799.438908282977 ----------
[1/200] Training loss: 0.15907794
[2/200] Training loss: 0.05871155
[3/200] Training loss: 0.05005177
[4/200] Training loss: 0.04539962
[5/200] Training loss: 0.04664423
[6/200] Training loss: 0.04356695
[7/200] Training loss: 0.04013716
[8/200] Training loss: 0.03886894
[9/200] Training loss: 0.03696532
[10/200] Training loss: 0.03444196
[50/200] Training loss: 0.01857394
[100/200] Training loss: 0.01532096
[150/200] Training loss: 0.01446184
[200/200] Training loss: 0.01304467
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16571.351664846174 ----------
[1/200] Training loss: 0.16343847
[2/200] Training loss: 0.05812668
[3/200] Training loss: 0.05461455
[4/200] Training loss: 0.05410269
[5/200] Training loss: 0.04823598
[6/200] Training loss: 0.04781446
[7/200] Training loss: 0.04417196
[8/200] Training loss: 0.04244760
[9/200] Training loss: 0.03972210
[10/200] Training loss: 0.03463474
[50/200] Training loss: 0.01836800
[100/200] Training loss: 0.01555307
[150/200] Training loss: 0.01269975
[200/200] Training loss: 0.01249142
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10624.192392836267 ----------
[1/200] Training loss: 0.16685892
[2/200] Training loss: 0.05810122
[3/200] Training loss: 0.05560023
[4/200] Training loss: 0.05206698
[5/200] Training loss: 0.04810890
[6/200] Training loss: 0.04541380
[7/200] Training loss: 0.04433789
[8/200] Training loss: 0.04128231
[9/200] Training loss: 0.03756428
[10/200] Training loss: 0.03560177
[50/200] Training loss: 0.01766468
[100/200] Training loss: 0.01506319
[150/200] Training loss: 0.01518417
[200/200] Training loss: 0.01325615
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7060.601390816508 ----------
[1/200] Training loss: 0.17685784
[2/200] Training loss: 0.06057342
[3/200] Training loss: 0.05456601
[4/200] Training loss: 0.04961416
[5/200] Training loss: 0.04821252
[6/200] Training loss: 0.04545134
[7/200] Training loss: 0.04165417
[8/200] Training loss: 0.04149690
[9/200] Training loss: 0.03819513
[10/200] Training loss: 0.03474814
[50/200] Training loss: 0.01852017
[100/200] Training loss: 0.01478475
[150/200] Training loss: 0.01331271
[200/200] Training loss: 0.01343873
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15533.341688123648 ----------
[1/200] Training loss: 0.18256887
[2/200] Training loss: 0.07005136
[3/200] Training loss: 0.05727996
[4/200] Training loss: 0.05428588
[5/200] Training loss: 0.05446004
[6/200] Training loss: 0.04904578
[7/200] Training loss: 0.04754294
[8/200] Training loss: 0.04647071
[9/200] Training loss: 0.04336776
[10/200] Training loss: 0.04167737
[50/200] Training loss: 0.01938082
[100/200] Training loss: 0.01555631
[150/200] Training loss: 0.01361980
[200/200] Training loss: 0.01287259
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7301.5507941806445 ----------
[1/200] Training loss: 0.16538641
[2/200] Training loss: 0.05903361
[3/200] Training loss: 0.04503386
[4/200] Training loss: 0.04493703
[5/200] Training loss: 0.04150074
[6/200] Training loss: 0.04142200
[7/200] Training loss: 0.03608168
[8/200] Training loss: 0.03320744
[9/200] Training loss: 0.03273000
[10/200] Training loss: 0.03043954
[50/200] Training loss: 0.01716664
[100/200] Training loss: 0.01553498
[150/200] Training loss: 0.01307546
[200/200] Training loss: 0.01151146
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4633.215298256709 ----------
[1/200] Training loss: 0.19096938
[2/200] Training loss: 0.06197977
[3/200] Training loss: 0.05615676
[4/200] Training loss: 0.05229883
[5/200] Training loss: 0.04942898
[6/200] Training loss: 0.04628444
[7/200] Training loss: 0.04315219
[8/200] Training loss: 0.04145678
[9/200] Training loss: 0.04005742
[10/200] Training loss: 0.03643412
[50/200] Training loss: 0.01914979
[100/200] Training loss: 0.01644791
[150/200] Training loss: 0.01502395
[200/200] Training loss: 0.01455866
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14422.915655303543 ----------
[1/200] Training loss: 0.15898175
[2/200] Training loss: 0.05976561
[3/200] Training loss: 0.05435267
[4/200] Training loss: 0.05317139
[5/200] Training loss: 0.04678843
[6/200] Training loss: 0.04495197
[7/200] Training loss: 0.04595960
[8/200] Training loss: 0.04117962
[9/200] Training loss: 0.04086279
[10/200] Training loss: 0.03758704
[50/200] Training loss: 0.01691179
[100/200] Training loss: 0.01456693
[150/200] Training loss: 0.01335902
[200/200] Training loss: 0.01211037
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18108.79211874718 ----------
[1/200] Training loss: 0.18323720
[2/200] Training loss: 0.05797534
[3/200] Training loss: 0.05149478
[4/200] Training loss: 0.04946276
[5/200] Training loss: 0.04682549
[6/200] Training loss: 0.04420097
[7/200] Training loss: 0.04384745
[8/200] Training loss: 0.03809822
[9/200] Training loss: 0.03699918
[10/200] Training loss: 0.03583000
[50/200] Training loss: 0.01722189
[100/200] Training loss: 0.01557112
[150/200] Training loss: 0.01422570
[200/200] Training loss: 0.01389494
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12826.465764192411 ----------
[1/200] Training loss: 0.16171513
[2/200] Training loss: 0.06587951
[3/200] Training loss: 0.05932130
[4/200] Training loss: 0.05160893
[5/200] Training loss: 0.05071549
[6/200] Training loss: 0.04634654
[7/200] Training loss: 0.04123496
[8/200] Training loss: 0.03789484
[9/200] Training loss: 0.03909740
[10/200] Training loss: 0.03296496
[50/200] Training loss: 0.01838421
[100/200] Training loss: 0.01537883
[150/200] Training loss: 0.01377528
[200/200] Training loss: 0.01280102
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10077.421495600945 ----------
[1/200] Training loss: 0.16403230
[2/200] Training loss: 0.06254916
[3/200] Training loss: 0.05127390
[4/200] Training loss: 0.05009434
[5/200] Training loss: 0.04676976
[6/200] Training loss: 0.04645300
[7/200] Training loss: 0.04269647
[8/200] Training loss: 0.03740223
[9/200] Training loss: 0.03342896
[10/200] Training loss: 0.03239658
[50/200] Training loss: 0.01820844
[100/200] Training loss: 0.01578125
[150/200] Training loss: 0.01403654
[200/200] Training loss: 0.01291509
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11266.998180527056 ----------
[1/200] Training loss: 0.14927155
[2/200] Training loss: 0.05619314
[3/200] Training loss: 0.04710727
[4/200] Training loss: 0.04496853
[5/200] Training loss: 0.04259820
[6/200] Training loss: 0.03919253
[7/200] Training loss: 0.03572168
[8/200] Training loss: 0.03456137
[9/200] Training loss: 0.03216835
[10/200] Training loss: 0.03217826
[50/200] Training loss: 0.01840369
[100/200] Training loss: 0.01521836
[150/200] Training loss: 0.01349068
[200/200] Training loss: 0.01320132
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15982.81577194707 ----------
[1/200] Training loss: 0.15134226
[2/200] Training loss: 0.05765823
[3/200] Training loss: 0.04978470
[4/200] Training loss: 0.04508605
[5/200] Training loss: 0.04239315
[6/200] Training loss: 0.04101107
[7/200] Training loss: 0.03942451
[8/200] Training loss: 0.03691390
[9/200] Training loss: 0.03237000
[10/200] Training loss: 0.03305718
[50/200] Training loss: 0.01728365
[100/200] Training loss: 0.01505199
[150/200] Training loss: 0.01348337
[200/200] Training loss: 0.01293505
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14640.337701023156 ----------
[1/200] Training loss: 0.16831120
[2/200] Training loss: 0.06082454
[3/200] Training loss: 0.06105977
[4/200] Training loss: 0.05025845
[5/200] Training loss: 0.04901322
[6/200] Training loss: 0.04402350
[7/200] Training loss: 0.04316112
[8/200] Training loss: 0.03942496
[9/200] Training loss: 0.03695953
[10/200] Training loss: 0.03687225
[50/200] Training loss: 0.01954431
[100/200] Training loss: 0.01726282
[150/200] Training loss: 0.01528126
[200/200] Training loss: 0.01394132
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7391.7881463148005 ----------
[1/200] Training loss: 0.19798943
[2/200] Training loss: 0.06549613
[3/200] Training loss: 0.05356096
[4/200] Training loss: 0.05208882
[5/200] Training loss: 0.04993399
[6/200] Training loss: 0.04332937
[7/200] Training loss: 0.04515764
[8/200] Training loss: 0.04296701
[9/200] Training loss: 0.03804960
[10/200] Training loss: 0.03617561
[50/200] Training loss: 0.01864860
[100/200] Training loss: 0.01658084
[150/200] Training loss: 0.01515358
[200/200] Training loss: 0.01352274
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13573.909090604666 ----------
[1/200] Training loss: 0.15762127
[2/200] Training loss: 0.05906462
[3/200] Training loss: 0.05372746
[4/200] Training loss: 0.05057277
[5/200] Training loss: 0.04730050
[6/200] Training loss: 0.04402677
[7/200] Training loss: 0.04031879
[8/200] Training loss: 0.03780377
[9/200] Training loss: 0.03744643
[10/200] Training loss: 0.03561993
[50/200] Training loss: 0.02046675
[100/200] Training loss: 0.01761614
[150/200] Training loss: 0.01643154
[200/200] Training loss: 0.01491957
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9429.475489124514 ----------
[1/200] Training loss: 0.17294849
[2/200] Training loss: 0.05883473
[3/200] Training loss: 0.05693029
[4/200] Training loss: 0.05234615
[5/200] Training loss: 0.04986058
[6/200] Training loss: 0.05040191
[7/200] Training loss: 0.04627432
[8/200] Training loss: 0.04240248
[9/200] Training loss: 0.04242434
[10/200] Training loss: 0.03752345
[50/200] Training loss: 0.01997349
[100/200] Training loss: 0.01571194
[150/200] Training loss: 0.01422604
[200/200] Training loss: 0.01362777
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14040.100854338618 ----------
[1/200] Training loss: 0.19002082
[2/200] Training loss: 0.06297980
[3/200] Training loss: 0.05434195
[4/200] Training loss: 0.05284224
[5/200] Training loss: 0.04984875
[6/200] Training loss: 0.04776771
[7/200] Training loss: 0.04717394
[8/200] Training loss: 0.04357056
[9/200] Training loss: 0.04029916
[10/200] Training loss: 0.04297576
[50/200] Training loss: 0.01913453
[100/200] Training loss: 0.01534064
[150/200] Training loss: 0.01331143
[200/200] Training loss: 0.01269940
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15360.873933471363 ----------
[1/200] Training loss: 0.19075607
[2/200] Training loss: 0.06405521
[3/200] Training loss: 0.05361984
[4/200] Training loss: 0.05293039
[5/200] Training loss: 0.04811449
[6/200] Training loss: 0.04566563
[7/200] Training loss: 0.04515099
[8/200] Training loss: 0.04185116
[9/200] Training loss: 0.03898998
[10/200] Training loss: 0.03933113
[50/200] Training loss: 0.02240051
[100/200] Training loss: 0.01597694
[150/200] Training loss: 0.01452166
[200/200] Training loss: 0.01322228
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8171.736412782781 ----------
[1/200] Training loss: 0.15676870
[2/200] Training loss: 0.05999600
[3/200] Training loss: 0.05296168
[4/200] Training loss: 0.04838324
[5/200] Training loss: 0.04731678
[6/200] Training loss: 0.04209324
[7/200] Training loss: 0.04033930
[8/200] Training loss: 0.04291645
[9/200] Training loss: 0.03810357
[10/200] Training loss: 0.03845156
[50/200] Training loss: 0.01716812
[100/200] Training loss: 0.01458336
[150/200] Training loss: 0.01320424
[200/200] Training loss: 0.01237101
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13074.692501164225 ----------
[1/200] Training loss: 0.15828527
[2/200] Training loss: 0.05518016
[3/200] Training loss: 0.04843322
[4/200] Training loss: 0.04520024
[5/200] Training loss: 0.03986035
[6/200] Training loss: 0.04000601
[7/200] Training loss: 0.03509312
[8/200] Training loss: 0.03447845
[9/200] Training loss: 0.03217731
[10/200] Training loss: 0.03014029
[50/200] Training loss: 0.01714143
[100/200] Training loss: 0.01368102
[150/200] Training loss: 0.01180731
[200/200] Training loss: 0.01125228
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21594.50300423698 ----------
[1/200] Training loss: 0.16241213
[2/200] Training loss: 0.06097483
[3/200] Training loss: 0.05478400
[4/200] Training loss: 0.05108341
[5/200] Training loss: 0.04825241
[6/200] Training loss: 0.04916681
[7/200] Training loss: 0.04518389
[8/200] Training loss: 0.04373203
[9/200] Training loss: 0.04318515
[10/200] Training loss: 0.04124480
[50/200] Training loss: 0.01793461
[100/200] Training loss: 0.01565260
[150/200] Training loss: 0.01356676
[200/200] Training loss: 0.01250807
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4233.05398028421 ----------
[1/200] Training loss: 0.16604996
[2/200] Training loss: 0.06387671
[3/200] Training loss: 0.05522515
[4/200] Training loss: 0.05093143
[5/200] Training loss: 0.05150646
[6/200] Training loss: 0.04991470
[7/200] Training loss: 0.04563329
[8/200] Training loss: 0.04346016
[9/200] Training loss: 0.04281197
[10/200] Training loss: 0.04003032
[50/200] Training loss: 0.01956072
[100/200] Training loss: 0.01538296
[150/200] Training loss: 0.01384127
[200/200] Training loss: 0.01215193
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13101.744616653157 ----------
[1/200] Training loss: 0.16249814
[2/200] Training loss: 0.06505645
[3/200] Training loss: 0.05313217
[4/200] Training loss: 0.05249837
[5/200] Training loss: 0.04825968
[6/200] Training loss: 0.04469243
[7/200] Training loss: 0.04254329
[8/200] Training loss: 0.04121597
[9/200] Training loss: 0.03981922
[10/200] Training loss: 0.03714801
[50/200] Training loss: 0.01783474
[100/200] Training loss: 0.01459019
[150/200] Training loss: 0.01352334
[200/200] Training loss: 0.01187947
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13612.138700439398 ----------
[1/200] Training loss: 0.17754712
[2/200] Training loss: 0.06022269
[3/200] Training loss: 0.05249858
[4/200] Training loss: 0.04663176
[5/200] Training loss: 0.04335561
[6/200] Training loss: 0.04177232
[7/200] Training loss: 0.03787885
[8/200] Training loss: 0.03463417
[9/200] Training loss: 0.03412205
[10/200] Training loss: 0.03175501
[50/200] Training loss: 0.01689240
[100/200] Training loss: 0.01517124
[150/200] Training loss: 0.01318695
[200/200] Training loss: 0.01237605
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14189.283279996915 ----------
[1/200] Training loss: 0.15843250
[2/200] Training loss: 0.05709406
[3/200] Training loss: 0.05220756
[4/200] Training loss: 0.04686919
[5/200] Training loss: 0.04169047
[6/200] Training loss: 0.03971458
[7/200] Training loss: 0.03611653
[8/200] Training loss: 0.03413939
[9/200] Training loss: 0.03318362
[10/200] Training loss: 0.02831807
[50/200] Training loss: 0.01746298
[100/200] Training loss: 0.01604000
[150/200] Training loss: 0.01354832
[200/200] Training loss: 0.01259121
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13081.560151602713 ----------
[1/200] Training loss: 0.18194115
[2/200] Training loss: 0.06068427
[3/200] Training loss: 0.05213463
[4/200] Training loss: 0.04998207
[5/200] Training loss: 0.04724043
[6/200] Training loss: 0.04020470
[7/200] Training loss: 0.04303669
[8/200] Training loss: 0.03782035
[9/200] Training loss: 0.03652036
[10/200] Training loss: 0.03268171
[50/200] Training loss: 0.01791094
[100/200] Training loss: 0.01479165
[150/200] Training loss: 0.01258889
[200/200] Training loss: 0.01180733
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5803.300440266728 ----------
[1/200] Training loss: 0.14777211
[2/200] Training loss: 0.05716167
[3/200] Training loss: 0.05158776
[4/200] Training loss: 0.04958938
[5/200] Training loss: 0.04630323
[6/200] Training loss: 0.04116433
[7/200] Training loss: 0.03796354
[8/200] Training loss: 0.03754393
[9/200] Training loss: 0.03548419
[10/200] Training loss: 0.03437358
[50/200] Training loss: 0.01711097
[100/200] Training loss: 0.01471853
[150/200] Training loss: 0.01281473
[200/200] Training loss: 0.01166052
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10093.941152988757 ----------
[1/200] Training loss: 0.13912893
[2/200] Training loss: 0.05354859
[3/200] Training loss: 0.04985668
[4/200] Training loss: 0.04549596
[5/200] Training loss: 0.04018862
[6/200] Training loss: 0.04005059
[7/200] Training loss: 0.03746022
[8/200] Training loss: 0.03377266
[9/200] Training loss: 0.03392981
[10/200] Training loss: 0.03071804
[50/200] Training loss: 0.01652702
[100/200] Training loss: 0.01363122
[150/200] Training loss: 0.01250129
[200/200] Training loss: 0.01173297
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7857.0309914114505 ----------
[1/200] Training loss: 0.19112408
[2/200] Training loss: 0.07169229
[3/200] Training loss: 0.05996162
[4/200] Training loss: 0.05282584
[5/200] Training loss: 0.04984442
[6/200] Training loss: 0.04912887
[7/200] Training loss: 0.04711299
[8/200] Training loss: 0.04699687
[9/200] Training loss: 0.04249324
[10/200] Training loss: 0.04309883
[50/200] Training loss: 0.01795281
[100/200] Training loss: 0.01663806
[150/200] Training loss: 0.01406362
[200/200] Training loss: 0.01364461
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11953.732136868384 ----------
[1/200] Training loss: 0.18467055
[2/200] Training loss: 0.06032246
[3/200] Training loss: 0.05094607
[4/200] Training loss: 0.04715207
[5/200] Training loss: 0.04533140
[6/200] Training loss: 0.04381516
[7/200] Training loss: 0.03927541
[8/200] Training loss: 0.03711431
[9/200] Training loss: 0.03555450
[10/200] Training loss: 0.03529997
[50/200] Training loss: 0.01772826
[100/200] Training loss: 0.01617275
[150/200] Training loss: 0.01463478
[200/200] Training loss: 0.01396857
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14672.107960344349 ----------
[1/200] Training loss: 0.13654575
[2/200] Training loss: 0.05414693
[3/200] Training loss: 0.05083163
[4/200] Training loss: 0.04683564
[5/200] Training loss: 0.04304483
[6/200] Training loss: 0.04354574
[7/200] Training loss: 0.03772572
[8/200] Training loss: 0.03828472
[9/200] Training loss: 0.03658638
[10/200] Training loss: 0.03140498
[50/200] Training loss: 0.01786481
[100/200] Training loss: 0.01571306
[150/200] Training loss: 0.01287529
[200/200] Training loss: 0.01335092
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 26751.300230082274 ----------
[1/200] Training loss: 0.15179200
[2/200] Training loss: 0.05924760
[3/200] Training loss: 0.05854594
[4/200] Training loss: 0.05030760
[5/200] Training loss: 0.04901261
[6/200] Training loss: 0.04577488
[7/200] Training loss: 0.04253679
[8/200] Training loss: 0.04226380
[9/200] Training loss: 0.04164617
[10/200] Training loss: 0.03899390
[50/200] Training loss: 0.01788017
[100/200] Training loss: 0.01486497
[150/200] Training loss: 0.01367699
[200/200] Training loss: 0.01220430
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8758.19159415915 ----------
[1/200] Training loss: 0.15613561
[2/200] Training loss: 0.06011355
[3/200] Training loss: 0.05236485
[4/200] Training loss: 0.04933096
[5/200] Training loss: 0.04575770
[6/200] Training loss: 0.04175901
[7/200] Training loss: 0.04104108
[8/200] Training loss: 0.03618014
[9/200] Training loss: 0.03554196
[10/200] Training loss: 0.03281361
[50/200] Training loss: 0.01807527
[100/200] Training loss: 0.01613445
[150/200] Training loss: 0.01509271
[200/200] Training loss: 0.01364048
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15582.08612477803 ----------
[1/200] Training loss: 0.18421976
[2/200] Training loss: 0.06698759
[3/200] Training loss: 0.05529386
[4/200] Training loss: 0.05343824
[5/200] Training loss: 0.05368582
[6/200] Training loss: 0.04903852
[7/200] Training loss: 0.04657524
[8/200] Training loss: 0.04623975
[9/200] Training loss: 0.04442430
[10/200] Training loss: 0.04156657
[50/200] Training loss: 0.01969942
[100/200] Training loss: 0.01572992
[150/200] Training loss: 0.01402271
[200/200] Training loss: 0.01292674
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7490.439506464223 ----------
[1/200] Training loss: 0.14798271
[2/200] Training loss: 0.05344710
[3/200] Training loss: 0.04744257
[4/200] Training loss: 0.04419142
[5/200] Training loss: 0.03898782
[6/200] Training loss: 0.03756030
[7/200] Training loss: 0.03346860
[8/200] Training loss: 0.03375819
[9/200] Training loss: 0.03090708
[10/200] Training loss: 0.02795766
[50/200] Training loss: 0.01763339
[100/200] Training loss: 0.01469107
[150/200] Training loss: 0.01332614
[200/200] Training loss: 0.01272312
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7061.248898034964 ----------
[1/200] Training loss: 0.15791264
[2/200] Training loss: 0.05968419
[3/200] Training loss: 0.05367613
[4/200] Training loss: 0.04599760
[5/200] Training loss: 0.04797133
[6/200] Training loss: 0.04386152
[7/200] Training loss: 0.03723848
[8/200] Training loss: 0.03351934
[9/200] Training loss: 0.03411124
[10/200] Training loss: 0.03329288
[50/200] Training loss: 0.01610651
[100/200] Training loss: 0.03311232
[150/200] Training loss: 0.01452833
[200/200] Training loss: 0.01284060
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22467.75111131508 ----------
[1/200] Training loss: 0.17316168
[2/200] Training loss: 0.06377316
[3/200] Training loss: 0.05775162
[4/200] Training loss: 0.05366862
[5/200] Training loss: 0.05072835
[6/200] Training loss: 0.04871586
[7/200] Training loss: 0.04425817
[8/200] Training loss: 0.04219483
[9/200] Training loss: 0.04313676
[10/200] Training loss: 0.03853078
[50/200] Training loss: 0.01846459
[100/200] Training loss: 0.01605671
[150/200] Training loss: 0.01459277
[200/200] Training loss: 0.01345283
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.31653039806811667 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7687.669868041941 ----------
[1/200] Training loss: 0.17764364
[2/200] Training loss: 0.05926557
[3/200] Training loss: 0.05782760
[4/200] Training loss: 0.05160092
[5/200] Training loss: 0.04729101
[6/200] Training loss: 0.04704245
[7/200] Training loss: 0.04289340
[8/200] Training loss: 0.04399662
[9/200] Training loss: 0.04257639
[10/200] Training loss: 0.03848369
[50/200] Training loss: 0.01848632
[100/200] Training loss: 0.01559946
[150/200] Training loss: 0.01484241
[200/200] Training loss: 0.01394251
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14926.033096573248 ----------
[1/200] Training loss: 0.17187354
[2/200] Training loss: 0.06094299
[3/200] Training loss: 0.05234002
[4/200] Training loss: 0.04477403
[5/200] Training loss: 0.04637028
[6/200] Training loss: 0.04150980
[7/200] Training loss: 0.03855380
[8/200] Training loss: 0.03582145
[9/200] Training loss: 0.03218595
[10/200] Training loss: 0.03320269
[50/200] Training loss: 0.01813164
[100/200] Training loss: 0.01349144
[150/200] Training loss: 0.01283827
[200/200] Training loss: 0.01154691
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6091.38604916812 ----------
[1/200] Training loss: 0.14967629
[2/200] Training loss: 0.05672760
[3/200] Training loss: 0.04913656
[4/200] Training loss: 0.04394787
[5/200] Training loss: 0.04204519
[6/200] Training loss: 0.04259377
[7/200] Training loss: 0.03546673
[8/200] Training loss: 0.03495668
[9/200] Training loss: 0.03567299
[10/200] Training loss: 0.03323235
[50/200] Training loss: 0.01724742
[100/200] Training loss: 0.01485980
[150/200] Training loss: 0.01298674
[200/200] Training loss: 0.01277794
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16251.286226019158 ----------
[1/200] Training loss: 0.13685574
[2/200] Training loss: 0.05583349
[3/200] Training loss: 0.04915555
[4/200] Training loss: 0.04769339
[5/200] Training loss: 0.04652978
[6/200] Training loss: 0.04371688
[7/200] Training loss: 0.04162082
[8/200] Training loss: 0.03915516
[9/200] Training loss: 0.03863316
[10/200] Training loss: 0.03422519
[50/200] Training loss: 0.01877638
[100/200] Training loss: 0.01642298
[150/200] Training loss: 0.01535068
[200/200] Training loss: 0.01475456
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.009927782639876225
----FITNESS-----------RMSE---- 10506.157051938639 ----------
[1/200] Training loss: 0.14797391
[2/200] Training loss: 0.06072039
[3/200] Training loss: 0.05815727
[4/200] Training loss: 0.04905404
[5/200] Training loss: 0.04908288
[6/200] Training loss: 0.04694695
[7/200] Training loss: 0.04198375
[8/200] Training loss: 0.04171009
[9/200] Training loss: 0.03877638
[10/200] Training loss: 0.03698021
[50/200] Training loss: 0.01774228
[100/200] Training loss: 0.01569431
[150/200] Training loss: 0.01448646
[200/200] Training loss: 0.01304042
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7446.145580097128 ----------
[1/200] Training loss: 0.15061226
[2/200] Training loss: 0.05856221
[3/200] Training loss: 0.04855920
[4/200] Training loss: 0.04531639
[5/200] Training loss: 0.04638498
[6/200] Training loss: 0.04106228
[7/200] Training loss: 0.03867465
[8/200] Training loss: 0.03687084
[9/200] Training loss: 0.03218715
[10/200] Training loss: 0.03155612
[50/200] Training loss: 0.01798896
[100/200] Training loss: 0.01486540
[150/200] Training loss: 0.01367586
[200/200] Training loss: 0.01300869
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12123.659513529732 ----------
[1/200] Training loss: 0.17262602
[2/200] Training loss: 0.05973290
[3/200] Training loss: 0.05824410
[4/200] Training loss: 0.05434778
[5/200] Training loss: 0.05107393
[6/200] Training loss: 0.05078919
[7/200] Training loss: 0.04827472
[8/200] Training loss: 0.04730706
[9/200] Training loss: 0.04522804
[10/200] Training loss: 0.04348349
[50/200] Training loss: 0.01923955
[100/200] Training loss: 0.01631353
[150/200] Training loss: 0.01536498
[200/200] Training loss: 0.01360827
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14026.976295695376 ----------
[1/200] Training loss: 0.14444353
[2/200] Training loss: 0.05596239
[3/200] Training loss: 0.05184129
[4/200] Training loss: 0.04550572
[5/200] Training loss: 0.04017592
[6/200] Training loss: 0.03897857
[7/200] Training loss: 0.03718121
[8/200] Training loss: 0.03081551
[9/200] Training loss: 0.02869379
[10/200] Training loss: 0.02793511
[50/200] Training loss: 0.01703374
[100/200] Training loss: 0.01449904
[150/200] Training loss: 0.01297675
[200/200] Training loss: 0.01157230
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13860.649335438799 ----------
[1/200] Training loss: 0.16080362
[2/200] Training loss: 0.06794322
[3/200] Training loss: 0.05176283
[4/200] Training loss: 0.05442320
[5/200] Training loss: 0.05241252
[6/200] Training loss: 0.04663407
[7/200] Training loss: 0.04026424
[8/200] Training loss: 0.03720523
[9/200] Training loss: 0.03629960
[10/200] Training loss: 0.03397177
[50/200] Training loss: 0.01878721
[100/200] Training loss: 0.01420351
[150/200] Training loss: 0.01392650
[200/200] Training loss: 0.01226744
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23542.10389918454 ----------
[1/200] Training loss: 0.15875978
[2/200] Training loss: 0.05540698
[3/200] Training loss: 0.05075266
[4/200] Training loss: 0.04570138
[5/200] Training loss: 0.04519060
[6/200] Training loss: 0.04003442
[7/200] Training loss: 0.03969939
[8/200] Training loss: 0.03684742
[9/200] Training loss: 0.03399358
[10/200] Training loss: 0.03546812
[50/200] Training loss: 0.01902573
[100/200] Training loss: 0.01420755
[150/200] Training loss: 0.01387793
[200/200] Training loss: 0.01213339
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5972.877363549331 ----------
[1/200] Training loss: 0.15314467
[2/200] Training loss: 0.05843835
[3/200] Training loss: 0.05417563
[4/200] Training loss: 0.04754023
[5/200] Training loss: 0.04591294
[6/200] Training loss: 0.03928205
[7/200] Training loss: 0.03717247
[8/200] Training loss: 0.03532790
[9/200] Training loss: 0.03202048
[10/200] Training loss: 0.03029848
[50/200] Training loss: 0.01746974
[100/200] Training loss: 0.01559303
[150/200] Training loss: 0.01401087
[200/200] Training loss: 0.01267158
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21225.41947759808 ----------
[1/200] Training loss: 0.17008892
[2/200] Training loss: 0.05787315
[3/200] Training loss: 0.04977047
[4/200] Training loss: 0.04559964
[5/200] Training loss: 0.04481137
[6/200] Training loss: 0.04292463
[7/200] Training loss: 0.03908457
[8/200] Training loss: 0.03579676
[9/200] Training loss: 0.03377337
[10/200] Training loss: 0.03387555
[50/200] Training loss: 0.01810141
[100/200] Training loss: 0.01626908
[150/200] Training loss: 0.01513870
[200/200] Training loss: 0.01426669
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18866.802166769015 ----------
[1/200] Training loss: 0.17205831
[2/200] Training loss: 0.05947399
[3/200] Training loss: 0.05164467
[4/200] Training loss: 0.04808691
[5/200] Training loss: 0.04332058
[6/200] Training loss: 0.04283725
[7/200] Training loss: 0.03878702
[8/200] Training loss: 0.03408879
[9/200] Training loss: 0.03097660
[10/200] Training loss: 0.02764758
[50/200] Training loss: 0.01779753
[100/200] Training loss: 0.01569526
[150/200] Training loss: 0.01360956
[200/200] Training loss: 0.01329485
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21945.188447584587 ----------
[1/200] Training loss: 0.16351248
[2/200] Training loss: 0.05599576
[3/200] Training loss: 0.04952525
[4/200] Training loss: 0.04552245
[5/200] Training loss: 0.04493178
[6/200] Training loss: 0.04052296
[7/200] Training loss: 0.03770260
[8/200] Training loss: 0.03547344
[9/200] Training loss: 0.03530791
[10/200] Training loss: 0.03396098
[50/200] Training loss: 0.01908587
[100/200] Training loss: 0.01608167
[150/200] Training loss: 0.01445891
[200/200] Training loss: 0.01304618
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16255.025561345636 ----------
[1/200] Training loss: 0.15087633
[2/200] Training loss: 0.05740511
[3/200] Training loss: 0.05319788
[4/200] Training loss: 0.05181481
[5/200] Training loss: 0.04490057
[6/200] Training loss: 0.04433302
[7/200] Training loss: 0.04037888
[8/200] Training loss: 0.03685832
[9/200] Training loss: 0.03550074
[10/200] Training loss: 0.03324228
[50/200] Training loss: 0.01751421
[100/200] Training loss: 0.01428624
[150/200] Training loss: 0.01272678
[200/200] Training loss: 0.01190280
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13995.670759202647 ----------
[1/200] Training loss: 0.17301501
[2/200] Training loss: 0.05418760
[3/200] Training loss: 0.05422765
[4/200] Training loss: 0.04706441
[5/200] Training loss: 0.04745617
[6/200] Training loss: 0.04351753
[7/200] Training loss: 0.03938136
[8/200] Training loss: 0.03633245
[9/200] Training loss: 0.03315910
[10/200] Training loss: 0.02961368
[50/200] Training loss: 0.01966903
[100/200] Training loss: 0.01616807
[150/200] Training loss: 0.01500257
[200/200] Training loss: 0.01358718
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 24903.646640602656 ----------
[1/200] Training loss: 0.17458949
[2/200] Training loss: 0.06175478
[3/200] Training loss: 0.05272584
[4/200] Training loss: 0.04968916
[5/200] Training loss: 0.04431171
[6/200] Training loss: 0.04479535
[7/200] Training loss: 0.04213931
[8/200] Training loss: 0.04012196
[9/200] Training loss: 0.03997895
[10/200] Training loss: 0.03840534
[50/200] Training loss: 0.01907962
[100/200] Training loss: 0.01558940
[150/200] Training loss: 0.01383195
[200/200] Training loss: 0.01300566
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.31653039806811667 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18087.464830649984 ----------
[1/200] Training loss: 0.18053339
[2/200] Training loss: 0.06279913
[3/200] Training loss: 0.05537153
[4/200] Training loss: 0.05177962
[5/200] Training loss: 0.05201338
[6/200] Training loss: 0.04953701
[7/200] Training loss: 0.04640862
[8/200] Training loss: 0.04707271
[9/200] Training loss: 0.04118430
[10/200] Training loss: 0.04045956
[50/200] Training loss: 0.01798479
[100/200] Training loss: 0.01554781
[150/200] Training loss: 0.01465298
[200/200] Training loss: 0.01290696
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19274.64655966485 ----------
[1/200] Training loss: 0.16062623
[2/200] Training loss: 0.05322570
[3/200] Training loss: 0.05090924
[4/200] Training loss: 0.04612327
[5/200] Training loss: 0.04073571
[6/200] Training loss: 0.03873718
[7/200] Training loss: 0.03621713
[8/200] Training loss: 0.03487172
[9/200] Training loss: 0.03245528
[10/200] Training loss: 0.02864210
[50/200] Training loss: 0.01737481
[100/200] Training loss: 0.01421406
[150/200] Training loss: 0.01267020
[200/200] Training loss: 0.01164458
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19021.051916232183 ----------
[1/200] Training loss: 0.14831129
[2/200] Training loss: 0.06229307
[3/200] Training loss: 0.05010953
[4/200] Training loss: 0.04950971
[5/200] Training loss: 0.04493338
[6/200] Training loss: 0.04314612
[7/200] Training loss: 0.04012565
[8/200] Training loss: 0.03728551
[9/200] Training loss: 0.03642088
[10/200] Training loss: 0.03281899
[50/200] Training loss: 0.01787392
[100/200] Training loss: 0.01513331
[150/200] Training loss: 0.01320785
[200/200] Training loss: 0.01244913
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 27653.337447765687 ----------
[1/200] Training loss: 0.14159407
[2/200] Training loss: 0.05807479
[3/200] Training loss: 0.05085623
[4/200] Training loss: 0.04746979
[5/200] Training loss: 0.04253759
[6/200] Training loss: 0.04053729
[7/200] Training loss: 0.03616501
[8/200] Training loss: 0.03296411
[9/200] Training loss: 0.03176443
[10/200] Training loss: 0.02783558
[50/200] Training loss: 0.01819378
[100/200] Training loss: 0.01546807
[150/200] Training loss: 0.01427159
[200/200] Training loss: 0.01260423
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13617.404745398442 ----------
[1/200] Training loss: 0.17576345
[2/200] Training loss: 0.06596593
[3/200] Training loss: 0.05456235
[4/200] Training loss: 0.05115832
[5/200] Training loss: 0.05112624
[6/200] Training loss: 0.04844875
[7/200] Training loss: 0.04487977
[8/200] Training loss: 0.04273706
[9/200] Training loss: 0.03695333
[10/200] Training loss: 0.03342755
[50/200] Training loss: 0.01757981
[100/200] Training loss: 0.01492566
[150/200] Training loss: 0.01342897
[200/200] Training loss: 0.01271861
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19884.89275807139 ----------
[1/200] Training loss: 0.18419760
[2/200] Training loss: 0.05805204
[3/200] Training loss: 0.05649670
[4/200] Training loss: 0.05500962
[5/200] Training loss: 0.05007553
[6/200] Training loss: 0.04707535
[7/200] Training loss: 0.04441965
[8/200] Training loss: 0.04398682
[9/200] Training loss: 0.04368846
[10/200] Training loss: 0.04032415
[50/200] Training loss: 0.01931006
[100/200] Training loss: 0.01714839
[150/200] Training loss: 0.01495646
[200/200] Training loss: 0.01387867
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23131.1931382711 ----------
[1/200] Training loss: 0.15646211
[2/200] Training loss: 0.05997624
[3/200] Training loss: 0.05291351
[4/200] Training loss: 0.04706173
[5/200] Training loss: 0.04486494
[6/200] Training loss: 0.04244001
[7/200] Training loss: 0.03999271
[8/200] Training loss: 0.03545529
[9/200] Training loss: 0.03557907
[10/200] Training loss: 0.03138255
[50/200] Training loss: 0.01558791
[100/200] Training loss: 0.01395810
[150/200] Training loss: 0.01231350
[200/200] Training loss: 0.01250824
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7771.200422071226 ----------
[1/200] Training loss: 0.18643072
[2/200] Training loss: 0.05781666
[3/200] Training loss: 0.05470111
[4/200] Training loss: 0.04813921
[5/200] Training loss: 0.04524615
[6/200] Training loss: 0.04491565
[7/200] Training loss: 0.04027027
[8/200] Training loss: 0.04141685
[9/200] Training loss: 0.03968830
[10/200] Training loss: 0.03738648
[50/200] Training loss: 0.01852098
[100/200] Training loss: 0.01638561
[150/200] Training loss: 0.01469177
[200/200] Training loss: 0.01362516
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.31653039806811667 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6128.820441161578 ----------
[1/200] Training loss: 0.13951898
[2/200] Training loss: 0.05465285
[3/200] Training loss: 0.05198796
[4/200] Training loss: 0.04533051
[5/200] Training loss: 0.04254735
[6/200] Training loss: 0.04140327
[7/200] Training loss: 0.04136852
[8/200] Training loss: 0.03745728
[9/200] Training loss: 0.03421224
[10/200] Training loss: 0.03386238
[50/200] Training loss: 0.01901867
[100/200] Training loss: 0.01365938
[150/200] Training loss: 0.01235159
[200/200] Training loss: 0.01116046
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11895.881640298881 ----------
[1/200] Training loss: 0.18040764
[2/200] Training loss: 0.05783514
[3/200] Training loss: 0.05445855
[4/200] Training loss: 0.04842782
[5/200] Training loss: 0.04838268
[6/200] Training loss: 0.04639771
[7/200] Training loss: 0.04429451
[8/200] Training loss: 0.04079689
[9/200] Training loss: 0.04025846
[10/200] Training loss: 0.03572817
[50/200] Training loss: 0.01980956
[100/200] Training loss: 0.01678491
[150/200] Training loss: 0.01470861
[200/200] Training loss: 0.01400832
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15859.22116624899 ----------
[1/200] Training loss: 0.15876556
[2/200] Training loss: 0.05554909
[3/200] Training loss: 0.05038374
[4/200] Training loss: 0.04648829
[5/200] Training loss: 0.04234385
[6/200] Training loss: 0.04089181
[7/200] Training loss: 0.03639033
[8/200] Training loss: 0.03646998
[9/200] Training loss: 0.03360944
[10/200] Training loss: 0.03357450
[50/200] Training loss: 0.01829632
[100/200] Training loss: 0.01564117
[150/200] Training loss: 0.01516391
[200/200] Training loss: 0.01255672
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 26180.741624331426 ----------
[1/200] Training loss: 0.15763684
[2/200] Training loss: 0.05934689
[3/200] Training loss: 0.05346321
[4/200] Training loss: 0.04988487
[5/200] Training loss: 0.04497937
[6/200] Training loss: 0.04316751
[7/200] Training loss: 0.04121613
[8/200] Training loss: 0.03722757
[9/200] Training loss: 0.03593650
[10/200] Training loss: 0.03268441
[50/200] Training loss: 0.01773790
[100/200] Training loss: 0.01465400
[150/200] Training loss: 0.01331159
[200/200] Training loss: 0.01243603
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10147.33107767752 ----------
[1/200] Training loss: 0.13108854
[2/200] Training loss: 0.05573564
[3/200] Training loss: 0.05097216
[4/200] Training loss: 0.04745089
[5/200] Training loss: 0.04545144
[6/200] Training loss: 0.04264097
[7/200] Training loss: 0.04204177
[8/200] Training loss: 0.03983042
[9/200] Training loss: 0.03311700
[10/200] Training loss: 0.03070973
[50/200] Training loss: 0.01634166
[100/200] Training loss: 0.01343540
[150/200] Training loss: 0.01237498
[200/200] Training loss: 0.01162981
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15711.404774876115 ----------
[1/200] Training loss: 0.16170456
[2/200] Training loss: 0.06139099
[3/200] Training loss: 0.05468390
[4/200] Training loss: 0.05076812
[5/200] Training loss: 0.04609445
[6/200] Training loss: 0.04428659
[7/200] Training loss: 0.03920378
[8/200] Training loss: 0.03595128
[9/200] Training loss: 0.03433533
[10/200] Training loss: 0.03420072
[50/200] Training loss: 0.01603960
[100/200] Training loss: 0.01239121
[150/200] Training loss: 0.01058733
[200/200] Training loss: 0.01010183
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17470.373550671433 ----------
[1/150] Training loss: 0.16839398
[2/150] Training loss: 0.05920026
[3/150] Training loss: 0.04915667
[4/150] Training loss: 0.04577842
[5/150] Training loss: 0.04129570
[6/150] Training loss: 0.03795800
[7/150] Training loss: 0.03575171
[8/150] Training loss: 0.03453963
[9/150] Training loss: 0.03228924
[10/150] Training loss: 0.03227879
[50/150] Training loss: 0.01906610
[100/150] Training loss: 0.01588169
[150/150] Training loss: 0.01435820
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 150
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10230.233623920814 ----------
[1/200] Training loss: 0.15451264
[2/200] Training loss: 0.06074239
[3/200] Training loss: 0.05263714
[4/200] Training loss: 0.05010818
[5/200] Training loss: 0.04592035
[6/200] Training loss: 0.04462514
[7/200] Training loss: 0.04171654
[8/200] Training loss: 0.04241298
[9/200] Training loss: 0.03894556
[10/200] Training loss: 0.03366359
[50/200] Training loss: 0.01622648
[100/200] Training loss: 0.01356747
[150/200] Training loss: 0.01200125
[200/200] Training loss: 0.01162540
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19829.058273150542 ----------
[1/200] Training loss: 0.19109003
[2/200] Training loss: 0.05991179
[3/200] Training loss: 0.05474668
[4/200] Training loss: 0.05045852
[5/200] Training loss: 0.04866126
[6/200] Training loss: 0.04625142
[7/200] Training loss: 0.04386978
[8/200] Training loss: 0.04098464
[9/200] Training loss: 0.04131598
[10/200] Training loss: 0.04041381
[50/200] Training loss: 0.02387784
[100/200] Training loss: 0.01534799
[150/200] Training loss: 0.01381297
[200/200] Training loss: 0.01293304
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.31653039806811667 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13051.575230599561 ----------
[1/200] Training loss: 0.18232127
[2/200] Training loss: 0.06229594
[3/200] Training loss: 0.04839418
[4/200] Training loss: 0.04736615
[5/200] Training loss: 0.04439474
[6/200] Training loss: 0.04398081
[7/200] Training loss: 0.03842209
[8/200] Training loss: 0.03552611
[9/200] Training loss: 0.03429035
[10/200] Training loss: 0.03401334
[50/200] Training loss: 0.01765736
[100/200] Training loss: 0.01411645
[150/200] Training loss: 0.01288754
[200/200] Training loss: 0.01193629
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4280.030841010378 ----------
[1/200] Training loss: 0.15685682
[2/200] Training loss: 0.05912166
[3/200] Training loss: 0.05667232
[4/200] Training loss: 0.05037580
[5/200] Training loss: 0.04515086
[6/200] Training loss: 0.04229290
[7/200] Training loss: 0.04158085
[8/200] Training loss: 0.03915660
[9/200] Training loss: 0.03638576
[10/200] Training loss: 0.03339272
[50/200] Training loss: 0.01888565
[100/200] Training loss: 0.01522513
[150/200] Training loss: 0.01380610
[200/200] Training loss: 0.01324162
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20204.09542642283 ----------
[1/200] Training loss: 0.18586562
[2/200] Training loss: 0.06402199
[3/200] Training loss: 0.05689216
[4/200] Training loss: 0.05462059
[5/200] Training loss: 0.05134736
[6/200] Training loss: 0.05102384
[7/200] Training loss: 0.04680390
[8/200] Training loss: 0.04598265
[9/200] Training loss: 0.04264155
[10/200] Training loss: 0.04424124
[50/200] Training loss: 0.02156789
[100/200] Training loss: 0.01629192
[150/200] Training loss: 0.01435730
[200/200] Training loss: 0.01254929
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16607.16375544 ----------
[1/200] Training loss: 0.17833734
[2/200] Training loss: 0.06005770
[3/200] Training loss: 0.05179256
[4/200] Training loss: 0.04860573
[5/200] Training loss: 0.04683580
[6/200] Training loss: 0.04046587
[7/200] Training loss: 0.04052435
[8/200] Training loss: 0.03935749
[9/200] Training loss: 0.03634447
[10/200] Training loss: 0.03375300
[50/200] Training loss: 0.01871781
[100/200] Training loss: 0.01536348
[150/200] Training loss: 0.01397793
[200/200] Training loss: 0.01292399
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8030.63982506998 ----------
[1/200] Training loss: 0.18444727
[2/200] Training loss: 0.06255550
[3/200] Training loss: 0.05739055
[4/200] Training loss: 0.05185823
[5/200] Training loss: 0.04958979
[6/200] Training loss: 0.04684478
[7/200] Training loss: 0.04655275
[8/200] Training loss: 0.04317598
[9/200] Training loss: 0.03968289
[10/200] Training loss: 0.03907242
[50/200] Training loss: 0.01634484
[100/200] Training loss: 0.01515499
[150/200] Training loss: 0.01333586
[200/200] Training loss: 0.01233079
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7206.567560218942 ----------
[1/200] Training loss: 0.14646309
[2/200] Training loss: 0.06317314
[3/200] Training loss: 0.05375386
[4/200] Training loss: 0.05317638
[5/200] Training loss: 0.04836313
[6/200] Training loss: 0.04843045
[7/200] Training loss: 0.04473881
[8/200] Training loss: 0.04361072
[9/200] Training loss: 0.04013588
[10/200] Training loss: 0.03718945
[50/200] Training loss: 0.01899420
[100/200] Training loss: 0.01512264
[150/200] Training loss: 0.01443763
[200/200] Training loss: 0.01217943
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8603.99721059927 ----------
[1/200] Training loss: 0.15141000
[2/200] Training loss: 0.06279851
[3/200] Training loss: 0.05243549
[4/200] Training loss: 0.05082002
[5/200] Training loss: 0.04796659
[6/200] Training loss: 0.04745756
[7/200] Training loss: 0.04150767
[8/200] Training loss: 0.04028815
[9/200] Training loss: 0.03948105
[10/200] Training loss: 0.03671344
[50/200] Training loss: 0.01841297
[100/200] Training loss: 0.01517102
[150/200] Training loss: 0.01354037
[200/200] Training loss: 0.01224001
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9469.592599473326 ----------
[1/200] Training loss: 0.13944068
[2/200] Training loss: 0.05900733
[3/200] Training loss: 0.05275198
[4/200] Training loss: 0.04611120
[5/200] Training loss: 0.04374621
[6/200] Training loss: 0.03972557
[7/200] Training loss: 0.03980001
[8/200] Training loss: 0.03028199
[9/200] Training loss: 0.03019794
[10/200] Training loss: 0.02845026
[50/200] Training loss: 0.01629923
[100/200] Training loss: 0.01359847
[150/200] Training loss: 0.01309670
[200/200] Training loss: 0.01157668
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13995.357515976504 ----------
[1/200] Training loss: 0.16046945
[2/200] Training loss: 0.05604898
[3/200] Training loss: 0.04849489
[4/200] Training loss: 0.04616818
[5/200] Training loss: 0.04627410
[6/200] Training loss: 0.03951372
[7/200] Training loss: 0.03629927
[8/200] Training loss: 0.03391777
[9/200] Training loss: 0.03130039
[10/200] Training loss: 0.03220763
[50/200] Training loss: 0.01785502
[100/200] Training loss: 0.01563026
[150/200] Training loss: 0.01448092
[200/200] Training loss: 0.01319800
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11086.05791072733 ----------
[1/200] Training loss: 0.17997194
[2/200] Training loss: 0.06367097
[3/200] Training loss: 0.05539881
[4/200] Training loss: 0.05373553
[5/200] Training loss: 0.04832610
[6/200] Training loss: 0.04260558
[7/200] Training loss: 0.04040379
[8/200] Training loss: 0.03324022
[9/200] Training loss: 0.03642311
[10/200] Training loss: 0.03266340
[50/200] Training loss: 0.01947246
[100/200] Training loss: 0.01705105
[150/200] Training loss: 0.01557757
[200/200] Training loss: 0.01367584
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9424.188028684488 ----------
[1/200] Training loss: 0.17290705
[2/200] Training loss: 0.06109546
[3/200] Training loss: 0.05386145
[4/200] Training loss: 0.04890170
[5/200] Training loss: 0.04589117
[6/200] Training loss: 0.04226070
[7/200] Training loss: 0.04329745
[8/200] Training loss: 0.03909918
[9/200] Training loss: 0.03853069
[10/200] Training loss: 0.03498209
[50/200] Training loss: 0.01789513
[100/200] Training loss: 0.01538020
[150/200] Training loss: 0.01426969
[200/200] Training loss: 0.01328057
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14352.902424248554 ----------
[1/200] Training loss: 0.17675384
[2/200] Training loss: 0.05718546
[3/200] Training loss: 0.05329443
[4/200] Training loss: 0.04708328
[5/200] Training loss: 0.04326372
[6/200] Training loss: 0.04335434
[7/200] Training loss: 0.03956991
[8/200] Training loss: 0.03649841
[9/200] Training loss: 0.03734756
[10/200] Training loss: 0.03483981
[50/200] Training loss: 0.01707691
[100/200] Training loss: 0.01526230
[150/200] Training loss: 0.01378801
[200/200] Training loss: 0.01285522
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8392.330784710526 ----------
[1/200] Training loss: 0.14991023
[2/200] Training loss: 0.06193491
[3/200] Training loss: 0.05446379
[4/200] Training loss: 0.04601125
[5/200] Training loss: 0.04656249
[6/200] Training loss: 0.04585477
[7/200] Training loss: 0.04367786
[8/200] Training loss: 0.03650005
[9/200] Training loss: 0.03688164
[10/200] Training loss: 0.03768633
[50/200] Training loss: 0.01967242
[100/200] Training loss: 0.01637102
[150/200] Training loss: 0.01483797
[200/200] Training loss: 0.01344450
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8536.868278238806 ----------
[1/200] Training loss: 0.17344432
[2/200] Training loss: 0.05423031
[3/200] Training loss: 0.05001713
[4/200] Training loss: 0.04713051
[5/200] Training loss: 0.04163968
[6/200] Training loss: 0.03798439
[7/200] Training loss: 0.03962510
[8/200] Training loss: 0.03555990
[9/200] Training loss: 0.03212428
[10/200] Training loss: 0.03024142
[50/200] Training loss: 0.01770383
[100/200] Training loss: 0.01478107
[150/200] Training loss: 0.01392108
[200/200] Training loss: 0.01316833
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15663.600607778531 ----------
[1/200] Training loss: 0.05691030
[2/200] Training loss: 0.00424240
[3/200] Training loss: 0.00417138
[4/200] Training loss: 0.00374387
[5/200] Training loss: 0.00337584
[6/200] Training loss: 0.00320774
[7/200] Training loss: 0.00288008
[8/200] Training loss: 0.00276748
[9/200] Training loss: 0.00254544
[10/200] Training loss: 0.00231087
[50/200] Training loss: 0.00058503
[100/200] Training loss: 0.00036794
[150/200] Training loss: 0.00029381
[200/200] Training loss: 0.00026194
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10083.813564321785 ----------
[1/200] Training loss: 0.14909284
[2/200] Training loss: 0.06244895
[3/200] Training loss: 0.05570110
[4/200] Training loss: 0.05268809
[5/200] Training loss: 0.05028022
[6/200] Training loss: 0.04552941
[7/200] Training loss: 0.04204189
[8/200] Training loss: 0.03804291
[9/200] Training loss: 0.03407783
[10/200] Training loss: 0.03312559
[50/200] Training loss: 0.01722226
[100/200] Training loss: 0.01466374
[150/200] Training loss: 0.01332891
[200/200] Training loss: 0.01233913
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16902.058572848455 ----------
[1/200] Training loss: 0.16857149
[2/200] Training loss: 0.05641317
[3/200] Training loss: 0.04908481
[4/200] Training loss: 0.04597632
[5/200] Training loss: 0.04124982
[6/200] Training loss: 0.03868359
[7/200] Training loss: 0.03656048
[8/200] Training loss: 0.03361096
[9/200] Training loss: 0.03367012
[10/200] Training loss: 0.03145381
[50/200] Training loss: 0.01786939
[100/200] Training loss: 0.01475395
[150/200] Training loss: 0.01352426
[200/200] Training loss: 0.01310051
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11685.876603832508 ----------
[1/200] Training loss: 0.15041223
[2/200] Training loss: 0.05244690
[3/200] Training loss: 0.04199069
[4/200] Training loss: 0.03533783
[5/200] Training loss: 0.03073547
[6/200] Training loss: 0.02796339
[7/200] Training loss: 0.03626360
[8/200] Training loss: 0.02992847
[9/200] Training loss: 0.02976529
[10/200] Training loss: 0.03002959
[50/200] Training loss: 0.01842338
[100/200] Training loss: 0.01547806
[150/200] Training loss: 0.01612049
[200/200] Training loss: 0.01422434
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adam ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14815.043165647545 ----------
[1/200] Training loss: 0.15669912
[2/200] Training loss: 0.05905361
[3/200] Training loss: 0.05152745
[4/200] Training loss: 0.04747579
[5/200] Training loss: 0.04765968
[6/200] Training loss: 0.04400952
[7/200] Training loss: 0.04232493
[8/200] Training loss: 0.04040890
[9/200] Training loss: 0.03684537
[10/200] Training loss: 0.03622705
[50/200] Training loss: 0.01889957
[100/200] Training loss: 0.01645745
[150/200] Training loss: 0.01336713
[200/200] Training loss: 0.01233468
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10346.996085821236 ----------
[1/200] Training loss: 0.16225567
[2/200] Training loss: 0.05687620
[3/200] Training loss: 0.04948123
[4/200] Training loss: 0.04707101
[5/200] Training loss: 0.04484278
[6/200] Training loss: 0.04042221
[7/200] Training loss: 0.04032876
[8/200] Training loss: 0.03617638
[9/200] Training loss: 0.03624178
[10/200] Training loss: 0.03257814
[50/200] Training loss: 0.01913503
[100/200] Training loss: 0.01537636
[150/200] Training loss: 0.01366014
[200/200] Training loss: 0.01366619
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8790.13811040532 ----------
[1/200] Training loss: 0.16405916
[2/200] Training loss: 0.05882066
[3/200] Training loss: 0.05086849
[4/200] Training loss: 0.04802286
[5/200] Training loss: 0.04607287
[6/200] Training loss: 0.04265701
[7/200] Training loss: 0.03739866
[8/200] Training loss: 0.04115752
[9/200] Training loss: 0.03427217
[10/200] Training loss: 0.03613005
[50/200] Training loss: 0.01878672
[100/200] Training loss: 0.01465729
[150/200] Training loss: 0.01294833
[200/200] Training loss: 0.01189633
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14986.662336891426 ----------
[1/200] Training loss: 0.16117476
[2/200] Training loss: 0.05734761
[3/200] Training loss: 0.05225453
[4/200] Training loss: 0.04904826
[5/200] Training loss: 0.04804034
[6/200] Training loss: 0.04419070
[7/200] Training loss: 0.04154994
[8/200] Training loss: 0.03587844
[9/200] Training loss: 0.03665650
[10/200] Training loss: 0.03442724
[50/200] Training loss: 0.01905317
[100/200] Training loss: 0.01585272
[150/200] Training loss: 0.01444373
[200/200] Training loss: 0.01291903
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14514.851428795268 ----------
[1/200] Training loss: 0.16797341
[2/200] Training loss: 0.05835693
[3/200] Training loss: 0.05483694
[4/200] Training loss: 0.05136203
[5/200] Training loss: 0.04399966
[6/200] Training loss: 0.04079605
[7/200] Training loss: 0.03848526
[8/200] Training loss: 0.03654802
[9/200] Training loss: 0.03407397
[10/200] Training loss: 0.03184273
[50/200] Training loss: 0.01592319
[100/200] Training loss: 0.01364849
[150/200] Training loss: 0.01246143
[200/200] Training loss: 0.01151060
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9319.992703859805 ----------
[1/200] Training loss: 0.15345008
[2/200] Training loss: 0.05953506
[3/200] Training loss: 0.05424728
[4/200] Training loss: 0.04900857
[5/200] Training loss: 0.04639906
[6/200] Training loss: 0.04363519
[7/200] Training loss: 0.04239582
[8/200] Training loss: 0.03681113
[9/200] Training loss: 0.03826389
[10/200] Training loss: 0.03745718
[50/200] Training loss: 0.01679591
[100/200] Training loss: 0.01351704
[150/200] Training loss: 0.01195605
[200/200] Training loss: 0.01115356
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15348.038571752419 ----------
[1/200] Training loss: 0.17624897
[2/200] Training loss: 0.06186944
[3/200] Training loss: 0.05271240
[4/200] Training loss: 0.05460589
[5/200] Training loss: 0.05063220
[6/200] Training loss: 0.04729218
[7/200] Training loss: 0.04550039
[8/200] Training loss: 0.04471426
[9/200] Training loss: 0.04098446
[10/200] Training loss: 0.03977136
[50/200] Training loss: 0.01759975
[100/200] Training loss: 0.01534642
[150/200] Training loss: 0.01394877
[200/200] Training loss: 0.01339671
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8544.536031874404 ----------
[1/200] Training loss: 0.17837938
[2/200] Training loss: 0.05995054
[3/200] Training loss: 0.06056261
[4/200] Training loss: 0.04969965
[5/200] Training loss: 0.04802527
[6/200] Training loss: 0.04679824
[7/200] Training loss: 0.04491754
[8/200] Training loss: 0.04235956
[9/200] Training loss: 0.04299719
[10/200] Training loss: 0.03946137
[50/200] Training loss: 0.01891609
[100/200] Training loss: 0.01553022
[150/200] Training loss: 0.01466366
[200/200] Training loss: 0.01357944
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21960.710371023975 ----------
[1/200] Training loss: 0.19938288
[2/200] Training loss: 0.06259232
[3/200] Training loss: 0.05754137
[4/200] Training loss: 0.05487297
[5/200] Training loss: 0.05378551
[6/200] Training loss: 0.04976829
[7/200] Training loss: 0.04985473
[8/200] Training loss: 0.04735141
[9/200] Training loss: 0.04732402
[10/200] Training loss: 0.04734616
[50/200] Training loss: 0.01958076
[100/200] Training loss: 0.01677505
[150/200] Training loss: 0.01572459
[200/200] Training loss: 0.01470812
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11452.023052718676 ----------
[1/200] Training loss: 0.14344154
[2/200] Training loss: 0.05360507
[3/200] Training loss: 0.04619001
[4/200] Training loss: 0.04404988
[5/200] Training loss: 0.04036801
[6/200] Training loss: 0.03789939
[7/200] Training loss: 0.03663347
[8/200] Training loss: 0.03557976
[9/200] Training loss: 0.03096154
[10/200] Training loss: 0.03077794
[50/200] Training loss: 0.01818337
[100/200] Training loss: 0.01561727
[150/200] Training loss: 0.01410335
[200/200] Training loss: 0.01325157
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10990.376517663079 ----------
[1/200] Training loss: 0.18591265
[2/200] Training loss: 0.06042942
[3/200] Training loss: 0.06033434
[4/200] Training loss: 0.05668898
[5/200] Training loss: 0.05329102
[6/200] Training loss: 0.05344654
[7/200] Training loss: 0.04941256
[8/200] Training loss: 0.04339864
[9/200] Training loss: 0.04470454
[10/200] Training loss: 0.04110542
[50/200] Training loss: 0.01849540
[100/200] Training loss: 0.01568942
[150/200] Training loss: 0.01402148
[200/200] Training loss: 0.01387625
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17202.159864389123 ----------
[1/200] Training loss: 0.19157250
[2/200] Training loss: 0.07349951
[3/200] Training loss: 0.06156368
[4/200] Training loss: 0.05954651
[5/200] Training loss: 0.05064756
[6/200] Training loss: 0.05143504
[7/200] Training loss: 0.04837819
[8/200] Training loss: 0.04731196
[9/200] Training loss: 0.04650618
[10/200] Training loss: 0.04220975
[50/200] Training loss: 0.01974399
[100/200] Training loss: 0.01708948
[150/200] Training loss: 0.01500066
[200/200] Training loss: 0.01432132
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8349.689814597905 ----------
[1/200] Training loss: 0.15521138
[2/200] Training loss: 0.05889993
[3/200] Training loss: 0.05336822
[4/200] Training loss: 0.04817667
[5/200] Training loss: 0.04310667
[6/200] Training loss: 0.04090716
[7/200] Training loss: 0.03591348
[8/200] Training loss: 0.03460510
[9/200] Training loss: 0.03366574
[10/200] Training loss: 0.03008784
[50/200] Training loss: 0.01691930
[100/200] Training loss: 0.01500465
[150/200] Training loss: 0.01327000
[200/200] Training loss: 0.01224827
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11520.987457679137 ----------
[1/200] Training loss: 0.18187504
[2/200] Training loss: 0.06635973
[3/200] Training loss: 0.05794731
[4/200] Training loss: 0.05540044
[5/200] Training loss: 0.05454387
[6/200] Training loss: 0.05013133
[7/200] Training loss: 0.05001617
[8/200] Training loss: 0.04814848
[9/200] Training loss: 0.04649245
[10/200] Training loss: 0.04418986
[50/200] Training loss: 0.01904849
[100/200] Training loss: 0.01522652
[150/200] Training loss: 0.01422478
[200/200] Training loss: 0.01306243
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15571.200595972039 ----------
[1/200] Training loss: 0.14751880
[2/200] Training loss: 0.05198620
[3/200] Training loss: 0.04686126
[4/200] Training loss: 0.04405046
[5/200] Training loss: 0.03764142
[6/200] Training loss: 0.03498423
[7/200] Training loss: 0.03520716
[8/200] Training loss: 0.03114134
[9/200] Training loss: 0.03110819
[10/200] Training loss: 0.02675833
[50/200] Training loss: 0.01639510
[100/200] Training loss: 0.01315635
[150/200] Training loss: 0.01173001
[200/200] Training loss: 0.01071087
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13140.798758066421 ----------
[1/200] Training loss: 0.13758934
[2/200] Training loss: 0.05791258
[3/200] Training loss: 0.05432452
[4/200] Training loss: 0.04742214
[5/200] Training loss: 0.04796530
[6/200] Training loss: 0.04523511
[7/200] Training loss: 0.03762268
[8/200] Training loss: 0.03685020
[9/200] Training loss: 0.03536586
[10/200] Training loss: 0.03116032
[50/200] Training loss: 0.01747117
[100/200] Training loss: 0.01526910
[150/200] Training loss: 0.01383251
[200/200] Training loss: 0.01320338
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18234.13238955997 ----------
[1/200] Training loss: 0.13521915
[2/200] Training loss: 0.06323288
[3/200] Training loss: 0.05408496
[4/200] Training loss: 0.05296810
[5/200] Training loss: 0.04746931
[6/200] Training loss: 0.04461568
[7/200] Training loss: 0.04191456
[8/200] Training loss: 0.04394758
[9/200] Training loss: 0.03762441
[10/200] Training loss: 0.03730195
[50/200] Training loss: 0.01661163
[100/200] Training loss: 0.01453290
[150/200] Training loss: 0.01341088
[200/200] Training loss: 0.01230688
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 26453.44166644484 ----------
[1/200] Training loss: 0.18407423
[2/200] Training loss: 0.06192296
[3/200] Training loss: 0.05431997
[4/200] Training loss: 0.05179217
[5/200] Training loss: 0.04899578
[6/200] Training loss: 0.04817928
[7/200] Training loss: 0.04111632
[8/200] Training loss: 0.04220649
[9/200] Training loss: 0.03790279
[10/200] Training loss: 0.03681374
[50/200] Training loss: 0.01929922
[100/200] Training loss: 0.01572350
[150/200] Training loss: 0.01427488
[200/200] Training loss: 0.01326264
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14386.65715168051 ----------
[1/200] Training loss: 0.17118040
[2/200] Training loss: 0.06242248
[3/200] Training loss: 0.05262523
[4/200] Training loss: 0.05119552
[5/200] Training loss: 0.04407766
[6/200] Training loss: 0.04238150
[7/200] Training loss: 0.03941983
[8/200] Training loss: 0.03930381
[9/200] Training loss: 0.03468782
[10/200] Training loss: 0.03483401
[50/200] Training loss: 0.01728061
[100/200] Training loss: 0.01487661
[150/200] Training loss: 0.01371859
[200/200] Training loss: 0.01334188
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7068.849694257192 ----------
[1/200] Training loss: 0.17421746
[2/200] Training loss: 0.05941483
[3/200] Training loss: 0.05386766
[4/200] Training loss: 0.05129027
[5/200] Training loss: 0.04765482
[6/200] Training loss: 0.04671665
[7/200] Training loss: 0.04481483
[8/200] Training loss: 0.04004317
[9/200] Training loss: 0.03943589
[10/200] Training loss: 0.03465227
[50/200] Training loss: 0.01652619
[100/200] Training loss: 0.01457201
[150/200] Training loss: 0.01371140
[200/200] Training loss: 0.01268950
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8424.88266980615 ----------
[1/200] Training loss: 0.17730836
[2/200] Training loss: 0.05861403
[3/200] Training loss: 0.05504980
[4/200] Training loss: 0.05170660
[5/200] Training loss: 0.04757339
[6/200] Training loss: 0.04613919
[7/200] Training loss: 0.04519894
[8/200] Training loss: 0.04222895
[9/200] Training loss: 0.03891675
[10/200] Training loss: 0.03701141
[50/200] Training loss: 0.01823631
[100/200] Training loss: 0.01646579
[150/200] Training loss: 0.01521620
[200/200] Training loss: 0.01413908
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21483.841742109347 ----------
[1/200] Training loss: 0.15306389
[2/200] Training loss: 0.05732886
[3/200] Training loss: 0.05598254
[4/200] Training loss: 0.04852678
[5/200] Training loss: 0.04557665
[6/200] Training loss: 0.04381660
[7/200] Training loss: 0.03710297
[8/200] Training loss: 0.03668065
[9/200] Training loss: 0.03583682
[10/200] Training loss: 0.03429357
[50/200] Training loss: 0.01785929
[100/200] Training loss: 0.01563076
[150/200] Training loss: 0.01365632
[200/200] Training loss: 0.01222418
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6048671496929195 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17166.73620697889 ----------
[1/200] Training loss: 0.02438001
[2/200] Training loss: 0.00246916
[3/200] Training loss: 0.00223739
[4/200] Training loss: 0.00203069
[5/200] Training loss: 0.00185592
[6/200] Training loss: 0.00161652
[7/200] Training loss: 0.00145685
[8/200] Training loss: 0.00128245
[9/200] Training loss: 0.00111681
[10/200] Training loss: 0.00102677
[50/200] Training loss: 0.00025803
[100/200] Training loss: 0.00016866
[150/200] Training loss: 0.00014074
[200/200] Training loss: 0.00013455
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15032.18573594672 ----------
[1/200] Training loss: 0.13687333
[2/200] Training loss: 0.05584953
[3/200] Training loss: 0.04836072
[4/200] Training loss: 0.04123132
[5/200] Training loss: 0.04359934
[6/200] Training loss: 0.03826183
[7/200] Training loss: 0.03767804
[8/200] Training loss: 0.03546564
[9/200] Training loss: 0.03448482
[10/200] Training loss: 0.03188803
[50/200] Training loss: 0.01653927
[100/200] Training loss: 0.01400544
[150/200] Training loss: 0.01240602
[200/200] Training loss: 0.01141947
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7755.085815128031 ----------
[1/200] Training loss: 0.16439902
[2/200] Training loss: 0.05678094
[3/200] Training loss: 0.05213835
[4/200] Training loss: 0.04629082
[5/200] Training loss: 0.04758924
[6/200] Training loss: 0.04358311
[7/200] Training loss: 0.04301726
[8/200] Training loss: 0.03916161
[9/200] Training loss: 0.03602843
[10/200] Training loss: 0.03497197
[50/200] Training loss: 0.01941058
[100/200] Training loss: 0.01535366
[150/200] Training loss: 0.01319960
[200/200] Training loss: 0.01224599
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6040.465544972507 ----------
[1/200] Training loss: 0.14935047
[2/200] Training loss: 0.05990828
[3/200] Training loss: 0.05318327
[4/200] Training loss: 0.05025252
[5/200] Training loss: 0.04466732
[6/200] Training loss: 0.04036344
[7/200] Training loss: 0.03871770
[8/200] Training loss: 0.03445601
[9/200] Training loss: 0.03300217
[10/200] Training loss: 0.03059317
[50/200] Training loss: 0.01949433
[100/200] Training loss: 0.01758144
[150/200] Training loss: 0.01576284
[200/200] Training loss: 0.01536496
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 29313.730298274902 ----------
[1/200] Training loss: 0.12784066
[2/200] Training loss: 0.06269040
[3/200] Training loss: 0.05114559
[4/200] Training loss: 0.05016926
[5/200] Training loss: 0.04301639
[6/200] Training loss: 0.03908420
[7/200] Training loss: 0.03911124
[8/200] Training loss: 0.03445134
[9/200] Training loss: 0.03204546
[10/200] Training loss: 0.03359265
[50/200] Training loss: 0.01889761
[100/200] Training loss: 0.01588026
[150/200] Training loss: 0.01399242
[200/200] Training loss: 0.01226442
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16571.811246813064 ----------
[1/200] Training loss: 0.15245132
[2/200] Training loss: 0.05750710
[3/200] Training loss: 0.05054164
[4/200] Training loss: 0.04706008
[5/200] Training loss: 0.04423627
[6/200] Training loss: 0.04083261
[7/200] Training loss: 0.03706594
[8/200] Training loss: 0.03498738
[9/200] Training loss: 0.02950518
[10/200] Training loss: 0.02903533
[50/200] Training loss: 0.01891396
[100/200] Training loss: 0.01655185
[150/200] Training loss: 0.01510588
[200/200] Training loss: 0.01316776
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16111.22043794324 ----------
[1/200] Training loss: 0.14383006
[2/200] Training loss: 0.05730370
[3/200] Training loss: 0.05199200
[4/200] Training loss: 0.04743602
[5/200] Training loss: 0.04580574
[6/200] Training loss: 0.04460603
[7/200] Training loss: 0.04212899
[8/200] Training loss: 0.03757491
[9/200] Training loss: 0.03739657
[10/200] Training loss: 0.03514844
[50/200] Training loss: 0.01975189
[100/200] Training loss: 0.01462485
[150/200] Training loss: 0.01283464
[200/200] Training loss: 0.01181933
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5809.775210797747 ----------
[1/200] Training loss: 0.17573689
[2/200] Training loss: 0.06979612
[3/200] Training loss: 0.05704019
[4/200] Training loss: 0.04804572
[5/200] Training loss: 0.04981133
[6/200] Training loss: 0.04550981
[7/200] Training loss: 0.04490205
[8/200] Training loss: 0.04234984
[9/200] Training loss: 0.04088606
[10/200] Training loss: 0.03699394
[50/200] Training loss: 0.01859988
[100/200] Training loss: 0.01522601
[150/200] Training loss: 0.01394791
[200/200] Training loss: 0.01340908
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7587.991829199607 ----------
[1/200] Training loss: 0.17946421
[2/200] Training loss: 0.06142488
[3/200] Training loss: 0.05586302
[4/200] Training loss: 0.05323000
[5/200] Training loss: 0.04907472
[6/200] Training loss: 0.04786285
[7/200] Training loss: 0.04417267
[8/200] Training loss: 0.04184551
[9/200] Training loss: 0.04017193
[10/200] Training loss: 0.03665784
[50/200] Training loss: 0.01659613
[100/200] Training loss: 0.01411272
[150/200] Training loss: 0.01307988
[200/200] Training loss: 0.01207954
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9686.935944869254 ----------
[1/200] Training loss: 0.17667543
[2/200] Training loss: 0.05703697
[3/200] Training loss: 0.04926352
[4/200] Training loss: 0.05148738
[5/200] Training loss: 0.04452255
[6/200] Training loss: 0.04212305
[7/200] Training loss: 0.04247029
[8/200] Training loss: 0.03776622
[9/200] Training loss: 0.03834687
[10/200] Training loss: 0.03699065
[50/200] Training loss: 0.01761144
[100/200] Training loss: 0.01542304
[150/200] Training loss: 0.01382740
[200/200] Training loss: 0.01304674
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9980.170740022437 ----------
[1/200] Training loss: 0.15372943
[2/200] Training loss: 0.05821443
[3/200] Training loss: 0.05207817
[4/200] Training loss: 0.04754414
[5/200] Training loss: 0.04635309
[6/200] Training loss: 0.04222194
[7/200] Training loss: 0.03992749
[8/200] Training loss: 0.03880651
[9/200] Training loss: 0.03650709
[10/200] Training loss: 0.03407137
[50/200] Training loss: 0.01698758
[100/200] Training loss: 0.01340522
[150/200] Training loss: 0.01238440
[200/200] Training loss: 0.01186831
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15463.581473901833 ----------
[1/200] Training loss: 0.15821296
[2/200] Training loss: 0.05746947
[3/200] Training loss: 0.04922886
[4/200] Training loss: 0.04523601
[5/200] Training loss: 0.04301602
[6/200] Training loss: 0.03950231
[7/200] Training loss: 0.03950231
[8/200] Training loss: 0.03519260
[9/200] Training loss: 0.03444493
[10/200] Training loss: 0.03554687
[50/200] Training loss: 0.01732115
[100/200] Training loss: 0.01536671
[150/200] Training loss: 0.01321169
[200/200] Training loss: 0.01177291
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 31850.073783274034 ----------
[1/200] Training loss: 0.10669737
[2/200] Training loss: 0.05539737
[3/200] Training loss: 0.05065564
[4/200] Training loss: 0.04352333
[5/200] Training loss: 0.03866793
[6/200] Training loss: 0.03395859
[7/200] Training loss: 0.03000812
[8/200] Training loss: 0.02825249
[9/200] Training loss: 0.02713643
[10/200] Training loss: 0.02591064
[50/200] Training loss: 0.01440967
[100/200] Training loss: 0.01240473
[150/200] Training loss: 0.01111926
[200/200] Training loss: 0.01046988
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16571.08904085667 ----------
[1/200] Training loss: 0.14713079
[2/200] Training loss: 0.05736086
[3/200] Training loss: 0.05040019
[4/200] Training loss: 0.04535487
[5/200] Training loss: 0.04357266
[6/200] Training loss: 0.03741052
[7/200] Training loss: 0.03679726
[8/200] Training loss: 0.03416825
[9/200] Training loss: 0.03014002
[10/200] Training loss: 0.03248882
[50/200] Training loss: 0.01740456
[100/200] Training loss: 0.01536180
[150/200] Training loss: 0.01327993
[200/200] Training loss: 0.01282444
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11682.824316063303 ----------
[1/200] Training loss: 0.15374589
[2/200] Training loss: 0.05674484
[3/200] Training loss: 0.05114103
[4/200] Training loss: 0.04723639
[5/200] Training loss: 0.04492214
[6/200] Training loss: 0.04230511
[7/200] Training loss: 0.03929996
[8/200] Training loss: 0.03420739
[9/200] Training loss: 0.03408139
[10/200] Training loss: 0.03145892
[50/200] Training loss: 0.01820745
[100/200] Training loss: 0.01530682
[150/200] Training loss: 0.01418570
[200/200] Training loss: 0.01329416
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8608.658896715562 ----------
[1/200] Training loss: 0.16784007
[2/200] Training loss: 0.06232921
[3/200] Training loss: 0.05300259
[4/200] Training loss: 0.05104718
[5/200] Training loss: 0.04722927
[6/200] Training loss: 0.04608533
[7/200] Training loss: 0.04273197
[8/200] Training loss: 0.04005903
[9/200] Training loss: 0.03993681
[10/200] Training loss: 0.03671022
[50/200] Training loss: 0.01861537
[100/200] Training loss: 0.01597967
[150/200] Training loss: 0.01482443
[200/200] Training loss: 0.01378402
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10365.229182222649 ----------
[1/200] Training loss: 0.18793401
[2/200] Training loss: 0.05875995
[3/200] Training loss: 0.05535622
[4/200] Training loss: 0.05166209
[5/200] Training loss: 0.04890980
[6/200] Training loss: 0.04248736
[7/200] Training loss: 0.04154630
[8/200] Training loss: 0.03944113
[9/200] Training loss: 0.03959664
[10/200] Training loss: 0.03621185
[50/200] Training loss: 0.01528871
[100/200] Training loss: 0.01422611
[150/200] Training loss: 0.01262144
[200/200] Training loss: 0.01176668
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19257.531305959234 ----------
[1/200] Training loss: 0.16213912
[2/200] Training loss: 0.06142210
[3/200] Training loss: 0.05382522
[4/200] Training loss: 0.05401766
[5/200] Training loss: 0.05042635
[6/200] Training loss: 0.04905946
[7/200] Training loss: 0.04744351
[8/200] Training loss: 0.04380180
[9/200] Training loss: 0.04333014
[10/200] Training loss: 0.03985056
[50/200] Training loss: 0.01898529
[100/200] Training loss: 0.01631219
[150/200] Training loss: 0.01529005
[200/200] Training loss: 0.01475804
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17125.731750789513 ----------
[1/200] Training loss: 0.15400643
[2/200] Training loss: 0.05724707
[3/200] Training loss: 0.04976616
[4/200] Training loss: 0.04834998
[5/200] Training loss: 0.04633395
[6/200] Training loss: 0.03860968
[7/200] Training loss: 0.03721891
[8/200] Training loss: 0.03252292
[9/200] Training loss: 0.03365591
[10/200] Training loss: 0.03392382
[50/200] Training loss: 0.01671365
[100/200] Training loss: 0.01429107
[150/200] Training loss: 0.01347854
[200/200] Training loss: 0.01208113
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 29504.25921795021 ----------
[1/200] Training loss: 0.16990969
[2/200] Training loss: 0.05755848
[3/200] Training loss: 0.05426581
[4/200] Training loss: 0.04823075
[5/200] Training loss: 0.04547792
[6/200] Training loss: 0.04287755
[7/200] Training loss: 0.03972729
[8/200] Training loss: 0.03876607
[9/200] Training loss: 0.03330493
[10/200] Training loss: 0.02963849
[50/200] Training loss: 0.01817269
[100/200] Training loss: 0.01731983
[150/200] Training loss: 0.01463949
[200/200] Training loss: 0.01317279
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12420.866958469525 ----------
[1/200] Training loss: 0.13972404
[2/200] Training loss: 0.05767346
[3/200] Training loss: 0.04788623
[4/200] Training loss: 0.04496111
[5/200] Training loss: 0.04286834
[6/200] Training loss: 0.04102628
[7/200] Training loss: 0.03679532
[8/200] Training loss: 0.03230929
[9/200] Training loss: 0.03444750
[10/200] Training loss: 0.03183790
[50/200] Training loss: 0.01824798
[100/200] Training loss: 0.01638130
[150/200] Training loss: 0.01396970
[200/200] Training loss: 0.01277546
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17218.679159563893 ----------
[1/200] Training loss: 0.16832148
[2/200] Training loss: 0.05614332
[3/200] Training loss: 0.05224890
[4/200] Training loss: 0.04612726
[5/200] Training loss: 0.04369527
[6/200] Training loss: 0.03865565
[7/200] Training loss: 0.03688800
[8/200] Training loss: 0.03334183
[9/200] Training loss: 0.03282905
[10/200] Training loss: 0.03234223
[50/200] Training loss: 0.01801768
[100/200] Training loss: 0.01578279
[150/200] Training loss: 0.01441868
[200/200] Training loss: 0.01337415
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10823.844049135223 ----------
[1/200] Training loss: 0.13104908
[2/200] Training loss: 0.05401128
[3/200] Training loss: 0.04996971
[4/200] Training loss: 0.04751473
[5/200] Training loss: 0.04279713
[6/200] Training loss: 0.04112080
[7/200] Training loss: 0.03972663
[8/200] Training loss: 0.03821038
[9/200] Training loss: 0.03626112
[10/200] Training loss: 0.03243576
[50/200] Training loss: 0.01721128
[100/200] Training loss: 0.01425243
[150/200] Training loss: 0.01330997
[200/200] Training loss: 0.01153199
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18638.181027128157 ----------
[1/200] Training loss: 0.14171227
[2/200] Training loss: 0.05671257
[3/200] Training loss: 0.04800603
[4/200] Training loss: 0.04814131
[5/200] Training loss: 0.04219955
[6/200] Training loss: 0.04207403
[7/200] Training loss: 0.03798506
[8/200] Training loss: 0.03729225
[9/200] Training loss: 0.03367806
[10/200] Training loss: 0.03473567
[50/200] Training loss: 0.01697190
[100/200] Training loss: 0.01384876
[150/200] Training loss: 0.01294566
[200/200] Training loss: 0.01226814
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13603.211973647989 ----------
[1/200] Training loss: 0.14045309
[2/200] Training loss: 0.05772844
[3/200] Training loss: 0.05061452
[4/200] Training loss: 0.04426803
[5/200] Training loss: 0.04183759
[6/200] Training loss: 0.04109479
[7/200] Training loss: 0.03790281
[8/200] Training loss: 0.03564051
[9/200] Training loss: 0.03223068
[10/200] Training loss: 0.02993999
[50/200] Training loss: 0.01663063
[100/200] Training loss: 0.01333272
[150/200] Training loss: 0.01269698
[200/200] Training loss: 0.01258926
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17834.80821315441 ----------
[1/200] Training loss: 0.14370878
[2/200] Training loss: 0.05780535
[3/200] Training loss: 0.05230899
[4/200] Training loss: 0.04839620
[5/200] Training loss: 0.04360853
[6/200] Training loss: 0.04225164
[7/200] Training loss: 0.03454498
[8/200] Training loss: 0.03332359
[9/200] Training loss: 0.02998265
[10/200] Training loss: 0.02796151
[50/200] Training loss: 0.01501842
[100/200] Training loss: 0.01245783
[150/200] Training loss: 0.01179038
[200/200] Training loss: 0.01067423
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6796.025603247827 ----------
[1/200] Training loss: 0.16983203
[2/200] Training loss: 0.05988238
[3/200] Training loss: 0.05358871
[4/200] Training loss: 0.05291446
[5/200] Training loss: 0.04646563
[6/200] Training loss: 0.04521174
[7/200] Training loss: 0.03934299
[8/200] Training loss: 0.03680473
[9/200] Training loss: 0.03561936
[10/200] Training loss: 0.03302326
[50/200] Training loss: 0.01696141
[100/200] Training loss: 0.01420983
[150/200] Training loss: 0.01275299
[200/200] Training loss: 0.01134655
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16258.388604040685 ----------
[1/200] Training loss: 0.17454609
[2/200] Training loss: 0.05876277
[3/200] Training loss: 0.05160370
[4/200] Training loss: 0.04797774
[5/200] Training loss: 0.04344669
[6/200] Training loss: 0.04183152
[7/200] Training loss: 0.03721989
[8/200] Training loss: 0.03454157
[9/200] Training loss: 0.03265465
[10/200] Training loss: 0.03147940
[50/200] Training loss: 0.01701384
[100/200] Training loss: 0.01454351
[150/200] Training loss: 0.01307531
[200/200] Training loss: 0.01248580
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11320.561116835155 ----------
[1/200] Training loss: 0.19997912
[2/200] Training loss: 0.06368285
[3/200] Training loss: 0.05271610
[4/200] Training loss: 0.05416586
[5/200] Training loss: 0.05202244
[6/200] Training loss: 0.04700996
[7/200] Training loss: 0.04330469
[8/200] Training loss: 0.03845288
[9/200] Training loss: 0.03644624
[10/200] Training loss: 0.03396296
[50/200] Training loss: 0.01745256
[100/200] Training loss: 0.01566772
[150/200] Training loss: 0.01386662
[200/200] Training loss: 0.01367466
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19294.743740200334 ----------
[1/200] Training loss: 0.16881485
[2/200] Training loss: 0.05785762
[3/200] Training loss: 0.05157710
[4/200] Training loss: 0.04650889
[5/200] Training loss: 0.04117389
[6/200] Training loss: 0.04150360
[7/200] Training loss: 0.03837859
[8/200] Training loss: 0.03353101
[9/200] Training loss: 0.03402925
[10/200] Training loss: 0.03070082
[50/200] Training loss: 0.01781597
[100/200] Training loss: 0.01647936
[150/200] Training loss: 0.01490490
[200/200] Training loss: 0.01374380
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7611.076402191743 ----------
[1/200] Training loss: 0.17102592
[2/200] Training loss: 0.06998605
[3/200] Training loss: 0.05974948
[4/200] Training loss: 0.05400309
[5/200] Training loss: 0.05233548
[6/200] Training loss: 0.05134080
[7/200] Training loss: 0.04850603
[8/200] Training loss: 0.04475387
[9/200] Training loss: 0.04304227
[10/200] Training loss: 0.04248669
[50/200] Training loss: 0.02066426
[100/200] Training loss: 0.01586628
[150/200] Training loss: 0.01322393
[200/200] Training loss: 0.01234016
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6027.936628731261 ----------
[1/200] Training loss: 0.15944671
[2/200] Training loss: 0.05554728
[3/200] Training loss: 0.05247937
[4/200] Training loss: 0.04797571
[5/200] Training loss: 0.04381067
[6/200] Training loss: 0.04294396
[7/200] Training loss: 0.03962494
[8/200] Training loss: 0.03669351
[9/200] Training loss: 0.03612966
[10/200] Training loss: 0.03471355
[50/200] Training loss: 0.02148246
[100/200] Training loss: 0.01571487
[150/200] Training loss: 0.01439466
[200/200] Training loss: 0.01389668
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14830.826275025947 ----------
[1/200] Training loss: 0.17532879
[2/200] Training loss: 0.06805603
[3/200] Training loss: 0.05758080
[4/200] Training loss: 0.05309697
[5/200] Training loss: 0.05316573
[6/200] Training loss: 0.05168038
[7/200] Training loss: 0.04802185
[8/200] Training loss: 0.04635489
[9/200] Training loss: 0.04465028
[10/200] Training loss: 0.04243760
[50/200] Training loss: 0.01845944
[100/200] Training loss: 0.01575927
[150/200] Training loss: 0.01486012
[200/200] Training loss: 0.01312653
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9724.062114157849 ----------
[1/200] Training loss: 0.15567302
[2/200] Training loss: 0.05545732
[3/200] Training loss: 0.04973787
[4/200] Training loss: 0.04429695
[5/200] Training loss: 0.04505749
[6/200] Training loss: 0.03971375
[7/200] Training loss: 0.03743164
[8/200] Training loss: 0.03736752
[9/200] Training loss: 0.03533116
[10/200] Training loss: 0.03412105
[50/200] Training loss: 0.01804760
[100/200] Training loss: 0.01463453
[150/200] Training loss: 0.01416880
[200/200] Training loss: 0.01340349
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17393.401967412814 ----------
[1/200] Training loss: 0.17364376
[2/200] Training loss: 0.06573947
[3/200] Training loss: 0.06074939
[4/200] Training loss: 0.05241882
[5/200] Training loss: 0.05235199
[6/200] Training loss: 0.05094031
[7/200] Training loss: 0.04907317
[8/200] Training loss: 0.04347167
[9/200] Training loss: 0.04176038
[10/200] Training loss: 0.04164264
[50/200] Training loss: 0.01981152
[100/200] Training loss: 0.01699097
[150/200] Training loss: 0.01588335
[200/200] Training loss: 0.01459067
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12022.661934862845 ----------
[1/200] Training loss: 0.17853493
[2/200] Training loss: 0.06623690
[3/200] Training loss: 0.05810964
[4/200] Training loss: 0.05294843
[5/200] Training loss: 0.05142074
[6/200] Training loss: 0.05061661
[7/200] Training loss: 0.04864125
[8/200] Training loss: 0.04605125
[9/200] Training loss: 0.04703329
[10/200] Training loss: 0.04633782
[50/200] Training loss: 0.01932724
[100/200] Training loss: 0.01664968
[150/200] Training loss: 0.01550593
[200/200] Training loss: 0.01387044
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22410.62033947298 ----------
[1/200] Training loss: 0.18146045
[2/200] Training loss: 0.06065044
[3/200] Training loss: 0.05777403
[4/200] Training loss: 0.05322406
[5/200] Training loss: 0.04961102
[6/200] Training loss: 0.04873907
[7/200] Training loss: 0.04499531
[8/200] Training loss: 0.04482140
[9/200] Training loss: 0.04188447
[10/200] Training loss: 0.04218697
[50/200] Training loss: 0.01869948
[100/200] Training loss: 0.01559970
[150/200] Training loss: 0.01423048
[200/200] Training loss: 0.01343217
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6341.126082960344 ----------
[1/200] Training loss: 0.16419248
[2/200] Training loss: 0.06142115
[3/200] Training loss: 0.05417460
[4/200] Training loss: 0.04991661
[5/200] Training loss: 0.04924775
[6/200] Training loss: 0.04590338
[7/200] Training loss: 0.04216292
[8/200] Training loss: 0.03922512
[9/200] Training loss: 0.03354747
[10/200] Training loss: 0.03180693
[50/200] Training loss: 0.01559569
[100/200] Training loss: 0.01364602
[150/200] Training loss: 0.01215556
[200/200] Training loss: 0.01173073
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13867.793479858286 ----------
[1/200] Training loss: 0.17144182
[2/200] Training loss: 0.05518529
[3/200] Training loss: 0.04975108
[4/200] Training loss: 0.05168159
[5/200] Training loss: 0.04467864
[6/200] Training loss: 0.04324230
[7/200] Training loss: 0.03967874
[8/200] Training loss: 0.03942951
[9/200] Training loss: 0.03447805
[10/200] Training loss: 0.03684508
[50/200] Training loss: 0.02049917
[100/200] Training loss: 0.01605035
[150/200] Training loss: 0.01438693
[200/200] Training loss: 0.01331925
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15631.88792180906 ----------
[1/200] Training loss: 0.16826266
[2/200] Training loss: 0.06245997
[3/200] Training loss: 0.05402174
[4/200] Training loss: 0.05067542
[5/200] Training loss: 0.04690416
[6/200] Training loss: 0.04325012
[7/200] Training loss: 0.04293791
[8/200] Training loss: 0.03849355
[9/200] Training loss: 0.03761330
[10/200] Training loss: 0.03763908
[50/200] Training loss: 0.01775175
[100/200] Training loss: 0.01500563
[150/200] Training loss: 0.01346154
[200/200] Training loss: 0.01272095
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12854.694395433911 ----------
[1/200] Training loss: 0.18947131
[2/200] Training loss: 0.06744237
[3/200] Training loss: 0.05645176
[4/200] Training loss: 0.05208917
[5/200] Training loss: 0.05262718
[6/200] Training loss: 0.04831413
[7/200] Training loss: 0.04584325
[8/200] Training loss: 0.04344868
[9/200] Training loss: 0.04023214
[10/200] Training loss: 0.03980992
[50/200] Training loss: 0.01890044
[100/200] Training loss: 0.01559904
[150/200] Training loss: 0.01327877
[200/200] Training loss: 0.01312500
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13471.451295239129 ----------
[1/200] Training loss: 0.14612996
[2/200] Training loss: 0.05413158
[3/200] Training loss: 0.04997852
[4/200] Training loss: 0.04680729
[5/200] Training loss: 0.04433546
[6/200] Training loss: 0.04114780
[7/200] Training loss: 0.03752269
[8/200] Training loss: 0.03705726
[9/200] Training loss: 0.03270253
[10/200] Training loss: 0.03116599
[50/200] Training loss: 0.01728121
[100/200] Training loss: 0.01354738
[150/200] Training loss: 0.01328062
[200/200] Training loss: 0.01266502
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5003.417831842549 ----------
[1/200] Training loss: 0.16160584
[2/200] Training loss: 0.06562452
[3/200] Training loss: 0.05271073
[4/200] Training loss: 0.05269428
[5/200] Training loss: 0.05114409
[6/200] Training loss: 0.04884765
[7/200] Training loss: 0.04465288
[8/200] Training loss: 0.04211720
[9/200] Training loss: 0.04271004
[10/200] Training loss: 0.03900434
[50/200] Training loss: 0.01859244
[100/200] Training loss: 0.01621318
[150/200] Training loss: 0.01516892
[200/200] Training loss: 0.01439381
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9208.129886138662 ----------
[1/200] Training loss: 0.18187395
[2/200] Training loss: 0.05873862
[3/200] Training loss: 0.05214540
[4/200] Training loss: 0.04705239
[5/200] Training loss: 0.04663731
[6/200] Training loss: 0.04455999
[7/200] Training loss: 0.03920198
[8/200] Training loss: 0.03477115
[9/200] Training loss: 0.03149302
[10/200] Training loss: 0.03075704
[50/200] Training loss: 0.01883103
[100/200] Training loss: 0.01606321
[150/200] Training loss: 0.01419057
[200/200] Training loss: 0.01335099
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7416.296649945983 ----------
[1/200] Training loss: 0.16631615
[2/200] Training loss: 0.05666526
[3/200] Training loss: 0.05394617
[4/200] Training loss: 0.04863609
[5/200] Training loss: 0.05060933
[6/200] Training loss: 0.04669714
[7/200] Training loss: 0.04391084
[8/200] Training loss: 0.04359410
[9/200] Training loss: 0.04078583
[10/200] Training loss: 0.03799885
[50/200] Training loss: 0.01783016
[100/200] Training loss: 0.01512751
[150/200] Training loss: 0.01417877
[200/200] Training loss: 0.01263174
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11961.136400860914 ----------
[1/200] Training loss: 0.14975697
[2/200] Training loss: 0.05705114
[3/200] Training loss: 0.05651371
[4/200] Training loss: 0.05436390
[5/200] Training loss: 0.05188497
[6/200] Training loss: 0.04744406
[7/200] Training loss: 0.04478734
[8/200] Training loss: 0.04406140
[9/200] Training loss: 0.04322728
[10/200] Training loss: 0.03824085
[50/200] Training loss: 0.01904109
[100/200] Training loss: 0.01497987
[150/200] Training loss: 0.01334811
[200/200] Training loss: 0.01229593
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14598.04918473698 ----------
[1/200] Training loss: 0.15476321
[2/200] Training loss: 0.06035382
[3/200] Training loss: 0.05593510
[4/200] Training loss: 0.05067968
[5/200] Training loss: 0.04605416
[6/200] Training loss: 0.04311528
[7/200] Training loss: 0.04057676
[8/200] Training loss: 0.03978052
[9/200] Training loss: 0.03645738
[10/200] Training loss: 0.03546896
[50/200] Training loss: 0.01938253
[100/200] Training loss: 0.01521043
[150/200] Training loss: 0.01452867
[200/200] Training loss: 0.01357942
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6134.88255796311 ----------
[1/200] Training loss: 0.16118977
[2/200] Training loss: 0.06390515
[3/200] Training loss: 0.05687348
[4/200] Training loss: 0.05284730
[5/200] Training loss: 0.05087191
[6/200] Training loss: 0.04937675
[7/200] Training loss: 0.04743585
[8/200] Training loss: 0.04534956
[9/200] Training loss: 0.04424383
[10/200] Training loss: 0.04186114
[50/200] Training loss: 0.01968947
[100/200] Training loss: 0.01590670
[150/200] Training loss: 0.01413633
[200/200] Training loss: 0.01364886
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11480.617753413795 ----------
[1/200] Training loss: 0.15100599
[2/200] Training loss: 0.05860411
[3/200] Training loss: 0.04930256
[4/200] Training loss: 0.04881687
[5/200] Training loss: 0.04439669
[6/200] Training loss: 0.04301593
[7/200] Training loss: 0.03843483
[8/200] Training loss: 0.03777652
[9/200] Training loss: 0.03490346
[10/200] Training loss: 0.03184088
[50/200] Training loss: 0.01840452
[100/200] Training loss: 0.01418605
[150/200] Training loss: 0.01348603
[200/200] Training loss: 0.01186414
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7091.662992556823 ----------
[1/200] Training loss: 0.14407378
[2/200] Training loss: 0.06048200
[3/200] Training loss: 0.05110393
[4/200] Training loss: 0.05048016
[5/200] Training loss: 0.04453964
[6/200] Training loss: 0.04310827
[7/200] Training loss: 0.03806077
[8/200] Training loss: 0.03770849
[9/200] Training loss: 0.03745815
[10/200] Training loss: 0.03339315
[50/200] Training loss: 0.01871156
[100/200] Training loss: 0.01559827
[150/200] Training loss: 0.01392878
[200/200] Training loss: 0.01304374
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21980.730470118593 ----------
[1/200] Training loss: 0.16458814
[2/200] Training loss: 0.05916408
[3/200] Training loss: 0.05008713
[4/200] Training loss: 0.04564537
[5/200] Training loss: 0.04667576
[6/200] Training loss: 0.04221159
[7/200] Training loss: 0.03569871
[8/200] Training loss: 0.03736711
[9/200] Training loss: 0.03330028
[10/200] Training loss: 0.03780907
[50/200] Training loss: 0.01812304
[100/200] Training loss: 0.01508478
[150/200] Training loss: 0.01312074
[200/200] Training loss: 0.01220924
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10490.037940827478 ----------
[1/200] Training loss: 0.16596894
[2/200] Training loss: 0.06420661
[3/200] Training loss: 0.05547056
[4/200] Training loss: 0.05545069
[5/200] Training loss: 0.05295971
[6/200] Training loss: 0.04961502
[7/200] Training loss: 0.04804249
[8/200] Training loss: 0.04505126
[9/200] Training loss: 0.04717413
[10/200] Training loss: 0.04088165
[50/200] Training loss: 0.01714311
[100/200] Training loss: 0.01511915
[150/200] Training loss: 0.01414962
[200/200] Training loss: 0.01281231
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7729.425075644372 ----------
[1/200] Training loss: 0.15880331
[2/200] Training loss: 0.05569484
[3/200] Training loss: 0.05172300
[4/200] Training loss: 0.04776962
[5/200] Training loss: 0.04460499
[6/200] Training loss: 0.04050653
[7/200] Training loss: 0.03871704
[8/200] Training loss: 0.03438972
[9/200] Training loss: 0.03425904
[10/200] Training loss: 0.03188143
[50/200] Training loss: 0.01719930
[100/200] Training loss: 0.01379275
[150/200] Training loss: 0.01313793
[200/200] Training loss: 0.01189719
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8794.435968269938 ----------
[1/200] Training loss: 0.16163060
[2/200] Training loss: 0.05886829
[3/200] Training loss: 0.05174876
[4/200] Training loss: 0.04733307
[5/200] Training loss: 0.04503083
[6/200] Training loss: 0.04023186
[7/200] Training loss: 0.03675178
[8/200] Training loss: 0.03615411
[9/200] Training loss: 0.03738919
[10/200] Training loss: 0.03189671
[50/200] Training loss: 0.01876085
[100/200] Training loss: 0.01600401
[150/200] Training loss: 0.01458726
[200/200] Training loss: 0.01396140
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 26981.33399222507 ----------
[1/200] Training loss: 0.18441025
[2/200] Training loss: 0.05997533
[3/200] Training loss: 0.05089277
[4/200] Training loss: 0.04802917
[5/200] Training loss: 0.04377683
[6/200] Training loss: 0.04318162
[7/200] Training loss: 0.03905336
[8/200] Training loss: 0.03679175
[9/200] Training loss: 0.03440074
[10/200] Training loss: 0.03170750
[50/200] Training loss: 0.01871508
[100/200] Training loss: 0.01605467
[150/200] Training loss: 0.01458705
[200/200] Training loss: 0.01355074
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14563.692388951367 ----------
[1/200] Training loss: 0.14831687
[2/200] Training loss: 0.05895821
[3/200] Training loss: 0.05471193
[4/200] Training loss: 0.05099362
[5/200] Training loss: 0.04588603
[6/200] Training loss: 0.04650675
[7/200] Training loss: 0.04195038
[8/200] Training loss: 0.04372193
[9/200] Training loss: 0.04106173
[10/200] Training loss: 0.03639430
[50/200] Training loss: 0.01721984
[100/200] Training loss: 0.01510864
[150/200] Training loss: 0.01269262
[200/200] Training loss: 0.01188411
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14345.547601956503 ----------
[1/200] Training loss: 0.15139346
[2/200] Training loss: 0.05976810
[3/200] Training loss: 0.04902245
[4/200] Training loss: 0.04675867
[5/200] Training loss: 0.04555388
[6/200] Training loss: 0.04010826
[7/200] Training loss: 0.03729233
[8/200] Training loss: 0.03665774
[9/200] Training loss: 0.03114176
[10/200] Training loss: 0.03040050
[50/200] Training loss: 0.01728206
[100/200] Training loss: 0.01438845
[150/200] Training loss: 0.01235485
[200/200] Training loss: 0.01183537
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15474.380892300667 ----------
[1/200] Training loss: 0.17239716
[2/200] Training loss: 0.05933869
[3/200] Training loss: 0.05731003
[4/200] Training loss: 0.05146328
[5/200] Training loss: 0.04887007
[6/200] Training loss: 0.04667628
[7/200] Training loss: 0.04625135
[8/200] Training loss: 0.04271000
[9/200] Training loss: 0.04115734
[10/200] Training loss: 0.03984521
[50/200] Training loss: 0.01906416
[100/200] Training loss: 0.01676024
[150/200] Training loss: 0.01562778
[200/200] Training loss: 0.01444033
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6959.324967265145 ----------
[1/200] Training loss: 0.14299256
[2/200] Training loss: 0.05631035
[3/200] Training loss: 0.04979467
[4/200] Training loss: 0.04878231
[5/200] Training loss: 0.04121241
[6/200] Training loss: 0.03627228
[7/200] Training loss: 0.03671160
[8/200] Training loss: 0.03526671
[9/200] Training loss: 0.03502463
[10/200] Training loss: 0.03162308
[50/200] Training loss: 0.01809785
[100/200] Training loss: 0.01559700
[150/200] Training loss: 0.01428944
[200/200] Training loss: 0.01274287
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6223.081551771598 ----------
[1/200] Training loss: 0.16416522
[2/200] Training loss: 0.05560564
[3/200] Training loss: 0.04987712
[4/200] Training loss: 0.04558498
[5/200] Training loss: 0.04374802
[6/200] Training loss: 0.04122549
[7/200] Training loss: 0.03856560
[8/200] Training loss: 0.03753373
[9/200] Training loss: 0.03651559
[10/200] Training loss: 0.03218466
[50/200] Training loss: 0.01748728
[100/200] Training loss: 0.01620443
[150/200] Training loss: 0.01463166
[200/200] Training loss: 0.01380542
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10070.019265125564 ----------
[1/200] Training loss: 0.17460456
[2/200] Training loss: 0.05766080
[3/200] Training loss: 0.05089805
[4/200] Training loss: 0.04822955
[5/200] Training loss: 0.04675241
[6/200] Training loss: 0.04567964
[7/200] Training loss: 0.04046703
[8/200] Training loss: 0.04079094
[9/200] Training loss: 0.03566130
[10/200] Training loss: 0.03138002
[50/200] Training loss: 0.01923328
[100/200] Training loss: 0.01686466
[150/200] Training loss: 0.01554143
[200/200] Training loss: 0.01477019
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13072.473063655552 ----------
[1/200] Training loss: 0.16967270
[2/200] Training loss: 0.05911362
[3/200] Training loss: 0.05695542
[4/200] Training loss: 0.05340650
[5/200] Training loss: 0.05011816
[6/200] Training loss: 0.04771044
[7/200] Training loss: 0.04700657
[8/200] Training loss: 0.04553710
[9/200] Training loss: 0.04262306
[10/200] Training loss: 0.04057042
[50/200] Training loss: 0.01907544
[100/200] Training loss: 0.01609675
[150/200] Training loss: 0.01445303
[200/200] Training loss: 0.01321847
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11686.281187785959 ----------
[1/200] Training loss: 0.13558365
[2/200] Training loss: 0.05868540
[3/200] Training loss: 0.05067080
[4/200] Training loss: 0.04637747
[5/200] Training loss: 0.04247659
[6/200] Training loss: 0.04271718
[7/200] Training loss: 0.03891446
[8/200] Training loss: 0.03739479
[9/200] Training loss: 0.03302932
[10/200] Training loss: 0.03208266
[50/200] Training loss: 0.01670004
[100/200] Training loss: 0.01422773
[150/200] Training loss: 0.01276752
[200/200] Training loss: 0.01154441
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15078.359061913867 ----------
[1/200] Training loss: 0.15339591
[2/200] Training loss: 0.05595971
[3/200] Training loss: 0.04564417
[4/200] Training loss: 0.04129858
[5/200] Training loss: 0.04043798
[6/200] Training loss: 0.03703767
[7/200] Training loss: 0.03227561
[8/200] Training loss: 0.03194515
[9/200] Training loss: 0.03069213
[10/200] Training loss: 0.02752289
[50/200] Training loss: 0.01692059
[100/200] Training loss: 0.01497091
[150/200] Training loss: 0.01404630
[200/200] Training loss: 0.01293830
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12448.76989907035 ----------
[1/200] Training loss: 0.14795596
[2/200] Training loss: 0.05555270
[3/200] Training loss: 0.04990648
[4/200] Training loss: 0.04813718
[5/200] Training loss: 0.04295982
[6/200] Training loss: 0.03817950
[7/200] Training loss: 0.03676425
[8/200] Training loss: 0.03660070
[9/200] Training loss: 0.03412351
[10/200] Training loss: 0.03282240
[50/200] Training loss: 0.01761673
[100/200] Training loss: 0.01431131
[150/200] Training loss: 0.01381213
[200/200] Training loss: 0.01374083
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23347.610413059407 ----------
[1/200] Training loss: 0.16376751
[2/200] Training loss: 0.06195096
[3/200] Training loss: 0.05431500
[4/200] Training loss: 0.05280547
[5/200] Training loss: 0.05285742
[6/200] Training loss: 0.04922457
[7/200] Training loss: 0.04582072
[8/200] Training loss: 0.04230166
[9/200] Training loss: 0.03854656
[10/200] Training loss: 0.03869824
[50/200] Training loss: 0.01727641
[100/200] Training loss: 0.01511711
[150/200] Training loss: 0.01362261
[200/200] Training loss: 0.01292691
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7916.387812632729 ----------
[1/200] Training loss: 0.18058012
[2/200] Training loss: 0.06170655
[3/200] Training loss: 0.05361628
[4/200] Training loss: 0.04945476
[5/200] Training loss: 0.04621756
[6/200] Training loss: 0.04406569
[7/200] Training loss: 0.04429658
[8/200] Training loss: 0.04054832
[9/200] Training loss: 0.04136905
[10/200] Training loss: 0.03717287
[50/200] Training loss: 0.01996749
[100/200] Training loss: 0.01741311
[150/200] Training loss: 0.01591495
[200/200] Training loss: 0.01434817
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6826.703157454555 ----------
[1/200] Training loss: 0.17162802
[2/200] Training loss: 0.05564704
[3/200] Training loss: 0.05440492
[4/200] Training loss: 0.04756620
[5/200] Training loss: 0.04610211
[6/200] Training loss: 0.04419602
[7/200] Training loss: 0.04046909
[8/200] Training loss: 0.03773240
[9/200] Training loss: 0.03621926
[10/200] Training loss: 0.03410806
[50/200] Training loss: 0.01824064
[100/200] Training loss: 0.01612620
[150/200] Training loss: 0.01529281
[200/200] Training loss: 0.01407719
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16520.03292974926 ----------
[1/200] Training loss: 0.14761337
[2/200] Training loss: 0.05802687
[3/200] Training loss: 0.04996747
[4/200] Training loss: 0.04501813
[5/200] Training loss: 0.04478099
[6/200] Training loss: 0.04183750
[7/200] Training loss: 0.03765427
[8/200] Training loss: 0.03686078
[9/200] Training loss: 0.03536077
[10/200] Training loss: 0.03243132
[50/200] Training loss: 0.01808790
[100/200] Training loss: 0.01439553
[150/200] Training loss: 0.01330732
[200/200] Training loss: 0.01210411
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10121.186886921909 ----------
[1/200] Training loss: 0.17180961
[2/200] Training loss: 0.05477384
[3/200] Training loss: 0.05119710
[4/200] Training loss: 0.04797526
[5/200] Training loss: 0.04595137
[6/200] Training loss: 0.04094230
[7/200] Training loss: 0.03985828
[8/200] Training loss: 0.03667652
[9/200] Training loss: 0.03545439
[10/200] Training loss: 0.03428718
[50/200] Training loss: 0.01720288
[100/200] Training loss: 0.01438151
[150/200] Training loss: 0.01268438
[200/200] Training loss: 0.01231648
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10605.609459149437 ----------
[1/200] Training loss: 0.17153612
[2/200] Training loss: 0.05712570
[3/200] Training loss: 0.05357393
[4/200] Training loss: 0.04474120
[5/200] Training loss: 0.04295194
[6/200] Training loss: 0.04091607
[7/200] Training loss: 0.03975417
[8/200] Training loss: 0.03745994
[9/200] Training loss: 0.03480739
[10/200] Training loss: 0.03350104
[50/200] Training loss: 0.01774401
[100/200] Training loss: 0.01472382
[150/200] Training loss: 0.01386511
[200/200] Training loss: 0.01309965
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10310.542177790652 ----------
[1/200] Training loss: 0.16164201
[2/200] Training loss: 0.05875454
[3/200] Training loss: 0.05159010
[4/200] Training loss: 0.04971974
[5/200] Training loss: 0.04603277
[6/200] Training loss: 0.04594543
[7/200] Training loss: 0.04193830
[8/200] Training loss: 0.04028802
[9/200] Training loss: 0.04117412
[10/200] Training loss: 0.03543126
[50/200] Training loss: 0.01843406
[100/200] Training loss: 0.01481359
[150/200] Training loss: 0.01378905
[200/200] Training loss: 0.01293392
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4598.8585540327285 ----------
[1/200] Training loss: 0.17408468
[2/200] Training loss: 0.06130578
[3/200] Training loss: 0.05025331
[4/200] Training loss: 0.04865815
[5/200] Training loss: 0.04625226
[6/200] Training loss: 0.04005838
[7/200] Training loss: 0.03646607
[8/200] Training loss: 0.03684552
[9/200] Training loss: 0.03297646
[10/200] Training loss: 0.03287703
[50/200] Training loss: 0.01744075
[100/200] Training loss: 0.01507395
[150/200] Training loss: 0.01362693
[200/200] Training loss: 0.01184024
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23004.19022699995 ----------
[1/200] Training loss: 0.14177379
[2/200] Training loss: 0.05604899
[3/200] Training loss: 0.04742210
[4/200] Training loss: 0.04575834
[5/200] Training loss: 0.04522279
[6/200] Training loss: 0.03968504
[7/200] Training loss: 0.03758033
[8/200] Training loss: 0.03727286
[9/200] Training loss: 0.03390780
[10/200] Training loss: 0.03265640
[50/200] Training loss: 0.01784666
[100/200] Training loss: 0.01465576
[150/200] Training loss: 0.01388984
[200/200] Training loss: 0.01283472
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 24425.95210017411 ----------
[1/200] Training loss: 0.15568882
[2/200] Training loss: 0.06157767
[3/200] Training loss: 0.05539534
[4/200] Training loss: 0.05438365
[5/200] Training loss: 0.04948177
[6/200] Training loss: 0.04667819
[7/200] Training loss: 0.04452186
[8/200] Training loss: 0.04020713
[9/200] Training loss: 0.04252206
[10/200] Training loss: 0.03714639
[50/200] Training loss: 0.01603578
[100/200] Training loss: 0.01428991
[150/200] Training loss: 0.01294545
[200/200] Training loss: 0.01232576
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18699.61967527682 ----------
[1/200] Training loss: 0.16442677
[2/200] Training loss: 0.05704962
[3/200] Training loss: 0.04876107
[4/200] Training loss: 0.04765748
[5/200] Training loss: 0.04157324
[6/200] Training loss: 0.04071262
[7/200] Training loss: 0.04211517
[8/200] Training loss: 0.03895567
[9/200] Training loss: 0.03567850
[10/200] Training loss: 0.03475721
[50/200] Training loss: 0.01806095
[100/200] Training loss: 0.01543093
[150/200] Training loss: 0.01257630
[200/200] Training loss: 0.01201677
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15486.559850399313 ----------
[1/200] Training loss: 0.17119544
[2/200] Training loss: 0.06182102
[3/200] Training loss: 0.05453918
[4/200] Training loss: 0.05168082
[5/200] Training loss: 0.05114604
[6/200] Training loss: 0.04578327
[7/200] Training loss: 0.04313860
[8/200] Training loss: 0.04484458
[9/200] Training loss: 0.04203224
[10/200] Training loss: 0.03952232
[50/200] Training loss: 0.02454062
[100/200] Training loss: 0.01718931
[150/200] Training loss: 0.01526991
[200/200] Training loss: 0.01387742
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13644.374665040534 ----------
[1/200] Training loss: 0.14763853
[2/200] Training loss: 0.06162305
[3/200] Training loss: 0.05383427
[4/200] Training loss: 0.05060187
[5/200] Training loss: 0.04380360
[6/200] Training loss: 0.03843361
[7/200] Training loss: 0.03816980
[8/200] Training loss: 0.03353640
[9/200] Training loss: 0.02970311
[10/200] Training loss: 0.03108979
[50/200] Training loss: 0.01551310
[100/200] Training loss: 0.01278918
[150/200] Training loss: 0.01214325
[200/200] Training loss: 0.01129241
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15775.86409677771 ----------
[1/200] Training loss: 0.16979502
[2/200] Training loss: 0.05969664
[3/200] Training loss: 0.05167861
[4/200] Training loss: 0.04670405
[5/200] Training loss: 0.04124578
[6/200] Training loss: 0.04254948
[7/200] Training loss: 0.03812632
[8/200] Training loss: 0.03480064
[9/200] Training loss: 0.03040070
[10/200] Training loss: 0.02980985
[50/200] Training loss: 0.01848089
[100/200] Training loss: 0.01568036
[150/200] Training loss: 0.01373588
[200/200] Training loss: 0.01305036
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7246.741060642363 ----------
[1/200] Training loss: 0.17033087
[2/200] Training loss: 0.06114108
[3/200] Training loss: 0.05215348
[4/200] Training loss: 0.05075122
[5/200] Training loss: 0.04702726
[6/200] Training loss: 0.04305784
[7/200] Training loss: 0.03983281
[8/200] Training loss: 0.03785306
[9/200] Training loss: 0.03671436
[10/200] Training loss: 0.03434933
[50/200] Training loss: 0.01759757
[100/200] Training loss: 0.01564683
[150/200] Training loss: 0.01379030
[200/200] Training loss: 0.01348165
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15050.176344481815 ----------
[1/200] Training loss: 0.15164563
[2/200] Training loss: 0.06150185
[3/200] Training loss: 0.05204665
[4/200] Training loss: 0.04722658
[5/200] Training loss: 0.04760375
[6/200] Training loss: 0.04009425
[7/200] Training loss: 0.04006319
[8/200] Training loss: 0.03668953
[9/200] Training loss: 0.03276637
[10/200] Training loss: 0.03055945
[50/200] Training loss: 0.01677932
[100/200] Training loss: 0.01393921
[150/200] Training loss: 0.01306475
[200/200] Training loss: 0.01217773
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16856.037019418294 ----------
[1/200] Training loss: 0.16431601
[2/200] Training loss: 0.06236637
[3/200] Training loss: 0.05453473
[4/200] Training loss: 0.04895243
[5/200] Training loss: 0.04433653
[6/200] Training loss: 0.04212650
[7/200] Training loss: 0.03910939
[8/200] Training loss: 0.03489584
[9/200] Training loss: 0.03289857
[10/200] Training loss: 0.03323996
[50/200] Training loss: 0.01878272
[100/200] Training loss: 0.01684606
[150/200] Training loss: 0.01543338
[200/200] Training loss: 0.01496725
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 25594.313118347207 ----------
[1/200] Training loss: 0.15978948
[2/200] Training loss: 0.05884997
[3/200] Training loss: 0.05095798
[4/200] Training loss: 0.05075752
[5/200] Training loss: 0.04369648
[6/200] Training loss: 0.04063950
[7/200] Training loss: 0.03738445
[8/200] Training loss: 0.03713886
[9/200] Training loss: 0.03279925
[10/200] Training loss: 0.03193479
[50/200] Training loss: 0.01904809
[100/200] Training loss: 0.01446307
[150/200] Training loss: 0.01355238
[200/200] Training loss: 0.01273417
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7682.548014819042 ----------
[1/200] Training loss: 0.17552400
[2/200] Training loss: 0.06279244
[3/200] Training loss: 0.05415324
[4/200] Training loss: 0.05165247
[5/200] Training loss: 0.05015623
[6/200] Training loss: 0.04815077
[7/200] Training loss: 0.04531445
[8/200] Training loss: 0.04093191
[9/200] Training loss: 0.03754525
[10/200] Training loss: 0.03523958
[50/200] Training loss: 0.01820961
[100/200] Training loss: 0.01492160
[150/200] Training loss: 0.01366517
[200/200] Training loss: 0.01258689
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 24397.11983001272 ----------
[1/200] Training loss: 0.15622517
[2/200] Training loss: 0.05479777
[3/200] Training loss: 0.05187590
[4/200] Training loss: 0.04308408
[5/200] Training loss: 0.04131730
[6/200] Training loss: 0.04058485
[7/200] Training loss: 0.03640513
[8/200] Training loss: 0.03228202
[9/200] Training loss: 0.03323100
[10/200] Training loss: 0.03255262
[50/200] Training loss: 0.01797622
[100/200] Training loss: 0.01534789
[150/200] Training loss: 0.01389756
[200/200] Training loss: 0.01257020
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12819.630571900268 ----------
[1/200] Training loss: 0.20058042
[2/200] Training loss: 0.05915871
[3/200] Training loss: 0.04718392
[4/200] Training loss: 0.04837072
[5/200] Training loss: 0.04616955
[6/200] Training loss: 0.04275888
[7/200] Training loss: 0.04491204
[8/200] Training loss: 0.04050833
[9/200] Training loss: 0.04141413
[10/200] Training loss: 0.03898835
[50/200] Training loss: 0.01969683
[100/200] Training loss: 0.01512977
[150/200] Training loss: 0.01410558
[200/200] Training loss: 0.01281318
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7060.17591848815 ----------
[1/200] Training loss: 0.16726996
[2/200] Training loss: 0.06367600
[3/200] Training loss: 0.05184501
[4/200] Training loss: 0.04735642
[5/200] Training loss: 0.04482860
[6/200] Training loss: 0.04235792
[7/200] Training loss: 0.04025365
[8/200] Training loss: 0.03888577
[9/200] Training loss: 0.03608438
[10/200] Training loss: 0.03318954
[50/200] Training loss: 0.01885922
[100/200] Training loss: 0.01576440
[150/200] Training loss: 0.01392845
[200/200] Training loss: 0.01265735
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17682.366357476025 ----------
[1/200] Training loss: 0.15732138
[2/200] Training loss: 0.06100553
[3/200] Training loss: 0.05242326
[4/200] Training loss: 0.04767300
[5/200] Training loss: 0.04630293
[6/200] Training loss: 0.04577840
[7/200] Training loss: 0.04117132
[8/200] Training loss: 0.03925744
[9/200] Training loss: 0.03940970
[10/200] Training loss: 0.03519711
[50/200] Training loss: 0.01969403
[100/200] Training loss: 0.01481014
[150/200] Training loss: 0.01430661
[200/200] Training loss: 0.01262733
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 28192.59704248617 ----------
[1/200] Training loss: 0.14992144
[2/200] Training loss: 0.05542790
[3/200] Training loss: 0.04886770
[4/200] Training loss: 0.04531315
[5/200] Training loss: 0.04253145
[6/200] Training loss: 0.03837134
[7/200] Training loss: 0.03633933
[8/200] Training loss: 0.03683302
[9/200] Training loss: 0.03237249
[10/200] Training loss: 0.03371960
[50/200] Training loss: 0.01641894
[100/200] Training loss: 0.01471581
[150/200] Training loss: 0.01261403
[200/200] Training loss: 0.01258573
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17887.85509780309 ----------
[1/200] Training loss: 0.17052733
[2/200] Training loss: 0.05588371
[3/200] Training loss: 0.04716058
[4/200] Training loss: 0.04251446
[5/200] Training loss: 0.04076607
[6/200] Training loss: 0.04065384
[7/200] Training loss: 0.03781867
[8/200] Training loss: 0.03575720
[9/200] Training loss: 0.03460501
[10/200] Training loss: 0.03125832
[50/200] Training loss: 0.01803120
[100/200] Training loss: 0.01547054
[150/200] Training loss: 0.01357122
[200/200] Training loss: 0.01249845
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10032.954101360177 ----------
[1/200] Training loss: 0.10963827
[2/200] Training loss: 0.05067184
[3/200] Training loss: 0.04316866
[4/200] Training loss: 0.03873192
[5/200] Training loss: 0.03423841
[6/200] Training loss: 0.03217980
[7/200] Training loss: 0.03136606
[8/200] Training loss: 0.02810478
[9/200] Training loss: 0.02740961
[10/200] Training loss: 0.02640039
[50/200] Training loss: 0.01545743
[100/200] Training loss: 0.01286734
[150/200] Training loss: 0.01190272
[200/200] Training loss: 0.01090412
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23452.20433136297 ----------
[1/200] Training loss: 0.17640927
[2/200] Training loss: 0.06160589
[3/200] Training loss: 0.05438203
[4/200] Training loss: 0.05079026
[5/200] Training loss: 0.04986059
[6/200] Training loss: 0.04654992
[7/200] Training loss: 0.04637074
[8/200] Training loss: 0.04476998
[9/200] Training loss: 0.04389231
[10/200] Training loss: 0.04035018
[50/200] Training loss: 0.01866222
[100/200] Training loss: 0.01615886
[150/200] Training loss: 0.01544115
[200/200] Training loss: 0.01424362
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22372.369387259812 ----------
[1/200] Training loss: 0.15979251
[2/200] Training loss: 0.06049887
[3/200] Training loss: 0.05370529
[4/200] Training loss: 0.04980323
[5/200] Training loss: 0.05000521
[6/200] Training loss: 0.04385918
[7/200] Training loss: 0.04307186
[8/200] Training loss: 0.03744820
[9/200] Training loss: 0.03688293
[10/200] Training loss: 0.03451044
[50/200] Training loss: 0.01853882
[100/200] Training loss: 0.01543521
[150/200] Training loss: 0.01368301
[200/200] Training loss: 0.01262508
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21056.74238812832 ----------
[1/200] Training loss: 0.17592678
[2/200] Training loss: 0.06519111
[3/200] Training loss: 0.05794161
[4/200] Training loss: 0.05238260
[5/200] Training loss: 0.04659125
[6/200] Training loss: 0.04660516
[7/200] Training loss: 0.04680368
[8/200] Training loss: 0.04149078
[9/200] Training loss: 0.04130859
[10/200] Training loss: 0.03608228
[50/200] Training loss: 0.01913214
[100/200] Training loss: 0.01702598
[150/200] Training loss: 0.01512340
[200/200] Training loss: 0.01389178
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14402.462011753407 ----------
[1/200] Training loss: 0.25396545
[2/200] Training loss: 0.07759589
[3/200] Training loss: 0.06282923
[4/200] Training loss: 0.05193462
[5/200] Training loss: 0.05152910
[6/200] Training loss: 0.04744177
[7/200] Training loss: 0.04889570
[8/200] Training loss: 0.04335071
[9/200] Training loss: 0.04322080
[10/200] Training loss: 0.04329851
[50/200] Training loss: 0.02023977
[100/200] Training loss: 0.01705453
[150/200] Training loss: 0.01551587
[200/200] Training loss: 0.01464286
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13993.699153547643 ----------
[1/200] Training loss: 0.15293690
[2/200] Training loss: 0.05446157
[3/200] Training loss: 0.05074355
[4/200] Training loss: 0.04609562
[5/200] Training loss: 0.04588949
[6/200] Training loss: 0.04114657
[7/200] Training loss: 0.03722686
[8/200] Training loss: 0.03654749
[9/200] Training loss: 0.03500034
[10/200] Training loss: 0.03061938
[50/200] Training loss: 0.01868243
[100/200] Training loss: 0.01468540
[150/200] Training loss: 0.01210231
[200/200] Training loss: 0.01150718
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5407.014333252687 ----------
[1/200] Training loss: 0.14910820
[2/200] Training loss: 0.05938813
[3/200] Training loss: 0.05686606
[4/200] Training loss: 0.05179901
[5/200] Training loss: 0.04963654
[6/200] Training loss: 0.04518389
[7/200] Training loss: 0.04499393
[8/200] Training loss: 0.03885998
[9/200] Training loss: 0.03619971
[10/200] Training loss: 0.03681569
[50/200] Training loss: 0.01753614
[100/200] Training loss: 0.01417083
[150/200] Training loss: 0.01354389
[200/200] Training loss: 0.01292987
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12400.445153299941 ----------
[1/200] Training loss: 0.27672021
[2/200] Training loss: 0.07022039
[3/200] Training loss: 0.06391163
[4/200] Training loss: 0.05262981
[5/200] Training loss: 0.05507722
[6/200] Training loss: 0.05313283
[7/200] Training loss: 0.05083217
[8/200] Training loss: 0.04770645
[9/200] Training loss: 0.04696831
[10/200] Training loss: 0.04100299
[50/200] Training loss: 0.02163857
[100/200] Training loss: 0.01764755
[150/200] Training loss: 0.01584283
[200/200] Training loss: 0.01477260
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9613.73226171813 ----------
[1/200] Training loss: 0.15516513
[2/200] Training loss: 0.05526856
[3/200] Training loss: 0.04660648
[4/200] Training loss: 0.04481951
[5/200] Training loss: 0.04279381
[6/200] Training loss: 0.04187802
[7/200] Training loss: 0.03826937
[8/200] Training loss: 0.03359092
[9/200] Training loss: 0.03544099
[10/200] Training loss: 0.03357646
[50/200] Training loss: 0.01886144
[100/200] Training loss: 0.01601180
[150/200] Training loss: 0.01457957
[200/200] Training loss: 0.01371739
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14670.29652052064 ----------
[1/200] Training loss: 0.17870456
[2/200] Training loss: 0.06517445
[3/200] Training loss: 0.05824112
[4/200] Training loss: 0.05839532
[5/200] Training loss: 0.05053785
[6/200] Training loss: 0.05318703
[7/200] Training loss: 0.04832163
[8/200] Training loss: 0.04509423
[9/200] Training loss: 0.04171954
[10/200] Training loss: 0.04110181
[50/200] Training loss: 0.01944484
[100/200] Training loss: 0.01702773
[150/200] Training loss: 0.01602930
[200/200] Training loss: 0.01445577
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10232.030492526887 ----------
[1/200] Training loss: 0.17565567
[2/200] Training loss: 0.05935990
[3/200] Training loss: 0.05147032
[4/200] Training loss: 0.04792771
[5/200] Training loss: 0.04384026
[6/200] Training loss: 0.04151699
[7/200] Training loss: 0.03982880
[8/200] Training loss: 0.03635370
[9/200] Training loss: 0.03690570
[10/200] Training loss: 0.03507811
[50/200] Training loss: 0.01769786
[100/200] Training loss: 0.01564066
[150/200] Training loss: 0.01431661
[200/200] Training loss: 0.01321676
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14160.5282387346 ----------
[1/200] Training loss: 0.03550149
[2/200] Training loss: 0.00219739
[3/200] Training loss: 0.00180979
[4/200] Training loss: 0.00171151
[5/200] Training loss: 0.00139234
[6/200] Training loss: 0.00127299
[7/200] Training loss: 0.00106258
[8/200] Training loss: 0.00083294
[9/200] Training loss: 0.00072396
[10/200] Training loss: 0.00080316
[50/200] Training loss: 0.00024620
[100/200] Training loss: 0.00014812
[150/200] Training loss: 0.00013590
[200/200] Training loss: 0.00011699
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15758.629889682668 ----------
[1/200] Training loss: 0.15698934
[2/200] Training loss: 0.05799071
[3/200] Training loss: 0.05082240
[4/200] Training loss: 0.04646598
[5/200] Training loss: 0.04391296
[6/200] Training loss: 0.03970359
[7/200] Training loss: 0.03850572
[8/200] Training loss: 0.03788909
[9/200] Training loss: 0.03565492
[10/200] Training loss: 0.03381880
[50/200] Training loss: 0.01665142
[100/200] Training loss: 0.01445338
[150/200] Training loss: 0.01333473
[200/200] Training loss: 0.01265879
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14241.542051336997 ----------
[1/200] Training loss: 0.16997711
[2/200] Training loss: 0.05701695
[3/200] Training loss: 0.05215434
[4/200] Training loss: 0.05219182
[5/200] Training loss: 0.04911756
[6/200] Training loss: 0.04349673
[7/200] Training loss: 0.04271745
[8/200] Training loss: 0.03977138
[9/200] Training loss: 0.03345373
[10/200] Training loss: 0.03554799
[50/200] Training loss: 0.01675025
[100/200] Training loss: 0.01449511
[150/200] Training loss: 0.01335017
[200/200] Training loss: 0.01261724
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6423.193598203311 ----------
[1/200] Training loss: 0.15414695
[2/200] Training loss: 0.06012196
[3/200] Training loss: 0.05347082
[4/200] Training loss: 0.05005896
[5/200] Training loss: 0.04864451
[6/200] Training loss: 0.04549324
[7/200] Training loss: 0.04300266
[8/200] Training loss: 0.03945722
[9/200] Training loss: 0.03645175
[10/200] Training loss: 0.03414218
[50/200] Training loss: 0.01775070
[100/200] Training loss: 0.01518357
[150/200] Training loss: 0.01434499
[200/200] Training loss: 0.01305475
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18687.511123742508 ----------
[1/200] Training loss: 0.15721750
[2/200] Training loss: 0.06117124
[3/200] Training loss: 0.05765256
[4/200] Training loss: 0.04566469
[5/200] Training loss: 0.04960808
[6/200] Training loss: 0.04291778
[7/200] Training loss: 0.04207718
[8/200] Training loss: 0.04131911
[9/200] Training loss: 0.03671179
[10/200] Training loss: 0.04026135
[50/200] Training loss: 0.01888504
[100/200] Training loss: 0.01462983
[150/200] Training loss: 0.01367082
[200/200] Training loss: 0.01197267
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18605.784046903264 ----------
[1/200] Training loss: 0.16107686
[2/200] Training loss: 0.05895848
[3/200] Training loss: 0.05180222
[4/200] Training loss: 0.04680647
[5/200] Training loss: 0.04080283
[6/200] Training loss: 0.03938408
[7/200] Training loss: 0.03564824
[8/200] Training loss: 0.03354673
[9/200] Training loss: 0.03357257
[10/200] Training loss: 0.03095802
[50/200] Training loss: 0.01770163
[100/200] Training loss: 0.01472065
[150/200] Training loss: 0.01305407
[200/200] Training loss: 0.01198739
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10579.361795495983 ----------
[1/200] Training loss: 0.13158482
[2/200] Training loss: 0.05114282
[3/200] Training loss: 0.04360144
[4/200] Training loss: 0.04211654
[5/200] Training loss: 0.03718103
[6/200] Training loss: 0.03711909
[7/200] Training loss: 0.03499828
[8/200] Training loss: 0.03365109
[9/200] Training loss: 0.03549827
[10/200] Training loss: 0.03076632
[50/200] Training loss: 0.01813644
[100/200] Training loss: 0.01480077
[150/200] Training loss: 0.01358456
[200/200] Training loss: 0.01308668
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01189641827273458
----FITNESS-----------RMSE---- 3889.649341521675 ----------
[1/200] Training loss: 0.18217439
[2/200] Training loss: 0.06659334
[3/200] Training loss: 0.06106831
[4/200] Training loss: 0.05585067
[5/200] Training loss: 0.05212487
[6/200] Training loss: 0.05220876
[7/200] Training loss: 0.04752080
[8/200] Training loss: 0.04475972
[9/200] Training loss: 0.04281776
[10/200] Training loss: 0.03984653
[50/200] Training loss: 0.01908728
[100/200] Training loss: 0.01681833
[150/200] Training loss: 0.01538884
[200/200] Training loss: 0.01417539
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6716.06313252042 ----------
[1/200] Training loss: 0.17054155
[2/200] Training loss: 0.05882395
[3/200] Training loss: 0.05107222
[4/200] Training loss: 0.04526601
[5/200] Training loss: 0.04342765
[6/200] Training loss: 0.04005193
[7/200] Training loss: 0.03456835
[8/200] Training loss: 0.03429905
[9/200] Training loss: 0.03345973
[10/200] Training loss: 0.03117612
[50/200] Training loss: 0.01795645
[100/200] Training loss: 0.01532531
[150/200] Training loss: 0.01370682
[200/200] Training loss: 0.01314730
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14836.193581913118 ----------
[1/200] Training loss: 0.28607516
[2/200] Training loss: 0.08288119
[3/200] Training loss: 0.05969988
[4/200] Training loss: 0.05057582
[5/200] Training loss: 0.05198337
[6/200] Training loss: 0.04779274
[7/200] Training loss: 0.04520643
[8/200] Training loss: 0.04204037
[9/200] Training loss: 0.04164024
[10/200] Training loss: 0.04374392
[50/200] Training loss: 0.02254065
[100/200] Training loss: 0.01657270
[150/200] Training loss: 0.01482136
[200/200] Training loss: 0.01404107
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10229.532931664084 ----------
[1/200] Training loss: 0.17329460
[2/200] Training loss: 0.05854980
[3/200] Training loss: 0.05380605
[4/200] Training loss: 0.04921161
[5/200] Training loss: 0.04645923
[6/200] Training loss: 0.04294853
[7/200] Training loss: 0.03937631
[8/200] Training loss: 0.03816840
[9/200] Training loss: 0.03851226
[10/200] Training loss: 0.03382651
[50/200] Training loss: 0.01705641
[100/200] Training loss: 0.01617505
[150/200] Training loss: 0.01411778
[200/200] Training loss: 0.01316275
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18895.77645930434 ----------
[1/200] Training loss: 0.17007628
[2/200] Training loss: 0.05654344
[3/200] Training loss: 0.04833745
[4/200] Training loss: 0.04585290
[5/200] Training loss: 0.04333167
[6/200] Training loss: 0.04134103
[7/200] Training loss: 0.03716269
[8/200] Training loss: 0.03495160
[9/200] Training loss: 0.03610871
[10/200] Training loss: 0.03381195
[50/200] Training loss: 0.01860521
[100/200] Training loss: 0.01537689
[150/200] Training loss: 0.01292999
[200/200] Training loss: 0.01224627
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21552.48922978504 ----------
[1/200] Training loss: 0.17161283
[2/200] Training loss: 0.06632680
[3/200] Training loss: 0.05331093
[4/200] Training loss: 0.04677536
[5/200] Training loss: 0.04622727
[6/200] Training loss: 0.03935122
[7/200] Training loss: 0.03893667
[8/200] Training loss: 0.03607732
[9/200] Training loss: 0.03274277
[10/200] Training loss: 0.03075449
[50/200] Training loss: 0.01795441
[100/200] Training loss: 0.01609711
[150/200] Training loss: 0.01504895
[200/200] Training loss: 0.01450790
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11588.805633023621 ----------
[1/200] Training loss: 0.16361895
[2/200] Training loss: 0.05300951
[3/200] Training loss: 0.05313659
[4/200] Training loss: 0.04786428
[5/200] Training loss: 0.04533695
[6/200] Training loss: 0.04112395
[7/200] Training loss: 0.03786167
[8/200] Training loss: 0.04025598
[9/200] Training loss: 0.03533611
[10/200] Training loss: 0.03376258
[50/200] Training loss: 0.01945452
[100/200] Training loss: 0.01569375
[150/200] Training loss: 0.01362833
[200/200] Training loss: 0.01403147
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13138.144161181974 ----------
[1/200] Training loss: 0.13511726
[2/200] Training loss: 0.05694283
[3/200] Training loss: 0.05391113
[4/200] Training loss: 0.04885657
[5/200] Training loss: 0.04817301
[6/200] Training loss: 0.04492228
[7/200] Training loss: 0.04229310
[8/200] Training loss: 0.03946072
[9/200] Training loss: 0.03642180
[10/200] Training loss: 0.03845365
[50/200] Training loss: 0.02072567
[100/200] Training loss: 0.01715448
[150/200] Training loss: 0.01545325
[200/200] Training loss: 0.01442435
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.006735897475658186
----FITNESS-----------RMSE---- 11022.57247651382 ----------
[1/200] Training loss: 0.17513336
[2/200] Training loss: 0.05781247
[3/200] Training loss: 0.05340745
[4/200] Training loss: 0.04784679
[5/200] Training loss: 0.04764998
[6/200] Training loss: 0.04154740
[7/200] Training loss: 0.04199972
[8/200] Training loss: 0.03879556
[9/200] Training loss: 0.03520982
[10/200] Training loss: 0.03492990
[50/200] Training loss: 0.01770553
[100/200] Training loss: 0.01526698
[150/200] Training loss: 0.01440102
[200/200] Training loss: 0.01304856
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18114.32317256154 ----------
[1/200] Training loss: 0.18418294
[2/200] Training loss: 0.05923681
[3/200] Training loss: 0.04959227
[4/200] Training loss: 0.04538370
[5/200] Training loss: 0.04219253
[6/200] Training loss: 0.03891883
[7/200] Training loss: 0.03491902
[8/200] Training loss: 0.03198849
[9/200] Training loss: 0.03228524
[10/200] Training loss: 0.02881888
[50/200] Training loss: 0.01859621
[100/200] Training loss: 0.01613425
[150/200] Training loss: 0.01441809
[200/200] Training loss: 0.01362631
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9468.650590237237 ----------
[1/200] Training loss: 0.18505683
[2/200] Training loss: 0.06349559
[3/200] Training loss: 0.05506786
[4/200] Training loss: 0.05167509
[5/200] Training loss: 0.05018954
[6/200] Training loss: 0.04766104
[7/200] Training loss: 0.04612122
[8/200] Training loss: 0.04083550
[9/200] Training loss: 0.04027791
[10/200] Training loss: 0.03804009
[50/200] Training loss: 0.01933408
[100/200] Training loss: 0.01580853
[150/200] Training loss: 0.01395356
[200/200] Training loss: 0.01306635
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 24501.37236972656 ----------
[1/200] Training loss: 0.17286894
[2/200] Training loss: 0.05678111
[3/200] Training loss: 0.04933577
[4/200] Training loss: 0.04807561
[5/200] Training loss: 0.04637721
[6/200] Training loss: 0.03981812
[7/200] Training loss: 0.03775754
[8/200] Training loss: 0.03547936
[9/200] Training loss: 0.03358060
[10/200] Training loss: 0.03209042
[50/200] Training loss: 0.01786469
[100/200] Training loss: 0.01550375
[150/200] Training loss: 0.01464044
[200/200] Training loss: 0.01396610
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 64 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18648.344055170153 ----------
[1/200] Training loss: 0.16397926
[2/200] Training loss: 0.06272069
[3/200] Training loss: 0.05115719
[4/200] Training loss: 0.05053296
[5/200] Training loss: 0.04683144
[6/200] Training loss: 0.04463086
[7/200] Training loss: 0.04004650
[8/200] Training loss: 0.03806608
[9/200] Training loss: 0.03621327
[10/200] Training loss: 0.03209482
[50/200] Training loss: 0.01792900
[100/200] Training loss: 0.01611985
[150/200] Training loss: 0.01473204
[200/200] Training loss: 0.01428839
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16324.314135668916 ----------
[1/200] Training loss: 0.15134524
[2/200] Training loss: 0.05768865
[3/200] Training loss: 0.05270013
[4/200] Training loss: 0.04731384
[5/200] Training loss: 0.04447806
[6/200] Training loss: 0.04477088
[7/200] Training loss: 0.03831557
[8/200] Training loss: 0.03567752
[9/200] Training loss: 0.03513713
[10/200] Training loss: 0.03237456
[50/200] Training loss: 0.01762581
[100/200] Training loss: 0.01466728
[150/200] Training loss: 0.01285166
[200/200] Training loss: 0.01244290
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4938.606483614583 ----------
[1/200] Training loss: 0.11934170
[2/200] Training loss: 0.05464105
[3/200] Training loss: 0.04950810
[4/200] Training loss: 0.04514030
[5/200] Training loss: 0.04359988
[6/200] Training loss: 0.04266021
[7/200] Training loss: 0.03949656
[8/200] Training loss: 0.03768971
[9/200] Training loss: 0.03766067
[10/200] Training loss: 0.03405933
[50/200] Training loss: 0.01768156
[100/200] Training loss: 0.01460360
[150/200] Training loss: 0.01201143
[200/200] Training loss: 0.01087343
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01189641827273458
----FITNESS-----------RMSE---- 18977.27230136091 ----------
[1/200] Training loss: 0.16968067
[2/200] Training loss: 0.06329075
[3/200] Training loss: 0.05291878
[4/200] Training loss: 0.04874356
[5/200] Training loss: 0.04766317
[6/200] Training loss: 0.04792055
[7/200] Training loss: 0.04283227
[8/200] Training loss: 0.04062222
[9/200] Training loss: 0.03985057
[10/200] Training loss: 0.03744631
[50/200] Training loss: 0.02048306
[100/200] Training loss: 0.01559879
[150/200] Training loss: 0.01396316
[200/200] Training loss: 0.01306482
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13880.820725014786 ----------
[1/200] Training loss: 0.14193657
[2/200] Training loss: 0.05351403
[3/200] Training loss: 0.05184962
[4/200] Training loss: 0.04451087
[5/200] Training loss: 0.04094690
[6/200] Training loss: 0.03677727
[7/200] Training loss: 0.03572097
[8/200] Training loss: 0.03297653
[9/200] Training loss: 0.02920786
[10/200] Training loss: 0.03037617
[50/200] Training loss: 0.01841182
[100/200] Training loss: 0.01580713
[150/200] Training loss: 0.01324303
[200/200] Training loss: 0.01246517
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21201.30486550297 ----------
[1/200] Training loss: 0.15532990
[2/200] Training loss: 0.06341265
[3/200] Training loss: 0.05298489
[4/200] Training loss: 0.05045574
[5/200] Training loss: 0.04642498
[6/200] Training loss: 0.04489968
[7/200] Training loss: 0.03852908
[8/200] Training loss: 0.04151212
[9/200] Training loss: 0.03549765
[10/200] Training loss: 0.03407484
[50/200] Training loss: 0.01777351
[100/200] Training loss: 0.01553208
[150/200] Training loss: 0.01361896
[200/200] Training loss: 0.01254433
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10089.085191433363 ----------
[1/200] Training loss: 0.18157624
[2/200] Training loss: 0.05633639
[3/200] Training loss: 0.05039831
[4/200] Training loss: 0.04433301
[5/200] Training loss: 0.03654708
[6/200] Training loss: 0.03504994
[7/200] Training loss: 0.03521164
[8/200] Training loss: 0.03526297
[9/200] Training loss: 0.03024445
[10/200] Training loss: 0.02681761
[50/200] Training loss: 0.01742238
[100/200] Training loss: 0.01435227
[150/200] Training loss: 0.01294458
[200/200] Training loss: 0.01175375
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20153.525150702542 ----------
[1/200] Training loss: 0.15156919
[2/200] Training loss: 0.05712775
[3/200] Training loss: 0.04778091
[4/200] Training loss: 0.04460835
[5/200] Training loss: 0.04155696
[6/200] Training loss: 0.03911535
[7/200] Training loss: 0.03222397
[8/200] Training loss: 0.03285468
[9/200] Training loss: 0.02983983
[10/200] Training loss: 0.03072375
[50/200] Training loss: 0.01679145
[100/200] Training loss: 0.01415462
[150/200] Training loss: 0.01359840
[200/200] Training loss: 0.01200065
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14165.72031348918 ----------
[1/200] Training loss: 0.14365270
[2/200] Training loss: 0.06097452
[3/200] Training loss: 0.05059574
[4/200] Training loss: 0.04986761
[5/200] Training loss: 0.04675954
[6/200] Training loss: 0.04378123
[7/200] Training loss: 0.03987408
[8/200] Training loss: 0.03786423
[9/200] Training loss: 0.03708026
[10/200] Training loss: 0.03349567
[50/200] Training loss: 0.01812226
[100/200] Training loss: 0.01538123
[150/200] Training loss: 0.01357704
[200/200] Training loss: 0.01240119
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20575.19127493108 ----------
[1/200] Training loss: 0.16937071
[2/200] Training loss: 0.05848576
[3/200] Training loss: 0.05320461
[4/200] Training loss: 0.04972021
[5/200] Training loss: 0.04718636
[6/200] Training loss: 0.04320247
[7/200] Training loss: 0.03954248
[8/200] Training loss: 0.03703965
[9/200] Training loss: 0.03685025
[10/200] Training loss: 0.03239357
[50/200] Training loss: 0.01772807
[100/200] Training loss: 0.01560199
[150/200] Training loss: 0.01355540
[200/200] Training loss: 0.01274403
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15338.435904615568 ----------
[1/150] Training loss: 0.15266327
[2/150] Training loss: 0.06073180
[3/150] Training loss: 0.05293221
[4/150] Training loss: 0.04928797
[5/150] Training loss: 0.04816351
[6/150] Training loss: 0.04527972
[7/150] Training loss: 0.04392901
[8/150] Training loss: 0.03984465
[9/150] Training loss: 0.03958479
[10/150] Training loss: 0.03479909
[50/150] Training loss: 0.01962098
[100/150] Training loss: 0.01664074
[150/150] Training loss: 0.01420108
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 150
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14560.771408136316 ----------
[1/200] Training loss: 0.17271003
[2/200] Training loss: 0.06071345
[3/200] Training loss: 0.05440112
[4/200] Training loss: 0.05241697
[5/200] Training loss: 0.04773821
[6/200] Training loss: 0.04422741
[7/200] Training loss: 0.04027949
[8/200] Training loss: 0.03628103
[9/200] Training loss: 0.03394250
[10/200] Training loss: 0.03275682
[50/200] Training loss: 0.01809671
[100/200] Training loss: 0.01580884
[150/200] Training loss: 0.01466895
[200/200] Training loss: 0.01309938
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7888.2299662218265 ----------
[1/200] Training loss: 0.14862119
[2/200] Training loss: 0.06057760
[3/200] Training loss: 0.05148081
[4/200] Training loss: 0.04425770
[5/200] Training loss: 0.04285606
[6/200] Training loss: 0.03813808
[7/200] Training loss: 0.03290515
[8/200] Training loss: 0.03431394
[9/200] Training loss: 0.02987266
[10/200] Training loss: 0.02875503
[50/200] Training loss: 0.01782711
[100/200] Training loss: 0.01448415
[150/200] Training loss: 0.01261229
[200/200] Training loss: 0.01124490
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10928.348457109152 ----------
[1/200] Training loss: 0.12956571
[2/200] Training loss: 0.05465647
[3/200] Training loss: 0.05161846
[4/200] Training loss: 0.05003738
[5/200] Training loss: 0.04398844
[6/200] Training loss: 0.04401283
[7/200] Training loss: 0.04187193
[8/200] Training loss: 0.04022068
[9/200] Training loss: 0.03539187
[10/200] Training loss: 0.03483060
[50/200] Training loss: 0.01833320
[100/200] Training loss: 0.01393371
[150/200] Training loss: 0.01294502
[200/200] Training loss: 0.01204127
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01189641827273458
----FITNESS-----------RMSE---- 11277.522068255952 ----------
[1/200] Training loss: 0.14391953
[2/200] Training loss: 0.05657879
[3/200] Training loss: 0.05229891
[4/200] Training loss: 0.04836778
[5/200] Training loss: 0.04659922
[6/200] Training loss: 0.04242898
[7/200] Training loss: 0.04005399
[8/200] Training loss: 0.03442677
[9/200] Training loss: 0.03247389
[10/200] Training loss: 0.03242969
[50/200] Training loss: 0.01772342
[100/200] Training loss: 0.01506875
[150/200] Training loss: 0.01345095
[200/200] Training loss: 0.01248900
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15844.338294797924 ----------
[1/200] Training loss: 0.14665308
[2/200] Training loss: 0.06367530
[3/200] Training loss: 0.05309066
[4/200] Training loss: 0.05130211
[5/200] Training loss: 0.04900553
[6/200] Training loss: 0.04672547
[7/200] Training loss: 0.04123724
[8/200] Training loss: 0.03852864
[9/200] Training loss: 0.03845541
[10/200] Training loss: 0.03826535
[50/200] Training loss: 0.01633218
[100/200] Training loss: 0.01368545
[150/200] Training loss: 0.01264153
[200/200] Training loss: 0.01146631
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18302.075947826248 ----------
[1/200] Training loss: 0.16488135
[2/200] Training loss: 0.06266189
[3/200] Training loss: 0.05142316
[4/200] Training loss: 0.04593545
[5/200] Training loss: 0.04573524
[6/200] Training loss: 0.04263562
[7/200] Training loss: 0.03764058
[8/200] Training loss: 0.03696158
[9/200] Training loss: 0.03563928
[10/200] Training loss: 0.03460071
[50/200] Training loss: 0.01919745
[100/200] Training loss: 0.01607646
[150/200] Training loss: 0.01345926
[200/200] Training loss: 0.01297093
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8490.127443095304 ----------
[1/200] Training loss: 0.14681536
[2/200] Training loss: 0.05522764
[3/200] Training loss: 0.05468665
[4/200] Training loss: 0.04727640
[5/200] Training loss: 0.04732126
[6/200] Training loss: 0.03939700
[7/200] Training loss: 0.04146260
[8/200] Training loss: 0.03714559
[9/200] Training loss: 0.03427268
[10/200] Training loss: 0.03506438
[50/200] Training loss: 0.01674308
[100/200] Training loss: 0.01421849
[150/200] Training loss: 0.01310807
[200/200] Training loss: 0.01149269
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15582.146707049063 ----------
[1/200] Training loss: 0.14543397
[2/200] Training loss: 0.05378071
[3/200] Training loss: 0.04627962
[4/200] Training loss: 0.04587669
[5/200] Training loss: 0.03986168
[6/200] Training loss: 0.03629229
[7/200] Training loss: 0.03447841
[8/200] Training loss: 0.03312920
[9/200] Training loss: 0.03306390
[10/200] Training loss: 0.03145037
[50/200] Training loss: 0.01742844
[100/200] Training loss: 0.01509726
[150/200] Training loss: 0.01346680
[200/200] Training loss: 0.01261140
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10716.072041564483 ----------
[1/200] Training loss: 0.16048598
[2/200] Training loss: 0.05957167
[3/200] Training loss: 0.05500824
[4/200] Training loss: 0.04930963
[5/200] Training loss: 0.04868323
[6/200] Training loss: 0.04512010
[7/200] Training loss: 0.04385383
[8/200] Training loss: 0.04195293
[9/200] Training loss: 0.04182767
[10/200] Training loss: 0.03875798
[50/200] Training loss: 0.01977458
[100/200] Training loss: 0.01711538
[150/200] Training loss: 0.01519846
[200/200] Training loss: 0.01479523
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12317.32893122531 ----------
[1/200] Training loss: 0.15397420
[2/200] Training loss: 0.05653811
[3/200] Training loss: 0.04986596
[4/200] Training loss: 0.04719375
[5/200] Training loss: 0.04404430
[6/200] Training loss: 0.04299180
[7/200] Training loss: 0.04270457
[8/200] Training loss: 0.03940737
[9/200] Training loss: 0.03600050
[10/200] Training loss: 0.03475320
[50/200] Training loss: 0.01907993
[100/200] Training loss: 0.01497330
[150/200] Training loss: 0.01346029
[200/200] Training loss: 0.01239870
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15403.347688083912 ----------
[1/200] Training loss: 0.14391198
[2/200] Training loss: 0.05540777
[3/200] Training loss: 0.05205635
[4/200] Training loss: 0.04870700
[5/200] Training loss: 0.04774285
[6/200] Training loss: 0.04177597
[7/200] Training loss: 0.04163414
[8/200] Training loss: 0.03763756
[9/200] Training loss: 0.03434332
[10/200] Training loss: 0.03397597
[50/200] Training loss: 0.01904078
[100/200] Training loss: 0.01530852
[150/200] Training loss: 0.01405178
[200/200] Training loss: 0.01222190
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10033.3288593567 ----------
[1/200] Training loss: 0.13894077
[2/200] Training loss: 0.05433695
[3/200] Training loss: 0.04586696
[4/200] Training loss: 0.04725062
[5/200] Training loss: 0.03746478
[6/200] Training loss: 0.03737512
[7/200] Training loss: 0.03347850
[8/200] Training loss: 0.03167181
[9/200] Training loss: 0.03031796
[10/200] Training loss: 0.02951526
[50/200] Training loss: 0.01700952
[100/200] Training loss: 0.01461959
[150/200] Training loss: 0.01249582
[200/200] Training loss: 0.01169136
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22683.879738704312 ----------
[1/200] Training loss: 0.16167968
[2/200] Training loss: 0.06245710
[3/200] Training loss: 0.05707483
[4/200] Training loss: 0.04859296
[5/200] Training loss: 0.04972587
[6/200] Training loss: 0.04884044
[7/200] Training loss: 0.04410129
[8/200] Training loss: 0.04145345
[9/200] Training loss: 0.03937166
[10/200] Training loss: 0.03667905
[50/200] Training loss: 0.01856061
[100/200] Training loss: 0.01536841
[150/200] Training loss: 0.01508531
[200/200] Training loss: 0.01411566
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9397.570749933198 ----------
[1/200] Training loss: 0.16299087
[2/200] Training loss: 0.05884222
[3/200] Training loss: 0.05582899
[4/200] Training loss: 0.05167192
[5/200] Training loss: 0.04920111
[6/200] Training loss: 0.04511018
[7/200] Training loss: 0.04468794
[8/200] Training loss: 0.04296311
[9/200] Training loss: 0.03909798
[10/200] Training loss: 0.03543665
[50/200] Training loss: 0.01845132
[100/200] Training loss: 0.01507369
[150/200] Training loss: 0.01388986
[200/200] Training loss: 0.01323945
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 32664.945369616034 ----------
[1/200] Training loss: 0.18395697
[2/200] Training loss: 0.06321835
[3/200] Training loss: 0.05672657
[4/200] Training loss: 0.05490336
[5/200] Training loss: 0.04965526
[6/200] Training loss: 0.04931826
[7/200] Training loss: 0.04560837
[8/200] Training loss: 0.04324815
[9/200] Training loss: 0.04186674
[10/200] Training loss: 0.03919394
[50/200] Training loss: 0.01714040
[100/200] Training loss: 0.01551954
[150/200] Training loss: 0.01382852
[200/200] Training loss: 0.01372850
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16930.968548786568 ----------
[1/200] Training loss: 0.14474127
[2/200] Training loss: 0.05546932
[3/200] Training loss: 0.04809429
[4/200] Training loss: 0.04740955
[5/200] Training loss: 0.04249763
[6/200] Training loss: 0.04170198
[7/200] Training loss: 0.03973698
[8/200] Training loss: 0.03737643
[9/200] Training loss: 0.03685184
[10/200] Training loss: 0.03439628
[50/200] Training loss: 0.01824839
[100/200] Training loss: 0.01422544
[150/200] Training loss: 0.01306931
[200/200] Training loss: 0.01239413
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7995.112256872945 ----------
[1/200] Training loss: 0.15098582
[2/200] Training loss: 0.05740830
[3/200] Training loss: 0.04854603
[4/200] Training loss: 0.04424465
[5/200] Training loss: 0.03720493
[6/200] Training loss: 0.03396461
[7/200] Training loss: 0.03188361
[8/200] Training loss: 0.03065641
[9/200] Training loss: 0.02807878
[10/200] Training loss: 0.02706633
[50/200] Training loss: 0.01671306
[100/200] Training loss: 0.01439555
[150/200] Training loss: 0.01236929
[200/200] Training loss: 0.01169891
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19034.243667663814 ----------
[1/200] Training loss: 0.14234528
[2/200] Training loss: 0.05774962
[3/200] Training loss: 0.05122390
[4/200] Training loss: 0.04582471
[5/200] Training loss: 0.04180187
[6/200] Training loss: 0.03971118
[7/200] Training loss: 0.03764250
[8/200] Training loss: 0.03882117
[9/200] Training loss: 0.03584379
[10/200] Training loss: 0.03581591
[50/200] Training loss: 0.01679934
[100/200] Training loss: 0.01389842
[150/200] Training loss: 0.01231312
[200/200] Training loss: 0.01119500
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8818.258331439378 ----------
[1/200] Training loss: 0.17853064
[2/200] Training loss: 0.05821618
[3/200] Training loss: 0.05134209
[4/200] Training loss: 0.04654679
[5/200] Training loss: 0.04413714
[6/200] Training loss: 0.03829799
[7/200] Training loss: 0.03753735
[8/200] Training loss: 0.03436956
[9/200] Training loss: 0.03123843
[10/200] Training loss: 0.03118322
[50/200] Training loss: 0.01937366
[100/200] Training loss: 0.01679853
[150/200] Training loss: 0.01503605
[200/200] Training loss: 0.01443089
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14257.016237628404 ----------
[1/200] Training loss: 0.16047802
[2/200] Training loss: 0.06273514
[3/200] Training loss: 0.05524860
[4/200] Training loss: 0.05168633
[5/200] Training loss: 0.05223071
[6/200] Training loss: 0.04792610
[7/200] Training loss: 0.04696151
[8/200] Training loss: 0.04416524
[9/200] Training loss: 0.04251759
[10/200] Training loss: 0.04072532
[50/200] Training loss: 0.01825099
[100/200] Training loss: 0.01628899
[150/200] Training loss: 0.01426347
[200/200] Training loss: 0.01354846
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9804.771491472915 ----------
[1/200] Training loss: 0.16144555
[2/200] Training loss: 0.05819755
[3/200] Training loss: 0.05054860
[4/200] Training loss: 0.04242205
[5/200] Training loss: 0.03919247
[6/200] Training loss: 0.03878707
[7/200] Training loss: 0.03372045
[8/200] Training loss: 0.03373158
[9/200] Training loss: 0.03288145
[10/200] Training loss: 0.03085161
[50/200] Training loss: 0.01631299
[100/200] Training loss: 0.01357707
[150/200] Training loss: 0.01229890
[200/200] Training loss: 0.01104539
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18986.57041174103 ----------
[1/200] Training loss: 0.15574695
[2/200] Training loss: 0.06003867
[3/200] Training loss: 0.05064451
[4/200] Training loss: 0.04503441
[5/200] Training loss: 0.04196510
[6/200] Training loss: 0.04184189
[7/200] Training loss: 0.03866777
[8/200] Training loss: 0.03109474
[9/200] Training loss: 0.03201974
[10/200] Training loss: 0.03027263
[50/200] Training loss: 0.01726346
[100/200] Training loss: 0.01382137
[150/200] Training loss: 0.01302139
[200/200] Training loss: 0.01231203
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18330.960913165465 ----------
[1/200] Training loss: 0.17295629
[2/200] Training loss: 0.06291334
[3/200] Training loss: 0.05581986
[4/200] Training loss: 0.05089378
[5/200] Training loss: 0.04697525
[6/200] Training loss: 0.04443510
[7/200] Training loss: 0.04272402
[8/200] Training loss: 0.04110765
[9/200] Training loss: 0.03965547
[10/200] Training loss: 0.03545161
[50/200] Training loss: 0.01814692
[100/200] Training loss: 0.01630466
[150/200] Training loss: 0.01414454
[200/200] Training loss: 0.01337133
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14914.133967481986 ----------
[1/200] Training loss: 0.17307777
[2/200] Training loss: 0.05862934
[3/200] Training loss: 0.05427341
[4/200] Training loss: 0.05204278
[5/200] Training loss: 0.04952246
[6/200] Training loss: 0.04739755
[7/200] Training loss: 0.04227665
[8/200] Training loss: 0.04265278
[9/200] Training loss: 0.04142315
[10/200] Training loss: 0.04047398
[50/200] Training loss: 0.01851341
[100/200] Training loss: 0.01590337
[150/200] Training loss: 0.01399175
[200/200] Training loss: 0.01261120
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23236.003787226407 ----------
[1/200] Training loss: 0.18270629
[2/200] Training loss: 0.06548560
[3/200] Training loss: 0.05438728
[4/200] Training loss: 0.05237181
[5/200] Training loss: 0.05115260
[6/200] Training loss: 0.04986882
[7/200] Training loss: 0.04195062
[8/200] Training loss: 0.04037130
[9/200] Training loss: 0.03878108
[10/200] Training loss: 0.03357470
[50/200] Training loss: 0.01880145
[100/200] Training loss: 0.01474399
[150/200] Training loss: 0.01454757
[200/200] Training loss: 0.01345271
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6740.190501758834 ----------
[1/200] Training loss: 0.16278406
[2/200] Training loss: 0.05619048
[3/200] Training loss: 0.05072111
[4/200] Training loss: 0.04689572
[5/200] Training loss: 0.04356391
[6/200] Training loss: 0.04205834
[7/200] Training loss: 0.03966748
[8/200] Training loss: 0.03617417
[9/200] Training loss: 0.03590142
[10/200] Training loss: 0.03348912
[50/200] Training loss: 0.01693158
[100/200] Training loss: 0.01523191
[150/200] Training loss: 0.01371929
[200/200] Training loss: 0.01317796
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12879.980124208267 ----------
[1/200] Training loss: 0.13624158
[2/200] Training loss: 0.05841909
[3/200] Training loss: 0.04690131
[4/200] Training loss: 0.04793371
[5/200] Training loss: 0.04434827
[6/200] Training loss: 0.04000853
[7/200] Training loss: 0.04031309
[8/200] Training loss: 0.03562373
[9/200] Training loss: 0.03328762
[10/200] Training loss: 0.03400953
[50/200] Training loss: 0.01757523
[100/200] Training loss: 0.01439782
[150/200] Training loss: 0.01271764
[200/200] Training loss: 0.01170945
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17802.815957033315 ----------
[1/200] Training loss: 0.15682501
[2/200] Training loss: 0.05934465
[3/200] Training loss: 0.05142165
[4/200] Training loss: 0.05086401
[5/200] Training loss: 0.04903227
[6/200] Training loss: 0.04777437
[7/200] Training loss: 0.04508327
[8/200] Training loss: 0.04180650
[9/200] Training loss: 0.04151673
[10/200] Training loss: 0.04055244
[50/200] Training loss: 0.01714249
[100/200] Training loss: 0.01453554
[150/200] Training loss: 0.01260211
[200/200] Training loss: 0.01222470
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17874.704361191543 ----------
[1/200] Training loss: 0.17468978
[2/200] Training loss: 0.06333318
[3/200] Training loss: 0.05518274
[4/200] Training loss: 0.05238293
[5/200] Training loss: 0.04856236
[6/200] Training loss: 0.04892824
[7/200] Training loss: 0.04399417
[8/200] Training loss: 0.04247965
[9/200] Training loss: 0.03962628
[10/200] Training loss: 0.03933315
[50/200] Training loss: 0.01868738
[100/200] Training loss: 0.01469756
[150/200] Training loss: 0.01366382
[200/200] Training loss: 0.01272849
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 26339.62186516731 ----------
[1/200] Training loss: 0.17254923
[2/200] Training loss: 0.05909601
[3/200] Training loss: 0.05467712
[4/200] Training loss: 0.05057510
[5/200] Training loss: 0.04592324
[6/200] Training loss: 0.04359249
[7/200] Training loss: 0.04246022
[8/200] Training loss: 0.04026371
[9/200] Training loss: 0.03548540
[10/200] Training loss: 0.03502408
[50/200] Training loss: 0.01721471
[100/200] Training loss: 0.01450446
[150/200] Training loss: 0.01329087
[200/200] Training loss: 0.01207081
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7780.583268624531 ----------
[1/200] Training loss: 0.14375165
[2/200] Training loss: 0.05827816
[3/200] Training loss: 0.05022370
[4/200] Training loss: 0.04577782
[5/200] Training loss: 0.04500144
[6/200] Training loss: 0.03921346
[7/200] Training loss: 0.03753393
[8/200] Training loss: 0.03411119
[9/200] Training loss: 0.03305643
[10/200] Training loss: 0.03146590
[50/200] Training loss: 0.01787377
[100/200] Training loss: 0.01526784
[150/200] Training loss: 0.01390293
[200/200] Training loss: 0.01293725
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17344.207563333646 ----------
[1/200] Training loss: 0.18345924
[2/200] Training loss: 0.05342935
[3/200] Training loss: 0.05265076
[4/200] Training loss: 0.05073981
[5/200] Training loss: 0.04281671
[6/200] Training loss: 0.04236153
[7/200] Training loss: 0.03831056
[8/200] Training loss: 0.03563262
[9/200] Training loss: 0.03680238
[10/200] Training loss: 0.03369073
[50/200] Training loss: 0.01899303
[100/200] Training loss: 0.01530590
[150/200] Training loss: 0.01458288
[200/200] Training loss: 0.01285487
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6879.983139514224 ----------
[1/200] Training loss: 0.14423975
[2/200] Training loss: 0.05573288
[3/200] Training loss: 0.05294006
[4/200] Training loss: 0.04831297
[5/200] Training loss: 0.04715875
[6/200] Training loss: 0.04555255
[7/200] Training loss: 0.04411555
[8/200] Training loss: 0.03634382
[9/200] Training loss: 0.03831137
[10/200] Training loss: 0.03557640
[50/200] Training loss: 0.01757595
[100/200] Training loss: 0.01462177
[150/200] Training loss: 0.01399591
[200/200] Training loss: 0.01245645
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18119.40219764438 ----------
[1/200] Training loss: 0.16128422
[2/200] Training loss: 0.05602204
[3/200] Training loss: 0.05325130
[4/200] Training loss: 0.04896262
[5/200] Training loss: 0.04814075
[6/200] Training loss: 0.04223308
[7/200] Training loss: 0.04283432
[8/200] Training loss: 0.03736462
[9/200] Training loss: 0.03550260
[10/200] Training loss: 0.03354001
[50/200] Training loss: 0.01841298
[100/200] Training loss: 0.01466501
[150/200] Training loss: 0.01379555
[200/200] Training loss: 0.01261337
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10788.403032886748 ----------
[1/200] Training loss: 0.16103293
[2/200] Training loss: 0.06387880
[3/200] Training loss: 0.05664589
[4/200] Training loss: 0.05385117
[5/200] Training loss: 0.04869277
[6/200] Training loss: 0.04723432
[7/200] Training loss: 0.04507739
[8/200] Training loss: 0.04608163
[9/200] Training loss: 0.03889116
[10/200] Training loss: 0.03708264
[50/200] Training loss: 0.01834409
[100/200] Training loss: 0.01475095
[150/200] Training loss: 0.01309103
[200/200] Training loss: 0.01220694
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20344.830498187985 ----------
[1/200] Training loss: 0.17338227
[2/200] Training loss: 0.06911008
[3/200] Training loss: 0.06509667
[4/200] Training loss: 0.05486436
[5/200] Training loss: 0.05312683
[6/200] Training loss: 0.05334413
[7/200] Training loss: 0.04847111
[8/200] Training loss: 0.04693616
[9/200] Training loss: 0.04435639
[10/200] Training loss: 0.04442994
[50/200] Training loss: 0.01852182
[100/200] Training loss: 0.01613409
[150/200] Training loss: 0.01419649
[200/200] Training loss: 0.01364936
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15830.676043681773 ----------
[1/200] Training loss: 0.17192088
[2/200] Training loss: 0.06122745
[3/200] Training loss: 0.05443999
[4/200] Training loss: 0.04978572
[5/200] Training loss: 0.04496283
[6/200] Training loss: 0.04228210
[7/200] Training loss: 0.04018963
[8/200] Training loss: 0.03378709
[9/200] Training loss: 0.03199897
[10/200] Training loss: 0.03356943
[50/200] Training loss: 0.01795185
[100/200] Training loss: 0.01555540
[150/200] Training loss: 0.01411579
[200/200] Training loss: 0.01350878
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10656.911747781343 ----------
[1/200] Training loss: 0.12863118
[2/200] Training loss: 0.05651539
[3/200] Training loss: 0.05595046
[4/200] Training loss: 0.05307641
[5/200] Training loss: 0.04931174
[6/200] Training loss: 0.04646620
[7/200] Training loss: 0.04290332
[8/200] Training loss: 0.04326321
[9/200] Training loss: 0.03791045
[10/200] Training loss: 0.03978380
[50/200] Training loss: 0.01831472
[100/200] Training loss: 0.01503522
[150/200] Training loss: 0.01417559
[200/200] Training loss: 0.01236908
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12172.546816504753 ----------
[1/200] Training loss: 0.16081498
[2/200] Training loss: 0.05533335
[3/200] Training loss: 0.05203994
[4/200] Training loss: 0.04713073
[5/200] Training loss: 0.04088581
[6/200] Training loss: 0.03759130
[7/200] Training loss: 0.03683443
[8/200] Training loss: 0.03201783
[9/200] Training loss: 0.03373034
[10/200] Training loss: 0.02966294
[50/200] Training loss: 0.01786382
[100/200] Training loss: 0.01519275
[150/200] Training loss: 0.01391593
[200/200] Training loss: 0.01248252
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13154.811743236769 ----------
[1/200] Training loss: 0.14566711
[2/200] Training loss: 0.05444435
[3/200] Training loss: 0.04776274
[4/200] Training loss: 0.04745428
[5/200] Training loss: 0.04434343
[6/200] Training loss: 0.04232013
[7/200] Training loss: 0.03882654
[8/200] Training loss: 0.03902775
[9/200] Training loss: 0.03632732
[10/200] Training loss: 0.03422303
[50/200] Training loss: 0.01924465
[100/200] Training loss: 0.01642191
[150/200] Training loss: 0.01493965
[200/200] Training loss: 0.01392428
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 24089.511742665105 ----------
[1/200] Training loss: 0.15156436
[2/200] Training loss: 0.05682446
[3/200] Training loss: 0.05182139
[4/200] Training loss: 0.05115163
[5/200] Training loss: 0.04645629
[6/200] Training loss: 0.04170399
[7/200] Training loss: 0.04122789
[8/200] Training loss: 0.04067149
[9/200] Training loss: 0.03861630
[10/200] Training loss: 0.03524620
[50/200] Training loss: 0.01629249
[100/200] Training loss: 0.01418192
[150/200] Training loss: 0.01338567
[200/200] Training loss: 0.01250866
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13624.322368470293 ----------
[1/200] Training loss: 0.15618690
[2/200] Training loss: 0.06008422
[3/200] Training loss: 0.05162392
[4/200] Training loss: 0.04733263
[5/200] Training loss: 0.04484804
[6/200] Training loss: 0.04128915
[7/200] Training loss: 0.04094738
[8/200] Training loss: 0.03860487
[9/200] Training loss: 0.03752592
[10/200] Training loss: 0.03390028
[50/200] Training loss: 0.01778776
[100/200] Training loss: 0.01616904
[150/200] Training loss: 0.01429036
[200/200] Training loss: 0.01335015
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11003.112286984988 ----------
[1/200] Training loss: 0.19476653
[2/200] Training loss: 0.06424716
[3/200] Training loss: 0.05553619
[4/200] Training loss: 0.05502159
[5/200] Training loss: 0.05243762
[6/200] Training loss: 0.04853076
[7/200] Training loss: 0.04640503
[8/200] Training loss: 0.04644783
[9/200] Training loss: 0.04251522
[10/200] Training loss: 0.04319722
[50/200] Training loss: 0.01900330
[100/200] Training loss: 0.01578388
[150/200] Training loss: 0.01450674
[200/200] Training loss: 0.01351181
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18160.380612751484 ----------
[1/200] Training loss: 0.17142590
[2/200] Training loss: 0.06089155
[3/200] Training loss: 0.05544547
[4/200] Training loss: 0.04941854
[5/200] Training loss: 0.05166008
[6/200] Training loss: 0.04962676
[7/200] Training loss: 0.04618570
[8/200] Training loss: 0.04390109
[9/200] Training loss: 0.04422835
[10/200] Training loss: 0.04229563
[50/200] Training loss: 0.01972421
[100/200] Training loss: 0.01704879
[150/200] Training loss: 0.01421620
[200/200] Training loss: 0.01376634
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12212.84438613708 ----------
[1/200] Training loss: 0.14723489
[2/200] Training loss: 0.05313965
[3/200] Training loss: 0.04936440
[4/200] Training loss: 0.04641281
[5/200] Training loss: 0.04401386
[6/200] Training loss: 0.03943246
[7/200] Training loss: 0.03872578
[8/200] Training loss: 0.03535687
[9/200] Training loss: 0.03496242
[10/200] Training loss: 0.03174440
[50/200] Training loss: 0.01947664
[100/200] Training loss: 0.01515595
[150/200] Training loss: 0.01341764
[200/200] Training loss: 0.01231056
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4243.3727151877665 ----------
[1/200] Training loss: 0.12813636
[2/200] Training loss: 0.05465311
[3/200] Training loss: 0.05247087
[4/200] Training loss: 0.04814480
[5/200] Training loss: 0.04716303
[6/200] Training loss: 0.04692064
[7/200] Training loss: 0.04500453
[8/200] Training loss: 0.04447646
[9/200] Training loss: 0.04418754
[10/200] Training loss: 0.04137279
[50/200] Training loss: 0.02353474
[100/200] Training loss: 0.02006304
[150/200] Training loss: 0.01808538
[200/200] Training loss: 0.01577854
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.0036513882958379602
----FITNESS-----------RMSE---- 17613.771884522634 ----------
[1/200] Training loss: 0.15797011
[2/200] Training loss: 0.05878780
[3/200] Training loss: 0.05400440
[4/200] Training loss: 0.04899968
[5/200] Training loss: 0.04475982
[6/200] Training loss: 0.04376752
[7/200] Training loss: 0.04214101
[8/200] Training loss: 0.03768163
[9/200] Training loss: 0.03779124
[10/200] Training loss: 0.03757988
[50/200] Training loss: 0.01695022
[100/200] Training loss: 0.01423358
[150/200] Training loss: 0.01354637
[200/200] Training loss: 0.01275433
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18586.172064198694 ----------
[1/200] Training loss: 0.13760255
[2/200] Training loss: 0.06057562
[3/200] Training loss: 0.05230849
[4/200] Training loss: 0.05191454
[5/200] Training loss: 0.04888676
[6/200] Training loss: 0.04587787
[7/200] Training loss: 0.04189534
[8/200] Training loss: 0.04089883
[9/200] Training loss: 0.03722201
[10/200] Training loss: 0.03665926
[50/200] Training loss: 0.01738002
[100/200] Training loss: 0.01399130
[150/200] Training loss: 0.01277690
[200/200] Training loss: 0.01151837
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15097.41805740306 ----------
[1/200] Training loss: 0.16343889
[2/200] Training loss: 0.05618587
[3/200] Training loss: 0.05196846
[4/200] Training loss: 0.05046049
[5/200] Training loss: 0.04766623
[6/200] Training loss: 0.04493142
[7/200] Training loss: 0.04332824
[8/200] Training loss: 0.04082766
[9/200] Training loss: 0.03977637
[10/200] Training loss: 0.03709436
[50/200] Training loss: 0.01856157
[100/200] Training loss: 0.01572611
[150/200] Training loss: 0.01423926
[200/200] Training loss: 0.01336841
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8329.387492487067 ----------
[1/200] Training loss: 0.15182609
[2/200] Training loss: 0.05889863
[3/200] Training loss: 0.05426079
[4/200] Training loss: 0.05200933
[5/200] Training loss: 0.04767621
[6/200] Training loss: 0.04453012
[7/200] Training loss: 0.04567830
[8/200] Training loss: 0.04053719
[9/200] Training loss: 0.03714654
[10/200] Training loss: 0.03893818
[50/200] Training loss: 0.01847458
[100/200] Training loss: 0.01543540
[150/200] Training loss: 0.01366784
[200/200] Training loss: 0.01332297
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12382.741538124747 ----------
[1/200] Training loss: 0.15412036
[2/200] Training loss: 0.05541327
[3/200] Training loss: 0.05248639
[4/200] Training loss: 0.04903363
[5/200] Training loss: 0.04215766
[6/200] Training loss: 0.04163971
[7/200] Training loss: 0.03827373
[8/200] Training loss: 0.03500525
[9/200] Training loss: 0.03384366
[10/200] Training loss: 0.03163334
[50/200] Training loss: 0.01782128
[100/200] Training loss: 0.01423821
[150/200] Training loss: 0.01291459
[200/200] Training loss: 0.01184283
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 24729.123235569838 ----------
[1/200] Training loss: 0.17495968
[2/200] Training loss: 0.05819240
[3/200] Training loss: 0.05307067
[4/200] Training loss: 0.05087956
[5/200] Training loss: 0.04611434
[6/200] Training loss: 0.04175983
[7/200] Training loss: 0.03629109
[8/200] Training loss: 0.03676968
[9/200] Training loss: 0.03109686
[10/200] Training loss: 0.03189094
[50/200] Training loss: 0.01793243
[100/200] Training loss: 0.01639808
[150/200] Training loss: 0.01449948
[200/200] Training loss: 0.01358498
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8235.987372501248 ----------
[1/200] Training loss: 0.17701204
[2/200] Training loss: 0.06295764
[3/200] Training loss: 0.05594268
[4/200] Training loss: 0.05265132
[5/200] Training loss: 0.04979987
[6/200] Training loss: 0.05030766
[7/200] Training loss: 0.04466093
[8/200] Training loss: 0.04507860
[9/200] Training loss: 0.04318140
[10/200] Training loss: 0.04170321
[50/200] Training loss: 0.01830097
[100/200] Training loss: 0.01522369
[150/200] Training loss: 0.01370987
[200/200] Training loss: 0.01321294
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9366.618600113918 ----------
[1/200] Training loss: 0.17647487
[2/200] Training loss: 0.05741847
[3/200] Training loss: 0.05288056
[4/200] Training loss: 0.04813722
[5/200] Training loss: 0.04158270
[6/200] Training loss: 0.04667567
[7/200] Training loss: 0.04156069
[8/200] Training loss: 0.03802246
[9/200] Training loss: 0.03762108
[10/200] Training loss: 0.03433458
[50/200] Training loss: 0.01802550
[100/200] Training loss: 0.01524331
[150/200] Training loss: 0.01364192
[200/200] Training loss: 0.01306718
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15721.823812776938 ----------
[1/200] Training loss: 0.18046254
[2/200] Training loss: 0.06354224
[3/200] Training loss: 0.05721137
[4/200] Training loss: 0.05812553
[5/200] Training loss: 0.05320749
[6/200] Training loss: 0.05107392
[7/200] Training loss: 0.04866197
[8/200] Training loss: 0.04727447
[9/200] Training loss: 0.04507626
[10/200] Training loss: 0.04280915
[50/200] Training loss: 0.02114171
[100/200] Training loss: 0.01662138
[150/200] Training loss: 0.01614650
[200/200] Training loss: 0.01399603
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6191.447649782723 ----------
[1/200] Training loss: 0.19917333
[2/200] Training loss: 0.06115418
[3/200] Training loss: 0.05282868
[4/200] Training loss: 0.04778893
[5/200] Training loss: 0.04392340
[6/200] Training loss: 0.04108535
[7/200] Training loss: 0.04221871
[8/200] Training loss: 0.03892546
[9/200] Training loss: 0.03564352
[10/200] Training loss: 0.03390741
[50/200] Training loss: 0.01836583
[100/200] Training loss: 0.01559367
[150/200] Training loss: 0.01418370
[200/200] Training loss: 0.01368754
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16529.214863386584 ----------
[1/200] Training loss: 0.14206445
[2/200] Training loss: 0.05290031
[3/200] Training loss: 0.04812316
[4/200] Training loss: 0.04604223
[5/200] Training loss: 0.04242309
[6/200] Training loss: 0.04343456
[7/200] Training loss: 0.03935895
[8/200] Training loss: 0.03520607
[9/200] Training loss: 0.03458446
[10/200] Training loss: 0.03288850
[50/200] Training loss: 0.01723891
[100/200] Training loss: 0.01461772
[150/200] Training loss: 0.01398994
[200/200] Training loss: 0.01241346
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18596.49171214829 ----------
[1/200] Training loss: 0.16702522
[2/200] Training loss: 0.05090516
[3/200] Training loss: 0.05297727
[4/200] Training loss: 0.04925507
[5/200] Training loss: 0.04490846
[6/200] Training loss: 0.04118242
[7/200] Training loss: 0.03742406
[8/200] Training loss: 0.03438609
[9/200] Training loss: 0.03460898
[10/200] Training loss: 0.03154337
[50/200] Training loss: 0.01848393
[100/200] Training loss: 0.01419790
[150/200] Training loss: 0.01302678
[200/200] Training loss: 0.01160183
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9336.50726985204 ----------
[1/200] Training loss: 0.18030733
[2/200] Training loss: 0.06275958
[3/200] Training loss: 0.05509656
[4/200] Training loss: 0.05080255
[5/200] Training loss: 0.04769573
[6/200] Training loss: 0.04660034
[7/200] Training loss: 0.04177588
[8/200] Training loss: 0.03889610
[9/200] Training loss: 0.03547501
[10/200] Training loss: 0.03643359
[50/200] Training loss: 0.01877573
[100/200] Training loss: 0.01540233
[150/200] Training loss: 0.01351426
[200/200] Training loss: 0.01342595
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19181.982379305846 ----------
[1/50] Training loss: 0.15731132
[2/50] Training loss: 0.05654930
[3/50] Training loss: 0.05364985
[4/50] Training loss: 0.04944488
[5/50] Training loss: 0.04614241
[6/50] Training loss: 0.04146271
[7/50] Training loss: 0.04113541
[8/50] Training loss: 0.04078188
[9/50] Training loss: 0.03579060
[10/50] Training loss: 0.03457331
[50/50] Training loss: 0.01713682
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 50
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12112.99335424568 ----------
[1/200] Training loss: 0.17354744
[2/200] Training loss: 0.06090269
[3/200] Training loss: 0.05726271
[4/200] Training loss: 0.05151001
[5/200] Training loss: 0.04653624
[6/200] Training loss: 0.04529084
[7/200] Training loss: 0.04072525
[8/200] Training loss: 0.04084317
[9/200] Training loss: 0.03787911
[10/200] Training loss: 0.03595430
[50/200] Training loss: 0.01764043
[100/200] Training loss: 0.01403339
[150/200] Training loss: 0.01312504
[200/200] Training loss: 0.01262098
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20202.753079716636 ----------
[1/200] Training loss: 0.15864884
[2/200] Training loss: 0.05935235
[3/200] Training loss: 0.05121826
[4/200] Training loss: 0.04612422
[5/200] Training loss: 0.04546326
[6/200] Training loss: 0.04017845
[7/200] Training loss: 0.03911639
[8/200] Training loss: 0.03648457
[9/200] Training loss: 0.03242325
[10/200] Training loss: 0.03486132
[50/200] Training loss: 0.01856910
[100/200] Training loss: 0.01510421
[150/200] Training loss: 0.01444231
[200/200] Training loss: 0.01370892
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12757.580334844064 ----------
[1/200] Training loss: 0.18154973
[2/200] Training loss: 0.05739447
[3/200] Training loss: 0.05466759
[4/200] Training loss: 0.05124494
[5/200] Training loss: 0.04949018
[6/200] Training loss: 0.04781555
[7/200] Training loss: 0.04532402
[8/200] Training loss: 0.04236080
[9/200] Training loss: 0.03875638
[10/200] Training loss: 0.03453712
[50/200] Training loss: 0.01834809
[100/200] Training loss: 0.01664892
[150/200] Training loss: 0.01540731
[200/200] Training loss: 0.01344713
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9955.378847638094 ----------
[1/200] Training loss: 0.15458492
[2/200] Training loss: 0.05664341
[3/200] Training loss: 0.05479378
[4/200] Training loss: 0.05072209
[5/200] Training loss: 0.05096981
[6/200] Training loss: 0.04751224
[7/200] Training loss: 0.04602947
[8/200] Training loss: 0.04166693
[9/200] Training loss: 0.03817513
[10/200] Training loss: 0.03470018
[50/200] Training loss: 0.01846369
[100/200] Training loss: 0.01616087
[150/200] Training loss: 0.01495084
[200/200] Training loss: 0.01395510
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17387.832067281994 ----------
[1/200] Training loss: 0.16290166
[2/200] Training loss: 0.05759341
[3/200] Training loss: 0.04972775
[4/200] Training loss: 0.04505665
[5/200] Training loss: 0.04348973
[6/200] Training loss: 0.04021431
[7/200] Training loss: 0.03689301
[8/200] Training loss: 0.03712130
[9/200] Training loss: 0.03229596
[10/200] Training loss: 0.03238094
[50/200] Training loss: 0.01787250
[100/200] Training loss: 0.01597158
[150/200] Training loss: 0.01347307
[200/200] Training loss: 0.01325849
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15013.52083956325 ----------
[1/200] Training loss: 0.14585797
[2/200] Training loss: 0.05510216
[3/200] Training loss: 0.05042595
[4/200] Training loss: 0.04748382
[5/200] Training loss: 0.04059430
[6/200] Training loss: 0.04063968
[7/200] Training loss: 0.03665040
[8/200] Training loss: 0.03278095
[9/200] Training loss: 0.03070999
[10/200] Training loss: 0.02825874
[50/200] Training loss: 0.01561019
[100/200] Training loss: 0.01351076
[150/200] Training loss: 0.01211766
[200/200] Training loss: 0.01055026
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14077.042871285148 ----------
[1/200] Training loss: 0.17041429
[2/200] Training loss: 0.06564330
[3/200] Training loss: 0.05520452
[4/200] Training loss: 0.05064339
[5/200] Training loss: 0.04561133
[6/200] Training loss: 0.04393724
[7/200] Training loss: 0.04314553
[8/200] Training loss: 0.03741855
[9/200] Training loss: 0.03650174
[10/200] Training loss: 0.03548035
[50/200] Training loss: 0.01922654
[100/200] Training loss: 0.01435340
[150/200] Training loss: 0.01286322
[200/200] Training loss: 0.01289020
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8480.31037167862 ----------
[1/200] Training loss: 0.15589477
[2/200] Training loss: 0.05835112
[3/200] Training loss: 0.05017177
[4/200] Training loss: 0.04746987
[5/200] Training loss: 0.04259109
[6/200] Training loss: 0.04128656
[7/200] Training loss: 0.03727738
[8/200] Training loss: 0.03562520
[9/200] Training loss: 0.03410055
[10/200] Training loss: 0.03318637
[50/200] Training loss: 0.01875702
[100/200] Training loss: 0.01613187
[150/200] Training loss: 0.01537796
[200/200] Training loss: 0.01468546
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16718.07165913581 ----------
[1/200] Training loss: 0.15391729
[2/200] Training loss: 0.05181668
[3/200] Training loss: 0.04681236
[4/200] Training loss: 0.04842728
[5/200] Training loss: 0.04169708
[6/200] Training loss: 0.03844161
[7/200] Training loss: 0.03436728
[8/200] Training loss: 0.03387396
[9/200] Training loss: 0.03346958
[10/200] Training loss: 0.03157442
[50/200] Training loss: 0.01793229
[100/200] Training loss: 0.01428730
[150/200] Training loss: 0.01302354
[200/200] Training loss: 0.01242697
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17003.307913462017 ----------
[1/200] Training loss: 0.15895373
[2/200] Training loss: 0.06117480
[3/200] Training loss: 0.04807068
[4/200] Training loss: 0.04923551
[5/200] Training loss: 0.04156278
[6/200] Training loss: 0.03963729
[7/200] Training loss: 0.03591085
[8/200] Training loss: 0.03469115
[9/200] Training loss: 0.03026924
[10/200] Training loss: 0.02905778
[50/200] Training loss: 0.01769191
[100/200] Training loss: 0.01549449
[150/200] Training loss: 0.01409677
[200/200] Training loss: 0.01343719
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8123.336260428962 ----------
[1/200] Training loss: 0.13979759
[2/200] Training loss: 0.05279611
[3/200] Training loss: 0.04792348
[4/200] Training loss: 0.04489185
[5/200] Training loss: 0.03998644
[6/200] Training loss: 0.03967662
[7/200] Training loss: 0.03567781
[8/200] Training loss: 0.03396742
[9/200] Training loss: 0.03238818
[10/200] Training loss: 0.03000281
[50/200] Training loss: 0.01703191
[100/200] Training loss: 0.01481466
[150/200] Training loss: 0.01311207
[200/200] Training loss: 0.01266218
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12635.25227290694 ----------
[1/200] Training loss: 0.14932293
[2/200] Training loss: 0.05764121
[3/200] Training loss: 0.05001660
[4/200] Training loss: 0.04519672
[5/200] Training loss: 0.04125298
[6/200] Training loss: 0.03757862
[7/200] Training loss: 0.03621923
[8/200] Training loss: 0.03344766
[9/200] Training loss: 0.03117737
[10/200] Training loss: 0.03063976
[50/200] Training loss: 0.01914791
[100/200] Training loss: 0.01594371
[150/200] Training loss: 0.01493863
[200/200] Training loss: 0.01406204
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9714.123738145403 ----------
[1/200] Training loss: 0.16613909
[2/200] Training loss: 0.06080722
[3/200] Training loss: 0.05490976
[4/200] Training loss: 0.04746407
[5/200] Training loss: 0.04629240
[6/200] Training loss: 0.04578289
[7/200] Training loss: 0.04263411
[8/200] Training loss: 0.03956126
[9/200] Training loss: 0.03536948
[10/200] Training loss: 0.03660557
[50/200] Training loss: 0.01861627
[100/200] Training loss: 0.01377921
[150/200] Training loss: 0.01247532
[200/200] Training loss: 0.01149121
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22560.598573619453 ----------
[1/200] Training loss: 0.14836820
[2/200] Training loss: 0.05984667
[3/200] Training loss: 0.05042113
[4/200] Training loss: 0.04753459
[5/200] Training loss: 0.04463798
[6/200] Training loss: 0.04386578
[7/200] Training loss: 0.04023184
[8/200] Training loss: 0.03649828
[9/200] Training loss: 0.03357890
[10/200] Training loss: 0.03489729
[50/200] Training loss: 0.01909915
[100/200] Training loss: 0.01508156
[150/200] Training loss: 0.01352174
[200/200] Training loss: 0.01292160
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12354.50881257527 ----------
[1/200] Training loss: 0.17253121
[2/200] Training loss: 0.05509277
[3/200] Training loss: 0.05320288
[4/200] Training loss: 0.04548648
[5/200] Training loss: 0.04609799
[6/200] Training loss: 0.04128751
[7/200] Training loss: 0.03480537
[8/200] Training loss: 0.03524108
[9/200] Training loss: 0.03328486
[10/200] Training loss: 0.03331245
[50/200] Training loss: 0.01759903
[100/200] Training loss: 0.01355596
[150/200] Training loss: 0.01224257
[200/200] Training loss: 0.01137832
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11612.668943873325 ----------
[1/200] Training loss: 0.14769651
[2/200] Training loss: 0.05795280
[3/200] Training loss: 0.05240061
[4/200] Training loss: 0.04618619
[5/200] Training loss: 0.04725672
[6/200] Training loss: 0.04096746
[7/200] Training loss: 0.04236340
[8/200] Training loss: 0.03791773
[9/200] Training loss: 0.03846655
[10/200] Training loss: 0.03572045
[50/200] Training loss: 0.01662378
[100/200] Training loss: 0.01478247
[150/200] Training loss: 0.01338430
[200/200] Training loss: 0.01246897
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13286.455659806343 ----------
[1/200] Training loss: 0.16881244
[2/200] Training loss: 0.06102564
[3/200] Training loss: 0.05405715
[4/200] Training loss: 0.04991921
[5/200] Training loss: 0.04566514
[6/200] Training loss: 0.04193336
[7/200] Training loss: 0.03904654
[8/200] Training loss: 0.03576231
[9/200] Training loss: 0.03442974
[10/200] Training loss: 0.03120601
[50/200] Training loss: 0.01708901
[100/200] Training loss: 0.01469130
[150/200] Training loss: 0.01421594
[200/200] Training loss: 0.01301353
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12090.87920707175 ----------
[1/200] Training loss: 0.15404469
[2/200] Training loss: 0.06425185
[3/200] Training loss: 0.05395562
[4/200] Training loss: 0.04877077
[5/200] Training loss: 0.04744787
[6/200] Training loss: 0.04364935
[7/200] Training loss: 0.04160508
[8/200] Training loss: 0.03763229
[9/200] Training loss: 0.04003551
[10/200] Training loss: 0.03573203
[50/200] Training loss: 0.01898728
[100/200] Training loss: 0.01462594
[150/200] Training loss: 0.01326106
[200/200] Training loss: 0.01303646
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14870.09724245272 ----------
[1/150] Training loss: 0.18756713
[2/150] Training loss: 0.06351916
[3/150] Training loss: 0.05681245
[4/150] Training loss: 0.05099275
[5/150] Training loss: 0.04972745
[6/150] Training loss: 0.05032091
[7/150] Training loss: 0.04457410
[8/150] Training loss: 0.04546707
[9/150] Training loss: 0.04319781
[10/150] Training loss: 0.03949567
[50/150] Training loss: 0.01903295
[100/150] Training loss: 0.01530291
[150/150] Training loss: 0.01410913
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 150
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13441.28267688765 ----------
[1/200] Training loss: 0.19105874
[2/200] Training loss: 0.06474015
[3/200] Training loss: 0.05576181
[4/200] Training loss: 0.05061743
[5/200] Training loss: 0.04572773
[6/200] Training loss: 0.04776451
[7/200] Training loss: 0.04545225
[8/200] Training loss: 0.04333015
[9/200] Training loss: 0.04108454
[10/200] Training loss: 0.04025332
[50/200] Training loss: 0.02521045
[100/200] Training loss: 0.01803639
[150/200] Training loss: 0.01600274
[200/200] Training loss: 0.01400458
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9519.137355874218 ----------
[1/200] Training loss: 0.15001981
[2/200] Training loss: 0.05718829
[3/200] Training loss: 0.04892015
[4/200] Training loss: 0.04962496
[5/200] Training loss: 0.04191033
[6/200] Training loss: 0.04203205
[7/200] Training loss: 0.03768792
[8/200] Training loss: 0.03319049
[9/200] Training loss: 0.03082187
[10/200] Training loss: 0.03102453
[50/200] Training loss: 0.01813232
[100/200] Training loss: 0.01585871
[150/200] Training loss: 0.01335189
[200/200] Training loss: 0.01253650
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10178.988161895071 ----------
[1/200] Training loss: 0.14651762
[2/200] Training loss: 0.05848297
[3/200] Training loss: 0.05012169
[4/200] Training loss: 0.04683502
[5/200] Training loss: 0.04524206
[6/200] Training loss: 0.04312725
[7/200] Training loss: 0.03886569
[8/200] Training loss: 0.03685427
[9/200] Training loss: 0.03685914
[10/200] Training loss: 0.03737364
[50/200] Training loss: 0.01726608
[100/200] Training loss: 0.01488616
[150/200] Training loss: 0.01363963
[200/200] Training loss: 0.01232873
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16019.547559154098 ----------
[1/200] Training loss: 0.16474528
[2/200] Training loss: 0.06094088
[3/200] Training loss: 0.05677806
[4/200] Training loss: 0.05341521
[5/200] Training loss: 0.05003730
[6/200] Training loss: 0.04436823
[7/200] Training loss: 0.04496856
[8/200] Training loss: 0.03919432
[9/200] Training loss: 0.03711674
[10/200] Training loss: 0.03255450
[50/200] Training loss: 0.01762366
[100/200] Training loss: 0.01526182
[150/200] Training loss: 0.01399608
[200/200] Training loss: 0.01268290
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23622.905494456012 ----------
[1/200] Training loss: 0.14205817
[2/200] Training loss: 0.05433736
[3/200] Training loss: 0.05646480
[4/200] Training loss: 0.04677490
[5/200] Training loss: 0.04511621
[6/200] Training loss: 0.04191610
[7/200] Training loss: 0.03874439
[8/200] Training loss: 0.03653636
[9/200] Training loss: 0.03449178
[10/200] Training loss: 0.03253955
[50/200] Training loss: 0.01680737
[100/200] Training loss: 0.01489205
[150/200] Training loss: 0.01342460
[200/200] Training loss: 0.01184853
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11754.50756093168 ----------
[1/200] Training loss: 0.16928470
[2/200] Training loss: 0.06126488
[3/200] Training loss: 0.05128529
[4/200] Training loss: 0.04940048
[5/200] Training loss: 0.04934561
[6/200] Training loss: 0.04370222
[7/200] Training loss: 0.04024196
[8/200] Training loss: 0.04026797
[9/200] Training loss: 0.03786968
[10/200] Training loss: 0.03220280
[50/200] Training loss: 0.01793528
[100/200] Training loss: 0.01503356
[150/200] Training loss: 0.01344790
[200/200] Training loss: 0.01303552
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17603.358770416515 ----------
[1/200] Training loss: 0.16529770
[2/200] Training loss: 0.05769603
[3/200] Training loss: 0.05290589
[4/200] Training loss: 0.04757756
[5/200] Training loss: 0.04325898
[6/200] Training loss: 0.04266093
[7/200] Training loss: 0.03850188
[8/200] Training loss: 0.03551755
[9/200] Training loss: 0.03709352
[10/200] Training loss: 0.03100993
[50/200] Training loss: 0.01681807
[100/200] Training loss: 0.01515487
[150/200] Training loss: 0.01344651
[200/200] Training loss: 0.01174133
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8125.665265072147 ----------
[1/200] Training loss: 0.16909572
[2/200] Training loss: 0.06177190
[3/200] Training loss: 0.05380482
[4/200] Training loss: 0.05503930
[5/200] Training loss: 0.05133573
[6/200] Training loss: 0.04809347
[7/200] Training loss: 0.04613239
[8/200] Training loss: 0.04600551
[9/200] Training loss: 0.03995842
[10/200] Training loss: 0.03901038
[50/200] Training loss: 0.01832359
[100/200] Training loss: 0.01623082
[150/200] Training loss: 0.01573492
[200/200] Training loss: 0.01376538
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17136.15872942358 ----------
[1/200] Training loss: 0.17083151
[2/200] Training loss: 0.05543394
[3/200] Training loss: 0.05159606
[4/200] Training loss: 0.04369780
[5/200] Training loss: 0.04019329
[6/200] Training loss: 0.03761867
[7/200] Training loss: 0.03618024
[8/200] Training loss: 0.03363079
[9/200] Training loss: 0.03250648
[10/200] Training loss: 0.02960593
[50/200] Training loss: 0.01739242
[100/200] Training loss: 0.01515478
[150/200] Training loss: 0.01343985
[200/200] Training loss: 0.01391964
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5887.096398055666 ----------
[1/200] Training loss: 0.19399989
[2/200] Training loss: 0.06049936
[3/200] Training loss: 0.05502828
[4/200] Training loss: 0.05303642
[5/200] Training loss: 0.04775232
[6/200] Training loss: 0.04683025
[7/200] Training loss: 0.04137300
[8/200] Training loss: 0.04212124
[9/200] Training loss: 0.03942362
[10/200] Training loss: 0.04067762
[50/200] Training loss: 0.01985204
[100/200] Training loss: 0.01538149
[150/200] Training loss: 0.01373085
[200/200] Training loss: 0.01255425
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 3824.427931076751 ----------
[1/200] Training loss: 0.18958543
[2/200] Training loss: 0.06212382
[3/200] Training loss: 0.05386732
[4/200] Training loss: 0.04976890
[5/200] Training loss: 0.04955121
[6/200] Training loss: 0.04573672
[7/200] Training loss: 0.04192173
[8/200] Training loss: 0.04148499
[9/200] Training loss: 0.03861680
[10/200] Training loss: 0.03530016
[50/200] Training loss: 0.01817314
[100/200] Training loss: 0.01579341
[150/200] Training loss: 0.01491310
[200/200] Training loss: 0.01388319
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17026.33348669055 ----------
[1/200] Training loss: 0.17812557
[2/200] Training loss: 0.05715275
[3/200] Training loss: 0.04555987
[4/200] Training loss: 0.04586058
[5/200] Training loss: 0.04239416
[6/200] Training loss: 0.03536059
[7/200] Training loss: 0.03512982
[8/200] Training loss: 0.03357775
[9/200] Training loss: 0.03171731
[10/200] Training loss: 0.03162172
[50/200] Training loss: 0.01837276
[100/200] Training loss: 0.01586005
[150/200] Training loss: 0.01427284
[200/200] Training loss: 0.01345180
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12672.852244068816 ----------
[1/200] Training loss: 0.16670030
[2/200] Training loss: 0.07013327
[3/200] Training loss: 0.05871298
[4/200] Training loss: 0.05439332
[5/200] Training loss: 0.05140384
[6/200] Training loss: 0.05030165
[7/200] Training loss: 0.04820650
[8/200] Training loss: 0.04741198
[9/200] Training loss: 0.04552806
[10/200] Training loss: 0.04384486
[50/200] Training loss: 0.01823081
[100/200] Training loss: 0.01524000
[150/200] Training loss: 0.01432747
[200/200] Training loss: 0.01334266
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18018.063824950783 ----------
[1/200] Training loss: 0.16516470
[2/200] Training loss: 0.06399578
[3/200] Training loss: 0.05316073
[4/200] Training loss: 0.04691276
[5/200] Training loss: 0.04285551
[6/200] Training loss: 0.04156028
[7/200] Training loss: 0.03779115
[8/200] Training loss: 0.03196227
[9/200] Training loss: 0.03142629
[10/200] Training loss: 0.03337354
[50/200] Training loss: 0.01960281
[100/200] Training loss: 0.01709412
[150/200] Training loss: 0.01581605
[200/200] Training loss: 0.01476599
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12193.44479628296 ----------
[1/200] Training loss: 0.15591862
[2/200] Training loss: 0.06264366
[3/200] Training loss: 0.05275898
[4/200] Training loss: 0.04974031
[5/200] Training loss: 0.04885642
[6/200] Training loss: 0.04383586
[7/200] Training loss: 0.03932078
[8/200] Training loss: 0.03476935
[9/200] Training loss: 0.03265148
[10/200] Training loss: 0.03281936
[50/200] Training loss: 0.01769258
[100/200] Training loss: 0.01433451
[150/200] Training loss: 0.01344332
[200/200] Training loss: 0.01227998
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16553.983448100942 ----------
[1/200] Training loss: 0.17524468
[2/200] Training loss: 0.06577747
[3/200] Training loss: 0.04837191
[4/200] Training loss: 0.04577608
[5/200] Training loss: 0.04192218
[6/200] Training loss: 0.03876572
[7/200] Training loss: 0.03577805
[8/200] Training loss: 0.03389916
[9/200] Training loss: 0.03292833
[10/200] Training loss: 0.03256632
[50/200] Training loss: 0.01910571
[100/200] Training loss: 0.01609164
[150/200] Training loss: 0.01540048
[200/200] Training loss: 0.01378458
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11813.340933029911 ----------
[1/200] Training loss: 0.15472626
[2/200] Training loss: 0.05953520
[3/200] Training loss: 0.05244248
[4/200] Training loss: 0.05080957
[5/200] Training loss: 0.04730510
[6/200] Training loss: 0.04547983
[7/200] Training loss: 0.04412785
[8/200] Training loss: 0.04321892
[9/200] Training loss: 0.04109977
[10/200] Training loss: 0.04006163
[50/200] Training loss: 0.01764258
[100/200] Training loss: 0.01577454
[150/200] Training loss: 0.01423181
[200/200] Training loss: 0.01321585
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12462.568595598581 ----------
[1/200] Training loss: 0.17575272
[2/200] Training loss: 0.05823236
[3/200] Training loss: 0.05457609
[4/200] Training loss: 0.05278761
[5/200] Training loss: 0.04954381
[6/200] Training loss: 0.04518195
[7/200] Training loss: 0.04615624
[8/200] Training loss: 0.04528747
[9/200] Training loss: 0.04083475
[10/200] Training loss: 0.04089796
[50/200] Training loss: 0.01974124
[100/200] Training loss: 0.01677097
[150/200] Training loss: 0.01478087
[200/200] Training loss: 0.01252443
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9340.617538471426 ----------
[1/200] Training loss: 0.15515216
[2/200] Training loss: 0.06172060
[3/200] Training loss: 0.05271135
[4/200] Training loss: 0.04759461
[5/200] Training loss: 0.04290418
[6/200] Training loss: 0.03704699
[7/200] Training loss: 0.03679094
[8/200] Training loss: 0.03433317
[9/200] Training loss: 0.03151377
[10/200] Training loss: 0.02723095
[50/200] Training loss: 0.01842193
[100/200] Training loss: 0.01669768
[150/200] Training loss: 0.01516339
[200/200] Training loss: 0.01447589
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13691.116535914813 ----------
[1/200] Training loss: 0.15239416
[2/200] Training loss: 0.05623446
[3/200] Training loss: 0.05183918
[4/200] Training loss: 0.04763373
[5/200] Training loss: 0.04327574
[6/200] Training loss: 0.04419965
[7/200] Training loss: 0.03953485
[8/200] Training loss: 0.03818411
[9/200] Training loss: 0.03455788
[10/200] Training loss: 0.03263709
[50/200] Training loss: 0.01810345
[100/200] Training loss: 0.01632824
[150/200] Training loss: 0.01567594
[200/200] Training loss: 0.01372459
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16231.903893259103 ----------
[1/200] Training loss: 0.16595016
[2/200] Training loss: 0.05793548
[3/200] Training loss: 0.05121474
[4/200] Training loss: 0.04719057
[5/200] Training loss: 0.04560009
[6/200] Training loss: 0.04240844
[7/200] Training loss: 0.03877898
[8/200] Training loss: 0.03495498
[9/200] Training loss: 0.03542306
[10/200] Training loss: 0.03436966
[50/200] Training loss: 0.01852887
[100/200] Training loss: 0.01627614
[150/200] Training loss: 0.01476609
[200/200] Training loss: 0.01330819
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16209.588273611394 ----------
[1/200] Training loss: 0.11928471
[2/200] Training loss: 0.05354401
[3/200] Training loss: 0.05078010
[4/200] Training loss: 0.04915988
[5/200] Training loss: 0.04793143
[6/200] Training loss: 0.04616893
[7/200] Training loss: 0.04489642
[8/200] Training loss: 0.04343022
[9/200] Training loss: 0.04391289
[10/200] Training loss: 0.04095767
[50/200] Training loss: 0.02500986
[100/200] Training loss: 0.02119130
[150/200] Training loss: 0.01907327
[200/200] Training loss: 0.01690776
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.005189123818832173
----FITNESS-----------RMSE---- 10200.634097937245 ----------
[1/200] Training loss: 0.17620405
[2/200] Training loss: 0.05696460
[3/200] Training loss: 0.05500512
[4/200] Training loss: 0.05168149
[5/200] Training loss: 0.04711058
[6/200] Training loss: 0.04762157
[7/200] Training loss: 0.04683117
[8/200] Training loss: 0.04353317
[9/200] Training loss: 0.03994244
[10/200] Training loss: 0.03946389
[50/200] Training loss: 0.01769722
[100/200] Training loss: 0.01545883
[150/200] Training loss: 0.01469382
[200/200] Training loss: 0.01327689
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9025.66961504796 ----------
[1/200] Training loss: 0.17442377
[2/200] Training loss: 0.05765880
[3/200] Training loss: 0.05335228
[4/200] Training loss: 0.04883682
[5/200] Training loss: 0.04500614
[6/200] Training loss: 0.04413615
[7/200] Training loss: 0.04170821
[8/200] Training loss: 0.03788847
[9/200] Training loss: 0.03107370
[10/200] Training loss: 0.03307392
[50/200] Training loss: 0.01853416
[100/200] Training loss: 0.01630626
[150/200] Training loss: 0.01503368
[200/200] Training loss: 0.01327905
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8051.051856745179 ----------
[1/200] Training loss: 0.13348716
[2/200] Training loss: 0.05567581
[3/200] Training loss: 0.05151472
[4/200] Training loss: 0.04589465
[5/200] Training loss: 0.04383941
[6/200] Training loss: 0.04380194
[7/200] Training loss: 0.04062122
[8/200] Training loss: 0.03710362
[9/200] Training loss: 0.03195460
[10/200] Training loss: 0.03394180
[50/200] Training loss: 0.01750236
[100/200] Training loss: 0.01418364
[150/200] Training loss: 0.01327982
[200/200] Training loss: 0.01211779
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10424.947002263369 ----------
[1/200] Training loss: 0.19025508
[2/200] Training loss: 0.05442676
[3/200] Training loss: 0.05148467
[4/200] Training loss: 0.05187659
[5/200] Training loss: 0.04641588
[6/200] Training loss: 0.04535508
[7/200] Training loss: 0.04518085
[8/200] Training loss: 0.03831176
[9/200] Training loss: 0.03628420
[10/200] Training loss: 0.03433096
[50/200] Training loss: 0.01908556
[100/200] Training loss: 0.01555123
[150/200] Training loss: 0.01364088
[200/200] Training loss: 0.01266440
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7760.801247293994 ----------
[1/200] Training loss: 0.15560210
[2/200] Training loss: 0.05130189
[3/200] Training loss: 0.04878161
[4/200] Training loss: 0.04221013
[5/200] Training loss: 0.04079668
[6/200] Training loss: 0.03649315
[7/200] Training loss: 0.03584374
[8/200] Training loss: 0.03419852
[9/200] Training loss: 0.03513784
[10/200] Training loss: 0.03032402
[50/200] Training loss: 0.01831074
[100/200] Training loss: 0.01514509
[150/200] Training loss: 0.01428792
[200/200] Training loss: 0.01302433
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14660.825624772979 ----------
[1/200] Training loss: 0.19265328
[2/200] Training loss: 0.06408857
[3/200] Training loss: 0.05589157
[4/200] Training loss: 0.05329134
[5/200] Training loss: 0.05277198
[6/200] Training loss: 0.05101728
[7/200] Training loss: 0.04711278
[8/200] Training loss: 0.04173191
[9/200] Training loss: 0.04203872
[10/200] Training loss: 0.04092601
[50/200] Training loss: 0.02160522
[100/200] Training loss: 0.01613641
[150/200] Training loss: 0.01393050
[200/200] Training loss: 0.01369305
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10614.308079191973 ----------
[1/200] Training loss: 0.16884658
[2/200] Training loss: 0.05618746
[3/200] Training loss: 0.04783712
[4/200] Training loss: 0.04800601
[5/200] Training loss: 0.04102628
[6/200] Training loss: 0.03616192
[7/200] Training loss: 0.03411716
[8/200] Training loss: 0.03394680
[9/200] Training loss: 0.03079892
[10/200] Training loss: 0.02810916
[50/200] Training loss: 0.01789376
[100/200] Training loss: 0.01527046
[150/200] Training loss: 0.01373830
[200/200] Training loss: 0.01290519
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17200.805562531077 ----------
[1/200] Training loss: 0.12634497
[2/200] Training loss: 0.05684410
[3/200] Training loss: 0.04935347
[4/200] Training loss: 0.04544866
[5/200] Training loss: 0.04145666
[6/200] Training loss: 0.04099527
[7/200] Training loss: 0.03730720
[8/200] Training loss: 0.03604009
[9/200] Training loss: 0.03194668
[10/200] Training loss: 0.03019986
[50/200] Training loss: 0.01651310
[100/200] Training loss: 0.01437336
[150/200] Training loss: 0.01296063
[200/200] Training loss: 0.01152219
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22126.135857849196 ----------
[1/100] Training loss: 0.19226318
[2/100] Training loss: 0.05932386
[3/100] Training loss: 0.05482398
[4/100] Training loss: 0.05583167
[5/100] Training loss: 0.04736015
[6/100] Training loss: 0.04875742
[7/100] Training loss: 0.04660098
[8/100] Training loss: 0.04281617
[9/100] Training loss: 0.04155923
[10/100] Training loss: 0.04064876
[50/100] Training loss: 0.01956747
[100/100] Training loss: 0.01555239
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12537.37452579287 ----------
[1/200] Training loss: 0.14482076
[2/200] Training loss: 0.06082210
[3/200] Training loss: 0.05046554
[4/200] Training loss: 0.04562681
[5/200] Training loss: 0.04236761
[6/200] Training loss: 0.04070314
[7/200] Training loss: 0.03694348
[8/200] Training loss: 0.03616628
[9/200] Training loss: 0.03324852
[10/200] Training loss: 0.03303967
[50/200] Training loss: 0.01881131
[100/200] Training loss: 0.01579524
[150/200] Training loss: 0.01316354
[200/200] Training loss: 0.01185900
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7844.5211453599895 ----------
[1/200] Training loss: 0.14049932
[2/200] Training loss: 0.05581614
[3/200] Training loss: 0.05020972
[4/200] Training loss: 0.04480595
[5/200] Training loss: 0.04113415
[6/200] Training loss: 0.03998083
[7/200] Training loss: 0.03853091
[8/200] Training loss: 0.03690160
[9/200] Training loss: 0.03578209
[10/200] Training loss: 0.03011538
[50/200] Training loss: 0.01847393
[100/200] Training loss: 0.01503986
[150/200] Training loss: 0.01359073
[200/200] Training loss: 0.01274281
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8556.5703409719 ----------
[1/200] Training loss: 0.16165243
[2/200] Training loss: 0.06337204
[3/200] Training loss: 0.05414728
[4/200] Training loss: 0.04870550
[5/200] Training loss: 0.04689272
[6/200] Training loss: 0.04482250
[7/200] Training loss: 0.04088014
[8/200] Training loss: 0.04020579
[9/200] Training loss: 0.03457929
[10/200] Training loss: 0.03256495
[50/200] Training loss: 0.01709847
[100/200] Training loss: 0.01456458
[150/200] Training loss: 0.01392249
[200/200] Training loss: 0.01264417
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 27893.102229762826 ----------
[1/200] Training loss: 0.15983896
[2/200] Training loss: 0.06035877
[3/200] Training loss: 0.05429442
[4/200] Training loss: 0.05126356
[5/200] Training loss: 0.04821763
[6/200] Training loss: 0.04363209
[7/200] Training loss: 0.04302811
[8/200] Training loss: 0.03809116
[9/200] Training loss: 0.03768328
[10/200] Training loss: 0.03362708
[50/200] Training loss: 0.01540451
[100/200] Training loss: 0.01296013
[150/200] Training loss: 0.01159965
[200/200] Training loss: 0.01076835
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14346.32886838999 ----------
[1/200] Training loss: 0.17220151
[2/200] Training loss: 0.06292748
[3/200] Training loss: 0.05866206
[4/200] Training loss: 0.05082813
[5/200] Training loss: 0.04958463
[6/200] Training loss: 0.04795976
[7/200] Training loss: 0.04527634
[8/200] Training loss: 0.04223563
[9/200] Training loss: 0.04064167
[10/200] Training loss: 0.03733832
[50/200] Training loss: 0.01699679
[100/200] Training loss: 0.01532073
[150/200] Training loss: 0.01337497
[200/200] Training loss: 0.01259731
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6335.55806539566 ----------
[1/200] Training loss: 0.17061576
[2/200] Training loss: 0.06557234
[3/200] Training loss: 0.05472021
[4/200] Training loss: 0.05158213
[5/200] Training loss: 0.04819198
[6/200] Training loss: 0.04502374
[7/200] Training loss: 0.04440188
[8/200] Training loss: 0.04243448
[9/200] Training loss: 0.03737389
[10/200] Training loss: 0.03265790
[50/200] Training loss: 0.01834134
[100/200] Training loss: 0.01519964
[150/200] Training loss: 0.01389122
[200/200] Training loss: 0.01260618
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15327.042767605237 ----------
[1/200] Training loss: 0.15236003
[2/200] Training loss: 0.05695572
[3/200] Training loss: 0.05231458
[4/200] Training loss: 0.04415868
[5/200] Training loss: 0.04199862
[6/200] Training loss: 0.03953704
[7/200] Training loss: 0.03628958
[8/200] Training loss: 0.03404059
[9/200] Training loss: 0.03080180
[10/200] Training loss: 0.02722907
[50/200] Training loss: 0.01702956
[100/200] Training loss: 0.01491258
[150/200] Training loss: 0.01321042
[200/200] Training loss: 0.01220808
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8497.9752882672 ----------
[1/200] Training loss: 0.22082515
[2/200] Training loss: 0.07046669
[3/200] Training loss: 0.06120819
[4/200] Training loss: 0.05591919
[5/200] Training loss: 0.05411580
[6/200] Training loss: 0.05308909
[7/200] Training loss: 0.05096090
[8/200] Training loss: 0.04925116
[9/200] Training loss: 0.04940740
[10/200] Training loss: 0.04850628
[50/200] Training loss: 0.03439788
[100/200] Training loss: 0.02531667
[150/200] Training loss: 0.02238255
[200/200] Training loss: 0.02080073
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: SGD ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8797.850191950303 ----------
[1/200] Training loss: 0.15301262
[2/200] Training loss: 0.05628248
[3/200] Training loss: 0.05126477
[4/200] Training loss: 0.04676518
[5/200] Training loss: 0.04717783
[6/200] Training loss: 0.03881015
[7/200] Training loss: 0.03779594
[8/200] Training loss: 0.03702410
[9/200] Training loss: 0.03515163
[10/200] Training loss: 0.03386488
[50/200] Training loss: 0.02052660
[100/200] Training loss: 0.01459312
[150/200] Training loss: 0.01291064
[200/200] Training loss: 0.01155524
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9627.380536781538 ----------
[1/200] Training loss: 0.17053433
[2/200] Training loss: 0.05607904
[3/200] Training loss: 0.05311638
[4/200] Training loss: 0.05309171
[5/200] Training loss: 0.04806644
[6/200] Training loss: 0.04663072
[7/200] Training loss: 0.04151279
[8/200] Training loss: 0.03878651
[9/200] Training loss: 0.03543778
[10/200] Training loss: 0.03708786
[50/200] Training loss: 0.01847414
[100/200] Training loss: 0.01586343
[150/200] Training loss: 0.01487137
[200/200] Training loss: 0.01345101
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19145.85615740388 ----------
[1/200] Training loss: 0.15284713
[2/200] Training loss: 0.05829392
[3/200] Training loss: 0.04967966
[4/200] Training loss: 0.04406611
[5/200] Training loss: 0.04331994
[6/200] Training loss: 0.04023793
[7/200] Training loss: 0.03654413
[8/200] Training loss: 0.03638262
[9/200] Training loss: 0.03286277
[10/200] Training loss: 0.03030491
[50/200] Training loss: 0.01699063
[100/200] Training loss: 0.01526331
[150/200] Training loss: 0.01428888
[200/200] Training loss: 0.01266650
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9788.811163772647 ----------
[1/200] Training loss: 0.14647699
[2/200] Training loss: 0.05709799
[3/200] Training loss: 0.05023345
[4/200] Training loss: 0.04622624
[5/200] Training loss: 0.04393402
[6/200] Training loss: 0.04129070
[7/200] Training loss: 0.04171054
[8/200] Training loss: 0.03528240
[9/200] Training loss: 0.03362186
[10/200] Training loss: 0.03586819
[50/200] Training loss: 0.01705621
[100/200] Training loss: 0.01487313
[150/200] Training loss: 0.01351323
[200/200] Training loss: 0.01244946
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19386.73773485369 ----------
[1/200] Training loss: 0.15687714
[2/200] Training loss: 0.05545346
[3/200] Training loss: 0.05223442
[4/200] Training loss: 0.04540324
[5/200] Training loss: 0.04348679
[6/200] Training loss: 0.03963144
[7/200] Training loss: 0.03868447
[8/200] Training loss: 0.03647023
[9/200] Training loss: 0.03573490
[10/200] Training loss: 0.03170834
[50/200] Training loss: 0.01791028
[100/200] Training loss: 0.01536199
[150/200] Training loss: 0.01395498
[200/200] Training loss: 0.01289320
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14865.181869052258 ----------
[1/200] Training loss: 0.15880849
[2/200] Training loss: 0.06003184
[3/200] Training loss: 0.05342858
[4/200] Training loss: 0.05034798
[5/200] Training loss: 0.04126032
[6/200] Training loss: 0.03721602
[7/200] Training loss: 0.03341833
[8/200] Training loss: 0.03310887
[9/200] Training loss: 0.02753247
[10/200] Training loss: 0.02754387
[50/200] Training loss: 0.01629600
[100/200] Training loss: 0.01366132
[150/200] Training loss: 0.01241015
[200/200] Training loss: 0.01176072
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18375.636914131712 ----------
[1/200] Training loss: 0.14722102
[2/200] Training loss: 0.05887923
[3/200] Training loss: 0.05232590
[4/200] Training loss: 0.04812459
[5/200] Training loss: 0.04568546
[6/200] Training loss: 0.04313647
[7/200] Training loss: 0.04133637
[8/200] Training loss: 0.03708160
[9/200] Training loss: 0.03595370
[10/200] Training loss: 0.03287931
[50/200] Training loss: 0.01720778
[100/200] Training loss: 0.01446647
[150/200] Training loss: 0.01263330
[200/200] Training loss: 0.01185709
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22174.103093473703 ----------
[1/200] Training loss: 0.16964498
[2/200] Training loss: 0.05870961
[3/200] Training loss: 0.05214989
[4/200] Training loss: 0.04843668
[5/200] Training loss: 0.04562993
[6/200] Training loss: 0.04308600
[7/200] Training loss: 0.03883019
[8/200] Training loss: 0.03656158
[9/200] Training loss: 0.03547205
[10/200] Training loss: 0.03251029
[50/200] Training loss: 0.01724421
[100/200] Training loss: 0.01531243
[150/200] Training loss: 0.01414929
[200/200] Training loss: 0.01257054
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5863.400378619901 ----------
[1/200] Training loss: 0.16552097
[2/200] Training loss: 0.06111661
[3/200] Training loss: 0.05157402
[4/200] Training loss: 0.05116285
[5/200] Training loss: 0.04638022
[6/200] Training loss: 0.04352217
[7/200] Training loss: 0.04181586
[8/200] Training loss: 0.03865882
[9/200] Training loss: 0.03482955
[10/200] Training loss: 0.03624520
[50/200] Training loss: 0.01778002
[100/200] Training loss: 0.01465058
[150/200] Training loss: 0.01228740
[200/200] Training loss: 0.01129234
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.022364818889473392
----FITNESS-----------RMSE---- 8721.383376506275 ----------
[1/200] Training loss: 0.15907676
[2/200] Training loss: 0.06387864
[3/200] Training loss: 0.05269668
[4/200] Training loss: 0.05072380
[5/200] Training loss: 0.04912252
[6/200] Training loss: 0.04959352
[7/200] Training loss: 0.04511821
[8/200] Training loss: 0.04499480
[9/200] Training loss: 0.04166789
[10/200] Training loss: 0.04165801
[50/200] Training loss: 0.01744920
[100/200] Training loss: 0.01540076
[150/200] Training loss: 0.01385412
[200/200] Training loss: 0.01320293
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9163.137017419307 ----------
[1/200] Training loss: 0.14785102
[2/200] Training loss: 0.05781972
[3/200] Training loss: 0.05061924
[4/200] Training loss: 0.04547827
[5/200] Training loss: 0.04068971
[6/200] Training loss: 0.03879308
[7/200] Training loss: 0.03774306
[8/200] Training loss: 0.03702074
[9/200] Training loss: 0.03125160
[10/200] Training loss: 0.03177287
[50/200] Training loss: 0.01725561
[100/200] Training loss: 0.01521748
[150/200] Training loss: 0.01348886
[200/200] Training loss: 0.01292175
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18009.67384490902 ----------
[1/200] Training loss: 0.15938624
[2/200] Training loss: 0.05811239
[3/200] Training loss: 0.04500973
[4/200] Training loss: 0.04257987
[5/200] Training loss: 0.03586665
[6/200] Training loss: 0.03530446
[7/200] Training loss: 0.03094882
[8/200] Training loss: 0.03059340
[9/200] Training loss: 0.02786518
[10/200] Training loss: 0.02700282
[50/200] Training loss: 0.01844534
[100/200] Training loss: 0.01595800
[150/200] Training loss: 0.01480297
[200/200] Training loss: 0.01333172
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20918.592686889813 ----------
[1/200] Training loss: 0.16598514
[2/200] Training loss: 0.05398600
[3/200] Training loss: 0.05229547
[4/200] Training loss: 0.04378152
[5/200] Training loss: 0.04153904
[6/200] Training loss: 0.04075538
[7/200] Training loss: 0.03745955
[8/200] Training loss: 0.03529808
[9/200] Training loss: 0.03120188
[10/200] Training loss: 0.03317128
[50/200] Training loss: 0.01787732
[100/200] Training loss: 0.01376947
[150/200] Training loss: 0.01376348
[200/200] Training loss: 0.01211188
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8297.16722743371 ----------
[1/200] Training loss: 0.16351341
[2/200] Training loss: 0.06175959
[3/200] Training loss: 0.05414321
[4/200] Training loss: 0.04778048
[5/200] Training loss: 0.04327059
[6/200] Training loss: 0.04280359
[7/200] Training loss: 0.03671950
[8/200] Training loss: 0.03512932
[9/200] Training loss: 0.03396397
[10/200] Training loss: 0.03383848
[50/200] Training loss: 0.01599831
[100/200] Training loss: 0.01501254
[150/200] Training loss: 0.01333814
[200/200] Training loss: 0.01142021
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10697.69395711057 ----------
[1/200] Training loss: 0.16888885
[2/200] Training loss: 0.05744885
[3/200] Training loss: 0.05527005
[4/200] Training loss: 0.04970228
[5/200] Training loss: 0.04883619
[6/200] Training loss: 0.04253564
[7/200] Training loss: 0.04193064
[8/200] Training loss: 0.04031889
[9/200] Training loss: 0.03908944
[10/200] Training loss: 0.03592884
[50/200] Training loss: 0.01750939
[100/200] Training loss: 0.01511369
[150/200] Training loss: 0.01359643
[200/200] Training loss: 0.01179441
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9547.870966870049 ----------
[1/200] Training loss: 0.17602976
[2/200] Training loss: 0.06347509
[3/200] Training loss: 0.05111143
[4/200] Training loss: 0.04798359
[5/200] Training loss: 0.04444640
[6/200] Training loss: 0.04580830
[7/200] Training loss: 0.03745013
[8/200] Training loss: 0.03843938
[9/200] Training loss: 0.03504572
[10/200] Training loss: 0.03357710
[50/200] Training loss: 0.01761530
[100/200] Training loss: 0.01510066
[150/200] Training loss: 0.01415934
[200/200] Training loss: 0.01237070
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9304.999946265449 ----------
[1/200] Training loss: 0.15025265
[2/200] Training loss: 0.05741504
[3/200] Training loss: 0.05289438
[4/200] Training loss: 0.04963060
[5/200] Training loss: 0.04534414
[6/200] Training loss: 0.03985616
[7/200] Training loss: 0.03718181
[8/200] Training loss: 0.03633855
[9/200] Training loss: 0.03277210
[10/200] Training loss: 0.03196530
[50/200] Training loss: 0.01718394
[100/200] Training loss: 0.01438294
[150/200] Training loss: 0.01285520
[200/200] Training loss: 0.01116133
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19851.036446493166 ----------
[1/200] Training loss: 0.15404153
[2/200] Training loss: 0.05722783
[3/200] Training loss: 0.05253982
[4/200] Training loss: 0.05142177
[5/200] Training loss: 0.04723481
[6/200] Training loss: 0.04508504
[7/200] Training loss: 0.04266135
[8/200] Training loss: 0.03796498
[9/200] Training loss: 0.03579821
[10/200] Training loss: 0.03442917
[50/200] Training loss: 0.01762321
[100/200] Training loss: 0.01476611
[150/200] Training loss: 0.01306995
[200/200] Training loss: 0.01264549
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16901.57483786644 ----------
[1/200] Training loss: 0.15606083
[2/200] Training loss: 0.06075167
[3/200] Training loss: 0.05043650
[4/200] Training loss: 0.04432393
[5/200] Training loss: 0.04500263
[6/200] Training loss: 0.04106355
[7/200] Training loss: 0.03791202
[8/200] Training loss: 0.03622353
[9/200] Training loss: 0.03529128
[10/200] Training loss: 0.03403041
[50/200] Training loss: 0.01744367
[100/200] Training loss: 0.01498082
[150/200] Training loss: 0.01315708
[200/200] Training loss: 0.01314856
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5292.453495308201 ----------
[1/200] Training loss: 0.15885177
[2/200] Training loss: 0.06046872
[3/200] Training loss: 0.04971240
[4/200] Training loss: 0.04614102
[5/200] Training loss: 0.04637406
[6/200] Training loss: 0.04380048
[7/200] Training loss: 0.04271934
[8/200] Training loss: 0.03644899
[9/200] Training loss: 0.03740330
[10/200] Training loss: 0.03564937
[50/200] Training loss: 0.01887046
[100/200] Training loss: 0.01669956
[150/200] Training loss: 0.01562256
[200/200] Training loss: 0.01464313
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11922.395061396011 ----------
[1/200] Training loss: 0.13264384
[2/200] Training loss: 0.05856496
[3/200] Training loss: 0.05023623
[4/200] Training loss: 0.04548658
[5/200] Training loss: 0.04388777
[6/200] Training loss: 0.04048520
[7/200] Training loss: 0.04212009
[8/200] Training loss: 0.03447832
[9/200] Training loss: 0.03726319
[10/200] Training loss: 0.03434999
[50/200] Training loss: 0.01685073
[100/200] Training loss: 0.01373495
[150/200] Training loss: 0.01185938
[200/200] Training loss: 0.01197031
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15226.415467863735 ----------
[1/200] Training loss: 0.15979415
[2/200] Training loss: 0.05735002
[3/200] Training loss: 0.05351958
[4/200] Training loss: 0.04918123
[5/200] Training loss: 0.04743877
[6/200] Training loss: 0.04433143
[7/200] Training loss: 0.03928672
[8/200] Training loss: 0.03881844
[9/200] Training loss: 0.03518788
[10/200] Training loss: 0.03194812
[50/200] Training loss: 0.01888206
[100/200] Training loss: 0.01477694
[150/200] Training loss: 0.01313755
[200/200] Training loss: 0.01298176
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10576.139560349986 ----------
[1/200] Training loss: 0.16637469
[2/200] Training loss: 0.05798685
[3/200] Training loss: 0.05331511
[4/200] Training loss: 0.04648618
[5/200] Training loss: 0.04445881
[6/200] Training loss: 0.04189699
[7/200] Training loss: 0.03699622
[8/200] Training loss: 0.03679661
[9/200] Training loss: 0.03019931
[10/200] Training loss: 0.03181333
[50/200] Training loss: 0.01877041
[100/200] Training loss: 0.01512890
[150/200] Training loss: 0.01387586
[200/200] Training loss: 0.01212848
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21782.524371614967 ----------
[1/200] Training loss: 0.12990348
[2/200] Training loss: 0.05208229
[3/200] Training loss: 0.04903329
[4/200] Training loss: 0.04464741
[5/200] Training loss: 0.04505446
[6/200] Training loss: 0.04085187
[7/200] Training loss: 0.03489591
[8/200] Training loss: 0.03531725
[9/200] Training loss: 0.03102813
[10/200] Training loss: 0.03145699
[50/200] Training loss: 0.01743908
[100/200] Training loss: 0.01497223
[150/200] Training loss: 0.01348918
[200/200] Training loss: 0.01198101
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 3580.0851945170243 ----------
[1/200] Training loss: 0.18758487
[2/200] Training loss: 0.06066446
[3/200] Training loss: 0.05196571
[4/200] Training loss: 0.04426335
[5/200] Training loss: 0.04426638
[6/200] Training loss: 0.04240011
[7/200] Training loss: 0.03868616
[8/200] Training loss: 0.03726767
[9/200] Training loss: 0.03411192
[10/200] Training loss: 0.03432859
[50/200] Training loss: 0.01876692
[100/200] Training loss: 0.01579265
[150/200] Training loss: 0.01363581
[200/200] Training loss: 0.01289248
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4570.366943692815 ----------
[1/200] Training loss: 0.16287968
[2/200] Training loss: 0.05212439
[3/200] Training loss: 0.04733240
[4/200] Training loss: 0.04361874
[5/200] Training loss: 0.04176100
[6/200] Training loss: 0.03474407
[7/200] Training loss: 0.03306816
[8/200] Training loss: 0.03322773
[9/200] Training loss: 0.03220322
[10/200] Training loss: 0.03061986
[50/200] Training loss: 0.01795543
[100/200] Training loss: 0.01355565
[150/200] Training loss: 0.01217161
[200/200] Training loss: 0.01175424
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10573.961225576722 ----------
[1/200] Training loss: 0.19137079
[2/200] Training loss: 0.07060322
[3/200] Training loss: 0.05871974
[4/200] Training loss: 0.04808076
[5/200] Training loss: 0.04652403
[6/200] Training loss: 0.04669811
[7/200] Training loss: 0.04489010
[8/200] Training loss: 0.04333939
[9/200] Training loss: 0.04065679
[10/200] Training loss: 0.04452890
[50/200] Training loss: 0.03180588
[100/200] Training loss: 0.02452189
[150/200] Training loss: 0.02189095
[200/200] Training loss: 0.02346458
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: SGD ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15435.42160098 ----------
[1/200] Training loss: 0.15500376
[2/200] Training loss: 0.06596558
[3/200] Training loss: 0.05511841
[4/200] Training loss: 0.04951794
[5/200] Training loss: 0.04839099
[6/200] Training loss: 0.04488975
[7/200] Training loss: 0.04010399
[8/200] Training loss: 0.03768383
[9/200] Training loss: 0.03344409
[10/200] Training loss: 0.03504248
[50/200] Training loss: 0.01731421
[100/200] Training loss: 0.01392597
[150/200] Training loss: 0.01297285
[200/200] Training loss: 0.01196876
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 24764.35373677254 ----------
[1/200] Training loss: 0.15027104
[2/200] Training loss: 0.06453109
[3/200] Training loss: 0.05436795
[4/200] Training loss: 0.05194124
[5/200] Training loss: 0.05009394
[6/200] Training loss: 0.04579107
[7/200] Training loss: 0.04436017
[8/200] Training loss: 0.04512255
[9/200] Training loss: 0.04018833
[10/200] Training loss: 0.03842829
[50/200] Training loss: 0.01791054
[100/200] Training loss: 0.01511123
[150/200] Training loss: 0.01334297
[200/200] Training loss: 0.01225600
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19065.044872750757 ----------
[1/200] Training loss: 0.17882442
[2/200] Training loss: 0.06356311
[3/200] Training loss: 0.05801958
[4/200] Training loss: 0.05252058
[5/200] Training loss: 0.05392366
[6/200] Training loss: 0.05012342
[7/200] Training loss: 0.05102535
[8/200] Training loss: 0.04849332
[9/200] Training loss: 0.04701644
[10/200] Training loss: 0.04744955
[50/200] Training loss: 0.01817249
[100/200] Training loss: 0.01657848
[150/200] Training loss: 0.01445314
[200/200] Training loss: 0.01373416
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16614.483320284144 ----------
[1/200] Training loss: 0.13726101
[2/200] Training loss: 0.05466128
[3/200] Training loss: 0.05421333
[4/200] Training loss: 0.04812534
[5/200] Training loss: 0.04456289
[6/200] Training loss: 0.04172226
[7/200] Training loss: 0.04234861
[8/200] Training loss: 0.03830528
[9/200] Training loss: 0.03457673
[10/200] Training loss: 0.03441690
[50/200] Training loss: 0.01830518
[100/200] Training loss: 0.01391994
[150/200] Training loss: 0.01237100
[200/200] Training loss: 0.01159970
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13943.935169097711 ----------
[1/200] Training loss: 0.16224138
[2/200] Training loss: 0.05527347
[3/200] Training loss: 0.05016050
[4/200] Training loss: 0.05011230
[5/200] Training loss: 0.04537849
[6/200] Training loss: 0.04244099
[7/200] Training loss: 0.04198041
[8/200] Training loss: 0.03967968
[9/200] Training loss: 0.03786761
[10/200] Training loss: 0.03729565
[50/200] Training loss: 0.01871762
[100/200] Training loss: 0.01599692
[150/200] Training loss: 0.01516527
[200/200] Training loss: 0.01444144
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13655.51492987357 ----------
[1/200] Training loss: 0.15151477
[2/200] Training loss: 0.05872053
[3/200] Training loss: 0.05052186
[4/200] Training loss: 0.04896742
[5/200] Training loss: 0.04129641
[6/200] Training loss: 0.04065201
[7/200] Training loss: 0.03947199
[8/200] Training loss: 0.03651457
[9/200] Training loss: 0.03424256
[10/200] Training loss: 0.03075079
[50/200] Training loss: 0.01605599
[100/200] Training loss: 0.01379235
[150/200] Training loss: 0.01237645
[200/200] Training loss: 0.01135659
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 24735.921086549417 ----------
[1/200] Training loss: 0.12436031
[2/200] Training loss: 0.05172790
[3/200] Training loss: 0.04396906
[4/200] Training loss: 0.04324098
[5/200] Training loss: 0.03989700
[6/200] Training loss: 0.03719280
[7/200] Training loss: 0.03412811
[8/200] Training loss: 0.03414974
[9/200] Training loss: 0.03110396
[10/200] Training loss: 0.03075335
[50/200] Training loss: 0.01681260
[100/200] Training loss: 0.01504058
[150/200] Training loss: 0.01370280
[200/200] Training loss: 0.01287915
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15926.848275788905 ----------
[1/200] Training loss: 0.13543023
[2/200] Training loss: 0.05632302
[3/200] Training loss: 0.05023890
[4/200] Training loss: 0.04726780
[5/200] Training loss: 0.04537296
[6/200] Training loss: 0.04237988
[7/200] Training loss: 0.03912292
[8/200] Training loss: 0.03611551
[9/200] Training loss: 0.03343284
[10/200] Training loss: 0.03663127
[50/200] Training loss: 0.01808048
[100/200] Training loss: 0.01524787
[150/200] Training loss: 0.01341343
[200/200] Training loss: 0.01271611
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17286.536726597376 ----------
[1/200] Training loss: 0.21554333
[2/200] Training loss: 0.07513060
[3/200] Training loss: 0.06463277
[4/200] Training loss: 0.05669262
[5/200] Training loss: 0.05375644
[6/200] Training loss: 0.05128069
[7/200] Training loss: 0.05016328
[8/200] Training loss: 0.04826616
[9/200] Training loss: 0.04751804
[10/200] Training loss: 0.04711024
[50/200] Training loss: 0.03039983
[100/200] Training loss: 0.02546447
[150/200] Training loss: 0.02485511
[200/200] Training loss: 0.02227924
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: SGD ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11726.035306104106 ----------
[1/200] Training loss: 0.14449259
[2/200] Training loss: 0.05324514
[3/200] Training loss: 0.04873456
[4/200] Training loss: 0.04285457
[5/200] Training loss: 0.04382082
[6/200] Training loss: 0.04038267
[7/200] Training loss: 0.03674540
[8/200] Training loss: 0.03841339
[9/200] Training loss: 0.03358615
[10/200] Training loss: 0.03473605
[50/200] Training loss: 0.01854562
[100/200] Training loss: 0.01594291
[150/200] Training loss: 0.01390910
[200/200] Training loss: 0.01155653
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15991.067006300736 ----------
[1/200] Training loss: 0.24817890
[2/200] Training loss: 0.08266804
[3/200] Training loss: 0.08053561
[4/200] Training loss: 0.07626998
[5/200] Training loss: 0.06939776
[6/200] Training loss: 0.06036152
[7/200] Training loss: 0.05462305
[8/200] Training loss: 0.05240932
[9/200] Training loss: 0.05173584
[10/200] Training loss: 0.04946631
[50/200] Training loss: 0.03419297
[100/200] Training loss: 0.02873586
[150/200] Training loss: 0.02580666
[200/200] Training loss: 0.02206426
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: SGD ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13484.969410421367 ----------
[1/200] Training loss: 0.16411281
[2/200] Training loss: 0.05502921
[3/200] Training loss: 0.05003650
[4/200] Training loss: 0.04358861
[5/200] Training loss: 0.04170105
[6/200] Training loss: 0.04047170
[7/200] Training loss: 0.04006203
[8/200] Training loss: 0.03514704
[9/200] Training loss: 0.03299205
[10/200] Training loss: 0.03363232
[50/200] Training loss: 0.01770158
[100/200] Training loss: 0.01597796
[150/200] Training loss: 0.01442322
[200/200] Training loss: 0.01345385
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8206.989216515396 ----------
[1/200] Training loss: 0.16096567
[2/200] Training loss: 0.05822303
[3/200] Training loss: 0.05163152
[4/200] Training loss: 0.05107354
[5/200] Training loss: 0.04550737
[6/200] Training loss: 0.04518915
[7/200] Training loss: 0.04390364
[8/200] Training loss: 0.04117110
[9/200] Training loss: 0.03822332
[10/200] Training loss: 0.03613947
[50/200] Training loss: 0.01921848
[100/200] Training loss: 0.01570915
[150/200] Training loss: 0.01416016
[200/200] Training loss: 0.01269248
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22469.630348539336 ----------
[1/200] Training loss: 0.19371630
[2/200] Training loss: 0.06839882
[3/200] Training loss: 0.05274242
[4/200] Training loss: 0.04986194
[5/200] Training loss: 0.04635205
[6/200] Training loss: 0.04385631
[7/200] Training loss: 0.04387621
[8/200] Training loss: 0.04010323
[9/200] Training loss: 0.03737789
[10/200] Training loss: 0.03695726
[50/200] Training loss: 0.01991602
[100/200] Training loss: 0.01552947
[150/200] Training loss: 0.01355829
[200/200] Training loss: 0.01327821
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18045.478547270504 ----------
[1/200] Training loss: 0.15005312
[2/200] Training loss: 0.05841217
[3/200] Training loss: 0.05064107
[4/200] Training loss: 0.04337814
[5/200] Training loss: 0.03987291
[6/200] Training loss: 0.03808539
[7/200] Training loss: 0.03512318
[8/200] Training loss: 0.03528495
[9/200] Training loss: 0.02898325
[10/200] Training loss: 0.03050959
[50/200] Training loss: 0.01645286
[100/200] Training loss: 0.01471062
[150/200] Training loss: 0.01426984
[200/200] Training loss: 0.01339893
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9052.160847002222 ----------
[1/200] Training loss: 0.19410911
[2/200] Training loss: 0.06502453
[3/200] Training loss: 0.05645036
[4/200] Training loss: 0.05123582
[5/200] Training loss: 0.05137514
[6/200] Training loss: 0.04745776
[7/200] Training loss: 0.04682923
[8/200] Training loss: 0.04169050
[9/200] Training loss: 0.03993051
[10/200] Training loss: 0.03967972
[50/200] Training loss: 0.02077742
[100/200] Training loss: 0.01632469
[150/200] Training loss: 0.01379464
[200/200] Training loss: 0.01352082
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9012.404784517837 ----------
[1/200] Training loss: 0.20299043
[2/200] Training loss: 0.06833627
[3/200] Training loss: 0.05611110
[4/200] Training loss: 0.05110231
[5/200] Training loss: 0.05465023
[6/200] Training loss: 0.05091107
[7/200] Training loss: 0.04748501
[8/200] Training loss: 0.04449322
[9/200] Training loss: 0.04229738
[10/200] Training loss: 0.03877556
[50/200] Training loss: 0.02111699
[100/200] Training loss: 0.01632085
[150/200] Training loss: 0.01396128
[200/200] Training loss: 0.01375646
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14505.793601178806 ----------
[1/200] Training loss: 0.16134511
[2/200] Training loss: 0.05487329
[3/200] Training loss: 0.04835694
[4/200] Training loss: 0.04733523
[5/200] Training loss: 0.04246440
[6/200] Training loss: 0.03763354
[7/200] Training loss: 0.03635421
[8/200] Training loss: 0.03427792
[9/200] Training loss: 0.03148066
[10/200] Training loss: 0.02997408
[50/200] Training loss: 0.01751643
[100/200] Training loss: 0.01585425
[150/200] Training loss: 0.01369786
[200/200] Training loss: 0.01301048
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22527.53337584921 ----------
[1/200] Training loss: 0.15554706
[2/200] Training loss: 0.05776670
[3/200] Training loss: 0.05356271
[4/200] Training loss: 0.04803235
[5/200] Training loss: 0.04831354
[6/200] Training loss: 0.04632488
[7/200] Training loss: 0.04109126
[8/200] Training loss: 0.04028229
[9/200] Training loss: 0.03796854
[10/200] Training loss: 0.03461396
[50/200] Training loss: 0.01775553
[100/200] Training loss: 0.01459335
[150/200] Training loss: 0.01275570
[200/200] Training loss: 0.01243207
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.022364818889473392
----FITNESS-----------RMSE---- 16079.670643393167 ----------
[1/200] Training loss: 0.17444613
[2/200] Training loss: 0.06428313
[3/200] Training loss: 0.05575040
[4/200] Training loss: 0.05549166
[5/200] Training loss: 0.05098368
[6/200] Training loss: 0.04967077
[7/200] Training loss: 0.04952058
[8/200] Training loss: 0.04348505
[9/200] Training loss: 0.04157896
[10/200] Training loss: 0.04015008
[50/200] Training loss: 0.01764975
[100/200] Training loss: 0.01571731
[150/200] Training loss: 0.01406089
[200/200] Training loss: 0.01269615
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10921.342042075232 ----------
[1/200] Training loss: 0.15988109
[2/200] Training loss: 0.05576745
[3/200] Training loss: 0.05146310
[4/200] Training loss: 0.04410849
[5/200] Training loss: 0.04389407
[6/200] Training loss: 0.03933208
[7/200] Training loss: 0.03822944
[8/200] Training loss: 0.03627528
[9/200] Training loss: 0.03336852
[10/200] Training loss: 0.03029645
[50/200] Training loss: 0.01730380
[100/200] Training loss: 0.01452766
[150/200] Training loss: 0.01375215
[200/200] Training loss: 0.01270127
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14572.981026543608 ----------
[1/200] Training loss: 0.15831110
[2/200] Training loss: 0.06374153
[3/200] Training loss: 0.05556289
[4/200] Training loss: 0.05177175
[5/200] Training loss: 0.05052333
[6/200] Training loss: 0.04876829
[7/200] Training loss: 0.04860366
[8/200] Training loss: 0.04454243
[9/200] Training loss: 0.04500206
[10/200] Training loss: 0.04215037
[50/200] Training loss: 0.01859593
[100/200] Training loss: 0.01472504
[150/200] Training loss: 0.01361885
[200/200] Training loss: 0.01292415
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15110.899377601587 ----------
[1/200] Training loss: 0.16643196
[2/200] Training loss: 0.06013531
[3/200] Training loss: 0.05475021
[4/200] Training loss: 0.05124336
[5/200] Training loss: 0.05042958
[6/200] Training loss: 0.04877930
[7/200] Training loss: 0.04646189
[8/200] Training loss: 0.04295325
[9/200] Training loss: 0.04025375
[10/200] Training loss: 0.03739964
[50/200] Training loss: 0.01755332
[100/200] Training loss: 0.01542393
[150/200] Training loss: 0.01395449
[200/200] Training loss: 0.01318758
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15318.371192786784 ----------
[1/200] Training loss: 0.16749848
[2/200] Training loss: 0.05910677
[3/200] Training loss: 0.05505783
[4/200] Training loss: 0.04887717
[5/200] Training loss: 0.04615091
[6/200] Training loss: 0.04363040
[7/200] Training loss: 0.04192704
[8/200] Training loss: 0.03821091
[9/200] Training loss: 0.03821917
[10/200] Training loss: 0.03770785
[50/200] Training loss: 0.01876481
[100/200] Training loss: 0.01511037
[150/200] Training loss: 0.01293280
[200/200] Training loss: 0.01276446
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11808.602288162643 ----------
[1/200] Training loss: 0.17857928
[2/200] Training loss: 0.06404460
[3/200] Training loss: 0.05089204
[4/200] Training loss: 0.04955688
[5/200] Training loss: 0.04689626
[6/200] Training loss: 0.04417848
[7/200] Training loss: 0.03851091
[8/200] Training loss: 0.03751779
[9/200] Training loss: 0.03759572
[10/200] Training loss: 0.03513819
[50/200] Training loss: 0.01893817
[100/200] Training loss: 0.01363643
[150/200] Training loss: 0.01296471
[200/200] Training loss: 0.01178392
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9974.176256714136 ----------
[1/200] Training loss: 0.16131330
[2/200] Training loss: 0.06034630
[3/200] Training loss: 0.05262441
[4/200] Training loss: 0.04654369
[5/200] Training loss: 0.04577503
[6/200] Training loss: 0.04330813
[7/200] Training loss: 0.03924556
[8/200] Training loss: 0.03704597
[9/200] Training loss: 0.03777477
[10/200] Training loss: 0.03598103
[50/200] Training loss: 0.01685405
[100/200] Training loss: 0.01448048
[150/200] Training loss: 0.01249568
[200/200] Training loss: 0.01177583
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14117.819661689973 ----------
[1/200] Training loss: 0.15588732
[2/200] Training loss: 0.05193900
[3/200] Training loss: 0.04802971
[4/200] Training loss: 0.04265827
[5/200] Training loss: 0.03852136
[6/200] Training loss: 0.03715289
[7/200] Training loss: 0.03450658
[8/200] Training loss: 0.03164447
[9/200] Training loss: 0.02961017
[10/200] Training loss: 0.02785328
[50/200] Training loss: 0.01720946
[100/200] Training loss: 0.01514297
[150/200] Training loss: 0.01413385
[200/200] Training loss: 0.01341878
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14706.82399432318 ----------
[1/200] Training loss: 0.14994958
[2/200] Training loss: 0.06488196
[3/200] Training loss: 0.05745747
[4/200] Training loss: 0.05267181
[5/200] Training loss: 0.04938551
[6/200] Training loss: 0.04354901
[7/200] Training loss: 0.04214441
[8/200] Training loss: 0.03961852
[9/200] Training loss: 0.03560922
[10/200] Training loss: 0.03511795
[50/200] Training loss: 0.01859274
[100/200] Training loss: 0.01432159
[150/200] Training loss: 0.01250067
[200/200] Training loss: 0.01213881
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 32813.376784476175 ----------
[1/200] Training loss: 0.16686289
[2/200] Training loss: 0.06137348
[3/200] Training loss: 0.04923188
[4/200] Training loss: 0.04540673
[5/200] Training loss: 0.04198318
[6/200] Training loss: 0.03911005
[7/200] Training loss: 0.03822669
[8/200] Training loss: 0.03509976
[9/200] Training loss: 0.03425669
[10/200] Training loss: 0.03393411
[50/200] Training loss: 0.01740797
[100/200] Training loss: 0.01489741
[150/200] Training loss: 0.01308995
[200/200] Training loss: 0.01280357
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15451.151413406058 ----------
[1/200] Training loss: 0.13990921
[2/200] Training loss: 0.05531359
[3/200] Training loss: 0.05464809
[4/200] Training loss: 0.04747214
[5/200] Training loss: 0.04579400
[6/200] Training loss: 0.04026504
[7/200] Training loss: 0.03819348
[8/200] Training loss: 0.03180307
[9/200] Training loss: 0.03440690
[10/200] Training loss: 0.03352246
[50/200] Training loss: 0.01675347
[100/200] Training loss: 0.01331857
[150/200] Training loss: 0.01239902
[200/200] Training loss: 0.01151898
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 29617.564518373216 ----------
[1/200] Training loss: 0.15141332
[2/200] Training loss: 0.05749961
[3/200] Training loss: 0.04886170
[4/200] Training loss: 0.04460662
[5/200] Training loss: 0.04118341
[6/200] Training loss: 0.04030385
[7/200] Training loss: 0.03935230
[8/200] Training loss: 0.03684839
[9/200] Training loss: 0.03273490
[10/200] Training loss: 0.03379065
[50/200] Training loss: 0.01748627
[100/200] Training loss: 0.01489360
[150/200] Training loss: 0.01367560
[200/200] Training loss: 0.01271506
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6614.003931054169 ----------
[1/200] Training loss: 0.16464648
[2/200] Training loss: 0.05510931
[3/200] Training loss: 0.04792439
[4/200] Training loss: 0.04612899
[5/200] Training loss: 0.04299207
[6/200] Training loss: 0.04116677
[7/200] Training loss: 0.03649929
[8/200] Training loss: 0.03709947
[9/200] Training loss: 0.03373849
[10/200] Training loss: 0.03449024
[50/200] Training loss: 0.01833248
[100/200] Training loss: 0.01574482
[150/200] Training loss: 0.01419498
[200/200] Training loss: 0.01318800
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.026666497286617918
----FITNESS-----------RMSE---- 14028.760173301132 ----------
[1/200] Training loss: 0.04255316
[2/200] Training loss: 0.00372705
[3/200] Training loss: 0.00351207
[4/200] Training loss: 0.00287112
[5/200] Training loss: 0.00226541
[6/200] Training loss: 0.00253095
[7/200] Training loss: 0.00163758
[8/200] Training loss: 0.00157253
[9/200] Training loss: 0.00154483
[10/200] Training loss: 0.00120994
[50/200] Training loss: 0.00047548
[100/200] Training loss: 0.00033224
[150/200] Training loss: 0.00023480
[200/200] Training loss: 0.00023657
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: MSELoss ---opt---: Adagrad ---learning_rate---: 0.022364818889473392
----FITNESS-----------RMSE---- 16163.797573590187 ----------
[1/200] Training loss: 0.14726679
[2/200] Training loss: 0.05723362
[3/200] Training loss: 0.05021169
[4/200] Training loss: 0.04797906
[5/200] Training loss: 0.04685559
[6/200] Training loss: 0.04316839
[7/200] Training loss: 0.04096828
[8/200] Training loss: 0.03893746
[9/200] Training loss: 0.03520819
[10/200] Training loss: 0.03403998
[50/200] Training loss: 0.01835676
[100/200] Training loss: 0.01540150
[150/200] Training loss: 0.01372331
[200/200] Training loss: 0.01302498
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.022364818889473392
----FITNESS-----------RMSE---- 4617.158866662485 ----------
[1/200] Training loss: 0.13253175
[2/200] Training loss: 0.05053802
[3/200] Training loss: 0.04858503
[4/200] Training loss: 0.04423879
[5/200] Training loss: 0.03912772
[6/200] Training loss: 0.03806322
[7/200] Training loss: 0.03815380
[8/200] Training loss: 0.03681657
[9/200] Training loss: 0.03516943
[10/200] Training loss: 0.03342259
[50/200] Training loss: 0.01730830
[100/200] Training loss: 0.01374527
[150/200] Training loss: 0.01201411
[200/200] Training loss: 0.01149757
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8767.253161623656 ----------
[1/200] Training loss: 0.14529040
[2/200] Training loss: 0.05612882
[3/200] Training loss: 0.05420688
[4/200] Training loss: 0.05135013
[5/200] Training loss: 0.04442991
[6/200] Training loss: 0.04540413
[7/200] Training loss: 0.04260343
[8/200] Training loss: 0.03973130
[9/200] Training loss: 0.03900150
[10/200] Training loss: 0.03881092
[50/200] Training loss: 0.01867549
[100/200] Training loss: 0.01485507
[150/200] Training loss: 0.01352149
[200/200] Training loss: 0.01209945
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20039.62953749395 ----------
[1/200] Training loss: 0.16987887
[2/200] Training loss: 0.05597298
[3/200] Training loss: 0.05474825
[4/200] Training loss: 0.04616892
[5/200] Training loss: 0.04353237
[6/200] Training loss: 0.04075289
[7/200] Training loss: 0.03976085
[8/200] Training loss: 0.03520892
[9/200] Training loss: 0.03499515
[10/200] Training loss: 0.03074117
[50/200] Training loss: 0.01811340
[100/200] Training loss: 0.01539203
[150/200] Training loss: 0.01437386
[200/200] Training loss: 0.01345738
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12574.313500147831 ----------
[1/200] Training loss: 0.15640694
[2/200] Training loss: 0.05679428
[3/200] Training loss: 0.04720413
[4/200] Training loss: 0.04359132
[5/200] Training loss: 0.04023234
[6/200] Training loss: 0.03811621
[7/200] Training loss: 0.03196775
[8/200] Training loss: 0.03496841
[9/200] Training loss: 0.03189322
[10/200] Training loss: 0.02971669
[50/200] Training loss: 0.01787049
[100/200] Training loss: 0.01567175
[150/200] Training loss: 0.01474822
[200/200] Training loss: 0.01361168
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8516.121182792082 ----------
[1/200] Training loss: 0.15006471
[2/200] Training loss: 0.06006353
[3/200] Training loss: 0.05495344
[4/200] Training loss: 0.05205092
[5/200] Training loss: 0.05095472
[6/200] Training loss: 0.04635655
[7/200] Training loss: 0.04468956
[8/200] Training loss: 0.04399500
[9/200] Training loss: 0.03962440
[10/200] Training loss: 0.03795796
[50/200] Training loss: 0.01879873
[100/200] Training loss: 0.01536716
[150/200] Training loss: 0.01309390
[200/200] Training loss: 0.01317145
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.022364818889473392
----FITNESS-----------RMSE---- 14836.164463903735 ----------
[1/200] Training loss: 0.14680085
[2/200] Training loss: 0.05966696
[3/200] Training loss: 0.05100197
[4/200] Training loss: 0.04611113
[5/200] Training loss: 0.04258366
[6/200] Training loss: 0.04036635
[7/200] Training loss: 0.04198721
[8/200] Training loss: 0.03668231
[9/200] Training loss: 0.03319570
[10/200] Training loss: 0.03412231
[50/200] Training loss: 0.01912819
[100/200] Training loss: 0.01511064
[150/200] Training loss: 0.01302144
[200/200] Training loss: 0.01194017
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7773.118807788802 ----------
[1/200] Training loss: 0.14939375
[2/200] Training loss: 0.05951756
[3/200] Training loss: 0.04925346
[4/200] Training loss: 0.04357242
[5/200] Training loss: 0.03859699
[6/200] Training loss: 0.03792839
[7/200] Training loss: 0.03445231
[8/200] Training loss: 0.03581095
[9/200] Training loss: 0.03205650
[10/200] Training loss: 0.02919149
[50/200] Training loss: 0.01845467
[100/200] Training loss: 0.01398606
[150/200] Training loss: 0.01358371
[200/200] Training loss: 0.01172953
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6637.923771782861 ----------
[1/200] Training loss: 0.13675893
[2/200] Training loss: 0.06079109
[3/200] Training loss: 0.05134464
[4/200] Training loss: 0.05079408
[5/200] Training loss: 0.04400692
[6/200] Training loss: 0.04350602
[7/200] Training loss: 0.04359680
[8/200] Training loss: 0.03769021
[9/200] Training loss: 0.03439407
[10/200] Training loss: 0.03535288
[50/200] Training loss: 0.01844200
[100/200] Training loss: 0.01516415
[150/200] Training loss: 0.01413962
[200/200] Training loss: 0.01351985
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11757.333711347994 ----------
[1/200] Training loss: 0.16040593
[2/200] Training loss: 0.05987446
[3/200] Training loss: 0.05482302
[4/200] Training loss: 0.04967398
[5/200] Training loss: 0.04518171
[6/200] Training loss: 0.04023822
[7/200] Training loss: 0.03814230
[8/200] Training loss: 0.03569498
[9/200] Training loss: 0.03448229
[10/200] Training loss: 0.03292251
[50/200] Training loss: 0.01792628
[100/200] Training loss: 0.01476975
[150/200] Training loss: 0.01379247
[200/200] Training loss: 0.01283244
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22856.31921373168 ----------
[1/200] Training loss: 0.13941114
[2/200] Training loss: 0.05795172
[3/200] Training loss: 0.05049274
[4/200] Training loss: 0.04554439
[5/200] Training loss: 0.04434201
[6/200] Training loss: 0.04387346
[7/200] Training loss: 0.04065562
[8/200] Training loss: 0.03816275
[9/200] Training loss: 0.03796102
[10/200] Training loss: 0.03402743
[50/200] Training loss: 0.01850454
[100/200] Training loss: 0.01506046
[150/200] Training loss: 0.01403679
[200/200] Training loss: 0.01236929
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9183.653303560626 ----------
[1/200] Training loss: 0.18784412
[2/200] Training loss: 0.06015436
[3/200] Training loss: 0.05548924
[4/200] Training loss: 0.05291397
[5/200] Training loss: 0.04691967
[6/200] Training loss: 0.04504019
[7/200] Training loss: 0.03885308
[8/200] Training loss: 0.03878721
[9/200] Training loss: 0.03798104
[10/200] Training loss: 0.03669719
[50/200] Training loss: 0.01830703
[100/200] Training loss: 0.01642186
[150/200] Training loss: 0.01413235
[200/200] Training loss: 0.01378020
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11908.002015451626 ----------
[1/200] Training loss: 0.18138308
[2/200] Training loss: 0.06256730
[3/200] Training loss: 0.05627402
[4/200] Training loss: 0.05386129
[5/200] Training loss: 0.05354011
[6/200] Training loss: 0.04930132
[7/200] Training loss: 0.04985795
[8/200] Training loss: 0.04350903
[9/200] Training loss: 0.04310088
[10/200] Training loss: 0.03730974
[50/200] Training loss: 0.02024010
[100/200] Training loss: 0.01714803
[150/200] Training loss: 0.01549506
[200/200] Training loss: 0.01366519
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17609.82362205823 ----------
[1/200] Training loss: 0.15426675
[2/200] Training loss: 0.05644236
[3/200] Training loss: 0.05055725
[4/200] Training loss: 0.04872557
[5/200] Training loss: 0.04124983
[6/200] Training loss: 0.03997787
[7/200] Training loss: 0.03693478
[8/200] Training loss: 0.03302512
[9/200] Training loss: 0.03209121
[10/200] Training loss: 0.02987803
[50/200] Training loss: 0.01684668
[100/200] Training loss: 0.01487797
[150/200] Training loss: 0.01399067
[200/200] Training loss: 0.01269622
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10519.080187925178 ----------
[1/200] Training loss: 0.16369052
[2/200] Training loss: 0.05886934
[3/200] Training loss: 0.05295905
[4/200] Training loss: 0.04626158
[5/200] Training loss: 0.04503824
[6/200] Training loss: 0.03847010
[7/200] Training loss: 0.04093390
[8/200] Training loss: 0.03873576
[9/200] Training loss: 0.04429268
[10/200] Training loss: 0.03602192
[50/200] Training loss: 0.01904368
[100/200] Training loss: 0.01585805
[150/200] Training loss: 0.01446499
[200/200] Training loss: 0.01301950
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13775.051070685728 ----------
[1/200] Training loss: 0.13716464
[2/200] Training loss: 0.06135144
[3/200] Training loss: 0.05648250
[4/200] Training loss: 0.05097589
[5/200] Training loss: 0.04700339
[6/200] Training loss: 0.04183893
[7/200] Training loss: 0.03933936
[8/200] Training loss: 0.03258911
[9/200] Training loss: 0.03240989
[10/200] Training loss: 0.03029357
[50/200] Training loss: 0.01683648
[100/200] Training loss: 0.01346860
[150/200] Training loss: 0.01158725
[200/200] Training loss: 0.01044638
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.022364818889473392
----FITNESS-----------RMSE---- 26842.093510007748 ----------
[1/200] Training loss: 0.15322377
[2/200] Training loss: 0.05773750
[3/200] Training loss: 0.05045456
[4/200] Training loss: 0.04604417
[5/200] Training loss: 0.04240562
[6/200] Training loss: 0.04056343
[7/200] Training loss: 0.03826149
[8/200] Training loss: 0.03642632
[9/200] Training loss: 0.03579591
[10/200] Training loss: 0.03336160
[50/200] Training loss: 0.01866224
[100/200] Training loss: 0.01557205
[150/200] Training loss: 0.01283391
[200/200] Training loss: 0.01187055
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22416.655950431144 ----------
[1/200] Training loss: 0.19070981
[2/200] Training loss: 0.06253529
[3/200] Training loss: 0.05398368
[4/200] Training loss: 0.04703423
[5/200] Training loss: 0.05045539
[6/200] Training loss: 0.04364031
[7/200] Training loss: 0.04177816
[8/200] Training loss: 0.04130276
[9/200] Training loss: 0.04089327
[10/200] Training loss: 0.03516496
[50/200] Training loss: 0.01816819
[100/200] Training loss: 0.01588046
[150/200] Training loss: 0.01443326
[200/200] Training loss: 0.01383332
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14986.201653521148 ----------
[1/200] Training loss: 0.17701604
[2/200] Training loss: 0.06773312
[3/200] Training loss: 0.05284932
[4/200] Training loss: 0.04949232
[5/200] Training loss: 0.04855070
[6/200] Training loss: 0.04574391
[7/200] Training loss: 0.04394991
[8/200] Training loss: 0.04090018
[9/200] Training loss: 0.03946455
[10/200] Training loss: 0.03738874
[50/200] Training loss: 0.01852401
[100/200] Training loss: 0.01607575
[150/200] Training loss: 0.01344168
[200/200] Training loss: 0.01294144
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12651.126748238672 ----------
[1/200] Training loss: 0.14515708
[2/200] Training loss: 0.05532623
[3/200] Training loss: 0.05437224
[4/200] Training loss: 0.04888576
[5/200] Training loss: 0.04371905
[6/200] Training loss: 0.04415115
[7/200] Training loss: 0.03769777
[8/200] Training loss: 0.03880789
[9/200] Training loss: 0.03372992
[10/200] Training loss: 0.03210580
[50/200] Training loss: 0.01780296
[100/200] Training loss: 0.01378911
[150/200] Training loss: 0.01206258
[200/200] Training loss: 0.01144914
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12521.477868047366 ----------
[1/200] Training loss: 0.17627188
[2/200] Training loss: 0.06186396
[3/200] Training loss: 0.05786828
[4/200] Training loss: 0.05001205
[5/200] Training loss: 0.04670971
[6/200] Training loss: 0.04478176
[7/200] Training loss: 0.04347231
[8/200] Training loss: 0.04262580
[9/200] Training loss: 0.04084650
[10/200] Training loss: 0.03615411
[50/200] Training loss: 0.01808552
[100/200] Training loss: 0.01638773
[150/200] Training loss: 0.01536447
[200/200] Training loss: 0.01451397
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12269.86552493547 ----------
[1/200] Training loss: 0.14570309
[2/200] Training loss: 0.05756813
[3/200] Training loss: 0.05330657
[4/200] Training loss: 0.05268960
[5/200] Training loss: 0.04610259
[6/200] Training loss: 0.04284763
[7/200] Training loss: 0.04126270
[8/200] Training loss: 0.03590241
[9/200] Training loss: 0.03622168
[10/200] Training loss: 0.03340223
[50/200] Training loss: 0.01803114
[100/200] Training loss: 0.01515190
[150/200] Training loss: 0.01305695
[200/200] Training loss: 0.01211513
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14269.33775618196 ----------
[1/200] Training loss: 0.17452083
[2/200] Training loss: 0.06291916
[3/200] Training loss: 0.05783917
[4/200] Training loss: 0.05738592
[5/200] Training loss: 0.05376961
[6/200] Training loss: 0.05258512
[7/200] Training loss: 0.05091068
[8/200] Training loss: 0.05086450
[9/200] Training loss: 0.04736417
[10/200] Training loss: 0.04601356
[50/200] Training loss: 0.02455810
[100/200] Training loss: 0.01722826
[150/200] Training loss: 0.01507355
[200/200] Training loss: 0.01431397
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 25954.51529117814 ----------
[1/200] Training loss: 0.15620982
[2/200] Training loss: 0.05806568
[3/200] Training loss: 0.04584257
[4/200] Training loss: 0.04411500
[5/200] Training loss: 0.04272924
[6/200] Training loss: 0.03837565
[7/200] Training loss: 0.03554147
[8/200] Training loss: 0.03306489
[9/200] Training loss: 0.03457416
[10/200] Training loss: 0.03301528
[50/200] Training loss: 0.01763958
[100/200] Training loss: 0.01480402
[150/200] Training loss: 0.01335649
[200/200] Training loss: 0.01249101
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4226.115947297234 ----------
[1/200] Training loss: 0.15521173
[2/200] Training loss: 0.06160744
[3/200] Training loss: 0.05564059
[4/200] Training loss: 0.04783691
[5/200] Training loss: 0.04545713
[6/200] Training loss: 0.04390008
[7/200] Training loss: 0.03803756
[8/200] Training loss: 0.03709965
[9/200] Training loss: 0.03697162
[10/200] Training loss: 0.03509550
[50/200] Training loss: 0.01763946
[100/200] Training loss: 0.01438203
[150/200] Training loss: 0.01290863
[200/200] Training loss: 0.01156291
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11210.461899493705 ----------
[1/200] Training loss: 0.15143392
[2/200] Training loss: 0.05850266
[3/200] Training loss: 0.04759160
[4/200] Training loss: 0.04792614
[5/200] Training loss: 0.04550860
[6/200] Training loss: 0.03952081
[7/200] Training loss: 0.03875478
[8/200] Training loss: 0.03360007
[9/200] Training loss: 0.03682111
[10/200] Training loss: 0.03560402
[50/200] Training loss: 0.01918783
[100/200] Training loss: 0.01448812
[150/200] Training loss: 0.01334967
[200/200] Training loss: 0.01281694
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9731.93834752358 ----------
[1/200] Training loss: 0.12908237
[2/200] Training loss: 0.03966927
[3/200] Training loss: 0.04504003
[4/200] Training loss: 0.03790856
[5/200] Training loss: 0.03375001
[6/200] Training loss: 0.03087834
[7/200] Training loss: 0.02899991
[8/200] Training loss: 0.02743442
[9/200] Training loss: 0.02745492
[10/200] Training loss: 0.02723161
[50/200] Training loss: 0.02598960
[100/200] Training loss: 0.02532453
[150/200] Training loss: 0.02472146
[200/200] Training loss: 0.02430398
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Rprop ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20486.88868520547 ----------
[1/200] Training loss: 0.17064636
[2/200] Training loss: 0.05985832
[3/200] Training loss: 0.05279495
[4/200] Training loss: 0.05058879
[5/200] Training loss: 0.04931996
[6/200] Training loss: 0.04551470
[7/200] Training loss: 0.04469968
[8/200] Training loss: 0.04310768
[9/200] Training loss: 0.04047545
[10/200] Training loss: 0.03933958
[50/200] Training loss: 0.01682221
[100/200] Training loss: 0.01453383
[150/200] Training loss: 0.01328947
[200/200] Training loss: 0.01235927
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9714.919659986901 ----------
[1/200] Training loss: 0.17252710
[2/200] Training loss: 0.06373447
[3/200] Training loss: 0.05362046
[4/200] Training loss: 0.05033342
[5/200] Training loss: 0.04694086
[6/200] Training loss: 0.04414748
[7/200] Training loss: 0.04063217
[8/200] Training loss: 0.03944896
[9/200] Training loss: 0.03777936
[10/200] Training loss: 0.03803720
[50/200] Training loss: 0.01823798
[100/200] Training loss: 0.01537944
[150/200] Training loss: 0.01360080
[200/200] Training loss: 0.01227891
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15314.472632121551 ----------
[1/200] Training loss: 0.15128360
[2/200] Training loss: 0.05869516
[3/200] Training loss: 0.05131411
[4/200] Training loss: 0.04722893
[5/200] Training loss: 0.04457324
[6/200] Training loss: 0.03896838
[7/200] Training loss: 0.04093781
[8/200] Training loss: 0.03820108
[9/200] Training loss: 0.03735569
[10/200] Training loss: 0.03501322
[50/200] Training loss: 0.01771698
[100/200] Training loss: 0.01424979
[150/200] Training loss: 0.01252918
[200/200] Training loss: 0.01183200
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11948.426507285383 ----------
[1/200] Training loss: 0.16715868
[2/200] Training loss: 0.05820914
[3/200] Training loss: 0.05207159
[4/200] Training loss: 0.04984462
[5/200] Training loss: 0.04376974
[6/200] Training loss: 0.03901441
[7/200] Training loss: 0.03574921
[8/200] Training loss: 0.03544180
[9/200] Training loss: 0.03166064
[10/200] Training loss: 0.03058372
[50/200] Training loss: 0.01771481
[100/200] Training loss: 0.01512683
[150/200] Training loss: 0.01406592
[200/200] Training loss: 0.01335382
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 32611.557215195964 ----------
[1/200] Training loss: 0.16790507
[2/200] Training loss: 0.05494720
[3/200] Training loss: 0.05197652
[4/200] Training loss: 0.04948183
[5/200] Training loss: 0.04530766
[6/200] Training loss: 0.04389203
[7/200] Training loss: 0.04222106
[8/200] Training loss: 0.03733355
[9/200] Training loss: 0.03658799
[10/200] Training loss: 0.03353199
[50/200] Training loss: 0.01800651
[100/200] Training loss: 0.01648507
[150/200] Training loss: 0.01503613
[200/200] Training loss: 0.01381561
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9984.687876944376 ----------
[1/200] Training loss: 0.16437263
[2/200] Training loss: 0.06374603
[3/200] Training loss: 0.05849522
[4/200] Training loss: 0.05175288
[5/200] Training loss: 0.04849667
[6/200] Training loss: 0.04573979
[7/200] Training loss: 0.04280802
[8/200] Training loss: 0.03900074
[9/200] Training loss: 0.03481854
[10/200] Training loss: 0.03233537
[50/200] Training loss: 0.01701669
[100/200] Training loss: 0.01453853
[150/200] Training loss: 0.01263568
[200/200] Training loss: 0.01170368
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16330.761647884032 ----------
[1/200] Training loss: 0.15055327
[2/200] Training loss: 0.06013986
[3/200] Training loss: 0.05271988
[4/200] Training loss: 0.05102425
[5/200] Training loss: 0.04891455
[6/200] Training loss: 0.04440261
[7/200] Training loss: 0.04216179
[8/200] Training loss: 0.03831250
[9/200] Training loss: 0.03643689
[10/200] Training loss: 0.03602290
[50/200] Training loss: 0.01775725
[100/200] Training loss: 0.01393049
[150/200] Training loss: 0.01220243
[200/200] Training loss: 0.01185370
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9044.850026396236 ----------
[1/200] Training loss: 0.16802074
[2/200] Training loss: 0.05927185
[3/200] Training loss: 0.05208006
[4/200] Training loss: 0.04625106
[5/200] Training loss: 0.04538828
[6/200] Training loss: 0.03982115
[7/200] Training loss: 0.03779569
[8/200] Training loss: 0.03924147
[9/200] Training loss: 0.03519864
[10/200] Training loss: 0.03302797
[50/200] Training loss: 0.01987575
[100/200] Training loss: 0.01702147
[150/200] Training loss: 0.01530230
[200/200] Training loss: 0.01411809
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19374.17993103192 ----------
[1/200] Training loss: 0.15124317
[2/200] Training loss: 0.05793468
[3/200] Training loss: 0.04934478
[4/200] Training loss: 0.04499810
[5/200] Training loss: 0.04225306
[6/200] Training loss: 0.03656081
[7/200] Training loss: 0.03666595
[8/200] Training loss: 0.03307255
[9/200] Training loss: 0.02896984
[10/200] Training loss: 0.03056493
[50/200] Training loss: 0.01725715
[100/200] Training loss: 0.01504776
[150/200] Training loss: 0.01450779
[200/200] Training loss: 0.01347240
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12822.801565960537 ----------
[1/200] Training loss: 0.17822592
[2/200] Training loss: 0.05892262
[3/200] Training loss: 0.05145122
[4/200] Training loss: 0.04859272
[5/200] Training loss: 0.04144515
[6/200] Training loss: 0.03920020
[7/200] Training loss: 0.03693101
[8/200] Training loss: 0.03720879
[9/200] Training loss: 0.03303630
[10/200] Training loss: 0.03236642
[50/200] Training loss: 0.01925399
[100/200] Training loss: 0.01698467
[150/200] Training loss: 0.01522314
[200/200] Training loss: 0.01491455
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17294.658597382026 ----------
[1/200] Training loss: 0.15349455
[2/200] Training loss: 0.05700172
[3/200] Training loss: 0.04855956
[4/200] Training loss: 0.04846954
[5/200] Training loss: 0.04254308
[6/200] Training loss: 0.04051855
[7/200] Training loss: 0.03733624
[8/200] Training loss: 0.03595484
[9/200] Training loss: 0.03136756
[10/200] Training loss: 0.03122060
[50/200] Training loss: 0.01922184
[100/200] Training loss: 0.01663162
[150/200] Training loss: 0.01482053
[200/200] Training loss: 0.01312616
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14401.728785114654 ----------
[1/200] Training loss: 0.17879425
[2/200] Training loss: 0.06086566
[3/200] Training loss: 0.05632736
[4/200] Training loss: 0.04882324
[5/200] Training loss: 0.04611559
[6/200] Training loss: 0.04146201
[7/200] Training loss: 0.03910163
[8/200] Training loss: 0.03612218
[9/200] Training loss: 0.03467175
[10/200] Training loss: 0.02959257
[50/200] Training loss: 0.01659151
[100/200] Training loss: 0.01409874
[150/200] Training loss: 0.01256231
[200/200] Training loss: 0.01162106
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15883.498355211297 ----------
[1/200] Training loss: 0.17883462
[2/200] Training loss: 0.06228474
[3/200] Training loss: 0.05819727
[4/200] Training loss: 0.05381912
[5/200] Training loss: 0.05162949
[6/200] Training loss: 0.04954783
[7/200] Training loss: 0.04813098
[8/200] Training loss: 0.04551707
[9/200] Training loss: 0.04518215
[10/200] Training loss: 0.04256493
[50/200] Training loss: 0.01979831
[100/200] Training loss: 0.01489755
[150/200] Training loss: 0.01410064
[200/200] Training loss: 0.01271706
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12840.456689697607 ----------
[1/200] Training loss: 0.17059398
[2/200] Training loss: 0.06286067
[3/200] Training loss: 0.05738688
[4/200] Training loss: 0.05336267
[5/200] Training loss: 0.04941578
[6/200] Training loss: 0.04719669
[7/200] Training loss: 0.04239912
[8/200] Training loss: 0.04106468
[9/200] Training loss: 0.03922168
[10/200] Training loss: 0.03381785
[50/200] Training loss: 0.01689379
[100/200] Training loss: 0.01507403
[150/200] Training loss: 0.01392075
[200/200] Training loss: 0.01202306
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16223.505909636178 ----------
[1/200] Training loss: 0.15971119
[2/200] Training loss: 0.06529749
[3/200] Training loss: 0.05430089
[4/200] Training loss: 0.04866695
[5/200] Training loss: 0.04098392
[6/200] Training loss: 0.04022477
[7/200] Training loss: 0.03783179
[8/200] Training loss: 0.03073157
[9/200] Training loss: 0.03426740
[10/200] Training loss: 0.02892230
[50/200] Training loss: 0.01821558
[100/200] Training loss: 0.01392345
[150/200] Training loss: 0.01290434
[200/200] Training loss: 0.01168531
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18306.189554355653 ----------
[1/200] Training loss: 0.14704074
[2/200] Training loss: 0.05916268
[3/200] Training loss: 0.05257365
[4/200] Training loss: 0.05168953
[5/200] Training loss: 0.04663574
[6/200] Training loss: 0.04310918
[7/200] Training loss: 0.04004940
[8/200] Training loss: 0.03955637
[9/200] Training loss: 0.03833136
[10/200] Training loss: 0.03642904
[50/200] Training loss: 0.01930617
[100/200] Training loss: 0.01730957
[150/200] Training loss: 0.01590479
[200/200] Training loss: 0.01405968
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20312.23788753962 ----------
[1/200] Training loss: 0.14625378
[2/200] Training loss: 0.05354847
[3/200] Training loss: 0.04631328
[4/200] Training loss: 0.04393256
[5/200] Training loss: 0.04323502
[6/200] Training loss: 0.04060245
[7/200] Training loss: 0.03635882
[8/200] Training loss: 0.03386601
[9/200] Training loss: 0.03260673
[10/200] Training loss: 0.03049171
[50/200] Training loss: 0.01646988
[100/200] Training loss: 0.01396599
[150/200] Training loss: 0.01231693
[200/200] Training loss: 0.01183499
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10449.01373336259 ----------
[1/200] Training loss: 0.16643522
[2/200] Training loss: 0.05838577
[3/200] Training loss: 0.05276459
[4/200] Training loss: 0.04704334
[5/200] Training loss: 0.04451462
[6/200] Training loss: 0.04346438
[7/200] Training loss: 0.04102719
[8/200] Training loss: 0.03818013
[9/200] Training loss: 0.03517114
[10/200] Training loss: 0.03497470
[50/200] Training loss: 0.01697262
[100/200] Training loss: 0.01359521
[150/200] Training loss: 0.01308798
[200/200] Training loss: 0.01124502
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16533.149246286986 ----------
[1/200] Training loss: 0.18047591
[2/200] Training loss: 0.06087842
[3/200] Training loss: 0.05423884
[4/200] Training loss: 0.05120136
[5/200] Training loss: 0.04975973
[6/200] Training loss: 0.04758036
[7/200] Training loss: 0.04537830
[8/200] Training loss: 0.04387242
[9/200] Training loss: 0.04118792
[10/200] Training loss: 0.03943511
[50/200] Training loss: 0.01820920
[100/200] Training loss: 0.01579598
[150/200] Training loss: 0.01429599
[200/200] Training loss: 0.01355455
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13309.886250453083 ----------
[1/200] Training loss: 0.16599122
[2/200] Training loss: 0.06167248
[3/200] Training loss: 0.05567314
[4/200] Training loss: 0.05130954
[5/200] Training loss: 0.05005386
[6/200] Training loss: 0.04824016
[7/200] Training loss: 0.04774565
[8/200] Training loss: 0.04457259
[9/200] Training loss: 0.04304137
[10/200] Training loss: 0.04503033
[50/200] Training loss: 0.01892617
[100/200] Training loss: 0.01647498
[150/200] Training loss: 0.01431714
[200/200] Training loss: 0.01290218
---batch_size---: 16 ---n_steps---: 2 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14465.891469245855 ----------
[1/200] Training loss: 0.16644257
[2/200] Training loss: 0.05925454
[3/200] Training loss: 0.05069999
[4/200] Training loss: 0.05233664
[5/200] Training loss: 0.04620354
[6/200] Training loss: 0.04542187
[7/200] Training loss: 0.04180788
[8/200] Training loss: 0.04205347
[9/200] Training loss: 0.03945837
[10/200] Training loss: 0.03949957
[50/200] Training loss: 0.01696068
[100/200] Training loss: 0.01473792
[150/200] Training loss: 0.01304179
[200/200] Training loss: 0.01192767
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15450.95776966593 ----------
[1/200] Training loss: 0.17012448
[2/200] Training loss: 0.05857336
[3/200] Training loss: 0.04896886
[4/200] Training loss: 0.04853940
[5/200] Training loss: 0.04739982
[6/200] Training loss: 0.04378347
[7/200] Training loss: 0.04362816
[8/200] Training loss: 0.03846918
[9/200] Training loss: 0.03718382
[10/200] Training loss: 0.03449592
[50/200] Training loss: 0.01792715
[100/200] Training loss: 0.01598606
[150/200] Training loss: 0.01480357
[200/200] Training loss: 0.01323378
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14614.485964275309 ----------
[1/200] Training loss: 0.13202524
[2/200] Training loss: 0.06135294
[3/200] Training loss: 0.05659678
[4/200] Training loss: 0.05290732
[5/200] Training loss: 0.04998661
[6/200] Training loss: 0.04849691
[7/200] Training loss: 0.04585151
[8/200] Training loss: 0.04084212
[9/200] Training loss: 0.03904468
[10/200] Training loss: 0.03558195
[50/200] Training loss: 0.01716413
[100/200] Training loss: 0.01405973
[150/200] Training loss: 0.01302810
[200/200] Training loss: 0.01236630
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12842.164303574378 ----------
[1/200] Training loss: 0.19535177
[2/200] Training loss: 0.05687883
[3/200] Training loss: 0.05467052
[4/200] Training loss: 0.05173257
[5/200] Training loss: 0.04592262
[6/200] Training loss: 0.03851052
[7/200] Training loss: 0.03622202
[8/200] Training loss: 0.03249910
[9/200] Training loss: 0.03224702
[10/200] Training loss: 0.03246214
[50/200] Training loss: 0.01923673
[100/200] Training loss: 0.01621097
[150/200] Training loss: 0.01489382
[200/200] Training loss: 0.01323922
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14970.336535963379 ----------
[1/200] Training loss: 0.14750021
[2/200] Training loss: 0.05672004
[3/200] Training loss: 0.04914644
[4/200] Training loss: 0.04438374
[5/200] Training loss: 0.03981972
[6/200] Training loss: 0.03951276
[7/200] Training loss: 0.03718235
[8/200] Training loss: 0.03486984
[9/200] Training loss: 0.03415706
[10/200] Training loss: 0.03297815
[50/200] Training loss: 0.01810720
[100/200] Training loss: 0.01511273
[150/200] Training loss: 0.01292601
[200/200] Training loss: 0.01216508
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7358.964601083498 ----------
[1/200] Training loss: 0.17017013
[2/200] Training loss: 0.05971671
[3/200] Training loss: 0.05331603
[4/200] Training loss: 0.05204077
[5/200] Training loss: 0.04836633
[6/200] Training loss: 0.04579452
[7/200] Training loss: 0.04250342
[8/200] Training loss: 0.04318196
[9/200] Training loss: 0.03869253
[10/200] Training loss: 0.03872111
[50/200] Training loss: 0.02226046
[100/200] Training loss: 0.01574667
[150/200] Training loss: 0.01405200
[200/200] Training loss: 0.01373850
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8475.384121088555 ----------
[1/200] Training loss: 0.14994103
[2/200] Training loss: 0.05511362
[3/200] Training loss: 0.04965178
[4/200] Training loss: 0.04632478
[5/200] Training loss: 0.04468487
[6/200] Training loss: 0.03953996
[7/200] Training loss: 0.03944994
[8/200] Training loss: 0.03644341
[9/200] Training loss: 0.03560434
[10/200] Training loss: 0.03084356
[50/200] Training loss: 0.01843464
[100/200] Training loss: 0.01471583
[150/200] Training loss: 0.01379227
[200/200] Training loss: 0.01291898
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10596.060399978853 ----------
[1/200] Training loss: 0.18247776
[2/200] Training loss: 0.06925419
[3/200] Training loss: 0.05836528
[4/200] Training loss: 0.05430678
[5/200] Training loss: 0.05424640
[6/200] Training loss: 0.05351506
[7/200] Training loss: 0.05052408
[8/200] Training loss: 0.04899124
[9/200] Training loss: 0.04879983
[10/200] Training loss: 0.04325759
[50/200] Training loss: 0.01952533
[100/200] Training loss: 0.01753690
[150/200] Training loss: 0.01628814
[200/200] Training loss: 0.01492701
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19562.58306052654 ----------
[1/200] Training loss: 0.17994234
[2/200] Training loss: 0.05922663
[3/200] Training loss: 0.05374810
[4/200] Training loss: 0.04855341
[5/200] Training loss: 0.05037787
[6/200] Training loss: 0.04731542
[7/200] Training loss: 0.04401240
[8/200] Training loss: 0.03689843
[9/200] Training loss: 0.03735949
[10/200] Training loss: 0.03617427
[50/200] Training loss: 0.01783131
[100/200] Training loss: 0.01470030
[150/200] Training loss: 0.01371381
[200/200] Training loss: 0.01269205
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12727.189791937575 ----------
[1/200] Training loss: 0.17134299
[2/200] Training loss: 0.05853430
[3/200] Training loss: 0.05068227
[4/200] Training loss: 0.04813450
[5/200] Training loss: 0.04573280
[6/200] Training loss: 0.04308184
[7/200] Training loss: 0.03999677
[8/200] Training loss: 0.03561368
[9/200] Training loss: 0.03701013
[10/200] Training loss: 0.03347920
[50/200] Training loss: 0.02098038
[100/200] Training loss: 0.01659521
[150/200] Training loss: 0.01405180
[200/200] Training loss: 0.01270510
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8358.82958314141 ----------
[1/200] Training loss: 0.16458455
[2/200] Training loss: 0.06471713
[3/200] Training loss: 0.05346561
[4/200] Training loss: 0.04922777
[5/200] Training loss: 0.04619717
[6/200] Training loss: 0.04265519
[7/200] Training loss: 0.03935680
[8/200] Training loss: 0.03802710
[9/200] Training loss: 0.03288832
[10/200] Training loss: 0.03388571
[50/200] Training loss: 0.01760256
[100/200] Training loss: 0.01458446
[150/200] Training loss: 0.01304445
[200/200] Training loss: 0.01225823
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10824.609369395277 ----------
[1/200] Training loss: 0.14687427
[2/200] Training loss: 0.05829161
[3/200] Training loss: 0.05433289
[4/200] Training loss: 0.05242599
[5/200] Training loss: 0.04960422
[6/200] Training loss: 0.04666994
[7/200] Training loss: 0.04576338
[8/200] Training loss: 0.04248123
[9/200] Training loss: 0.04072862
[10/200] Training loss: 0.03711050
[50/200] Training loss: 0.01678829
[100/200] Training loss: 0.01342020
[150/200] Training loss: 0.01199432
[200/200] Training loss: 0.01118980
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13336.405512730933 ----------
[1/200] Training loss: 0.17686373
[2/200] Training loss: 0.06365280
[3/200] Training loss: 0.05179938
[4/200] Training loss: 0.04859936
[5/200] Training loss: 0.04639797
[6/200] Training loss: 0.04180768
[7/200] Training loss: 0.04280277
[8/200] Training loss: 0.03462986
[9/200] Training loss: 0.03437546
[10/200] Training loss: 0.03308031
[50/200] Training loss: 0.01807206
[100/200] Training loss: 0.01558332
[150/200] Training loss: 0.01445813
[200/200] Training loss: 0.01324108
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16132.930297996083 ----------
[1/200] Training loss: 0.19271240
[2/200] Training loss: 0.06345425
[3/200] Training loss: 0.05568652
[4/200] Training loss: 0.05132677
[5/200] Training loss: 0.05054102
[6/200] Training loss: 0.04429165
[7/200] Training loss: 0.04305821
[8/200] Training loss: 0.03731111
[9/200] Training loss: 0.03719936
[10/200] Training loss: 0.03582020
[50/200] Training loss: 0.01727901
[100/200] Training loss: 0.01499230
[150/200] Training loss: 0.01408883
[200/200] Training loss: 0.01327512
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13421.25001629878 ----------
[1/200] Training loss: 0.17377163
[2/200] Training loss: 0.05770227
[3/200] Training loss: 0.04936091
[4/200] Training loss: 0.04796359
[5/200] Training loss: 0.04310725
[6/200] Training loss: 0.04309025
[7/200] Training loss: 0.03957047
[8/200] Training loss: 0.03784970
[9/200] Training loss: 0.03583379
[10/200] Training loss: 0.03454706
[50/200] Training loss: 0.01746468
[100/200] Training loss: 0.01473407
[150/200] Training loss: 0.01303187
[200/200] Training loss: 0.01162384
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15061.509087737522 ----------
[1/200] Training loss: 0.18263312
[2/200] Training loss: 0.06585574
[3/200] Training loss: 0.05841440
[4/200] Training loss: 0.05521862
[5/200] Training loss: 0.05087084
[6/200] Training loss: 0.05137652
[7/200] Training loss: 0.04784456
[8/200] Training loss: 0.04620483
[9/200] Training loss: 0.04031904
[10/200] Training loss: 0.03910093
[50/200] Training loss: 0.01921984
[100/200] Training loss: 0.01539576
[150/200] Training loss: 0.01397453
[200/200] Training loss: 0.01348416
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14860.124898532986 ----------
[1/200] Training loss: 0.17843062
[2/200] Training loss: 0.06096107
[3/200] Training loss: 0.05336937
[4/200] Training loss: 0.04976703
[5/200] Training loss: 0.04436589
[6/200] Training loss: 0.04033073
[7/200] Training loss: 0.03668360
[8/200] Training loss: 0.03216297
[9/200] Training loss: 0.03155018
[10/200] Training loss: 0.02790062
[50/200] Training loss: 0.01759379
[100/200] Training loss: 0.01497143
[150/200] Training loss: 0.01430379
[200/200] Training loss: 0.01277430
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15175.285697475352 ----------
[1/200] Training loss: 0.15617813
[2/200] Training loss: 0.06383482
[3/200] Training loss: 0.05320258
[4/200] Training loss: 0.04974095
[5/200] Training loss: 0.04876533
[6/200] Training loss: 0.03955117
[7/200] Training loss: 0.03629762
[8/200] Training loss: 0.03445168
[9/200] Training loss: 0.03390839
[10/200] Training loss: 0.03065098
[50/200] Training loss: 0.01687426
[100/200] Training loss: 0.01412343
[150/200] Training loss: 0.01299498
[200/200] Training loss: 0.01260379
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15456.153725943592 ----------
[1/200] Training loss: 0.14622416
[2/200] Training loss: 0.05892441
[3/200] Training loss: 0.05197052
[4/200] Training loss: 0.05275780
[5/200] Training loss: 0.04660145
[6/200] Training loss: 0.04380462
[7/200] Training loss: 0.04185684
[8/200] Training loss: 0.03965591
[9/200] Training loss: 0.04119655
[10/200] Training loss: 0.03453619
[50/200] Training loss: 0.01638305
[100/200] Training loss: 0.01431702
[150/200] Training loss: 0.01292880
[200/200] Training loss: 0.01151922
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19935.746386829866 ----------
[1/200] Training loss: 0.17308564
[2/200] Training loss: 0.06497242
[3/200] Training loss: 0.05644533
[4/200] Training loss: 0.05159676
[5/200] Training loss: 0.04894142
[6/200] Training loss: 0.04695615
[7/200] Training loss: 0.04727621
[8/200] Training loss: 0.04347663
[9/200] Training loss: 0.04113576
[10/200] Training loss: 0.04055285
[50/200] Training loss: 0.01857101
[100/200] Training loss: 0.01516465
[150/200] Training loss: 0.01316833
[200/200] Training loss: 0.01251131
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7617.77657850373 ----------
[1/200] Training loss: 0.16013253
[2/200] Training loss: 0.06178002
[3/200] Training loss: 0.05407844
[4/200] Training loss: 0.04819761
[5/200] Training loss: 0.04849121
[6/200] Training loss: 0.04808946
[7/200] Training loss: 0.04487561
[8/200] Training loss: 0.04082315
[9/200] Training loss: 0.03917715
[10/200] Training loss: 0.03719156
[50/200] Training loss: 0.01748494
[100/200] Training loss: 0.01414167
[150/200] Training loss: 0.01266018
[200/200] Training loss: 0.01190286
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11733.798702892427 ----------
[1/200] Training loss: 0.16138350
[2/200] Training loss: 0.05997431
[3/200] Training loss: 0.05309406
[4/200] Training loss: 0.04936995
[5/200] Training loss: 0.05202372
[6/200] Training loss: 0.04247122
[7/200] Training loss: 0.04537953
[8/200] Training loss: 0.03769087
[9/200] Training loss: 0.03838361
[10/200] Training loss: 0.03428809
[50/200] Training loss: 0.01717549
[100/200] Training loss: 0.01521558
[150/200] Training loss: 0.01428465
[200/200] Training loss: 0.01291862
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10388.658431193126 ----------
[1/200] Training loss: 0.16277526
[2/200] Training loss: 0.05952908
[3/200] Training loss: 0.05168581
[4/200] Training loss: 0.05092855
[5/200] Training loss: 0.04623896
[6/200] Training loss: 0.04330906
[7/200] Training loss: 0.04300980
[8/200] Training loss: 0.04058188
[9/200] Training loss: 0.03905871
[10/200] Training loss: 0.03734518
[50/200] Training loss: 0.01935742
[100/200] Training loss: 0.01637752
[150/200] Training loss: 0.01513958
[200/200] Training loss: 0.01424487
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14010.558304364606 ----------
[1/200] Training loss: 0.18474987
[2/200] Training loss: 0.06379415
[3/200] Training loss: 0.05444374
[4/200] Training loss: 0.05116388
[5/200] Training loss: 0.04736613
[6/200] Training loss: 0.04641417
[7/200] Training loss: 0.04444277
[8/200] Training loss: 0.04148104
[9/200] Training loss: 0.03734599
[10/200] Training loss: 0.03585076
[50/200] Training loss: 0.01775389
[100/200] Training loss: 0.01553771
[150/200] Training loss: 0.01427852
[200/200] Training loss: 0.01371757
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10772.440392037452 ----------
[1/200] Training loss: 0.14968104
[2/200] Training loss: 0.05716250
[3/200] Training loss: 0.05364821
[4/200] Training loss: 0.04391836
[5/200] Training loss: 0.04376415
[6/200] Training loss: 0.04005662
[7/200] Training loss: 0.03712953
[8/200] Training loss: 0.03470897
[9/200] Training loss: 0.03283666
[10/200] Training loss: 0.03035948
[50/200] Training loss: 0.01691895
[100/200] Training loss: 0.01501010
[150/200] Training loss: 0.01345365
[200/200] Training loss: 0.01239363
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6731.283384318328 ----------
[1/200] Training loss: 0.13724795
[2/200] Training loss: 0.05755941
[3/200] Training loss: 0.05170730
[4/200] Training loss: 0.04625726
[5/200] Training loss: 0.04739538
[6/200] Training loss: 0.04312566
[7/200] Training loss: 0.04203206
[8/200] Training loss: 0.04017582
[9/200] Training loss: 0.03598118
[10/200] Training loss: 0.03779014
[50/200] Training loss: 0.01740965
[100/200] Training loss: 0.01372957
[150/200] Training loss: 0.01273328
[200/200] Training loss: 0.01243347
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11056.879849216053 ----------
[1/200] Training loss: 0.15353808
[2/200] Training loss: 0.05659797
[3/200] Training loss: 0.04981484
[4/200] Training loss: 0.04836401
[5/200] Training loss: 0.04241946
[6/200] Training loss: 0.04016109
[7/200] Training loss: 0.03791225
[8/200] Training loss: 0.03657309
[9/200] Training loss: 0.03615866
[10/200] Training loss: 0.03295039
[50/200] Training loss: 0.01805573
[100/200] Training loss: 0.01431171
[150/200] Training loss: 0.01238324
[200/200] Training loss: 0.01153690
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15055.183824849168 ----------
[1/200] Training loss: 0.15944838
[2/200] Training loss: 0.06101311
[3/200] Training loss: 0.05319333
[4/200] Training loss: 0.05224731
[5/200] Training loss: 0.04821594
[6/200] Training loss: 0.04407304
[7/200] Training loss: 0.03994739
[8/200] Training loss: 0.03668154
[9/200] Training loss: 0.03478721
[10/200] Training loss: 0.03301465
[50/200] Training loss: 0.01877284
[100/200] Training loss: 0.01508657
[150/200] Training loss: 0.01405247
[200/200] Training loss: 0.01287247
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13127.23337188762 ----------
[1/200] Training loss: 0.14586212
[2/200] Training loss: 0.05919405
[3/200] Training loss: 0.05168916
[4/200] Training loss: 0.04962970
[5/200] Training loss: 0.04425898
[6/200] Training loss: 0.04106343
[7/200] Training loss: 0.03781580
[8/200] Training loss: 0.03814379
[9/200] Training loss: 0.03293925
[10/200] Training loss: 0.03149326
[50/200] Training loss: 0.01925225
[100/200] Training loss: 0.01461527
[150/200] Training loss: 0.01294874
[200/200] Training loss: 0.01179011
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23665.565871113246 ----------
[1/200] Training loss: 0.15569713
[2/200] Training loss: 0.05571612
[3/200] Training loss: 0.05108671
[4/200] Training loss: 0.04930751
[5/200] Training loss: 0.04674680
[6/200] Training loss: 0.04222208
[7/200] Training loss: 0.04364204
[8/200] Training loss: 0.03713091
[9/200] Training loss: 0.03668148
[10/200] Training loss: 0.03561236
[50/200] Training loss: 0.01884555
[100/200] Training loss: 0.01453745
[150/200] Training loss: 0.01246647
[200/200] Training loss: 0.01147448
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11037.49138165009 ----------
[1/200] Training loss: 0.15543250
[2/200] Training loss: 0.06159755
[3/200] Training loss: 0.05088686
[4/200] Training loss: 0.04684052
[5/200] Training loss: 0.04479005
[6/200] Training loss: 0.04021863
[7/200] Training loss: 0.03898087
[8/200] Training loss: 0.03950698
[9/200] Training loss: 0.03831489
[10/200] Training loss: 0.03740869
[50/200] Training loss: 0.01717190
[100/200] Training loss: 0.01506533
[150/200] Training loss: 0.01350230
[200/200] Training loss: 0.01285061
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11228.358740261196 ----------
[1/200] Training loss: 0.16662368
[2/200] Training loss: 0.05698382
[3/200] Training loss: 0.05033702
[4/200] Training loss: 0.04687543
[5/200] Training loss: 0.04288350
[6/200] Training loss: 0.04171575
[7/200] Training loss: 0.03618719
[8/200] Training loss: 0.03482838
[9/200] Training loss: 0.03341353
[10/200] Training loss: 0.03256265
[50/200] Training loss: 0.01588364
[100/200] Training loss: 0.01380382
[150/200] Training loss: 0.01262998
[200/200] Training loss: 0.01178802
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6660298665042367 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6303.02752651454 ----------
[1/200] Training loss: 0.13536344
[2/200] Training loss: 0.05602546
[3/200] Training loss: 0.05016426
[4/200] Training loss: 0.05086738
[5/200] Training loss: 0.04414490
[6/200] Training loss: 0.04109130
[7/200] Training loss: 0.03984731
[8/200] Training loss: 0.03818317
[9/200] Training loss: 0.03473080
[10/200] Training loss: 0.03404433
[50/200] Training loss: 0.01764283
[100/200] Training loss: 0.01465074
[150/200] Training loss: 0.01244389
[200/200] Training loss: 0.01182377
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10747.375865763699 ----------
[1/200] Training loss: 0.17688639
[2/200] Training loss: 0.06104398
[3/200] Training loss: 0.05427312
[4/200] Training loss: 0.05166292
[5/200] Training loss: 0.04457218
[6/200] Training loss: 0.04493721
[7/200] Training loss: 0.04068740
[8/200] Training loss: 0.03847650
[9/200] Training loss: 0.03490071
[10/200] Training loss: 0.03608348
[50/200] Training loss: 0.01941501
[100/200] Training loss: 0.01482229
[150/200] Training loss: 0.01309260
[200/200] Training loss: 0.01182929
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22010.012267147875 ----------
[1/200] Training loss: 0.15521781
[2/200] Training loss: 0.05858517
[3/200] Training loss: 0.05452499
[4/200] Training loss: 0.05275104
[5/200] Training loss: 0.05070568
[6/200] Training loss: 0.04791393
[7/200] Training loss: 0.04634666
[8/200] Training loss: 0.04429750
[9/200] Training loss: 0.04106416
[10/200] Training loss: 0.03996188
[50/200] Training loss: 0.01812382
[100/200] Training loss: 0.01578329
[150/200] Training loss: 0.01507948
[200/200] Training loss: 0.01434222
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20089.73867425856 ----------
[1/200] Training loss: 0.14669960
[2/200] Training loss: 0.06096986
[3/200] Training loss: 0.05467439
[4/200] Training loss: 0.05183111
[5/200] Training loss: 0.04784428
[6/200] Training loss: 0.04285488
[7/200] Training loss: 0.04067441
[8/200] Training loss: 0.03853635
[9/200] Training loss: 0.03721259
[10/200] Training loss: 0.03490042
[50/200] Training loss: 0.01878922
[100/200] Training loss: 0.01599694
[150/200] Training loss: 0.01448478
[200/200] Training loss: 0.01286607
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15736.044229729401 ----------
[1/200] Training loss: 0.16103674
[2/200] Training loss: 0.05914740
[3/200] Training loss: 0.05532595
[4/200] Training loss: 0.04759379
[5/200] Training loss: 0.04349121
[6/200] Training loss: 0.04199595
[7/200] Training loss: 0.04252465
[8/200] Training loss: 0.03951689
[9/200] Training loss: 0.03463899
[10/200] Training loss: 0.03759985
[50/200] Training loss: 0.01819027
[100/200] Training loss: 0.01504356
[150/200] Training loss: 0.01291168
[200/200] Training loss: 0.01209685
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10862.997744637527 ----------
[1/200] Training loss: 0.18755615
[2/200] Training loss: 0.06032591
[3/200] Training loss: 0.05565789
[4/200] Training loss: 0.05579198
[5/200] Training loss: 0.05019103
[6/200] Training loss: 0.04979822
[7/200] Training loss: 0.04781587
[8/200] Training loss: 0.04455251
[9/200] Training loss: 0.04312910
[10/200] Training loss: 0.04080454
[50/200] Training loss: 0.01846263
[100/200] Training loss: 0.01571284
[150/200] Training loss: 0.01458415
[200/200] Training loss: 0.01360683
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19531.004275254254 ----------
[1/200] Training loss: 0.13525041
[2/200] Training loss: 0.05556490
[3/200] Training loss: 0.04715435
[4/200] Training loss: 0.04582965
[5/200] Training loss: 0.04074340
[6/200] Training loss: 0.03853204
[7/200] Training loss: 0.03480336
[8/200] Training loss: 0.03173392
[9/200] Training loss: 0.03173238
[10/200] Training loss: 0.03069088
[50/200] Training loss: 0.01622564
[100/200] Training loss: 0.01413690
[150/200] Training loss: 0.01274646
[200/200] Training loss: 0.01181663
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5611.073515825648 ----------
[1/200] Training loss: 0.15243485
[2/200] Training loss: 0.06021638
[3/200] Training loss: 0.05241736
[4/200] Training loss: 0.05090308
[5/200] Training loss: 0.04496938
[6/200] Training loss: 0.04005503
[7/200] Training loss: 0.03650683
[8/200] Training loss: 0.03500271
[9/200] Training loss: 0.03218703
[10/200] Training loss: 0.03397461
[50/200] Training loss: 0.01892396
[100/200] Training loss: 0.01666302
[150/200] Training loss: 0.01498714
[200/200] Training loss: 0.01412901
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13642.917576530323 ----------
[1/200] Training loss: 0.18141563
[2/200] Training loss: 0.06054232
[3/200] Training loss: 0.05195042
[4/200] Training loss: 0.04650964
[5/200] Training loss: 0.04202096
[6/200] Training loss: 0.04052107
[7/200] Training loss: 0.03864395
[8/200] Training loss: 0.03609398
[9/200] Training loss: 0.03493812
[10/200] Training loss: 0.03295811
[50/200] Training loss: 0.01768771
[100/200] Training loss: 0.01501397
[150/200] Training loss: 0.01299427
[200/200] Training loss: 0.01204431
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17277.910058800513 ----------
[1/200] Training loss: 0.13419812
[2/200] Training loss: 0.05709747
[3/200] Training loss: 0.05003097
[4/200] Training loss: 0.04634966
[5/200] Training loss: 0.04258308
[6/200] Training loss: 0.03840054
[7/200] Training loss: 0.03860419
[8/200] Training loss: 0.03994726
[9/200] Training loss: 0.03785446
[10/200] Training loss: 0.03395844
[50/200] Training loss: 0.01879482
[100/200] Training loss: 0.01447945
[150/200] Training loss: 0.01260928
[200/200] Training loss: 0.01137492
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7000.860518536275 ----------
[1/200] Training loss: 0.18342377
[2/200] Training loss: 0.06066822
[3/200] Training loss: 0.05314176
[4/200] Training loss: 0.05275963
[5/200] Training loss: 0.04955584
[6/200] Training loss: 0.04422194
[7/200] Training loss: 0.04365086
[8/200] Training loss: 0.03960658
[9/200] Training loss: 0.03867880
[10/200] Training loss: 0.03889219
[50/200] Training loss: 0.01786582
[100/200] Training loss: 0.01509566
[150/200] Training loss: 0.01445493
[200/200] Training loss: 0.01286639
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18198.529171336897 ----------
[1/200] Training loss: 0.18635686
[2/200] Training loss: 0.06443314
[3/200] Training loss: 0.05492975
[4/200] Training loss: 0.04703027
[5/200] Training loss: 0.04696982
[6/200] Training loss: 0.04437822
[7/200] Training loss: 0.04345799
[8/200] Training loss: 0.04069912
[9/200] Training loss: 0.03812325
[10/200] Training loss: 0.03291914
[50/200] Training loss: 0.01725658
[100/200] Training loss: 0.01565660
[150/200] Training loss: 0.01477902
[200/200] Training loss: 0.01354526
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10673.339121380899 ----------
[1/200] Training loss: 0.17624393
[2/200] Training loss: 0.05594256
[3/200] Training loss: 0.05202880
[4/200] Training loss: 0.04738627
[5/200] Training loss: 0.04843654
[6/200] Training loss: 0.04052318
[7/200] Training loss: 0.03795348
[8/200] Training loss: 0.03895879
[9/200] Training loss: 0.03320440
[10/200] Training loss: 0.03364116
[50/200] Training loss: 0.01676892
[100/200] Training loss: 0.01373486
[150/200] Training loss: 0.01220492
[200/200] Training loss: 0.01203643
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9765.75936627562 ----------
[1/200] Training loss: 0.15416515
[2/200] Training loss: 0.06044755
[3/200] Training loss: 0.05188730
[4/200] Training loss: 0.04854297
[5/200] Training loss: 0.04448311
[6/200] Training loss: 0.04406084
[7/200] Training loss: 0.04195772
[8/200] Training loss: 0.03874235
[9/200] Training loss: 0.03573161
[10/200] Training loss: 0.03733069
[50/200] Training loss: 0.01805427
[100/200] Training loss: 0.01535063
[150/200] Training loss: 0.01324704
[200/200] Training loss: 0.01313869
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11740.190799130993 ----------
[1/200] Training loss: 0.15718027
[2/200] Training loss: 0.05464802
[3/200] Training loss: 0.05154216
[4/200] Training loss: 0.04408575
[5/200] Training loss: 0.04054697
[6/200] Training loss: 0.03851034
[7/200] Training loss: 0.03753983
[8/200] Training loss: 0.03349052
[9/200] Training loss: 0.03074813
[10/200] Training loss: 0.03068325
[50/200] Training loss: 0.01946502
[100/200] Training loss: 0.01695760
[150/200] Training loss: 0.01505420
[200/200] Training loss: 0.01306320
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16482.41050332141 ----------
[1/200] Training loss: 0.16909081
[2/200] Training loss: 0.05811473
[3/200] Training loss: 0.04954305
[4/200] Training loss: 0.04705839
[5/200] Training loss: 0.04360737
[6/200] Training loss: 0.04059139
[7/200] Training loss: 0.03533794
[8/200] Training loss: 0.03503302
[9/200] Training loss: 0.03516540
[10/200] Training loss: 0.02960454
[50/200] Training loss: 0.01863290
[100/200] Training loss: 0.01505820
[150/200] Training loss: 0.01387695
[200/200] Training loss: 0.01310401
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20759.632366687038 ----------
[1/200] Training loss: 0.17177006
[2/200] Training loss: 0.05872668
[3/200] Training loss: 0.05938993
[4/200] Training loss: 0.05542267
[5/200] Training loss: 0.05089163
[6/200] Training loss: 0.04887923
[7/200] Training loss: 0.05024020
[8/200] Training loss: 0.04454626
[9/200] Training loss: 0.04345573
[10/200] Training loss: 0.04117752
[50/200] Training loss: 0.01732993
[100/200] Training loss: 0.01492132
[150/200] Training loss: 0.01397354
[200/200] Training loss: 0.01301698
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9086.124806538814 ----------
[1/200] Training loss: 0.17707702
[2/200] Training loss: 0.06019019
[3/200] Training loss: 0.05962490
[4/200] Training loss: 0.05524582
[5/200] Training loss: 0.05075041
[6/200] Training loss: 0.04781435
[7/200] Training loss: 0.04691709
[8/200] Training loss: 0.04681439
[9/200] Training loss: 0.04388075
[10/200] Training loss: 0.04107524
[50/200] Training loss: 0.01794599
[100/200] Training loss: 0.01510728
[150/200] Training loss: 0.01290046
[200/200] Training loss: 0.01206430
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7279.166161037952 ----------
[1/200] Training loss: 0.13920880
[2/200] Training loss: 0.05889103
[3/200] Training loss: 0.05310015
[4/200] Training loss: 0.04566312
[5/200] Training loss: 0.04312908
[6/200] Training loss: 0.03949327
[7/200] Training loss: 0.03631377
[8/200] Training loss: 0.03494122
[9/200] Training loss: 0.03353890
[10/200] Training loss: 0.03197158
[50/200] Training loss: 0.01886213
[100/200] Training loss: 0.01679600
[150/200] Training loss: 0.01400599
[200/200] Training loss: 0.01198755
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8926.458200204603 ----------
[1/200] Training loss: 0.14844642
[2/200] Training loss: 0.05697166
[3/200] Training loss: 0.05098967
[4/200] Training loss: 0.04506249
[5/200] Training loss: 0.04328833
[6/200] Training loss: 0.04070083
[7/200] Training loss: 0.03797518
[8/200] Training loss: 0.03484916
[9/200] Training loss: 0.03508454
[10/200] Training loss: 0.03117923
[50/200] Training loss: 0.01814845
[100/200] Training loss: 0.01490695
[150/200] Training loss: 0.01337183
[200/200] Training loss: 0.01168278
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7943.136911825201 ----------
[1/200] Training loss: 0.15123140
[2/200] Training loss: 0.05789341
[3/200] Training loss: 0.04733503
[4/200] Training loss: 0.04785162
[5/200] Training loss: 0.04022145
[6/200] Training loss: 0.03839264
[7/200] Training loss: 0.03530425
[8/200] Training loss: 0.03369412
[9/200] Training loss: 0.03099045
[10/200] Training loss: 0.02906335
[50/200] Training loss: 0.01759894
[100/200] Training loss: 0.01427635
[150/200] Training loss: 0.01368114
[200/200] Training loss: 0.01235596
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19456.504105311415 ----------
[1/200] Training loss: 0.15611418
[2/200] Training loss: 0.05419983
[3/200] Training loss: 0.05255829
[4/200] Training loss: 0.04660156
[5/200] Training loss: 0.04331408
[6/200] Training loss: 0.04045870
[7/200] Training loss: 0.03609699
[8/200] Training loss: 0.03830869
[9/200] Training loss: 0.03556548
[10/200] Training loss: 0.03185826
[50/200] Training loss: 0.01749234
[100/200] Training loss: 0.01476742
[150/200] Training loss: 0.01369992
[200/200] Training loss: 0.01279308
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18874.40510320789 ----------
[1/200] Training loss: 0.15110818
[2/200] Training loss: 0.05785745
[3/200] Training loss: 0.04928042
[4/200] Training loss: 0.05067729
[5/200] Training loss: 0.04712751
[6/200] Training loss: 0.04152882
[7/200] Training loss: 0.04062813
[8/200] Training loss: 0.03775204
[9/200] Training loss: 0.03820778
[10/200] Training loss: 0.03456674
[50/200] Training loss: 0.01622289
[100/200] Training loss: 0.01355020
[150/200] Training loss: 0.01213836
[200/200] Training loss: 0.01160760
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4961.74787751252 ----------
[1/200] Training loss: 0.14184427
[2/200] Training loss: 0.05183508
[3/200] Training loss: 0.04531240
[4/200] Training loss: 0.04510484
[5/200] Training loss: 0.03997265
[6/200] Training loss: 0.03958126
[7/200] Training loss: 0.03574110
[8/200] Training loss: 0.03381387
[9/200] Training loss: 0.03087171
[10/200] Training loss: 0.03071476
[50/200] Training loss: 0.01832562
[100/200] Training loss: 0.01345749
[150/200] Training loss: 0.01217927
[200/200] Training loss: 0.01116566
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5322.037391826555 ----------
[1/200] Training loss: 0.12044302
[2/200] Training loss: 0.05409405
[3/200] Training loss: 0.04980085
[4/200] Training loss: 0.04413143
[5/200] Training loss: 0.04115247
[6/200] Training loss: 0.03844433
[7/200] Training loss: 0.03559152
[8/200] Training loss: 0.03536583
[9/200] Training loss: 0.03285754
[10/200] Training loss: 0.03027517
[50/200] Training loss: 0.01867331
[100/200] Training loss: 0.01499567
[150/200] Training loss: 0.01341366
[200/200] Training loss: 0.01202874
---batch_size---: 8 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14250.740893020264 ----------
[1/200] Training loss: 0.16417865
[2/200] Training loss: 0.06111876
[3/200] Training loss: 0.05002202
[4/200] Training loss: 0.04599977
[5/200] Training loss: 0.04553475
[6/200] Training loss: 0.04492806
[7/200] Training loss: 0.04049937
[8/200] Training loss: 0.03508849
[9/200] Training loss: 0.03210844
[10/200] Training loss: 0.03172228
[50/200] Training loss: 0.01749705
[100/200] Training loss: 0.01538904
[150/200] Training loss: 0.01297844
[200/200] Training loss: 0.01157757
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17086.58140179012 ----------
[1/200] Training loss: 0.15419553
[2/200] Training loss: 0.05925361
[3/200] Training loss: 0.05386314
[4/200] Training loss: 0.05124731
[5/200] Training loss: 0.04496333
[6/200] Training loss: 0.04251303
[7/200] Training loss: 0.03979210
[8/200] Training loss: 0.03520495
[9/200] Training loss: 0.03060063
[10/200] Training loss: 0.03172861
[50/200] Training loss: 0.01763328
[100/200] Training loss: 0.01393367
[150/200] Training loss: 0.01267439
[200/200] Training loss: 0.01143481
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19887.741752144713 ----------
[1/200] Training loss: 0.15147041
[2/200] Training loss: 0.05832125
[3/200] Training loss: 0.05417765
[4/200] Training loss: 0.04826108
[5/200] Training loss: 0.04480516
[6/200] Training loss: 0.04067915
[7/200] Training loss: 0.03700310
[8/200] Training loss: 0.03435260
[9/200] Training loss: 0.03378563
[10/200] Training loss: 0.03088909
[50/200] Training loss: 0.01766402
[100/200] Training loss: 0.01470121
[150/200] Training loss: 0.01332634
[200/200] Training loss: 0.01286428
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15048.787858163196 ----------
[1/200] Training loss: 0.16608986
[2/200] Training loss: 0.05752718
[3/200] Training loss: 0.05176105
[4/200] Training loss: 0.05371911
[5/200] Training loss: 0.04531387
[6/200] Training loss: 0.04617429
[7/200] Training loss: 0.04234633
[8/200] Training loss: 0.03924310
[9/200] Training loss: 0.03760257
[10/200] Training loss: 0.03380717
[50/200] Training loss: 0.01982027
[100/200] Training loss: 0.01741412
[150/200] Training loss: 0.01622917
[200/200] Training loss: 0.01565331
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14915.609541684846 ----------
[1/200] Training loss: 0.13331822
[2/200] Training loss: 0.05719839
[3/200] Training loss: 0.04826386
[4/200] Training loss: 0.04640485
[5/200] Training loss: 0.04315188
[6/200] Training loss: 0.03909808
[7/200] Training loss: 0.03558246
[8/200] Training loss: 0.03461430
[9/200] Training loss: 0.03499707
[10/200] Training loss: 0.02987289
[50/200] Training loss: 0.01899023
[100/200] Training loss: 0.01375176
[150/200] Training loss: 0.01194339
[200/200] Training loss: 0.01121288
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 25713.315461060247 ----------
[1/200] Training loss: 0.19155841
[2/200] Training loss: 0.05915099
[3/200] Training loss: 0.05368659
[4/200] Training loss: 0.05041499
[5/200] Training loss: 0.04886580
[6/200] Training loss: 0.04951987
[7/200] Training loss: 0.04627211
[8/200] Training loss: 0.04424404
[9/200] Training loss: 0.04306598
[10/200] Training loss: 0.03715671
[50/200] Training loss: 0.01917586
[100/200] Training loss: 0.01576034
[150/200] Training loss: 0.01329350
[200/200] Training loss: 0.01284421
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17801.857880569656 ----------
[1/200] Training loss: 0.16614039
[2/200] Training loss: 0.05932986
[3/200] Training loss: 0.05493900
[4/200] Training loss: 0.05142575
[5/200] Training loss: 0.05026884
[6/200] Training loss: 0.04845774
[7/200] Training loss: 0.04588511
[8/200] Training loss: 0.04512981
[9/200] Training loss: 0.04072681
[10/200] Training loss: 0.03815894
[50/200] Training loss: 0.01910008
[100/200] Training loss: 0.01611988
[150/200] Training loss: 0.01477755
[200/200] Training loss: 0.01381034
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17881.015631109996 ----------
[1/200] Training loss: 0.16902363
[2/200] Training loss: 0.05846186
[3/200] Training loss: 0.04904745
[4/200] Training loss: 0.04464067
[5/200] Training loss: 0.04453413
[6/200] Training loss: 0.04239978
[7/200] Training loss: 0.04004045
[8/200] Training loss: 0.03727691
[9/200] Training loss: 0.03565173
[10/200] Training loss: 0.03339187
[50/200] Training loss: 0.01754971
[100/200] Training loss: 0.01468943
[150/200] Training loss: 0.01317888
[200/200] Training loss: 0.01239756
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7099.892675245169 ----------
[1/200] Training loss: 0.15665284
[2/200] Training loss: 0.05928064
[3/200] Training loss: 0.05104538
[4/200] Training loss: 0.04649031
[5/200] Training loss: 0.04580113
[6/200] Training loss: 0.04337387
[7/200] Training loss: 0.04455105
[8/200] Training loss: 0.03865559
[9/200] Training loss: 0.03553053
[10/200] Training loss: 0.03523889
[50/200] Training loss: 0.01811817
[100/200] Training loss: 0.01428278
[150/200] Training loss: 0.01240771
[200/200] Training loss: 0.01072649
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5766.014047849693 ----------
[1/200] Training loss: 0.14191038
[2/200] Training loss: 0.05917219
[3/200] Training loss: 0.05205240
[4/200] Training loss: 0.05037985
[5/200] Training loss: 0.04764642
[6/200] Training loss: 0.04199683
[7/200] Training loss: 0.04078626
[8/200] Training loss: 0.03987009
[9/200] Training loss: 0.03589895
[10/200] Training loss: 0.03376721
[50/200] Training loss: 0.01986597
[100/200] Training loss: 0.01571874
[150/200] Training loss: 0.01421166
[200/200] Training loss: 0.01263895
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8153.187597498294 ----------
[1/200] Training loss: 0.17694423
[2/200] Training loss: 0.05902413
[3/200] Training loss: 0.05485427
[4/200] Training loss: 0.04909079
[5/200] Training loss: 0.04622398
[6/200] Training loss: 0.04438189
[7/200] Training loss: 0.04149776
[8/200] Training loss: 0.04026319
[9/200] Training loss: 0.03679867
[10/200] Training loss: 0.03696265
[50/200] Training loss: 0.01925037
[100/200] Training loss: 0.01570792
[150/200] Training loss: 0.01335768
[200/200] Training loss: 0.01239597
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8580.156641926766 ----------
[1/200] Training loss: 0.18208710
[2/200] Training loss: 0.05685651
[3/200] Training loss: 0.05076428
[4/200] Training loss: 0.04582692
[5/200] Training loss: 0.04682216
[6/200] Training loss: 0.04282371
[7/200] Training loss: 0.03920675
[8/200] Training loss: 0.04058689
[9/200] Training loss: 0.03684761
[10/200] Training loss: 0.03705216
[50/200] Training loss: 0.01888784
[100/200] Training loss: 0.01654327
[150/200] Training loss: 0.01531267
[200/200] Training loss: 0.01441730
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10789.21498534532 ----------
[1/200] Training loss: 0.20026327
[2/200] Training loss: 0.06319935
[3/200] Training loss: 0.05869938
[4/200] Training loss: 0.05662708
[5/200] Training loss: 0.05264605
[6/200] Training loss: 0.04743367
[7/200] Training loss: 0.04941107
[8/200] Training loss: 0.04737329
[9/200] Training loss: 0.04468857
[10/200] Training loss: 0.04319016
[50/200] Training loss: 0.01971691
[100/200] Training loss: 0.01617422
[150/200] Training loss: 0.01497672
[200/200] Training loss: 0.01393167
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10797.395982365379 ----------
[1/200] Training loss: 0.13382217
[2/200] Training loss: 0.05578234
[3/200] Training loss: 0.05248505
[4/200] Training loss: 0.04542846
[5/200] Training loss: 0.04351295
[6/200] Training loss: 0.04190902
[7/200] Training loss: 0.04110698
[8/200] Training loss: 0.04051264
[9/200] Training loss: 0.03868407
[10/200] Training loss: 0.03667850
[50/200] Training loss: 0.01746921
[100/200] Training loss: 0.01334563
[150/200] Training loss: 0.01134961
[200/200] Training loss: 0.01056784
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19311.363701199352 ----------
[1/200] Training loss: 0.13878692
[2/200] Training loss: 0.05085998
[3/200] Training loss: 0.04547814
[4/200] Training loss: 0.04031753
[5/200] Training loss: 0.04065894
[6/200] Training loss: 0.03599612
[7/200] Training loss: 0.03225147
[8/200] Training loss: 0.03213770
[9/200] Training loss: 0.02866537
[10/200] Training loss: 0.02762401
[50/200] Training loss: 0.01569879
[100/200] Training loss: 0.01296562
[150/200] Training loss: 0.01196389
[200/200] Training loss: 0.01076122
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9332.240459825283 ----------
[1/200] Training loss: 0.14951401
[2/200] Training loss: 0.06197373
[3/200] Training loss: 0.05408924
[4/200] Training loss: 0.04821508
[5/200] Training loss: 0.04063314
[6/200] Training loss: 0.03724373
[7/200] Training loss: 0.03432815
[8/200] Training loss: 0.03243575
[9/200] Training loss: 0.03251786
[10/200] Training loss: 0.03005623
[50/200] Training loss: 0.01752556
[100/200] Training loss: 0.01642676
[150/200] Training loss: 0.01441102
[200/200] Training loss: 0.01302992
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15122.103028348934 ----------
[1/200] Training loss: 0.16404428
[2/200] Training loss: 0.06267349
[3/200] Training loss: 0.05407157
[4/200] Training loss: 0.04827575
[5/200] Training loss: 0.04639652
[6/200] Training loss: 0.04724111
[7/200] Training loss: 0.04124570
[8/200] Training loss: 0.04005969
[9/200] Training loss: 0.04022143
[10/200] Training loss: 0.03715184
[50/200] Training loss: 0.01752699
[100/200] Training loss: 0.01549222
[150/200] Training loss: 0.01357942
[200/200] Training loss: 0.01294771
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 3516.4032760762807 ----------
[1/200] Training loss: 0.16863836
[2/200] Training loss: 0.06017076
[3/200] Training loss: 0.05094758
[4/200] Training loss: 0.04590466
[5/200] Training loss: 0.04603427
[6/200] Training loss: 0.04216328
[7/200] Training loss: 0.03887453
[8/200] Training loss: 0.03759309
[9/200] Training loss: 0.03095783
[10/200] Training loss: 0.03253017
[50/200] Training loss: 0.01789888
[100/200] Training loss: 0.01527517
[150/200] Training loss: 0.01407520
[200/200] Training loss: 0.01276807
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20385.714998498337 ----------
[1/200] Training loss: 0.17420696
[2/200] Training loss: 0.06019569
[3/200] Training loss: 0.04909143
[4/200] Training loss: 0.04490354
[5/200] Training loss: 0.04654861
[6/200] Training loss: 0.04116074
[7/200] Training loss: 0.03832024
[8/200] Training loss: 0.03789565
[9/200] Training loss: 0.03539825
[10/200] Training loss: 0.03601315
[50/200] Training loss: 0.01970735
[100/200] Training loss: 0.01564062
[150/200] Training loss: 0.01470288
[200/200] Training loss: 0.01412990
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12329.694237895765 ----------
[1/200] Training loss: 0.17742929
[2/200] Training loss: 0.06251835
[3/200] Training loss: 0.05329609
[4/200] Training loss: 0.05287039
[5/200] Training loss: 0.04485971
[6/200] Training loss: 0.04715797
[7/200] Training loss: 0.04074143
[8/200] Training loss: 0.03741956
[9/200] Training loss: 0.03672087
[10/200] Training loss: 0.03305464
[50/200] Training loss: 0.01704135
[100/200] Training loss: 0.01464668
[150/200] Training loss: 0.01357213
[200/200] Training loss: 0.01319902
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17425.745550764823 ----------
[1/200] Training loss: 0.16440748
[2/200] Training loss: 0.05824004
[3/200] Training loss: 0.05263008
[4/200] Training loss: 0.04902961
[5/200] Training loss: 0.04528039
[6/200] Training loss: 0.04492588
[7/200] Training loss: 0.04081425
[8/200] Training loss: 0.03662334
[9/200] Training loss: 0.03531338
[10/200] Training loss: 0.03018353
[50/200] Training loss: 0.01565753
[100/200] Training loss: 0.01271479
[150/200] Training loss: 0.01245151
[200/200] Training loss: 0.01134493
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13629.492727170737 ----------
[1/200] Training loss: 0.17914535
[2/200] Training loss: 0.05993675
[3/200] Training loss: 0.05076465
[4/200] Training loss: 0.04837453
[5/200] Training loss: 0.04429768
[6/200] Training loss: 0.04217328
[7/200] Training loss: 0.03827127
[8/200] Training loss: 0.03700061
[9/200] Training loss: 0.03548520
[10/200] Training loss: 0.03369192
[50/200] Training loss: 0.01842868
[100/200] Training loss: 0.01512061
[150/200] Training loss: 0.01384791
[200/200] Training loss: 0.01318572
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20574.257313448765 ----------
[1/200] Training loss: 0.18501960
[2/200] Training loss: 0.07084533
[3/200] Training loss: 0.06143277
[4/200] Training loss: 0.05502901
[5/200] Training loss: 0.05134470
[6/200] Training loss: 0.04943896
[7/200] Training loss: 0.04599570
[8/200] Training loss: 0.04695477
[9/200] Training loss: 0.04595005
[10/200] Training loss: 0.04408757
[50/200] Training loss: 0.01796523
[100/200] Training loss: 0.01607667
[150/200] Training loss: 0.01478711
[200/200] Training loss: 0.01386828
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6913.208806335882 ----------
[1/200] Training loss: 0.17127415
[2/200] Training loss: 0.06031232
[3/200] Training loss: 0.04835258
[4/200] Training loss: 0.04646577
[5/200] Training loss: 0.04395365
[6/200] Training loss: 0.03976433
[7/200] Training loss: 0.03833116
[8/200] Training loss: 0.03741706
[9/200] Training loss: 0.03671828
[10/200] Training loss: 0.03394527
[50/200] Training loss: 0.01679994
[100/200] Training loss: 0.01440778
[150/200] Training loss: 0.01292112
[200/200] Training loss: 0.01210365
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11694.138018682694 ----------
[1/200] Training loss: 0.15409515
[2/200] Training loss: 0.05646741
[3/200] Training loss: 0.05536253
[4/200] Training loss: 0.04712397
[5/200] Training loss: 0.04755845
[6/200] Training loss: 0.04207074
[7/200] Training loss: 0.03955762
[8/200] Training loss: 0.04141473
[9/200] Training loss: 0.03674161
[10/200] Training loss: 0.03264975
[50/200] Training loss: 0.01838959
[100/200] Training loss: 0.01601565
[150/200] Training loss: 0.01360189
[200/200] Training loss: 0.01264654
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16193.70692584005 ----------
[1/200] Training loss: 0.17732005
[2/200] Training loss: 0.05807212
[3/200] Training loss: 0.05391875
[4/200] Training loss: 0.04667805
[5/200] Training loss: 0.04401045
[6/200] Training loss: 0.04112535
[7/200] Training loss: 0.03875042
[8/200] Training loss: 0.03612769
[9/200] Training loss: 0.03377610
[10/200] Training loss: 0.03447780
[50/200] Training loss: 0.01848970
[100/200] Training loss: 0.01629098
[150/200] Training loss: 0.01362063
[200/200] Training loss: 0.01275473
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11929.643749919776 ----------
[1/200] Training loss: 0.15842173
[2/200] Training loss: 0.05841752
[3/200] Training loss: 0.05251349
[4/200] Training loss: 0.04390855
[5/200] Training loss: 0.03989898
[6/200] Training loss: 0.03565163
[7/200] Training loss: 0.03310277
[8/200] Training loss: 0.03151976
[9/200] Training loss: 0.02935639
[10/200] Training loss: 0.02709525
[50/200] Training loss: 0.01741973
[100/200] Training loss: 0.01494590
[150/200] Training loss: 0.01411248
[200/200] Training loss: 0.01313155
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16626.471423606392 ----------
[1/200] Training loss: 0.17904019
[2/200] Training loss: 0.05723832
[3/200] Training loss: 0.04950498
[4/200] Training loss: 0.04811711
[5/200] Training loss: 0.04407651
[6/200] Training loss: 0.04124787
[7/200] Training loss: 0.03589797
[8/200] Training loss: 0.03700386
[9/200] Training loss: 0.03347909
[10/200] Training loss: 0.03578742
[50/200] Training loss: 0.01827620
[100/200] Training loss: 0.01486713
[150/200] Training loss: 0.01334310
[200/200] Training loss: 0.01256844
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17017.490296750577 ----------
[1/200] Training loss: 0.14536947
[2/200] Training loss: 0.06415096
[3/200] Training loss: 0.05330105
[4/200] Training loss: 0.05299766
[5/200] Training loss: 0.04766170
[6/200] Training loss: 0.04550193
[7/200] Training loss: 0.04244895
[8/200] Training loss: 0.04052995
[9/200] Training loss: 0.03587875
[10/200] Training loss: 0.03236167
[50/200] Training loss: 0.01773511
[100/200] Training loss: 0.01567663
[150/200] Training loss: 0.01301711
[200/200] Training loss: 0.01261735
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17651.97779287069 ----------
[1/200] Training loss: 0.15062013
[2/200] Training loss: 0.06353105
[3/200] Training loss: 0.05196425
[4/200] Training loss: 0.04644383
[5/200] Training loss: 0.04805616
[6/200] Training loss: 0.04032546
[7/200] Training loss: 0.03890299
[8/200] Training loss: 0.03585605
[9/200] Training loss: 0.03314500
[10/200] Training loss: 0.03434729
[50/200] Training loss: 0.01786442
[100/200] Training loss: 0.01427704
[150/200] Training loss: 0.01228747
[200/200] Training loss: 0.01153379
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22316.981157853766 ----------
[1/200] Training loss: 0.17349749
[2/200] Training loss: 0.06210645
[3/200] Training loss: 0.05125422
[4/200] Training loss: 0.04573190
[5/200] Training loss: 0.04487896
[6/200] Training loss: 0.04227296
[7/200] Training loss: 0.03711763
[8/200] Training loss: 0.03643974
[9/200] Training loss: 0.03409637
[10/200] Training loss: 0.03061106
[50/200] Training loss: 0.01754822
[100/200] Training loss: 0.01468188
[150/200] Training loss: 0.01347833
[200/200] Training loss: 0.01239144
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16984.25812333291 ----------
[1/200] Training loss: 0.15761511
[2/200] Training loss: 0.05508302
[3/200] Training loss: 0.04393563
[4/200] Training loss: 0.04193867
[5/200] Training loss: 0.03874830
[6/200] Training loss: 0.03772786
[7/200] Training loss: 0.03223079
[8/200] Training loss: 0.03155797
[9/200] Training loss: 0.03351555
[10/200] Training loss: 0.02849058
[50/200] Training loss: 0.01941662
[100/200] Training loss: 0.01612712
[150/200] Training loss: 0.01456091
[200/200] Training loss: 0.01305569
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9589.12258759893 ----------
[1/200] Training loss: 0.16143161
[2/200] Training loss: 0.05564979
[3/200] Training loss: 0.05381228
[4/200] Training loss: 0.04805940
[5/200] Training loss: 0.04561566
[6/200] Training loss: 0.04282205
[7/200] Training loss: 0.03801495
[8/200] Training loss: 0.03644341
[9/200] Training loss: 0.03069487
[10/200] Training loss: 0.03192437
[50/200] Training loss: 0.01737284
[100/200] Training loss: 0.01588175
[150/200] Training loss: 0.01458283
[200/200] Training loss: 0.01322157
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18464.447134967242 ----------
[1/200] Training loss: 0.24255833
[2/200] Training loss: 0.08202621
[3/200] Training loss: 0.05906563
[4/200] Training loss: 0.05665547
[5/200] Training loss: 0.05560920
[6/200] Training loss: 0.05309743
[7/200] Training loss: 0.05149623
[8/200] Training loss: 0.04811507
[9/200] Training loss: 0.04656179
[10/200] Training loss: 0.04470295
[50/200] Training loss: 0.02289153
[100/200] Training loss: 0.01670206
[150/200] Training loss: 0.01546405
[200/200] Training loss: 0.01392529
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9917.17903438271 ----------
[1/200] Training loss: 0.16823371
[2/200] Training loss: 0.05920549
[3/200] Training loss: 0.05385680
[4/200] Training loss: 0.05047872
[5/200] Training loss: 0.04931395
[6/200] Training loss: 0.04767011
[7/200] Training loss: 0.04402899
[8/200] Training loss: 0.04408021
[9/200] Training loss: 0.04144599
[10/200] Training loss: 0.03895464
[50/200] Training loss: 0.01898160
[100/200] Training loss: 0.01730183
[150/200] Training loss: 0.01599757
[200/200] Training loss: 0.01519238
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17011.405115392437 ----------
[1/200] Training loss: 0.16057076
[2/200] Training loss: 0.06252705
[3/200] Training loss: 0.05117994
[4/200] Training loss: 0.05348927
[5/200] Training loss: 0.04953461
[6/200] Training loss: 0.04324125
[7/200] Training loss: 0.04003284
[8/200] Training loss: 0.03356362
[9/200] Training loss: 0.03325922
[10/200] Training loss: 0.02926293
[50/200] Training loss: 0.01562260
[100/200] Training loss: 0.01391689
[150/200] Training loss: 0.01310667
[200/200] Training loss: 0.01177456
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11717.920634651866 ----------
[1/200] Training loss: 0.17082370
[2/200] Training loss: 0.06032237
[3/200] Training loss: 0.05696763
[4/200] Training loss: 0.05378870
[5/200] Training loss: 0.05148367
[6/200] Training loss: 0.04834709
[7/200] Training loss: 0.04450374
[8/200] Training loss: 0.04032947
[9/200] Training loss: 0.03800595
[10/200] Training loss: 0.03373954
[50/200] Training loss: 0.01933984
[100/200] Training loss: 0.01461239
[150/200] Training loss: 0.01321221
[200/200] Training loss: 0.01254503
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 25849.307302130943 ----------
[1/200] Training loss: 0.16786475
[2/200] Training loss: 0.06200971
[3/200] Training loss: 0.05344969
[4/200] Training loss: 0.04878306
[5/200] Training loss: 0.04932483
[6/200] Training loss: 0.04622970
[7/200] Training loss: 0.04319633
[8/200] Training loss: 0.04144131
[9/200] Training loss: 0.03655799
[10/200] Training loss: 0.03619396
[50/200] Training loss: 0.01727539
[100/200] Training loss: 0.01523706
[150/200] Training loss: 0.01400306
[200/200] Training loss: 0.01245911
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7583.877108708974 ----------
[1/200] Training loss: 0.17493898
[2/200] Training loss: 0.06046363
[3/200] Training loss: 0.05107592
[4/200] Training loss: 0.05023499
[5/200] Training loss: 0.04813703
[6/200] Training loss: 0.04151927
[7/200] Training loss: 0.03857706
[8/200] Training loss: 0.03701199
[9/200] Training loss: 0.03738279
[10/200] Training loss: 0.03367060
[50/200] Training loss: 0.01653196
[100/200] Training loss: 0.01477767
[150/200] Training loss: 0.01281428
[200/200] Training loss: 0.01196967
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8517.065222246452 ----------
[1/200] Training loss: 0.18230307
[2/200] Training loss: 0.06713070
[3/200] Training loss: 0.05874785
[4/200] Training loss: 0.05395992
[5/200] Training loss: 0.05044937
[6/200] Training loss: 0.05075887
[7/200] Training loss: 0.04722185
[8/200] Training loss: 0.04418435
[9/200] Training loss: 0.04164446
[10/200] Training loss: 0.03767304
[50/200] Training loss: 0.01752355
[100/200] Training loss: 0.01529498
[150/200] Training loss: 0.01502860
[200/200] Training loss: 0.01341871
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12453.599640264658 ----------
[1/200] Training loss: 0.15134528
[2/200] Training loss: 0.05178226
[3/200] Training loss: 0.04683687
[4/200] Training loss: 0.04337948
[5/200] Training loss: 0.03753472
[6/200] Training loss: 0.03673867
[7/200] Training loss: 0.03412477
[8/200] Training loss: 0.03140571
[9/200] Training loss: 0.02922342
[10/200] Training loss: 0.02985103
[50/200] Training loss: 0.01762332
[100/200] Training loss: 0.01539259
[150/200] Training loss: 0.01465362
[200/200] Training loss: 0.01344976
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11792.702150058738 ----------
[1/200] Training loss: 0.15542206
[2/200] Training loss: 0.05563579
[3/200] Training loss: 0.05773052
[4/200] Training loss: 0.04798368
[5/200] Training loss: 0.04818634
[6/200] Training loss: 0.04781211
[7/200] Training loss: 0.04336823
[8/200] Training loss: 0.04097896
[9/200] Training loss: 0.04067965
[10/200] Training loss: 0.03593875
[50/200] Training loss: 0.01713163
[100/200] Training loss: 0.01548491
[150/200] Training loss: 0.01435505
[200/200] Training loss: 0.01324729
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 26194.685262472616 ----------
[1/200] Training loss: 0.16447485
[2/200] Training loss: 0.05674039
[3/200] Training loss: 0.05122017
[4/200] Training loss: 0.04766977
[5/200] Training loss: 0.04621515
[6/200] Training loss: 0.04159085
[7/200] Training loss: 0.04303038
[8/200] Training loss: 0.04066393
[9/200] Training loss: 0.03344539
[10/200] Training loss: 0.03563464
[50/200] Training loss: 0.01877241
[100/200] Training loss: 0.01600672
[150/200] Training loss: 0.01481595
[200/200] Training loss: 0.01392696
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10596.902943785039 ----------
[1/200] Training loss: 0.16261912
[2/200] Training loss: 0.05756934
[3/200] Training loss: 0.04743587
[4/200] Training loss: 0.04555074
[5/200] Training loss: 0.04319227
[6/200] Training loss: 0.03929662
[7/200] Training loss: 0.03787878
[8/200] Training loss: 0.03540231
[9/200] Training loss: 0.03329657
[10/200] Training loss: 0.03239541
[50/200] Training loss: 0.01898412
[100/200] Training loss: 0.01613522
[150/200] Training loss: 0.01530728
[200/200] Training loss: 0.01346686
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18161.72679014856 ----------
[1/200] Training loss: 0.15592851
[2/200] Training loss: 0.05319711
[3/200] Training loss: 0.05318373
[4/200] Training loss: 0.04754545
[5/200] Training loss: 0.04382624
[6/200] Training loss: 0.04201676
[7/200] Training loss: 0.03825527
[8/200] Training loss: 0.03862815
[9/200] Training loss: 0.03564425
[10/200] Training loss: 0.03548488
[50/200] Training loss: 0.02054687
[100/200] Training loss: 0.01595688
[150/200] Training loss: 0.01438284
[200/200] Training loss: 0.01350451
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13627.305529707624 ----------
[1/200] Training loss: 0.16535865
[2/200] Training loss: 0.05692296
[3/200] Training loss: 0.05404680
[4/200] Training loss: 0.05329826
[5/200] Training loss: 0.04644832
[6/200] Training loss: 0.04313382
[7/200] Training loss: 0.04207463
[8/200] Training loss: 0.03812360
[9/200] Training loss: 0.03677557
[10/200] Training loss: 0.03388363
[50/200] Training loss: 0.01903527
[100/200] Training loss: 0.01602118
[150/200] Training loss: 0.01470003
[200/200] Training loss: 0.01396668
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22684.8495697018 ----------
[1/200] Training loss: 0.15745026
[2/200] Training loss: 0.06167206
[3/200] Training loss: 0.05014481
[4/200] Training loss: 0.05067262
[5/200] Training loss: 0.04892734
[6/200] Training loss: 0.04209171
[7/200] Training loss: 0.04156200
[8/200] Training loss: 0.03731472
[9/200] Training loss: 0.03570218
[10/200] Training loss: 0.03561704
[50/200] Training loss: 0.01858842
[100/200] Training loss: 0.01582482
[150/200] Training loss: 0.01347944
[200/200] Training loss: 0.01277609
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 3821.009552461234 ----------
[1/200] Training loss: 0.14105083
[2/200] Training loss: 0.05509884
[3/200] Training loss: 0.04922467
[4/200] Training loss: 0.04707995
[5/200] Training loss: 0.04205587
[6/200] Training loss: 0.03794565
[7/200] Training loss: 0.03705720
[8/200] Training loss: 0.03467856
[9/200] Training loss: 0.03495670
[10/200] Training loss: 0.02961136
[50/200] Training loss: 0.01671598
[100/200] Training loss: 0.01417241
[150/200] Training loss: 0.01245129
[200/200] Training loss: 0.01196213
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14959.070559362972 ----------
[1/200] Training loss: 0.15313723
[2/200] Training loss: 0.06240182
[3/200] Training loss: 0.05219235
[4/200] Training loss: 0.05013440
[5/200] Training loss: 0.04654859
[6/200] Training loss: 0.04870163
[7/200] Training loss: 0.04369500
[8/200] Training loss: 0.04457492
[9/200] Training loss: 0.03934954
[10/200] Training loss: 0.04114347
[50/200] Training loss: 0.01790400
[100/200] Training loss: 0.01490630
[150/200] Training loss: 0.01317407
[200/200] Training loss: 0.01195651
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.7175923143023218 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11427.839340837794 ----------
[1/200] Training loss: 0.16019885
[2/200] Training loss: 0.06069449
[3/200] Training loss: 0.05422032
[4/200] Training loss: 0.04587750
[5/200] Training loss: 0.04169471
[6/200] Training loss: 0.04148343
[7/200] Training loss: 0.04033435
[8/200] Training loss: 0.03724082
[9/200] Training loss: 0.03381623
[10/200] Training loss: 0.03378348
[50/200] Training loss: 0.01879926
[100/200] Training loss: 0.01527383
[150/200] Training loss: 0.01326571
[200/200] Training loss: 0.01241982
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12521.965340951874 ----------
[1/200] Training loss: 0.14819018
[2/200] Training loss: 0.05752329
[3/200] Training loss: 0.05232042
[4/200] Training loss: 0.04620449
[5/200] Training loss: 0.04315193
[6/200] Training loss: 0.03968835
[7/200] Training loss: 0.03785921
[8/200] Training loss: 0.03461914
[9/200] Training loss: 0.03394917
[10/200] Training loss: 0.03141607
[50/200] Training loss: 0.01751609
[100/200] Training loss: 0.01516605
[150/200] Training loss: 0.01425594
[200/200] Training loss: 0.01257209
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18122.920735907886 ----------
[1/200] Training loss: 0.15783466
[2/200] Training loss: 0.05669632
[3/200] Training loss: 0.05219015
[4/200] Training loss: 0.04939957
[5/200] Training loss: 0.04780774
[6/200] Training loss: 0.04520383
[7/200] Training loss: 0.04226540
[8/200] Training loss: 0.04298656
[9/200] Training loss: 0.03689129
[10/200] Training loss: 0.03527467
[50/200] Training loss: 0.01863590
[100/200] Training loss: 0.01576971
[150/200] Training loss: 0.01436334
[200/200] Training loss: 0.01313073
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9387.396657220786 ----------
[1/200] Training loss: 0.15472546
[2/200] Training loss: 0.06025809
[3/200] Training loss: 0.05069132
[4/200] Training loss: 0.04935489
[5/200] Training loss: 0.04761386
[6/200] Training loss: 0.04510051
[7/200] Training loss: 0.04199479
[8/200] Training loss: 0.04410885
[9/200] Training loss: 0.03883321
[10/200] Training loss: 0.03779615
[50/200] Training loss: 0.01980863
[100/200] Training loss: 0.01469328
[150/200] Training loss: 0.01210478
[200/200] Training loss: 0.01073356
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15126.043236748994 ----------
[1/200] Training loss: 0.14233310
[2/200] Training loss: 0.05694206
[3/200] Training loss: 0.04941160
[4/200] Training loss: 0.04414410
[5/200] Training loss: 0.04509148
[6/200] Training loss: 0.04010005
[7/200] Training loss: 0.04087051
[8/200] Training loss: 0.03397836
[9/200] Training loss: 0.03613303
[10/200] Training loss: 0.03484780
[50/200] Training loss: 0.01715002
[100/200] Training loss: 0.01491454
[150/200] Training loss: 0.01272409
[200/200] Training loss: 0.01207828
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7926.701205419566 ----------
[1/200] Training loss: 0.16930907
[2/200] Training loss: 0.05877882
[3/200] Training loss: 0.05268868
[4/200] Training loss: 0.04725955
[5/200] Training loss: 0.04106405
[6/200] Training loss: 0.03574755
[7/200] Training loss: 0.03437415
[8/200] Training loss: 0.03303158
[9/200] Training loss: 0.02825575
[10/200] Training loss: 0.02886473
[50/200] Training loss: 0.01546957
[100/200] Training loss: 0.01158482
[150/200] Training loss: 0.01035344
[200/200] Training loss: 0.00961831
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22886.117364026602 ----------
[1/200] Training loss: 0.17698833
[2/200] Training loss: 0.05963814
[3/200] Training loss: 0.05468779
[4/200] Training loss: 0.04993733
[5/200] Training loss: 0.05100264
[6/200] Training loss: 0.04514071
[7/200] Training loss: 0.04189375
[8/200] Training loss: 0.04232179
[9/200] Training loss: 0.03789724
[10/200] Training loss: 0.03456543
[50/200] Training loss: 0.02066016
[100/200] Training loss: 0.01883600
[150/200] Training loss: 0.01726894
[200/200] Training loss: 0.01496141
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17630.462727903654 ----------
[1/200] Training loss: 0.26578826
[2/200] Training loss: 0.07041443
[3/200] Training loss: 0.05862268
[4/200] Training loss: 0.05461453
[5/200] Training loss: 0.05631203
[6/200] Training loss: 0.04958104
[7/200] Training loss: 0.04871202
[8/200] Training loss: 0.04695781
[9/200] Training loss: 0.04461762
[10/200] Training loss: 0.04483193
[50/200] Training loss: 0.02134455
[100/200] Training loss: 0.01809632
[150/200] Training loss: 0.01606137
[200/200] Training loss: 0.01556500
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15255.154145402792 ----------
[1/200] Training loss: 0.13497668
[2/200] Training loss: 0.05686795
[3/200] Training loss: 0.04383581
[4/200] Training loss: 0.04111168
[5/200] Training loss: 0.03882466
[6/200] Training loss: 0.03552346
[7/200] Training loss: 0.03254055
[8/200] Training loss: 0.03265268
[9/200] Training loss: 0.02844621
[10/200] Training loss: 0.02899782
[50/200] Training loss: 0.01915526
[100/200] Training loss: 0.01663043
[150/200] Training loss: 0.01499844
[200/200] Training loss: 0.01427429
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16656.50815747406 ----------
[1/200] Training loss: 0.16164344
[2/200] Training loss: 0.05543790
[3/200] Training loss: 0.05109112
[4/200] Training loss: 0.04584282
[5/200] Training loss: 0.04403698
[6/200] Training loss: 0.04257866
[7/200] Training loss: 0.04208559
[8/200] Training loss: 0.03783350
[9/200] Training loss: 0.03578517
[10/200] Training loss: 0.03520683
[50/200] Training loss: 0.01891305
[100/200] Training loss: 0.01632846
[150/200] Training loss: 0.01530021
[200/200] Training loss: 0.01384707
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15528.183924722169 ----------
[1/200] Training loss: 0.17488461
[2/200] Training loss: 0.06118388
[3/200] Training loss: 0.05401345
[4/200] Training loss: 0.05302666
[5/200] Training loss: 0.04827240
[6/200] Training loss: 0.04695540
[7/200] Training loss: 0.04465755
[8/200] Training loss: 0.04340381
[9/200] Training loss: 0.04118591
[10/200] Training loss: 0.04005591
[50/200] Training loss: 0.01746652
[100/200] Training loss: 0.01515103
[150/200] Training loss: 0.01311305
[200/200] Training loss: 0.01165337
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17565.099942784273 ----------
[1/200] Training loss: 0.17578280
[2/200] Training loss: 0.05945185
[3/200] Training loss: 0.05013265
[4/200] Training loss: 0.04927097
[5/200] Training loss: 0.04256192
[6/200] Training loss: 0.04090772
[7/200] Training loss: 0.03965702
[8/200] Training loss: 0.03514334
[9/200] Training loss: 0.03514237
[10/200] Training loss: 0.03353836
[50/200] Training loss: 0.01895712
[100/200] Training loss: 0.01616390
[150/200] Training loss: 0.01491145
[200/200] Training loss: 0.01422275
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7606.706251722883 ----------
[1/200] Training loss: 0.17319014
[2/200] Training loss: 0.06192004
[3/200] Training loss: 0.05236715
[4/200] Training loss: 0.05160680
[5/200] Training loss: 0.04574309
[6/200] Training loss: 0.04433659
[7/200] Training loss: 0.04138883
[8/200] Training loss: 0.03731574
[9/200] Training loss: 0.03499334
[10/200] Training loss: 0.03204020
[50/200] Training loss: 0.01951432
[100/200] Training loss: 0.01604365
[150/200] Training loss: 0.01478372
[200/200] Training loss: 0.01310632
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10167.622731002562 ----------
[1/200] Training loss: 0.14403800
[2/200] Training loss: 0.05656472
[3/200] Training loss: 0.04933099
[4/200] Training loss: 0.04775972
[5/200] Training loss: 0.04371702
[6/200] Training loss: 0.04014870
[7/200] Training loss: 0.03772674
[8/200] Training loss: 0.03569792
[9/200] Training loss: 0.03484696
[10/200] Training loss: 0.03124234
[50/200] Training loss: 0.01872712
[100/200] Training loss: 0.01518258
[150/200] Training loss: 0.01270201
[200/200] Training loss: 0.01223174
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13337.199706085232 ----------
[1/200] Training loss: 0.15731410
[2/200] Training loss: 0.05878343
[3/200] Training loss: 0.05231842
[4/200] Training loss: 0.04854040
[5/200] Training loss: 0.04303123
[6/200] Training loss: 0.03830402
[7/200] Training loss: 0.03982201
[8/200] Training loss: 0.03718693
[9/200] Training loss: 0.03511745
[10/200] Training loss: 0.03377643
[50/200] Training loss: 0.01647865
[100/200] Training loss: 0.01420516
[150/200] Training loss: 0.01153593
[200/200] Training loss: 0.01204491
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6154.323033445677 ----------
[1/200] Training loss: 0.17127811
[2/200] Training loss: 0.05535840
[3/200] Training loss: 0.05069562
[4/200] Training loss: 0.04703937
[5/200] Training loss: 0.04197490
[6/200] Training loss: 0.03616074
[7/200] Training loss: 0.03422743
[8/200] Training loss: 0.03349319
[9/200] Training loss: 0.03141201
[10/200] Training loss: 0.03075724
[50/200] Training loss: 0.01744193
[100/200] Training loss: 0.01499858
[150/200] Training loss: 0.01388819
[200/200] Training loss: 0.01301687
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12823.93761681645 ----------
[1/200] Training loss: 0.16673672
[2/200] Training loss: 0.05547468
[3/200] Training loss: 0.04923336
[4/200] Training loss: 0.04434001
[5/200] Training loss: 0.03759850
[6/200] Training loss: 0.04057085
[7/200] Training loss: 0.03625155
[8/200] Training loss: 0.03517056
[9/200] Training loss: 0.03378949
[10/200] Training loss: 0.02994708
[50/200] Training loss: 0.01660627
[100/200] Training loss: 0.01391938
[150/200] Training loss: 0.01279966
[200/200] Training loss: 0.01263006
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8524.263721870646 ----------
[1/200] Training loss: 0.14427750
[2/200] Training loss: 0.05880513
[3/200] Training loss: 0.05539415
[4/200] Training loss: 0.05225502
[5/200] Training loss: 0.04996640
[6/200] Training loss: 0.04810966
[7/200] Training loss: 0.04530723
[8/200] Training loss: 0.04276324
[9/200] Training loss: 0.03972259
[10/200] Training loss: 0.03885242
[50/200] Training loss: 0.01642299
[100/200] Training loss: 0.01439531
[150/200] Training loss: 0.01254920
[200/200] Training loss: 0.01257348
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15007.356062944598 ----------
[1/200] Training loss: 0.18644436
[2/200] Training loss: 0.06523419
[3/200] Training loss: 0.05280467
[4/200] Training loss: 0.05064719
[5/200] Training loss: 0.04803717
[6/200] Training loss: 0.04638491
[7/200] Training loss: 0.04358558
[8/200] Training loss: 0.04125695
[9/200] Training loss: 0.03815908
[10/200] Training loss: 0.03925162
[50/200] Training loss: 0.01790368
[100/200] Training loss: 0.01538438
[150/200] Training loss: 0.01381162
[200/200] Training loss: 0.01307628
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12331.98248458049 ----------
[1/200] Training loss: 0.15803891
[2/200] Training loss: 0.05808531
[3/200] Training loss: 0.05273784
[4/200] Training loss: 0.04870308
[5/200] Training loss: 0.04857801
[6/200] Training loss: 0.04503442
[7/200] Training loss: 0.04288499
[8/200] Training loss: 0.03939808
[9/200] Training loss: 0.03843886
[10/200] Training loss: 0.03777905
[50/200] Training loss: 0.01725992
[100/200] Training loss: 0.01540518
[150/200] Training loss: 0.01386019
[200/200] Training loss: 0.01299963
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14113.45003888135 ----------
[1/200] Training loss: 0.15645248
[2/200] Training loss: 0.05440297
[3/200] Training loss: 0.05115206
[4/200] Training loss: 0.05011402
[5/200] Training loss: 0.04103979
[6/200] Training loss: 0.03686307
[7/200] Training loss: 0.03589674
[8/200] Training loss: 0.03174173
[9/200] Training loss: 0.03159604
[10/200] Training loss: 0.02886580
[50/200] Training loss: 0.01679363
[100/200] Training loss: 0.01398060
[150/200] Training loss: 0.01206445
[200/200] Training loss: 0.01151236
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18167.310422844654 ----------
[1/200] Training loss: 0.19516798
[2/200] Training loss: 0.06573670
[3/200] Training loss: 0.05418176
[4/200] Training loss: 0.05155134
[5/200] Training loss: 0.04654463
[6/200] Training loss: 0.04200415
[7/200] Training loss: 0.04215506
[8/200] Training loss: 0.04056233
[9/200] Training loss: 0.03963071
[10/200] Training loss: 0.03614756
[50/200] Training loss: 0.01920501
[100/200] Training loss: 0.01715242
[150/200] Training loss: 0.01523578
[200/200] Training loss: 0.01405560
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11365.366338134465 ----------
[1/200] Training loss: 0.16526292
[2/200] Training loss: 0.05699818
[3/200] Training loss: 0.04995827
[4/200] Training loss: 0.04484244
[5/200] Training loss: 0.04639486
[6/200] Training loss: 0.03997920
[7/200] Training loss: 0.03718839
[8/200] Training loss: 0.03915457
[9/200] Training loss: 0.03486276
[10/200] Training loss: 0.03315274
[50/200] Training loss: 0.01834593
[100/200] Training loss: 0.01557079
[150/200] Training loss: 0.01334744
[200/200] Training loss: 0.01229590
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7863.595615238617 ----------
[1/200] Training loss: 0.19350142
[2/200] Training loss: 0.06100892
[3/200] Training loss: 0.05297405
[4/200] Training loss: 0.04869214
[5/200] Training loss: 0.04594088
[6/200] Training loss: 0.04211360
[7/200] Training loss: 0.04411097
[8/200] Training loss: 0.03859910
[9/200] Training loss: 0.03814976
[10/200] Training loss: 0.03166350
[50/200] Training loss: 0.01764407
[100/200] Training loss: 0.01586590
[150/200] Training loss: 0.01456951
[200/200] Training loss: 0.01313033
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7295.2718934937575 ----------
[1/200] Training loss: 0.16583785
[2/200] Training loss: 0.06458349
[3/200] Training loss: 0.05300874
[4/200] Training loss: 0.04185763
[5/200] Training loss: 0.03605100
[6/200] Training loss: 0.03799441
[7/200] Training loss: 0.03217662
[8/200] Training loss: 0.03078028
[9/200] Training loss: 0.03014155
[10/200] Training loss: 0.02743703
[50/200] Training loss: 0.01757249
[100/200] Training loss: 0.01494644
[150/200] Training loss: 0.01389603
[200/200] Training loss: 0.01262467
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12952.052501437754 ----------
[1/200] Training loss: 0.17937995
[2/200] Training loss: 0.06442032
[3/200] Training loss: 0.05471789
[4/200] Training loss: 0.05112030
[5/200] Training loss: 0.05101287
[6/200] Training loss: 0.04358511
[7/200] Training loss: 0.04239432
[8/200] Training loss: 0.04421495
[9/200] Training loss: 0.03939618
[10/200] Training loss: 0.03830495
[50/200] Training loss: 0.01894601
[100/200] Training loss: 0.01606438
[150/200] Training loss: 0.01400407
[200/200] Training loss: 0.01301414
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16923.3802770014 ----------
[1/200] Training loss: 0.18213525
[2/200] Training loss: 0.06492751
[3/200] Training loss: 0.05497857
[4/200] Training loss: 0.04748294
[5/200] Training loss: 0.04901142
[6/200] Training loss: 0.04395375
[7/200] Training loss: 0.04098354
[8/200] Training loss: 0.03939724
[9/200] Training loss: 0.03619696
[10/200] Training loss: 0.03240669
[50/200] Training loss: 0.01847735
[100/200] Training loss: 0.01574338
[150/200] Training loss: 0.01367420
[200/200] Training loss: 0.01282376
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14897.981608258215 ----------
[1/200] Training loss: 0.16373241
[2/200] Training loss: 0.05370568
[3/200] Training loss: 0.04809423
[4/200] Training loss: 0.04405810
[5/200] Training loss: 0.03776009
[6/200] Training loss: 0.03744075
[7/200] Training loss: 0.03273688
[8/200] Training loss: 0.03256903
[9/200] Training loss: 0.03168690
[10/200] Training loss: 0.02924199
[50/200] Training loss: 0.01685576
[100/200] Training loss: 0.01330132
[150/200] Training loss: 0.01224834
[200/200] Training loss: 0.01179549
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18576.875087053795 ----------
[1/200] Training loss: 0.16361817
[2/200] Training loss: 0.06402054
[3/200] Training loss: 0.05264384
[4/200] Training loss: 0.04751909
[5/200] Training loss: 0.04832985
[6/200] Training loss: 0.04346718
[7/200] Training loss: 0.04284876
[8/200] Training loss: 0.03922499
[9/200] Training loss: 0.03656549
[10/200] Training loss: 0.03458200
[50/200] Training loss: 0.01860102
[100/200] Training loss: 0.01513772
[150/200] Training loss: 0.01392208
[200/200] Training loss: 0.01267564
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12540.59647704207 ----------
[1/200] Training loss: 0.15973109
[2/200] Training loss: 0.06214190
[3/200] Training loss: 0.05301990
[4/200] Training loss: 0.04545591
[5/200] Training loss: 0.04302233
[6/200] Training loss: 0.04225370
[7/200] Training loss: 0.03850101
[8/200] Training loss: 0.03790179
[9/200] Training loss: 0.03594214
[10/200] Training loss: 0.03491178
[50/200] Training loss: 0.01873559
[100/200] Training loss: 0.01658487
[150/200] Training loss: 0.01526019
[200/200] Training loss: 0.01409578
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10842.326318645828 ----------
[1/200] Training loss: 0.13328620
[2/200] Training loss: 0.05981834
[3/200] Training loss: 0.05347780
[4/200] Training loss: 0.05051853
[5/200] Training loss: 0.04453846
[6/200] Training loss: 0.04299130
[7/200] Training loss: 0.03941111
[8/200] Training loss: 0.03472183
[9/200] Training loss: 0.03591505
[10/200] Training loss: 0.03193168
[50/200] Training loss: 0.01753311
[100/200] Training loss: 0.01461303
[150/200] Training loss: 0.01397716
[200/200] Training loss: 0.01181176
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5889.694049778816 ----------
[1/200] Training loss: 0.16120118
[2/200] Training loss: 0.05774736
[3/200] Training loss: 0.05110565
[4/200] Training loss: 0.04344291
[5/200] Training loss: 0.04280807
[6/200] Training loss: 0.03671633
[7/200] Training loss: 0.03868777
[8/200] Training loss: 0.03373466
[9/200] Training loss: 0.03666804
[10/200] Training loss: 0.03110820
[50/200] Training loss: 0.01710647
[100/200] Training loss: 0.01435911
[150/200] Training loss: 0.01306098
[200/200] Training loss: 0.01235403
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15614.400532841471 ----------
[1/200] Training loss: 0.15561963
[2/200] Training loss: 0.05698248
[3/200] Training loss: 0.05001802
[4/200] Training loss: 0.04623997
[5/200] Training loss: 0.04317237
[6/200] Training loss: 0.03993276
[7/200] Training loss: 0.03945645
[8/200] Training loss: 0.03608784
[9/200] Training loss: 0.03444819
[10/200] Training loss: 0.03136828
[50/200] Training loss: 0.01742747
[100/200] Training loss: 0.01453423
[150/200] Training loss: 0.01327762
[200/200] Training loss: 0.01193489
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5946.301371440906 ----------
[1/200] Training loss: 0.16751883
[2/200] Training loss: 0.05794465
[3/200] Training loss: 0.05067247
[4/200] Training loss: 0.04804258
[5/200] Training loss: 0.04676611
[6/200] Training loss: 0.04287785
[7/200] Training loss: 0.04033806
[8/200] Training loss: 0.04212102
[9/200] Training loss: 0.03760992
[10/200] Training loss: 0.03559385
[50/200] Training loss: 0.01868253
[100/200] Training loss: 0.01497240
[150/200] Training loss: 0.01437086
[200/200] Training loss: 0.01245866
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5335.007403931132 ----------
[1/200] Training loss: 0.13489755
[2/200] Training loss: 0.05784821
[3/200] Training loss: 0.04959589
[4/200] Training loss: 0.05321321
[5/200] Training loss: 0.04650562
[6/200] Training loss: 0.04269040
[7/200] Training loss: 0.04173061
[8/200] Training loss: 0.03964537
[9/200] Training loss: 0.03551140
[10/200] Training loss: 0.03622933
[50/200] Training loss: 0.01761037
[100/200] Training loss: 0.01447284
[150/200] Training loss: 0.01277934
[200/200] Training loss: 0.01114564
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15636.449213296477 ----------
[1/200] Training loss: 0.15411950
[2/200] Training loss: 0.05284596
[3/200] Training loss: 0.04347357
[4/200] Training loss: 0.04132430
[5/200] Training loss: 0.03655057
[6/200] Training loss: 0.03597367
[7/200] Training loss: 0.03292614
[8/200] Training loss: 0.03187652
[9/200] Training loss: 0.02816857
[10/200] Training loss: 0.02565043
[50/200] Training loss: 0.01651264
[100/200] Training loss: 0.01380283
[150/200] Training loss: 0.01319098
[200/200] Training loss: 0.01254348
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16019.134558396094 ----------
[1/200] Training loss: 0.15250666
[2/200] Training loss: 0.05932297
[3/200] Training loss: 0.05031275
[4/200] Training loss: 0.05042104
[5/200] Training loss: 0.04662605
[6/200] Training loss: 0.04154001
[7/200] Training loss: 0.04141054
[8/200] Training loss: 0.03986430
[9/200] Training loss: 0.03817728
[10/200] Training loss: 0.03568729
[50/200] Training loss: 0.01753513
[100/200] Training loss: 0.01436611
[150/200] Training loss: 0.01335887
[200/200] Training loss: 0.01260179
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9962.192128241655 ----------
[1/200] Training loss: 0.17341904
[2/200] Training loss: 0.05426815
[3/200] Training loss: 0.05015129
[4/200] Training loss: 0.04447514
[5/200] Training loss: 0.04283347
[6/200] Training loss: 0.04095893
[7/200] Training loss: 0.03671732
[8/200] Training loss: 0.03383447
[9/200] Training loss: 0.03250874
[10/200] Training loss: 0.03349435
[50/200] Training loss: 0.01876921
[100/200] Training loss: 0.01615500
[150/200] Training loss: 0.01408427
[200/200] Training loss: 0.01300273
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13879.03743060015 ----------
[1/200] Training loss: 0.15422484
[2/200] Training loss: 0.05447466
[3/200] Training loss: 0.05110342
[4/200] Training loss: 0.04571500
[5/200] Training loss: 0.04109134
[6/200] Training loss: 0.03974766
[7/200] Training loss: 0.03663042
[8/200] Training loss: 0.03432697
[9/200] Training loss: 0.03240060
[10/200] Training loss: 0.02948801
[50/200] Training loss: 0.01773460
[100/200] Training loss: 0.01543615
[150/200] Training loss: 0.01395860
[200/200] Training loss: 0.01302163
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20298.729812478414 ----------
[1/200] Training loss: 0.13119106
[2/200] Training loss: 0.06343167
[3/200] Training loss: 0.05308589
[4/200] Training loss: 0.04839142
[5/200] Training loss: 0.04114172
[6/200] Training loss: 0.03657984
[7/200] Training loss: 0.03732379
[8/200] Training loss: 0.03343026
[9/200] Training loss: 0.03099758
[10/200] Training loss: 0.02993969
[50/200] Training loss: 0.01853055
[100/200] Training loss: 0.01608622
[150/200] Training loss: 0.01486442
[200/200] Training loss: 0.01374358
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12104.492389191708 ----------
[1/200] Training loss: 0.17577626
[2/200] Training loss: 0.06071182
[3/200] Training loss: 0.05264333
[4/200] Training loss: 0.05133541
[5/200] Training loss: 0.05025697
[6/200] Training loss: 0.04607351
[7/200] Training loss: 0.04517285
[8/200] Training loss: 0.04222075
[9/200] Training loss: 0.04036871
[10/200] Training loss: 0.04163285
[50/200] Training loss: 0.01922329
[100/200] Training loss: 0.01638656
[150/200] Training loss: 0.01383641
[200/200] Training loss: 0.01243592
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7352.568530792488 ----------
[1/200] Training loss: 0.15954589
[2/200] Training loss: 0.06085750
[3/200] Training loss: 0.05057397
[4/200] Training loss: 0.04986692
[5/200] Training loss: 0.04798957
[6/200] Training loss: 0.04751806
[7/200] Training loss: 0.04383227
[8/200] Training loss: 0.03635961
[9/200] Training loss: 0.03608446
[10/200] Training loss: 0.03366708
[50/200] Training loss: 0.01918888
[100/200] Training loss: 0.01732887
[150/200] Training loss: 0.01587339
[200/200] Training loss: 0.01485660
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9707.688087284223 ----------
[1/200] Training loss: 0.17505301
[2/200] Training loss: 0.06514808
[3/200] Training loss: 0.05418715
[4/200] Training loss: 0.05002621
[5/200] Training loss: 0.05183800
[6/200] Training loss: 0.04668639
[7/200] Training loss: 0.04306431
[8/200] Training loss: 0.04484654
[9/200] Training loss: 0.03987764
[10/200] Training loss: 0.03674954
[50/200] Training loss: 0.01846236
[100/200] Training loss: 0.01649264
[150/200] Training loss: 0.01426873
[200/200] Training loss: 0.01335794
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13333.03206326303 ----------
[1/200] Training loss: 0.17705043
[2/200] Training loss: 0.06192501
[3/200] Training loss: 0.05177332
[4/200] Training loss: 0.04909559
[5/200] Training loss: 0.04816125
[6/200] Training loss: 0.04540390
[7/200] Training loss: 0.04157198
[8/200] Training loss: 0.04095716
[9/200] Training loss: 0.03880201
[10/200] Training loss: 0.03582572
[50/200] Training loss: 0.01855131
[100/200] Training loss: 0.01627889
[150/200] Training loss: 0.01526320
[200/200] Training loss: 0.01379823
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9228.615497462228 ----------
[1/200] Training loss: 0.17514252
[2/200] Training loss: 0.06542112
[3/200] Training loss: 0.05229759
[4/200] Training loss: 0.05160075
[5/200] Training loss: 0.04759394
[6/200] Training loss: 0.04543357
[7/200] Training loss: 0.04516444
[8/200] Training loss: 0.03989213
[9/200] Training loss: 0.03641290
[10/200] Training loss: 0.03743581
[50/200] Training loss: 0.01889602
[100/200] Training loss: 0.01615356
[150/200] Training loss: 0.01360441
[200/200] Training loss: 0.01259206
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8052.009438643251 ----------
[1/200] Training loss: 0.16085793
[2/200] Training loss: 0.06555505
[3/200] Training loss: 0.05689556
[4/200] Training loss: 0.05101975
[5/200] Training loss: 0.04775990
[6/200] Training loss: 0.04121339
[7/200] Training loss: 0.04241530
[8/200] Training loss: 0.03675739
[9/200] Training loss: 0.03623511
[10/200] Training loss: 0.03657359
[50/200] Training loss: 0.01752835
[100/200] Training loss: 0.01503456
[150/200] Training loss: 0.01361279
[200/200] Training loss: 0.01253360
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6330.520989618469 ----------
[1/200] Training loss: 0.15832236
[2/200] Training loss: 0.05991148
[3/200] Training loss: 0.05204303
[4/200] Training loss: 0.05026072
[5/200] Training loss: 0.04735322
[6/200] Training loss: 0.04384390
[7/200] Training loss: 0.04169293
[8/200] Training loss: 0.03987592
[9/200] Training loss: 0.03929334
[10/200] Training loss: 0.03838863
[50/200] Training loss: 0.01795070
[100/200] Training loss: 0.01596028
[150/200] Training loss: 0.01408341
[200/200] Training loss: 0.01287355
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9731.176290664967 ----------
[1/200] Training loss: 0.16754063
[2/200] Training loss: 0.05820052
[3/200] Training loss: 0.05349793
[4/200] Training loss: 0.04948690
[5/200] Training loss: 0.04573636
[6/200] Training loss: 0.04312047
[7/200] Training loss: 0.04036275
[8/200] Training loss: 0.03613795
[9/200] Training loss: 0.03289156
[10/200] Training loss: 0.03260995
[50/200] Training loss: 0.01778691
[100/200] Training loss: 0.01580036
[150/200] Training loss: 0.01453830
[200/200] Training loss: 0.01447736
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22092.14448621953 ----------
[1/200] Training loss: 0.13658545
[2/200] Training loss: 0.05699227
[3/200] Training loss: 0.05044846
[4/200] Training loss: 0.04775117
[5/200] Training loss: 0.04272313
[6/200] Training loss: 0.04334147
[7/200] Training loss: 0.04028912
[8/200] Training loss: 0.03543197
[9/200] Training loss: 0.03380172
[10/200] Training loss: 0.03384239
[50/200] Training loss: 0.01816906
[100/200] Training loss: 0.01473589
[150/200] Training loss: 0.01333930
[200/200] Training loss: 0.01143010
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14355.47226670025 ----------
[1/200] Training loss: 0.15094181
[2/200] Training loss: 0.05969877
[3/200] Training loss: 0.05139929
[4/200] Training loss: 0.04722571
[5/200] Training loss: 0.04641273
[6/200] Training loss: 0.04481100
[7/200] Training loss: 0.03961585
[8/200] Training loss: 0.04089764
[9/200] Training loss: 0.03538440
[10/200] Training loss: 0.03226566
[50/200] Training loss: 0.01752073
[100/200] Training loss: 0.01552364
[150/200] Training loss: 0.01357956
[200/200] Training loss: 0.01273796
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22781.82714358091 ----------
[1/200] Training loss: 0.17153808
[2/200] Training loss: 0.06233741
[3/200] Training loss: 0.05492096
[4/200] Training loss: 0.04781991
[5/200] Training loss: 0.04796804
[6/200] Training loss: 0.04680989
[7/200] Training loss: 0.03917608
[8/200] Training loss: 0.03724296
[9/200] Training loss: 0.03801164
[10/200] Training loss: 0.03437466
[50/200] Training loss: 0.01726490
[100/200] Training loss: 0.01512093
[150/200] Training loss: 0.01362940
[200/200] Training loss: 0.01306349
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6173.970197530921 ----------
[1/200] Training loss: 0.15277398
[2/200] Training loss: 0.06181900
[3/200] Training loss: 0.05426704
[4/200] Training loss: 0.05235896
[5/200] Training loss: 0.04937466
[6/200] Training loss: 0.04438342
[7/200] Training loss: 0.04104966
[8/200] Training loss: 0.04006375
[9/200] Training loss: 0.03389224
[10/200] Training loss: 0.03227901
[50/200] Training loss: 0.01583203
[100/200] Training loss: 0.01257218
[150/200] Training loss: 0.01201621
[200/200] Training loss: 0.01048443
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23839.0483031517 ----------
[1/200] Training loss: 0.15522656
[2/200] Training loss: 0.05768033
[3/200] Training loss: 0.05304421
[4/200] Training loss: 0.04677384
[5/200] Training loss: 0.04470909
[6/200] Training loss: 0.03930281
[7/200] Training loss: 0.04038340
[8/200] Training loss: 0.03626321
[9/200] Training loss: 0.03433418
[10/200] Training loss: 0.03414611
[50/200] Training loss: 0.01659788
[100/200] Training loss: 0.01437251
[150/200] Training loss: 0.01343093
[200/200] Training loss: 0.01173894
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15609.003555640571 ----------
[1/200] Training loss: 0.17195923
[2/200] Training loss: 0.06233613
[3/200] Training loss: 0.05560499
[4/200] Training loss: 0.05203816
[5/200] Training loss: 0.05278349
[6/200] Training loss: 0.04831473
[7/200] Training loss: 0.04753967
[8/200] Training loss: 0.04461042
[9/200] Training loss: 0.04262140
[10/200] Training loss: 0.03957186
[50/200] Training loss: 0.01819819
[100/200] Training loss: 0.01589595
[150/200] Training loss: 0.01496897
[200/200] Training loss: 0.01440352
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7664.020093919378 ----------
[1/200] Training loss: 0.14722068
[2/200] Training loss: 0.06183993
[3/200] Training loss: 0.05126992
[4/200] Training loss: 0.04693307
[5/200] Training loss: 0.04477210
[6/200] Training loss: 0.04036218
[7/200] Training loss: 0.04002523
[8/200] Training loss: 0.03894122
[9/200] Training loss: 0.03703594
[10/200] Training loss: 0.03627412
[50/200] Training loss: 0.01774726
[100/200] Training loss: 0.01455226
[150/200] Training loss: 0.01311845
[200/200] Training loss: 0.01243068
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5178.2439108253675 ----------
[1/200] Training loss: 0.15886673
[2/200] Training loss: 0.05722353
[3/200] Training loss: 0.05353705
[4/200] Training loss: 0.04966146
[5/200] Training loss: 0.04338572
[6/200] Training loss: 0.04379487
[7/200] Training loss: 0.04223246
[8/200] Training loss: 0.04114934
[9/200] Training loss: 0.03866405
[10/200] Training loss: 0.03853587
[50/200] Training loss: 0.01733864
[100/200] Training loss: 0.01582736
[150/200] Training loss: 0.01448476
[200/200] Training loss: 0.01273651
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7626.058746167643 ----------
[1/200] Training loss: 0.14438370
[2/200] Training loss: 0.05947018
[3/200] Training loss: 0.05281047
[4/200] Training loss: 0.04871003
[5/200] Training loss: 0.04399436
[6/200] Training loss: 0.03879521
[7/200] Training loss: 0.03886374
[8/200] Training loss: 0.03585638
[9/200] Training loss: 0.03334228
[10/200] Training loss: 0.03000638
[50/200] Training loss: 0.01788653
[100/200] Training loss: 0.01351757
[150/200] Training loss: 0.01233390
[200/200] Training loss: 0.01164596
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13143.118047099782 ----------
[1/200] Training loss: 0.17350475
[2/200] Training loss: 0.05738093
[3/200] Training loss: 0.05349922
[4/200] Training loss: 0.05190172
[5/200] Training loss: 0.04473875
[6/200] Training loss: 0.04223309
[7/200] Training loss: 0.03912764
[8/200] Training loss: 0.03757088
[9/200] Training loss: 0.03335763
[10/200] Training loss: 0.03389971
[50/200] Training loss: 0.01988311
[100/200] Training loss: 0.01532467
[150/200] Training loss: 0.01390724
[200/200] Training loss: 0.01264532
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17101.563905093593 ----------
[1/200] Training loss: 0.16559638
[2/200] Training loss: 0.06028772
[3/200] Training loss: 0.05404471
[4/200] Training loss: 0.04994665
[5/200] Training loss: 0.04751908
[6/200] Training loss: 0.04458144
[7/200] Training loss: 0.04140896
[8/200] Training loss: 0.04038632
[9/200] Training loss: 0.03899211
[10/200] Training loss: 0.03636350
[50/200] Training loss: 0.01765287
[100/200] Training loss: 0.01539512
[150/200] Training loss: 0.01446876
[200/200] Training loss: 0.01300658
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6245.746712763815 ----------
[1/200] Training loss: 0.15224190
[2/200] Training loss: 0.05719029
[3/200] Training loss: 0.04915842
[4/200] Training loss: 0.04995457
[5/200] Training loss: 0.04593063
[6/200] Training loss: 0.04318793
[7/200] Training loss: 0.04087533
[8/200] Training loss: 0.03926036
[9/200] Training loss: 0.03480966
[10/200] Training loss: 0.03281262
[50/200] Training loss: 0.01731100
[100/200] Training loss: 0.01461027
[150/200] Training loss: 0.01243581
[200/200] Training loss: 0.01221844
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13468.587453775544 ----------
[1/200] Training loss: 0.14499463
[2/200] Training loss: 0.05551521
[3/200] Training loss: 0.05050745
[4/200] Training loss: 0.04879877
[5/200] Training loss: 0.04436721
[6/200] Training loss: 0.03788150
[7/200] Training loss: 0.04026779
[8/200] Training loss: 0.03508583
[9/200] Training loss: 0.03527988
[10/200] Training loss: 0.03182707
[50/200] Training loss: 0.01761121
[100/200] Training loss: 0.01397944
[150/200] Training loss: 0.01274782
[200/200] Training loss: 0.01151356
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10397.26233197951 ----------
[1/200] Training loss: 0.18161162
[2/200] Training loss: 0.06347105
[3/200] Training loss: 0.05419758
[4/200] Training loss: 0.05304927
[5/200] Training loss: 0.04893661
[6/200] Training loss: 0.04691342
[7/200] Training loss: 0.04454472
[8/200] Training loss: 0.04278419
[9/200] Training loss: 0.03967501
[10/200] Training loss: 0.03706678
[50/200] Training loss: 0.01838260
[100/200] Training loss: 0.01661735
[150/200] Training loss: 0.01432939
[200/200] Training loss: 0.01261118
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16240.154679066329 ----------
[1/200] Training loss: 0.18874837
[2/200] Training loss: 0.06040961
[3/200] Training loss: 0.05329117
[4/200] Training loss: 0.05267026
[5/200] Training loss: 0.04532036
[6/200] Training loss: 0.04629444
[7/200] Training loss: 0.04402442
[8/200] Training loss: 0.04108793
[9/200] Training loss: 0.03911579
[10/200] Training loss: 0.03479358
[50/200] Training loss: 0.01939467
[100/200] Training loss: 0.01707464
[150/200] Training loss: 0.01538832
[200/200] Training loss: 0.01474519
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12713.697809842737 ----------
[1/200] Training loss: 0.14411165
[2/200] Training loss: 0.05993699
[3/200] Training loss: 0.05552439
[4/200] Training loss: 0.05027001
[5/200] Training loss: 0.04808337
[6/200] Training loss: 0.04709720
[7/200] Training loss: 0.04427214
[8/200] Training loss: 0.04019715
[9/200] Training loss: 0.04151210
[10/200] Training loss: 0.03841189
[50/200] Training loss: 0.01728231
[100/200] Training loss: 0.01395909
[150/200] Training loss: 0.01295947
[200/200] Training loss: 0.01169341
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17906.16519526166 ----------
[1/200] Training loss: 0.13922043
[2/200] Training loss: 0.05150925
[3/200] Training loss: 0.04828237
[4/200] Training loss: 0.04711603
[5/200] Training loss: 0.04302145
[6/200] Training loss: 0.03954734
[7/200] Training loss: 0.03748741
[8/200] Training loss: 0.03656056
[9/200] Training loss: 0.03724988
[10/200] Training loss: 0.03302424
[50/200] Training loss: 0.01654609
[100/200] Training loss: 0.01308217
[150/200] Training loss: 0.01226985
[200/200] Training loss: 0.01132485
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8612.931208363387 ----------
[1/200] Training loss: 0.16912515
[2/200] Training loss: 0.05801777
[3/200] Training loss: 0.05559861
[4/200] Training loss: 0.04732623
[5/200] Training loss: 0.04467848
[6/200] Training loss: 0.04277876
[7/200] Training loss: 0.03565763
[8/200] Training loss: 0.03647498
[9/200] Training loss: 0.03355020
[10/200] Training loss: 0.03138174
[50/200] Training loss: 0.01724015
[100/200] Training loss: 0.01363509
[150/200] Training loss: 0.01275579
[200/200] Training loss: 0.01221243
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21913.111326326984 ----------
[1/200] Training loss: 0.16420865
[2/200] Training loss: 0.05956505
[3/200] Training loss: 0.05467517
[4/200] Training loss: 0.04928893
[5/200] Training loss: 0.04694966
[6/200] Training loss: 0.04594371
[7/200] Training loss: 0.04329161
[8/200] Training loss: 0.03948715
[9/200] Training loss: 0.03837494
[10/200] Training loss: 0.03625853
[50/200] Training loss: 0.01691792
[100/200] Training loss: 0.01462680
[150/200] Training loss: 0.01343374
[200/200] Training loss: 0.01245260
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5207.146819516423 ----------
[1/200] Training loss: 0.18070626
[2/200] Training loss: 0.06925670
[3/200] Training loss: 0.05598325
[4/200] Training loss: 0.05552968
[5/200] Training loss: 0.04932391
[6/200] Training loss: 0.04743137
[7/200] Training loss: 0.04718243
[8/200] Training loss: 0.04534553
[9/200] Training loss: 0.04170117
[10/200] Training loss: 0.03939159
[50/200] Training loss: 0.01880385
[100/200] Training loss: 0.01676892
[150/200] Training loss: 0.01521849
[200/200] Training loss: 0.01389165
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17698.796343254533 ----------
[1/200] Training loss: 0.15465478
[2/200] Training loss: 0.05510155
[3/200] Training loss: 0.05334988
[4/200] Training loss: 0.05002686
[5/200] Training loss: 0.04733960
[6/200] Training loss: 0.04382720
[7/200] Training loss: 0.04180295
[8/200] Training loss: 0.04122228
[9/200] Training loss: 0.03747902
[10/200] Training loss: 0.03524315
[50/200] Training loss: 0.01795946
[100/200] Training loss: 0.01461416
[150/200] Training loss: 0.01282106
[200/200] Training loss: 0.01147440
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14593.076714661647 ----------
[1/200] Training loss: 0.15027505
[2/200] Training loss: 0.06236025
[3/200] Training loss: 0.05337433
[4/200] Training loss: 0.04837826
[5/200] Training loss: 0.04236564
[6/200] Training loss: 0.04052522
[7/200] Training loss: 0.04043689
[8/200] Training loss: 0.03894023
[9/200] Training loss: 0.03418633
[10/200] Training loss: 0.03200527
[50/200] Training loss: 0.01880656
[100/200] Training loss: 0.01479965
[150/200] Training loss: 0.01367800
[200/200] Training loss: 0.01228183
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13823.685181600455 ----------
[1/200] Training loss: 0.17296838
[2/200] Training loss: 0.05595741
[3/200] Training loss: 0.05425422
[4/200] Training loss: 0.04796719
[5/200] Training loss: 0.04418514
[6/200] Training loss: 0.04382464
[7/200] Training loss: 0.03938204
[8/200] Training loss: 0.03887430
[9/200] Training loss: 0.03523405
[10/200] Training loss: 0.03597496
[50/200] Training loss: 0.01904604
[100/200] Training loss: 0.01518799
[150/200] Training loss: 0.01320574
[200/200] Training loss: 0.01240791
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4244.6205955303 ----------
[1/200] Training loss: 0.14343813
[2/200] Training loss: 0.06062753
[3/200] Training loss: 0.05234467
[4/200] Training loss: 0.05066509
[5/200] Training loss: 0.04705784
[6/200] Training loss: 0.04127567
[7/200] Training loss: 0.04096744
[8/200] Training loss: 0.03664181
[9/200] Training loss: 0.03763546
[10/200] Training loss: 0.03258251
[50/200] Training loss: 0.01627431
[100/200] Training loss: 0.01476541
[150/200] Training loss: 0.01334631
[200/200] Training loss: 0.01229238
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 32797.9218853878 ----------
[1/200] Training loss: 0.15754347
[2/200] Training loss: 0.05394492
[3/200] Training loss: 0.05162292
[4/200] Training loss: 0.04762122
[5/200] Training loss: 0.04259967
[6/200] Training loss: 0.03889560
[7/200] Training loss: 0.03862517
[8/200] Training loss: 0.03807018
[9/200] Training loss: 0.03638751
[10/200] Training loss: 0.03676289
[50/200] Training loss: 0.01830883
[100/200] Training loss: 0.01591954
[150/200] Training loss: 0.01473964
[200/200] Training loss: 0.01369603
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11762.19163251475 ----------
[1/200] Training loss: 0.15323048
[2/200] Training loss: 0.05636026
[3/200] Training loss: 0.05536969
[4/200] Training loss: 0.04935103
[5/200] Training loss: 0.04553302
[6/200] Training loss: 0.04314210
[7/200] Training loss: 0.04005375
[8/200] Training loss: 0.03728249
[9/200] Training loss: 0.03816550
[10/200] Training loss: 0.03308342
[50/200] Training loss: 0.01725854
[100/200] Training loss: 0.01333276
[150/200] Training loss: 0.01296382
[200/200] Training loss: 0.01175310
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16632.507928752057 ----------
[1/200] Training loss: 0.13510528
[2/200] Training loss: 0.05772938
[3/200] Training loss: 0.05174188
[4/200] Training loss: 0.04861972
[5/200] Training loss: 0.04651273
[6/200] Training loss: 0.04047933
[7/200] Training loss: 0.04181145
[8/200] Training loss: 0.03761455
[9/200] Training loss: 0.03785171
[10/200] Training loss: 0.03246101
[50/200] Training loss: 0.01854183
[100/200] Training loss: 0.01520065
[150/200] Training loss: 0.01401089
[200/200] Training loss: 0.01346172
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 24260.626207911453 ----------
[1/200] Training loss: 0.17759316
[2/200] Training loss: 0.05993252
[3/200] Training loss: 0.05292218
[4/200] Training loss: 0.05071066
[5/200] Training loss: 0.04874239
[6/200] Training loss: 0.04672922
[7/200] Training loss: 0.04480575
[8/200] Training loss: 0.04093959
[9/200] Training loss: 0.04148924
[10/200] Training loss: 0.03869522
[50/200] Training loss: 0.01882910
[100/200] Training loss: 0.01607030
[150/200] Training loss: 0.01482558
[200/200] Training loss: 0.01341123
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17210.715731775945 ----------
[1/200] Training loss: 0.16930595
[2/200] Training loss: 0.06132743
[3/200] Training loss: 0.05108087
[4/200] Training loss: 0.05064262
[5/200] Training loss: 0.04725190
[6/200] Training loss: 0.04591100
[7/200] Training loss: 0.04014512
[8/200] Training loss: 0.03961442
[9/200] Training loss: 0.03843701
[10/200] Training loss: 0.03402064
[50/200] Training loss: 0.01673362
[100/200] Training loss: 0.01531521
[150/200] Training loss: 0.01331986
[200/200] Training loss: 0.01222345
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11770.560564391146 ----------
[1/200] Training loss: 0.15012237
[2/200] Training loss: 0.05908468
[3/200] Training loss: 0.05561221
[4/200] Training loss: 0.04976537
[5/200] Training loss: 0.04694731
[6/200] Training loss: 0.04379425
[7/200] Training loss: 0.04253695
[8/200] Training loss: 0.04199319
[9/200] Training loss: 0.03810362
[10/200] Training loss: 0.03548968
[50/200] Training loss: 0.01974669
[100/200] Training loss: 0.01606289
[150/200] Training loss: 0.01370100
[200/200] Training loss: 0.01241829
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6624.9821131834005 ----------
[1/200] Training loss: 0.16969653
[2/200] Training loss: 0.06125139
[3/200] Training loss: 0.05417483
[4/200] Training loss: 0.05188179
[5/200] Training loss: 0.04835423
[6/200] Training loss: 0.04696528
[7/200] Training loss: 0.04522015
[8/200] Training loss: 0.04238592
[9/200] Training loss: 0.03997971
[10/200] Training loss: 0.03738820
[50/200] Training loss: 0.01759228
[100/200] Training loss: 0.01562448
[150/200] Training loss: 0.01486171
[200/200] Training loss: 0.01345435
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16447.28403110982 ----------
[1/200] Training loss: 0.16136938
[2/200] Training loss: 0.05920690
[3/200] Training loss: 0.05207244
[4/200] Training loss: 0.04428606
[5/200] Training loss: 0.04244844
[6/200] Training loss: 0.03678914
[7/200] Training loss: 0.03454966
[8/200] Training loss: 0.03236212
[9/200] Training loss: 0.03306734
[10/200] Training loss: 0.03081778
[50/200] Training loss: 0.01893266
[100/200] Training loss: 0.01693764
[150/200] Training loss: 0.01595503
[200/200] Training loss: 0.01506007
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12454.7558787798 ----------
[1/200] Training loss: 0.14392185
[2/200] Training loss: 0.05409769
[3/200] Training loss: 0.04994955
[4/200] Training loss: 0.04712316
[5/200] Training loss: 0.04582498
[6/200] Training loss: 0.04180804
[7/200] Training loss: 0.03644129
[8/200] Training loss: 0.03424830
[9/200] Training loss: 0.03614686
[10/200] Training loss: 0.02999159
[50/200] Training loss: 0.01851458
[100/200] Training loss: 0.01526593
[150/200] Training loss: 0.01376337
[200/200] Training loss: 0.01234419
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10338.30160132698 ----------
[1/200] Training loss: 0.16163433
[2/200] Training loss: 0.05837238
[3/200] Training loss: 0.04888080
[4/200] Training loss: 0.04552569
[5/200] Training loss: 0.04678171
[6/200] Training loss: 0.04108689
[7/200] Training loss: 0.03913137
[8/200] Training loss: 0.03916469
[9/200] Training loss: 0.03444268
[10/200] Training loss: 0.03333452
[50/200] Training loss: 0.01735180
[100/200] Training loss: 0.01566489
[150/200] Training loss: 0.01359968
[200/200] Training loss: 0.01354176
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13404.814060627623 ----------
[1/200] Training loss: 0.15823543
[2/200] Training loss: 0.06159413
[3/200] Training loss: 0.05109521
[4/200] Training loss: 0.04788976
[5/200] Training loss: 0.04449926
[6/200] Training loss: 0.04200701
[7/200] Training loss: 0.03758287
[8/200] Training loss: 0.03848833
[9/200] Training loss: 0.03447359
[10/200] Training loss: 0.03225246
[50/200] Training loss: 0.01584087
[100/200] Training loss: 0.01409466
[150/200] Training loss: 0.01216890
[200/200] Training loss: 0.01166045
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17334.73461002504 ----------
[1/200] Training loss: 0.16635373
[2/200] Training loss: 0.05712023
[3/200] Training loss: 0.05206094
[4/200] Training loss: 0.04652988
[5/200] Training loss: 0.04514238
[6/200] Training loss: 0.04270617
[7/200] Training loss: 0.04172208
[8/200] Training loss: 0.03746322
[9/200] Training loss: 0.03629318
[10/200] Training loss: 0.03313248
[50/200] Training loss: 0.01885063
[100/200] Training loss: 0.01616094
[150/200] Training loss: 0.01403246
[200/200] Training loss: 0.01305816
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 3992.567970617407 ----------
[1/200] Training loss: 0.14256425
[2/200] Training loss: 0.05405824
[3/200] Training loss: 0.05178325
[4/200] Training loss: 0.04481698
[5/200] Training loss: 0.04666938
[6/200] Training loss: 0.04252828
[7/200] Training loss: 0.03924861
[8/200] Training loss: 0.03756329
[9/200] Training loss: 0.03612744
[10/200] Training loss: 0.03732959
[50/200] Training loss: 0.01821071
[100/200] Training loss: 0.01434057
[150/200] Training loss: 0.01308977
[200/200] Training loss: 0.01220352
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7259.7264411271035 ----------
[1/200] Training loss: 0.19599564
[2/200] Training loss: 0.06452495
[3/200] Training loss: 0.05594589
[4/200] Training loss: 0.05237124
[5/200] Training loss: 0.04633421
[6/200] Training loss: 0.04422218
[7/200] Training loss: 0.04159372
[8/200] Training loss: 0.04182795
[9/200] Training loss: 0.03904053
[10/200] Training loss: 0.03554272
[50/200] Training loss: 0.01782650
[100/200] Training loss: 0.01570061
[150/200] Training loss: 0.01424775
[200/200] Training loss: 0.01314539
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4434.231838774333 ----------
[1/200] Training loss: 0.16169295
[2/200] Training loss: 0.06540572
[3/200] Training loss: 0.05260599
[4/200] Training loss: 0.05069731
[5/200] Training loss: 0.04153107
[6/200] Training loss: 0.03613775
[7/200] Training loss: 0.03548377
[8/200] Training loss: 0.03377653
[9/200] Training loss: 0.03145700
[10/200] Training loss: 0.03106272
[50/200] Training loss: 0.01742505
[100/200] Training loss: 0.01520744
[150/200] Training loss: 0.01445619
[200/200] Training loss: 0.01293213
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5428.185516358114 ----------
[1/200] Training loss: 0.17918443
[2/200] Training loss: 0.06534313
[3/200] Training loss: 0.05801401
[4/200] Training loss: 0.05157752
[5/200] Training loss: 0.05039556
[6/200] Training loss: 0.04757861
[7/200] Training loss: 0.04383006
[8/200] Training loss: 0.04454840
[9/200] Training loss: 0.03871343
[10/200] Training loss: 0.03789496
[50/200] Training loss: 0.01756034
[100/200] Training loss: 0.01488148
[150/200] Training loss: 0.01371730
[200/200] Training loss: 0.01274951
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6423.407506923409 ----------
[1/200] Training loss: 0.17363639
[2/200] Training loss: 0.05652012
[3/200] Training loss: 0.05201059
[4/200] Training loss: 0.04681980
[5/200] Training loss: 0.04277168
[6/200] Training loss: 0.03971990
[7/200] Training loss: 0.03711187
[8/200] Training loss: 0.03472241
[9/200] Training loss: 0.03394062
[10/200] Training loss: 0.03283829
[50/200] Training loss: 0.01928749
[100/200] Training loss: 0.01597246
[150/200] Training loss: 0.01457359
[200/200] Training loss: 0.01338525
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18549.944689944496 ----------
[1/200] Training loss: 0.17672948
[2/200] Training loss: 0.05957532
[3/200] Training loss: 0.05338966
[4/200] Training loss: 0.04988706
[5/200] Training loss: 0.04645761
[6/200] Training loss: 0.04220934
[7/200] Training loss: 0.04154349
[8/200] Training loss: 0.03850844
[9/200] Training loss: 0.03480787
[10/200] Training loss: 0.03660813
[50/200] Training loss: 0.01893711
[100/200] Training loss: 0.01580542
[150/200] Training loss: 0.01333984
[200/200] Training loss: 0.01199147
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17087.63670025788 ----------
[1/200] Training loss: 0.17828050
[2/200] Training loss: 0.06077372
[3/200] Training loss: 0.05446702
[4/200] Training loss: 0.05023347
[5/200] Training loss: 0.04941935
[6/200] Training loss: 0.04715270
[7/200] Training loss: 0.04029607
[8/200] Training loss: 0.04066193
[9/200] Training loss: 0.03673031
[10/200] Training loss: 0.03886422
[50/200] Training loss: 0.01895677
[100/200] Training loss: 0.01614779
[150/200] Training loss: 0.01428949
[200/200] Training loss: 0.01425856
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6248.185016466782 ----------
[1/200] Training loss: 0.17787409
[2/200] Training loss: 0.06262691
[3/200] Training loss: 0.05849775
[4/200] Training loss: 0.05325037
[5/200] Training loss: 0.05266142
[6/200] Training loss: 0.05203575
[7/200] Training loss: 0.04837743
[8/200] Training loss: 0.04743644
[9/200] Training loss: 0.04492397
[10/200] Training loss: 0.04179820
[50/200] Training loss: 0.01881896
[100/200] Training loss: 0.01507096
[150/200] Training loss: 0.01369089
[200/200] Training loss: 0.01248194
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 25263.208347318043 ----------
[1/200] Training loss: 0.15868412
[2/200] Training loss: 0.06335373
[3/200] Training loss: 0.05293826
[4/200] Training loss: 0.04915465
[5/200] Training loss: 0.04495604
[6/200] Training loss: 0.04302049
[7/200] Training loss: 0.04165046
[8/200] Training loss: 0.03855825
[9/200] Training loss: 0.03552520
[10/200] Training loss: 0.03459133
[50/200] Training loss: 0.01826539
[100/200] Training loss: 0.01589680
[150/200] Training loss: 0.01410387
[200/200] Training loss: 0.01283519
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.5696294478685805 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15517.136849303095 ----------
[1/50] Training loss: 0.18351859
[2/50] Training loss: 0.06635041
[3/50] Training loss: 0.05898897
[4/50] Training loss: 0.05445876
[5/50] Training loss: 0.05689531
[6/50] Training loss: 0.05433352
[7/50] Training loss: 0.05403247
[8/50] Training loss: 0.04955663
[9/50] Training loss: 0.04694925
[10/50] Training loss: 0.04243642
[50/50] Training loss: 0.01990531
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 50
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12345.633722089766 ----------
[1/200] Training loss: 0.19156536
[2/200] Training loss: 0.06660413
[3/200] Training loss: 0.05775188
[4/200] Training loss: 0.05444540
[5/200] Training loss: 0.04734239
[6/200] Training loss: 0.04562920
[7/200] Training loss: 0.04277112
[8/200] Training loss: 0.04093482
[9/200] Training loss: 0.03875580
[10/200] Training loss: 0.03496076
[50/200] Training loss: 0.01838000
[100/200] Training loss: 0.01563453
[150/200] Training loss: 0.01511500
[200/200] Training loss: 0.01362051
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16276.254114506813 ----------
[1/200] Training loss: 0.17768538
[2/200] Training loss: 0.05780684
[3/200] Training loss: 0.05633157
[4/200] Training loss: 0.04897889
[5/200] Training loss: 0.04910194
[6/200] Training loss: 0.04559897
[7/200] Training loss: 0.04357097
[8/200] Training loss: 0.03963435
[9/200] Training loss: 0.03944447
[10/200] Training loss: 0.03586286
[50/200] Training loss: 0.01767992
[100/200] Training loss: 0.01425485
[150/200] Training loss: 0.01343875
[200/200] Training loss: 0.01186332
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15417.423390437198 ----------
[1/200] Training loss: 0.17085735
[2/200] Training loss: 0.06345731
[3/200] Training loss: 0.05489895
[4/200] Training loss: 0.05223291
[5/200] Training loss: 0.04935290
[6/200] Training loss: 0.04586658
[7/200] Training loss: 0.04560678
[8/200] Training loss: 0.04139265
[9/200] Training loss: 0.04202791
[10/200] Training loss: 0.03632168
[50/200] Training loss: 0.01818228
[100/200] Training loss: 0.01525741
[150/200] Training loss: 0.01316745
[200/200] Training loss: 0.01261506
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5487.504168563337 ----------
[1/200] Training loss: 0.12828507
[2/200] Training loss: 0.05936138
[3/200] Training loss: 0.05045344
[4/200] Training loss: 0.04903627
[5/200] Training loss: 0.04271038
[6/200] Training loss: 0.04202436
[7/200] Training loss: 0.04245648
[8/200] Training loss: 0.03897322
[9/200] Training loss: 0.03339826
[10/200] Training loss: 0.03446538
[50/200] Training loss: 0.01771513
[100/200] Training loss: 0.01464571
[150/200] Training loss: 0.01291767
[200/200] Training loss: 0.01290625
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6685.404998951073 ----------
[1/200] Training loss: 0.18664523
[2/200] Training loss: 0.05792795
[3/200] Training loss: 0.05627535
[4/200] Training loss: 0.04954768
[5/200] Training loss: 0.04770277
[6/200] Training loss: 0.04410759
[7/200] Training loss: 0.04261139
[8/200] Training loss: 0.04194800
[9/200] Training loss: 0.03903887
[10/200] Training loss: 0.03677943
[50/200] Training loss: 0.01772445
[100/200] Training loss: 0.01531129
[150/200] Training loss: 0.01430101
[200/200] Training loss: 0.01323551
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6013.954439468261 ----------
[1/200] Training loss: 0.15579398
[2/200] Training loss: 0.05720996
[3/200] Training loss: 0.05495099
[4/200] Training loss: 0.05023313
[5/200] Training loss: 0.05043012
[6/200] Training loss: 0.04527674
[7/200] Training loss: 0.03935471
[8/200] Training loss: 0.03871900
[9/200] Training loss: 0.03717255
[10/200] Training loss: 0.03442811
[50/200] Training loss: 0.01767246
[100/200] Training loss: 0.01517516
[150/200] Training loss: 0.01381641
[200/200] Training loss: 0.01337038
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10756.163256477656 ----------
[1/200] Training loss: 0.19002122
[2/200] Training loss: 0.06148268
[3/200] Training loss: 0.05475853
[4/200] Training loss: 0.05517801
[5/200] Training loss: 0.05127650
[6/200] Training loss: 0.05175080
[7/200] Training loss: 0.05083974
[8/200] Training loss: 0.05033057
[9/200] Training loss: 0.04839731
[10/200] Training loss: 0.04732342
[50/200] Training loss: 0.02002824
[100/200] Training loss: 0.01514049
[150/200] Training loss: 0.01324782
[200/200] Training loss: 0.01097084
---batch_size---: 16 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13641.937105851206 ----------
[1/200] Training loss: 0.14906804
[2/200] Training loss: 0.05499126
[3/200] Training loss: 0.05300552
[4/200] Training loss: 0.04789421
[5/200] Training loss: 0.04135634
[6/200] Training loss: 0.04079105
[7/200] Training loss: 0.03879113
[8/200] Training loss: 0.03660140
[9/200] Training loss: 0.03708123
[10/200] Training loss: 0.03093677
[50/200] Training loss: 0.01760444
[100/200] Training loss: 0.01557655
[150/200] Training loss: 0.01373847
[200/200] Training loss: 0.01243973
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 24081.974669864594 ----------
[1/200] Training loss: 0.14141448
[2/200] Training loss: 0.05507586
[3/200] Training loss: 0.04800358
[4/200] Training loss: 0.04594964
[5/200] Training loss: 0.04039461
[6/200] Training loss: 0.04003586
[7/200] Training loss: 0.03939006
[8/200] Training loss: 0.03511849
[9/200] Training loss: 0.03081904
[10/200] Training loss: 0.02883199
[50/200] Training loss: 0.01823428
[100/200] Training loss: 0.01584850
[150/200] Training loss: 0.01449890
[200/200] Training loss: 0.01428513
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19483.870662678914 ----------
[1/200] Training loss: 0.17557133
[2/200] Training loss: 0.06153969
[3/200] Training loss: 0.05593574
[4/200] Training loss: 0.05380957
[5/200] Training loss: 0.04932270
[6/200] Training loss: 0.04820718
[7/200] Training loss: 0.04919827
[8/200] Training loss: 0.04428886
[9/200] Training loss: 0.04306288
[10/200] Training loss: 0.04005600
[50/200] Training loss: 0.02048298
[100/200] Training loss: 0.01717433
[150/200] Training loss: 0.01477918
[200/200] Training loss: 0.01375776
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22844.127823140894 ----------
[1/200] Training loss: 0.15684879
[2/200] Training loss: 0.05994333
[3/200] Training loss: 0.05301114
[4/200] Training loss: 0.05118196
[5/200] Training loss: 0.04446467
[6/200] Training loss: 0.04098449
[7/200] Training loss: 0.03527316
[8/200] Training loss: 0.03521256
[9/200] Training loss: 0.03314042
[10/200] Training loss: 0.03036654
[50/200] Training loss: 0.01577279
[100/200] Training loss: 0.01333809
[150/200] Training loss: 0.01184935
[200/200] Training loss: 0.01067178
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16917.454181997953 ----------
[1/200] Training loss: 0.16066408
[2/200] Training loss: 0.06257241
[3/200] Training loss: 0.05621060
[4/200] Training loss: 0.04971259
[5/200] Training loss: 0.04628659
[6/200] Training loss: 0.04055608
[7/200] Training loss: 0.03979804
[8/200] Training loss: 0.03917936
[9/200] Training loss: 0.03658852
[10/200] Training loss: 0.03503130
[50/200] Training loss: 0.01693162
[100/200] Training loss: 0.01498777
[150/200] Training loss: 0.01350788
[200/200] Training loss: 0.01226271
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21618.444717416653 ----------
[1/200] Training loss: 0.12752802
[2/200] Training loss: 0.05445578
[3/200] Training loss: 0.05028872
[4/200] Training loss: 0.04548657
[5/200] Training loss: 0.04238705
[6/200] Training loss: 0.04043001
[7/200] Training loss: 0.03815327
[8/200] Training loss: 0.03460397
[9/200] Training loss: 0.03236679
[10/200] Training loss: 0.03120533
[50/200] Training loss: 0.01786981
[100/200] Training loss: 0.01481674
[150/200] Training loss: 0.01330963
[200/200] Training loss: 0.01276061
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12622.829476785306 ----------
[1/200] Training loss: 0.17328062
[2/200] Training loss: 0.06068025
[3/200] Training loss: 0.05448268
[4/200] Training loss: 0.05308446
[5/200] Training loss: 0.04716979
[6/200] Training loss: 0.04708536
[7/200] Training loss: 0.04507935
[8/200] Training loss: 0.04186025
[9/200] Training loss: 0.04304043
[10/200] Training loss: 0.04026843
[50/200] Training loss: 0.01904020
[100/200] Training loss: 0.01616722
[150/200] Training loss: 0.01454370
[200/200] Training loss: 0.01320279
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8888.289824257532 ----------
[1/200] Training loss: 0.16456598
[2/200] Training loss: 0.05813304
[3/200] Training loss: 0.04850442
[4/200] Training loss: 0.04448616
[5/200] Training loss: 0.04054607
[6/200] Training loss: 0.03982488
[7/200] Training loss: 0.03740479
[8/200] Training loss: 0.03563129
[9/200] Training loss: 0.03468077
[10/200] Training loss: 0.03279226
[50/200] Training loss: 0.01817767
[100/200] Training loss: 0.01424666
[150/200] Training loss: 0.01288814
[200/200] Training loss: 0.01224574
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17032.225456469274 ----------
[1/200] Training loss: 0.17956225
[2/200] Training loss: 0.06153746
[3/200] Training loss: 0.05400122
[4/200] Training loss: 0.04981183
[5/200] Training loss: 0.04825818
[6/200] Training loss: 0.04340358
[7/200] Training loss: 0.04273643
[8/200] Training loss: 0.04144110
[9/200] Training loss: 0.03861871
[10/200] Training loss: 0.03601646
[50/200] Training loss: 0.01878055
[100/200] Training loss: 0.01542984
[150/200] Training loss: 0.01466393
[200/200] Training loss: 0.01246301
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17901.29470178065 ----------
[1/200] Training loss: 0.17317023
[2/200] Training loss: 0.05871742
[3/200] Training loss: 0.05125596
[4/200] Training loss: 0.05151437
[5/200] Training loss: 0.04552994
[6/200] Training loss: 0.04395179
[7/200] Training loss: 0.03989282
[8/200] Training loss: 0.03863683
[9/200] Training loss: 0.03877964
[10/200] Training loss: 0.03732320
[50/200] Training loss: 0.01730299
[100/200] Training loss: 0.01526994
[150/200] Training loss: 0.01301787
[200/200] Training loss: 0.01288462
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7163.013890814396 ----------
[1/200] Training loss: 0.14918320
[2/200] Training loss: 0.05555599
[3/200] Training loss: 0.05219797
[4/200] Training loss: 0.05065357
[5/200] Training loss: 0.04584818
[6/200] Training loss: 0.04539155
[7/200] Training loss: 0.04267575
[8/200] Training loss: 0.03891244
[9/200] Training loss: 0.03835770
[10/200] Training loss: 0.03406459
[50/200] Training loss: 0.01717713
[100/200] Training loss: 0.01558531
[150/200] Training loss: 0.01366027
[200/200] Training loss: 0.01169361
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18426.358511653896 ----------
[1/200] Training loss: 0.15395185
[2/200] Training loss: 0.05750739
[3/200] Training loss: 0.05439304
[4/200] Training loss: 0.04692913
[5/200] Training loss: 0.04827475
[6/200] Training loss: 0.04451751
[7/200] Training loss: 0.04289887
[8/200] Training loss: 0.03987498
[9/200] Training loss: 0.03726431
[10/200] Training loss: 0.03912742
[50/200] Training loss: 0.01774231
[100/200] Training loss: 0.01540893
[150/200] Training loss: 0.01435190
[200/200] Training loss: 0.01244437
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 3998.090044008514 ----------
[1/200] Training loss: 0.17589061
[2/200] Training loss: 0.06085344
[3/200] Training loss: 0.05399036
[4/200] Training loss: 0.05265072
[5/200] Training loss: 0.04900742
[6/200] Training loss: 0.04831731
[7/200] Training loss: 0.04552687
[8/200] Training loss: 0.04322507
[9/200] Training loss: 0.04276472
[10/200] Training loss: 0.04271464
[50/200] Training loss: 0.01862395
[100/200] Training loss: 0.01581327
[150/200] Training loss: 0.01391167
[200/200] Training loss: 0.01255352
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8601.939781235393 ----------
[1/200] Training loss: 0.17305508
[2/200] Training loss: 0.06058513
[3/200] Training loss: 0.05446064
[4/200] Training loss: 0.04783498
[5/200] Training loss: 0.04326053
[6/200] Training loss: 0.04114671
[7/200] Training loss: 0.03822821
[8/200] Training loss: 0.03375610
[9/200] Training loss: 0.03076390
[10/200] Training loss: 0.03155049
[50/200] Training loss: 0.01804453
[100/200] Training loss: 0.01550795
[150/200] Training loss: 0.01355894
[200/200] Training loss: 0.01241150
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16207.308968487027 ----------
[1/200] Training loss: 0.15931846
[2/200] Training loss: 0.05659071
[3/200] Training loss: 0.05288974
[4/200] Training loss: 0.04947681
[5/200] Training loss: 0.04751674
[6/200] Training loss: 0.04441399
[7/200] Training loss: 0.04086036
[8/200] Training loss: 0.03944427
[9/200] Training loss: 0.03672197
[10/200] Training loss: 0.03541733
[50/200] Training loss: 0.01774471
[100/200] Training loss: 0.01579682
[150/200] Training loss: 0.01466787
[200/200] Training loss: 0.01343605
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16931.785966046227 ----------
[1/200] Training loss: 0.15817298
[2/200] Training loss: 0.06376225
[3/200] Training loss: 0.05299478
[4/200] Training loss: 0.05186379
[5/200] Training loss: 0.04950901
[6/200] Training loss: 0.04883575
[7/200] Training loss: 0.04539474
[8/200] Training loss: 0.04364507
[9/200] Training loss: 0.04048267
[10/200] Training loss: 0.04071159
[50/200] Training loss: 0.01700952
[100/200] Training loss: 0.01495782
[150/200] Training loss: 0.01364964
[200/200] Training loss: 0.01290971
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10565.559899976906 ----------
[1/200] Training loss: 0.16107454
[2/200] Training loss: 0.05675174
[3/200] Training loss: 0.05380785
[4/200] Training loss: 0.05397449
[5/200] Training loss: 0.04715159
[6/200] Training loss: 0.04564049
[7/200] Training loss: 0.04469751
[8/200] Training loss: 0.04267405
[9/200] Training loss: 0.03850150
[10/200] Training loss: 0.03842940
[50/200] Training loss: 0.01833633
[100/200] Training loss: 0.01484989
[150/200] Training loss: 0.01318916
[200/200] Training loss: 0.01281855
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8438.039108702922 ----------
[1/200] Training loss: 0.17509020
[2/200] Training loss: 0.05497898
[3/200] Training loss: 0.05375217
[4/200] Training loss: 0.04827178
[5/200] Training loss: 0.04748242
[6/200] Training loss: 0.04246390
[7/200] Training loss: 0.04182940
[8/200] Training loss: 0.04003441
[9/200] Training loss: 0.03674209
[10/200] Training loss: 0.03585707
[50/200] Training loss: 0.01851004
[100/200] Training loss: 0.01616852
[150/200] Training loss: 0.01485887
[200/200] Training loss: 0.01376547
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13163.357626380892 ----------
[1/200] Training loss: 0.17174156
[2/200] Training loss: 0.06365082
[3/200] Training loss: 0.05577776
[4/200] Training loss: 0.05560403
[5/200] Training loss: 0.05155058
[6/200] Training loss: 0.04931724
[7/200] Training loss: 0.04752377
[8/200] Training loss: 0.04454681
[9/200] Training loss: 0.04287342
[10/200] Training loss: 0.04264830
[50/200] Training loss: 0.01902357
[100/200] Training loss: 0.01653545
[150/200] Training loss: 0.01467359
[200/200] Training loss: 0.01349438
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10124.652685401115 ----------
[1/200] Training loss: 0.16864914
[2/200] Training loss: 0.05563331
[3/200] Training loss: 0.05083221
[4/200] Training loss: 0.04755708
[5/200] Training loss: 0.04311320
[6/200] Training loss: 0.04180144
[7/200] Training loss: 0.03924568
[8/200] Training loss: 0.03936431
[9/200] Training loss: 0.03469835
[10/200] Training loss: 0.03348371
[50/200] Training loss: 0.01829620
[100/200] Training loss: 0.01433861
[150/200] Training loss: 0.01340119
[200/200] Training loss: 0.01210442
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13843.555612630738 ----------
[1/200] Training loss: 0.14632093
[2/200] Training loss: 0.05504409
[3/200] Training loss: 0.04985176
[4/200] Training loss: 0.04790384
[5/200] Training loss: 0.04363157
[6/200] Training loss: 0.04085174
[7/200] Training loss: 0.03699456
[8/200] Training loss: 0.03647017
[9/200] Training loss: 0.03445529
[10/200] Training loss: 0.03316405
[50/200] Training loss: 0.01803223
[100/200] Training loss: 0.01541463
[150/200] Training loss: 0.01413915
[200/200] Training loss: 0.01307984
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13372.001794794975 ----------
[1/200] Training loss: 0.17828467
[2/200] Training loss: 0.06506643
[3/200] Training loss: 0.05679440
[4/200] Training loss: 0.05577595
[5/200] Training loss: 0.05096425
[6/200] Training loss: 0.04913444
[7/200] Training loss: 0.04990980
[8/200] Training loss: 0.04799950
[9/200] Training loss: 0.04228974
[10/200] Training loss: 0.04093836
[50/200] Training loss: 0.01738802
[100/200] Training loss: 0.01514149
[150/200] Training loss: 0.01339432
[200/200] Training loss: 0.01305630
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9113.012674192876 ----------
[1/200] Training loss: 0.15565614
[2/200] Training loss: 0.05621341
[3/200] Training loss: 0.04980060
[4/200] Training loss: 0.04721317
[5/200] Training loss: 0.04398973
[6/200] Training loss: 0.04126923
[7/200] Training loss: 0.03836442
[8/200] Training loss: 0.03593491
[9/200] Training loss: 0.03633499
[10/200] Training loss: 0.03518361
[50/200] Training loss: 0.01676078
[100/200] Training loss: 0.01287959
[150/200] Training loss: 0.01245534
[200/200] Training loss: 0.01106283
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17331.228692738434 ----------
[1/200] Training loss: 0.14917339
[2/200] Training loss: 0.05502753
[3/200] Training loss: 0.05233961
[4/200] Training loss: 0.04704875
[5/200] Training loss: 0.04474377
[6/200] Training loss: 0.04255683
[7/200] Training loss: 0.03769231
[8/200] Training loss: 0.03630747
[9/200] Training loss: 0.03424201
[10/200] Training loss: 0.03205433
[50/200] Training loss: 0.01778325
[100/200] Training loss: 0.01524487
[150/200] Training loss: 0.01358969
[200/200] Training loss: 0.01138363
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20980.861374119035 ----------
[1/200] Training loss: 0.17040073
[2/200] Training loss: 0.06147815
[3/200] Training loss: 0.05423884
[4/200] Training loss: 0.05059646
[5/200] Training loss: 0.04672654
[6/200] Training loss: 0.04403979
[7/200] Training loss: 0.04236036
[8/200] Training loss: 0.03917887
[9/200] Training loss: 0.03942370
[10/200] Training loss: 0.03556547
[50/200] Training loss: 0.01658997
[100/200] Training loss: 0.01469272
[150/200] Training loss: 0.01285949
[200/200] Training loss: 0.01205554
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17898.030282687534 ----------
[1/200] Training loss: 0.13404306
[2/200] Training loss: 0.05862135
[3/200] Training loss: 0.05240144
[4/200] Training loss: 0.04592486
[5/200] Training loss: 0.04195177
[6/200] Training loss: 0.03920355
[7/200] Training loss: 0.03656266
[8/200] Training loss: 0.03305714
[9/200] Training loss: 0.03501607
[10/200] Training loss: 0.03179686
[50/200] Training loss: 0.01959654
[100/200] Training loss: 0.01507241
[150/200] Training loss: 0.01344697
[200/200] Training loss: 0.01257001
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17825.104992678163 ----------
[1/200] Training loss: 0.13018760
[2/200] Training loss: 0.05617848
[3/200] Training loss: 0.05019376
[4/200] Training loss: 0.04700594
[5/200] Training loss: 0.04074594
[6/200] Training loss: 0.03924831
[7/200] Training loss: 0.03787296
[8/200] Training loss: 0.03605689
[9/200] Training loss: 0.03276796
[10/200] Training loss: 0.02992665
[50/200] Training loss: 0.01790256
[100/200] Training loss: 0.01475714
[150/200] Training loss: 0.01307330
[200/200] Training loss: 0.01191078
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8952.589346105406 ----------
[1/200] Training loss: 0.15821302
[2/200] Training loss: 0.05714206
[3/200] Training loss: 0.04791231
[4/200] Training loss: 0.04287049
[5/200] Training loss: 0.04303958
[6/200] Training loss: 0.03814688
[7/200] Training loss: 0.04009083
[8/200] Training loss: 0.03727165
[9/200] Training loss: 0.03285179
[10/200] Training loss: 0.03547762
[50/200] Training loss: 0.01806549
[100/200] Training loss: 0.01462663
[150/200] Training loss: 0.01366333
[200/200] Training loss: 0.01247639
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4540.632555052214 ----------
[1/200] Training loss: 0.18066447
[2/200] Training loss: 0.06355098
[3/200] Training loss: 0.05801115
[4/200] Training loss: 0.05228981
[5/200] Training loss: 0.05192599
[6/200] Training loss: 0.04620010
[7/200] Training loss: 0.04659600
[8/200] Training loss: 0.04207720
[9/200] Training loss: 0.03971828
[10/200] Training loss: 0.04101902
[50/200] Training loss: 0.02063932
[100/200] Training loss: 0.01605165
[150/200] Training loss: 0.01508087
[200/200] Training loss: 0.01456223
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9378.335033469428 ----------
[1/200] Training loss: 0.14507677
[2/200] Training loss: 0.05961267
[3/200] Training loss: 0.05219947
[4/200] Training loss: 0.05045276
[5/200] Training loss: 0.04435504
[6/200] Training loss: 0.04730333
[7/200] Training loss: 0.04103738
[8/200] Training loss: 0.03333570
[9/200] Training loss: 0.03374946
[10/200] Training loss: 0.03229161
[50/200] Training loss: 0.01709956
[100/200] Training loss: 0.01387549
[150/200] Training loss: 0.01234936
[200/200] Training loss: 0.01150624
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20445.481065506872 ----------
[1/200] Training loss: 0.15245727
[2/200] Training loss: 0.05472102
[3/200] Training loss: 0.05120123
[4/200] Training loss: 0.04905925
[5/200] Training loss: 0.04267672
[6/200] Training loss: 0.04177170
[7/200] Training loss: 0.03992761
[8/200] Training loss: 0.03667562
[9/200] Training loss: 0.03703428
[10/200] Training loss: 0.03484930
[50/200] Training loss: 0.01781205
[100/200] Training loss: 0.01431346
[150/200] Training loss: 0.01270155
[200/200] Training loss: 0.01135274
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5010.497180919275 ----------
[1/200] Training loss: 0.19737734
[2/200] Training loss: 0.06111553
[3/200] Training loss: 0.05730951
[4/200] Training loss: 0.05327021
[5/200] Training loss: 0.05121539
[6/200] Training loss: 0.04957137
[7/200] Training loss: 0.04472769
[8/200] Training loss: 0.04428559
[9/200] Training loss: 0.04192622
[10/200] Training loss: 0.03928095
[50/200] Training loss: 0.02113207
[100/200] Training loss: 0.01570251
[150/200] Training loss: 0.01490178
[200/200] Training loss: 0.01402832
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13642.99673825366 ----------
[1/200] Training loss: 0.16588462
[2/200] Training loss: 0.05638694
[3/200] Training loss: 0.05547378
[4/200] Training loss: 0.04496518
[5/200] Training loss: 0.04263859
[6/200] Training loss: 0.04088712
[7/200] Training loss: 0.03901143
[8/200] Training loss: 0.03773234
[9/200] Training loss: 0.03342850
[10/200] Training loss: 0.03129199
[50/200] Training loss: 0.01768508
[100/200] Training loss: 0.01587270
[150/200] Training loss: 0.01474061
[200/200] Training loss: 0.01275944
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12195.605110038616 ----------
[1/200] Training loss: 0.16894562
[2/200] Training loss: 0.05798531
[3/200] Training loss: 0.05338556
[4/200] Training loss: 0.04582828
[5/200] Training loss: 0.04534214
[6/200] Training loss: 0.04153702
[7/200] Training loss: 0.03845000
[8/200] Training loss: 0.03760670
[9/200] Training loss: 0.03127003
[10/200] Training loss: 0.03366034
[50/200] Training loss: 0.01668473
[100/200] Training loss: 0.01520264
[150/200] Training loss: 0.01381064
[200/200] Training loss: 0.01314877
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 24358.08071256847 ----------
[1/200] Training loss: 0.15851926
[2/200] Training loss: 0.05707635
[3/200] Training loss: 0.04831019
[4/200] Training loss: 0.04873778
[5/200] Training loss: 0.04364482
[6/200] Training loss: 0.04175753
[7/200] Training loss: 0.03777593
[8/200] Training loss: 0.03661336
[9/200] Training loss: 0.03478890
[10/200] Training loss: 0.03334986
[50/200] Training loss: 0.01775013
[100/200] Training loss: 0.01406969
[150/200] Training loss: 0.01328652
[200/200] Training loss: 0.01167314
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17325.287876396167 ----------
[1/200] Training loss: 0.13780622
[2/200] Training loss: 0.05603008
[3/200] Training loss: 0.04868892
[4/200] Training loss: 0.04389505
[5/200] Training loss: 0.04155783
[6/200] Training loss: 0.03856268
[7/200] Training loss: 0.03771505
[8/200] Training loss: 0.03447836
[9/200] Training loss: 0.03594165
[10/200] Training loss: 0.03453130
[50/200] Training loss: 0.01742545
[100/200] Training loss: 0.01447451
[150/200] Training loss: 0.01360426
[200/200] Training loss: 0.01163439
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7218.741164496757 ----------
[1/200] Training loss: 0.20900699
[2/200] Training loss: 0.06728434
[3/200] Training loss: 0.06003454
[4/200] Training loss: 0.05277491
[5/200] Training loss: 0.04965048
[6/200] Training loss: 0.04627684
[7/200] Training loss: 0.04536307
[8/200] Training loss: 0.04795372
[9/200] Training loss: 0.04057277
[10/200] Training loss: 0.03987756
[50/200] Training loss: 0.01969423
[100/200] Training loss: 0.01609567
[150/200] Training loss: 0.01444228
[200/200] Training loss: 0.01423265
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10495.35668760238 ----------
[1/200] Training loss: 0.15086228
[2/200] Training loss: 0.05621887
[3/200] Training loss: 0.05146825
[4/200] Training loss: 0.04818998
[5/200] Training loss: 0.04293328
[6/200] Training loss: 0.03908061
[7/200] Training loss: 0.04008457
[8/200] Training loss: 0.03507206
[9/200] Training loss: 0.03430255
[10/200] Training loss: 0.03072855
[50/200] Training loss: 0.01752675
[100/200] Training loss: 0.01572480
[150/200] Training loss: 0.01342893
[200/200] Training loss: 0.01264079
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16601.401386629987 ----------
[1/200] Training loss: 0.17137516
[2/200] Training loss: 0.06324780
[3/200] Training loss: 0.05150795
[4/200] Training loss: 0.04958866
[5/200] Training loss: 0.04774906
[6/200] Training loss: 0.04696414
[7/200] Training loss: 0.04043647
[8/200] Training loss: 0.04220685
[9/200] Training loss: 0.03948151
[10/200] Training loss: 0.03807977
[50/200] Training loss: 0.01866922
[100/200] Training loss: 0.01551619
[150/200] Training loss: 0.01384214
[200/200] Training loss: 0.01273546
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6218.955539316871 ----------
[1/200] Training loss: 0.15940025
[2/200] Training loss: 0.05652585
[3/200] Training loss: 0.05093153
[4/200] Training loss: 0.04772562
[5/200] Training loss: 0.04178052
[6/200] Training loss: 0.03765812
[7/200] Training loss: 0.03412166
[8/200] Training loss: 0.03429770
[9/200] Training loss: 0.03176473
[10/200] Training loss: 0.03231650
[50/200] Training loss: 0.01713657
[100/200] Training loss: 0.01522998
[150/200] Training loss: 0.01362752
[200/200] Training loss: 0.01280322
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.01825706333502782
----FITNESS-----------RMSE---- 10179.175998085504 ----------
[1/200] Training loss: 0.17258133
[2/200] Training loss: 0.06188209
[3/200] Training loss: 0.05952924
[4/200] Training loss: 0.05358864
[5/200] Training loss: 0.05175933
[6/200] Training loss: 0.04871094
[7/200] Training loss: 0.04737305
[8/200] Training loss: 0.04430267
[9/200] Training loss: 0.04515529
[10/200] Training loss: 0.04118769
[50/200] Training loss: 0.01980029
[100/200] Training loss: 0.01580524
[150/200] Training loss: 0.01382358
[200/200] Training loss: 0.01342536
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15271.785750199615 ----------
[1/200] Training loss: 0.16121945
[2/200] Training loss: 0.06246873
[3/200] Training loss: 0.05515674
[4/200] Training loss: 0.05129757
[5/200] Training loss: 0.04628722
[6/200] Training loss: 0.04231421
[7/200] Training loss: 0.03482962
[8/200] Training loss: 0.03635817
[9/200] Training loss: 0.03202051
[10/200] Training loss: 0.03104071
[50/200] Training loss: 0.01854849
[100/200] Training loss: 0.01655419
[150/200] Training loss: 0.01561288
[200/200] Training loss: 0.01463164
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11764.79970080239 ----------
[1/200] Training loss: 0.16605207
[2/200] Training loss: 0.06022442
[3/200] Training loss: 0.05725525
[4/200] Training loss: 0.05422703
[5/200] Training loss: 0.04973640
[6/200] Training loss: 0.04866302
[7/200] Training loss: 0.04646403
[8/200] Training loss: 0.04303458
[9/200] Training loss: 0.04169511
[10/200] Training loss: 0.03716973
[50/200] Training loss: 0.01761513
[100/200] Training loss: 0.01566013
[150/200] Training loss: 0.01405571
[200/200] Training loss: 0.01302817
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10779.439688592352 ----------
[1/200] Training loss: 0.14486341
[2/200] Training loss: 0.05888979
[3/200] Training loss: 0.05243063
[4/200] Training loss: 0.04482399
[5/200] Training loss: 0.04462884
[6/200] Training loss: 0.04070756
[7/200] Training loss: 0.04078972
[8/200] Training loss: 0.03897398
[9/200] Training loss: 0.03708888
[10/200] Training loss: 0.03406372
[50/200] Training loss: 0.01821624
[100/200] Training loss: 0.01489403
[150/200] Training loss: 0.01315276
[200/200] Training loss: 0.01195688
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17263.76181485368 ----------
[1/200] Training loss: 0.14364706
[2/200] Training loss: 0.05773472
[3/200] Training loss: 0.05002968
[4/200] Training loss: 0.04651585
[5/200] Training loss: 0.03937581
[6/200] Training loss: 0.04041387
[7/200] Training loss: 0.03773034
[8/200] Training loss: 0.03526793
[9/200] Training loss: 0.03179659
[10/200] Training loss: 0.03156112
[50/200] Training loss: 0.01854693
[100/200] Training loss: 0.01355554
[150/200] Training loss: 0.01220160
[200/200] Training loss: 0.01156325
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15085.82964241609 ----------
[1/200] Training loss: 0.16977425
[2/200] Training loss: 0.06035298
[3/200] Training loss: 0.05295634
[4/200] Training loss: 0.05111196
[5/200] Training loss: 0.05050821
[6/200] Training loss: 0.04675221
[7/200] Training loss: 0.04424654
[8/200] Training loss: 0.04355199
[9/200] Training loss: 0.03962320
[10/200] Training loss: 0.03727835
[50/200] Training loss: 0.01669617
[100/200] Training loss: 0.01473904
[150/200] Training loss: 0.01354465
[200/200] Training loss: 0.01274989
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14999.288516459706 ----------
[1/200] Training loss: 0.16205420
[2/200] Training loss: 0.05720337
[3/200] Training loss: 0.05222259
[4/200] Training loss: 0.04780745
[5/200] Training loss: 0.04349019
[6/200] Training loss: 0.03970925
[7/200] Training loss: 0.03611074
[8/200] Training loss: 0.03518459
[9/200] Training loss: 0.03359537
[10/200] Training loss: 0.02981917
[50/200] Training loss: 0.01782044
[100/200] Training loss: 0.01448957
[150/200] Training loss: 0.01367049
[200/200] Training loss: 0.01247976
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12454.02360685092 ----------
[1/200] Training loss: 0.18735095
[2/200] Training loss: 0.06332280
[3/200] Training loss: 0.05389311
[4/200] Training loss: 0.05224043
[5/200] Training loss: 0.05049015
[6/200] Training loss: 0.04842200
[7/200] Training loss: 0.04560263
[8/200] Training loss: 0.04320530
[9/200] Training loss: 0.04087417
[10/200] Training loss: 0.03978946
[50/200] Training loss: 0.01771168
[100/200] Training loss: 0.01538974
[150/200] Training loss: 0.01358467
[200/200] Training loss: 0.01273615
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7774.847136760954 ----------
[1/200] Training loss: 0.16612456
[2/200] Training loss: 0.06005521
[3/200] Training loss: 0.05619971
[4/200] Training loss: 0.05441785
[5/200] Training loss: 0.04797920
[6/200] Training loss: 0.04729434
[7/200] Training loss: 0.04367309
[8/200] Training loss: 0.04182545
[9/200] Training loss: 0.03700933
[10/200] Training loss: 0.03870926
[50/200] Training loss: 0.01890759
[100/200] Training loss: 0.01641434
[150/200] Training loss: 0.01657204
[200/200] Training loss: 0.01424000
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18293.968404914227 ----------
[1/200] Training loss: 0.15363158
[2/200] Training loss: 0.05517932
[3/200] Training loss: 0.05315745
[4/200] Training loss: 0.04696155
[5/200] Training loss: 0.04619079
[6/200] Training loss: 0.04351097
[7/200] Training loss: 0.03870427
[8/200] Training loss: 0.03420066
[9/200] Training loss: 0.03271008
[10/200] Training loss: 0.03180475
[50/200] Training loss: 0.01495469
[100/200] Training loss: 0.01339826
[150/200] Training loss: 0.01153890
[200/200] Training loss: 0.01104675
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17687.01489794137 ----------
[1/200] Training loss: 0.15680785
[2/200] Training loss: 0.06238065
[3/200] Training loss: 0.05263885
[4/200] Training loss: 0.05252007
[5/200] Training loss: 0.04678861
[6/200] Training loss: 0.04545173
[7/200] Training loss: 0.03915575
[8/200] Training loss: 0.03744209
[9/200] Training loss: 0.03409988
[10/200] Training loss: 0.03226341
[50/200] Training loss: 0.01750135
[100/200] Training loss: 0.01479132
[150/200] Training loss: 0.01357289
[200/200] Training loss: 0.01277082
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10928.309658863076 ----------
[1/200] Training loss: 0.18067529
[2/200] Training loss: 0.05812985
[3/200] Training loss: 0.05165253
[4/200] Training loss: 0.04880354
[5/200] Training loss: 0.04430107
[6/200] Training loss: 0.04362876
[7/200] Training loss: 0.04196500
[8/200] Training loss: 0.04005178
[9/200] Training loss: 0.03835027
[10/200] Training loss: 0.03668838
[50/200] Training loss: 0.01990868
[100/200] Training loss: 0.01563181
[150/200] Training loss: 0.01442339
[200/200] Training loss: 0.01236893
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20607.618785293947 ----------
[1/200] Training loss: 0.15844643
[2/200] Training loss: 0.06195248
[3/200] Training loss: 0.05742644
[4/200] Training loss: 0.05391735
[5/200] Training loss: 0.04848213
[6/200] Training loss: 0.04663820
[7/200] Training loss: 0.04112343
[8/200] Training loss: 0.03770828
[9/200] Training loss: 0.03369393
[10/200] Training loss: 0.03164497
[50/200] Training loss: 0.01765498
[100/200] Training loss: 0.01389810
[150/200] Training loss: 0.01237498
[200/200] Training loss: 0.01141494
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12696.679250890762 ----------
[1/200] Training loss: 0.15619628
[2/200] Training loss: 0.06102706
[3/200] Training loss: 0.05511017
[4/200] Training loss: 0.05199444
[5/200] Training loss: 0.04812102
[6/200] Training loss: 0.04700076
[7/200] Training loss: 0.04402096
[8/200] Training loss: 0.04009391
[9/200] Training loss: 0.03493636
[10/200] Training loss: 0.03559099
[50/200] Training loss: 0.01788617
[100/200] Training loss: 0.01499616
[150/200] Training loss: 0.01340347
[200/200] Training loss: 0.01283311
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13438.595759974329 ----------
[1/200] Training loss: 0.16352081
[2/200] Training loss: 0.05486926
[3/200] Training loss: 0.04652539
[4/200] Training loss: 0.04593705
[5/200] Training loss: 0.04366832
[6/200] Training loss: 0.04212655
[7/200] Training loss: 0.03910466
[8/200] Training loss: 0.03842696
[9/200] Training loss: 0.03686007
[10/200] Training loss: 0.03542052
[50/200] Training loss: 0.01773330
[100/200] Training loss: 0.01480507
[150/200] Training loss: 0.01363350
[200/200] Training loss: 0.01193877
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11260.471748554764 ----------
[1/200] Training loss: 0.13845580
[2/200] Training loss: 0.05559492
[3/200] Training loss: 0.05340000
[4/200] Training loss: 0.04670335
[5/200] Training loss: 0.04588330
[6/200] Training loss: 0.04522209
[7/200] Training loss: 0.04154487
[8/200] Training loss: 0.04035852
[9/200] Training loss: 0.03828786
[10/200] Training loss: 0.03639177
[50/200] Training loss: 0.01704603
[100/200] Training loss: 0.01494944
[150/200] Training loss: 0.01294915
[200/200] Training loss: 0.01126182
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16077.273400673386 ----------
[1/200] Training loss: 0.15535560
[2/200] Training loss: 0.05741120
[3/200] Training loss: 0.05147411
[4/200] Training loss: 0.04681665
[5/200] Training loss: 0.04233260
[6/200] Training loss: 0.04014120
[7/200] Training loss: 0.03631967
[8/200] Training loss: 0.03637940
[9/200] Training loss: 0.03393519
[10/200] Training loss: 0.03201715
[50/200] Training loss: 0.01716827
[100/200] Training loss: 0.01492707
[150/200] Training loss: 0.01429153
[200/200] Training loss: 0.01261639
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18954.484429812383 ----------
[1/200] Training loss: 0.15818948
[2/200] Training loss: 0.05646679
[3/200] Training loss: 0.04746363
[4/200] Training loss: 0.04536049
[5/200] Training loss: 0.04266308
[6/200] Training loss: 0.04052038
[7/200] Training loss: 0.03971336
[8/200] Training loss: 0.03769010
[9/200] Training loss: 0.03448360
[10/200] Training loss: 0.03315152
[50/200] Training loss: 0.01628142
[100/200] Training loss: 0.01321880
[150/200] Training loss: 0.01245116
[200/200] Training loss: 0.01142361
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5707.972669871502 ----------
[1/200] Training loss: 0.18043613
[2/200] Training loss: 0.06371421
[3/200] Training loss: 0.05678488
[4/200] Training loss: 0.05386185
[5/200] Training loss: 0.05069457
[6/200] Training loss: 0.05166340
[7/200] Training loss: 0.04744074
[8/200] Training loss: 0.04705235
[9/200] Training loss: 0.04318802
[10/200] Training loss: 0.04435696
[50/200] Training loss: 0.01797521
[100/200] Training loss: 0.01588800
[150/200] Training loss: 0.01388883
[200/200] Training loss: 0.01254981
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14073.866277608297 ----------
[1/200] Training loss: 0.17348315
[2/200] Training loss: 0.05900850
[3/200] Training loss: 0.05414357
[4/200] Training loss: 0.04797971
[5/200] Training loss: 0.04786354
[6/200] Training loss: 0.04376129
[7/200] Training loss: 0.04024726
[8/200] Training loss: 0.03707968
[9/200] Training loss: 0.03653473
[10/200] Training loss: 0.03102140
[50/200] Training loss: 0.01876748
[100/200] Training loss: 0.01537806
[150/200] Training loss: 0.01330092
[200/200] Training loss: 0.01255613
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17662.919351001976 ----------
[1/200] Training loss: 0.13973438
[2/200] Training loss: 0.05330593
[3/200] Training loss: 0.05035089
[4/200] Training loss: 0.04415357
[5/200] Training loss: 0.04804006
[6/200] Training loss: 0.04150815
[7/200] Training loss: 0.04002937
[8/200] Training loss: 0.03990674
[9/200] Training loss: 0.03590282
[10/200] Training loss: 0.03503255
[50/200] Training loss: 0.01656662
[100/200] Training loss: 0.01414887
[150/200] Training loss: 0.01239085
[200/200] Training loss: 0.01226269
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12700.599040990153 ----------
[1/200] Training loss: 0.16055942
[2/200] Training loss: 0.06188414
[3/200] Training loss: 0.05493921
[4/200] Training loss: 0.05660284
[5/200] Training loss: 0.05229753
[6/200] Training loss: 0.04665884
[7/200] Training loss: 0.04676882
[8/200] Training loss: 0.04255522
[9/200] Training loss: 0.04068501
[10/200] Training loss: 0.04011069
[50/200] Training loss: 0.01793665
[100/200] Training loss: 0.01574843
[150/200] Training loss: 0.01439821
[200/200] Training loss: 0.01426705
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16375.262807051373 ----------
[1/200] Training loss: 0.15601264
[2/200] Training loss: 0.06465317
[3/200] Training loss: 0.05065427
[4/200] Training loss: 0.04672319
[5/200] Training loss: 0.04439896
[6/200] Training loss: 0.04344338
[7/200] Training loss: 0.04024244
[8/200] Training loss: 0.03535852
[9/200] Training loss: 0.03399903
[10/200] Training loss: 0.03182252
[50/200] Training loss: 0.01624844
[100/200] Training loss: 0.01391918
[150/200] Training loss: 0.01290040
[200/200] Training loss: 0.01113230
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23759.523227539732 ----------
[1/200] Training loss: 0.14676700
[2/200] Training loss: 0.06011879
[3/200] Training loss: 0.05287844
[4/200] Training loss: 0.05147712
[5/200] Training loss: 0.04891815
[6/200] Training loss: 0.04745014
[7/200] Training loss: 0.04354867
[8/200] Training loss: 0.03943670
[9/200] Training loss: 0.03762442
[10/200] Training loss: 0.03233516
[50/200] Training loss: 0.01705168
[100/200] Training loss: 0.01423895
[150/200] Training loss: 0.01323575
[200/200] Training loss: 0.01162001
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17948.77332855925 ----------
[1/200] Training loss: 0.18056348
[2/200] Training loss: 0.05915012
[3/200] Training loss: 0.05135460
[4/200] Training loss: 0.04961274
[5/200] Training loss: 0.04711606
[6/200] Training loss: 0.04250777
[7/200] Training loss: 0.03973609
[8/200] Training loss: 0.03934502
[9/200] Training loss: 0.03619034
[10/200] Training loss: 0.03323308
[50/200] Training loss: 0.01971454
[100/200] Training loss: 0.01714611
[150/200] Training loss: 0.01587989
[200/200] Training loss: 0.01457233
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9501.565344720837 ----------
[1/200] Training loss: 0.18327536
[2/200] Training loss: 0.06345321
[3/200] Training loss: 0.05357216
[4/200] Training loss: 0.05198943
[5/200] Training loss: 0.04667974
[6/200] Training loss: 0.04642794
[7/200] Training loss: 0.04301104
[8/200] Training loss: 0.04009332
[9/200] Training loss: 0.03571447
[10/200] Training loss: 0.03701926
[50/200] Training loss: 0.01765880
[100/200] Training loss: 0.01550920
[150/200] Training loss: 0.01377576
[200/200] Training loss: 0.01295790
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11822.78207529852 ----------
[1/200] Training loss: 0.17279009
[2/200] Training loss: 0.05897254
[3/200] Training loss: 0.05501391
[4/200] Training loss: 0.05214059
[5/200] Training loss: 0.04962889
[6/200] Training loss: 0.04629764
[7/200] Training loss: 0.04289453
[8/200] Training loss: 0.04246314
[9/200] Training loss: 0.03816107
[10/200] Training loss: 0.03453771
[50/200] Training loss: 0.01670059
[100/200] Training loss: 0.01490993
[150/200] Training loss: 0.01351372
[200/200] Training loss: 0.01323863
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17048.975101160773 ----------
[1/200] Training loss: 0.14201745
[2/200] Training loss: 0.05537176
[3/200] Training loss: 0.05288781
[4/200] Training loss: 0.04912351
[5/200] Training loss: 0.04392007
[6/200] Training loss: 0.04214844
[7/200] Training loss: 0.03680700
[8/200] Training loss: 0.03599881
[9/200] Training loss: 0.03318300
[10/200] Training loss: 0.03089666
[50/200] Training loss: 0.01888164
[100/200] Training loss: 0.01606064
[150/200] Training loss: 0.01441658
[200/200] Training loss: 0.01255751
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16591.119552338834 ----------
[1/200] Training loss: 0.16598360
[2/200] Training loss: 0.06193087
[3/200] Training loss: 0.05552353
[4/200] Training loss: 0.05366886
[5/200] Training loss: 0.04398206
[6/200] Training loss: 0.04120632
[7/200] Training loss: 0.03937087
[8/200] Training loss: 0.03467968
[9/200] Training loss: 0.03480294
[10/200] Training loss: 0.03160122
[50/200] Training loss: 0.01744656
[100/200] Training loss: 0.01349332
[150/200] Training loss: 0.01180556
[200/200] Training loss: 0.01094089
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17422.683145830324 ----------
[1/200] Training loss: 0.12925580
[2/200] Training loss: 0.05129772
[3/200] Training loss: 0.04741094
[4/200] Training loss: 0.04136365
[5/200] Training loss: 0.03346769
[6/200] Training loss: 0.03396053
[7/200] Training loss: 0.03016810
[8/200] Training loss: 0.02611747
[9/200] Training loss: 0.02600967
[10/200] Training loss: 0.02580658
[50/200] Training loss: 0.01772123
[100/200] Training loss: 0.01577760
[150/200] Training loss: 0.01485516
[200/200] Training loss: 0.01386390
---batch_size---: 16 ---n_steps---: 8 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.017691104822453053
----FITNESS-----------RMSE---- 22187.91526935327 ----------
[1/200] Training loss: 0.14269115
[2/200] Training loss: 0.05524929
[3/200] Training loss: 0.04593922
[4/200] Training loss: 0.04742982
[5/200] Training loss: 0.04316446
[6/200] Training loss: 0.03856519
[7/200] Training loss: 0.03486504
[8/200] Training loss: 0.03543415
[9/200] Training loss: 0.03270840
[10/200] Training loss: 0.03100577
[50/200] Training loss: 0.01715356
[100/200] Training loss: 0.01430936
[150/200] Training loss: 0.01288909
[200/200] Training loss: 0.01223405
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5111.943074800422 ----------
[1/200] Training loss: 0.17882597
[2/200] Training loss: 0.05726577
[3/200] Training loss: 0.05033173
[4/200] Training loss: 0.04816752
[5/200] Training loss: 0.04529499
[6/200] Training loss: 0.04264186
[7/200] Training loss: 0.03733299
[8/200] Training loss: 0.03946307
[9/200] Training loss: 0.03540621
[10/200] Training loss: 0.03395655
[50/200] Training loss: 0.01828819
[100/200] Training loss: 0.01543243
[150/200] Training loss: 0.01394878
[200/200] Training loss: 0.01206644
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11050.75382044139 ----------
[1/200] Training loss: 0.16187612
[2/200] Training loss: 0.05787477
[3/200] Training loss: 0.05545012
[4/200] Training loss: 0.04956305
[5/200] Training loss: 0.04748481
[6/200] Training loss: 0.04428875
[7/200] Training loss: 0.04218420
[8/200] Training loss: 0.04000282
[9/200] Training loss: 0.03479037
[10/200] Training loss: 0.03448918
[50/200] Training loss: 0.01799220
[100/200] Training loss: 0.01423836
[150/200] Training loss: 0.01346714
[200/200] Training loss: 0.01279525
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13063.281057988455 ----------
[1/200] Training loss: 0.17587199
[2/200] Training loss: 0.06127891
[3/200] Training loss: 0.04975715
[4/200] Training loss: 0.04559322
[5/200] Training loss: 0.04079715
[6/200] Training loss: 0.03705353
[7/200] Training loss: 0.03404010
[8/200] Training loss: 0.03196200
[9/200] Training loss: 0.03096311
[10/200] Training loss: 0.02799629
[50/200] Training loss: 0.01464946
[100/200] Training loss: 0.01222425
[150/200] Training loss: 0.01081429
[200/200] Training loss: 0.01019190
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 24379.68465751762 ----------
[1/200] Training loss: 0.18152328
[2/200] Training loss: 0.06412394
[3/200] Training loss: 0.05674788
[4/200] Training loss: 0.05158450
[5/200] Training loss: 0.04948123
[6/200] Training loss: 0.04661563
[7/200] Training loss: 0.04511674
[8/200] Training loss: 0.04254878
[9/200] Training loss: 0.03964002
[10/200] Training loss: 0.03634949
[50/200] Training loss: 0.01701674
[100/200] Training loss: 0.01542862
[150/200] Training loss: 0.01524228
[200/200] Training loss: 0.01359767
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17725.766104741426 ----------
[1/200] Training loss: 0.18750004
[2/200] Training loss: 0.06533848
[3/200] Training loss: 0.05897048
[4/200] Training loss: 0.05434587
[5/200] Training loss: 0.05397206
[6/200] Training loss: 0.04975970
[7/200] Training loss: 0.05086659
[8/200] Training loss: 0.04811887
[9/200] Training loss: 0.04518359
[10/200] Training loss: 0.04328885
[50/200] Training loss: 0.01909986
[100/200] Training loss: 0.01749288
[150/200] Training loss: 0.01540014
[200/200] Training loss: 0.01465843
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8526.428091528129 ----------
[1/200] Training loss: 0.16524110
[2/200] Training loss: 0.06198186
[3/200] Training loss: 0.05444643
[4/200] Training loss: 0.05350255
[5/200] Training loss: 0.05196252
[6/200] Training loss: 0.04869346
[7/200] Training loss: 0.04696655
[8/200] Training loss: 0.04685094
[9/200] Training loss: 0.04404518
[10/200] Training loss: 0.04361152
[50/200] Training loss: 0.01851825
[100/200] Training loss: 0.01587752
[150/200] Training loss: 0.01482685
[200/200] Training loss: 0.01388606
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17990.16264517917 ----------
[1/200] Training loss: 0.15997824
[2/200] Training loss: 0.05679922
[3/200] Training loss: 0.05206221
[4/200] Training loss: 0.04676106
[5/200] Training loss: 0.04729115
[6/200] Training loss: 0.03985941
[7/200] Training loss: 0.03881344
[8/200] Training loss: 0.03716051
[9/200] Training loss: 0.03343336
[10/200] Training loss: 0.03122096
[50/200] Training loss: 0.01724331
[100/200] Training loss: 0.01443654
[150/200] Training loss: 0.01328722
[200/200] Training loss: 0.01146776
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7139.567774032263 ----------
[1/200] Training loss: 0.14803657
[2/200] Training loss: 0.05887485
[3/200] Training loss: 0.05347231
[4/200] Training loss: 0.05314739
[5/200] Training loss: 0.05037967
[6/200] Training loss: 0.04669832
[7/200] Training loss: 0.04232553
[8/200] Training loss: 0.04247757
[9/200] Training loss: 0.04007287
[10/200] Training loss: 0.03836316
[50/200] Training loss: 0.01810794
[100/200] Training loss: 0.01584934
[150/200] Training loss: 0.01429376
[200/200] Training loss: 0.01339269
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18071.093381419952 ----------
[1/200] Training loss: 0.12706566
[2/200] Training loss: 0.05472187
[3/200] Training loss: 0.05029731
[4/200] Training loss: 0.04911798
[5/200] Training loss: 0.04526741
[6/200] Training loss: 0.04351992
[7/200] Training loss: 0.03941103
[8/200] Training loss: 0.03823029
[9/200] Training loss: 0.03770435
[10/200] Training loss: 0.03792150
[50/200] Training loss: 0.01753125
[100/200] Training loss: 0.01477090
[150/200] Training loss: 0.01291272
[200/200] Training loss: 0.01179538
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15990.015884920189 ----------
[1/200] Training loss: 0.15629503
[2/200] Training loss: 0.06086097
[3/200] Training loss: 0.05484477
[4/200] Training loss: 0.05522879
[5/200] Training loss: 0.05013818
[6/200] Training loss: 0.04912855
[7/200] Training loss: 0.04672778
[8/200] Training loss: 0.04476610
[9/200] Training loss: 0.04238621
[10/200] Training loss: 0.04160235
[50/200] Training loss: 0.02080602
[100/200] Training loss: 0.01638100
[150/200] Training loss: 0.01538683
[200/200] Training loss: 0.01378465
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18279.025794609515 ----------
[1/200] Training loss: 0.15372403
[2/200] Training loss: 0.05427257
[3/200] Training loss: 0.05253415
[4/200] Training loss: 0.04746278
[5/200] Training loss: 0.04644335
[6/200] Training loss: 0.04348007
[7/200] Training loss: 0.04157200
[8/200] Training loss: 0.03774708
[9/200] Training loss: 0.04043563
[10/200] Training loss: 0.03508825
[50/200] Training loss: 0.01857129
[100/200] Training loss: 0.01498305
[150/200] Training loss: 0.01320873
[200/200] Training loss: 0.01251331
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 36903.77888509522 ----------
[1/200] Training loss: 0.17562532
[2/200] Training loss: 0.06117443
[3/200] Training loss: 0.05223508
[4/200] Training loss: 0.04778247
[5/200] Training loss: 0.04846333
[6/200] Training loss: 0.04301840
[7/200] Training loss: 0.04338288
[8/200] Training loss: 0.03826178
[9/200] Training loss: 0.03751344
[10/200] Training loss: 0.03615731
[50/200] Training loss: 0.01700363
[100/200] Training loss: 0.01479658
[150/200] Training loss: 0.01320751
[200/200] Training loss: 0.01253897
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17220.412074047472 ----------
[1/200] Training loss: 0.16851624
[2/200] Training loss: 0.05837286
[3/200] Training loss: 0.05623343
[4/200] Training loss: 0.05079012
[5/200] Training loss: 0.05036515
[6/200] Training loss: 0.04394579
[7/200] Training loss: 0.04283575
[8/200] Training loss: 0.03983161
[9/200] Training loss: 0.03487383
[10/200] Training loss: 0.03624186
[50/200] Training loss: 0.01877827
[100/200] Training loss: 0.01597772
[150/200] Training loss: 0.01523451
[200/200] Training loss: 0.01341982
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10313.444817324616 ----------
[1/200] Training loss: 0.19396457
[2/200] Training loss: 0.06870395
[3/200] Training loss: 0.05770004
[4/200] Training loss: 0.05184975
[5/200] Training loss: 0.04929313
[6/200] Training loss: 0.04813680
[7/200] Training loss: 0.04484669
[8/200] Training loss: 0.04238755
[9/200] Training loss: 0.03822254
[10/200] Training loss: 0.03871902
[50/200] Training loss: 0.01815722
[100/200] Training loss: 0.01570711
[150/200] Training loss: 0.01410671
[200/200] Training loss: 0.01318826
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13305.198983855897 ----------
[1/200] Training loss: 0.16931252
[2/200] Training loss: 0.06415170
[3/200] Training loss: 0.05688588
[4/200] Training loss: 0.05286738
[5/200] Training loss: 0.05039858
[6/200] Training loss: 0.04842602
[7/200] Training loss: 0.04746721
[8/200] Training loss: 0.04306538
[9/200] Training loss: 0.04416100
[10/200] Training loss: 0.04084840
[50/200] Training loss: 0.01925118
[100/200] Training loss: 0.01614347
[150/200] Training loss: 0.01502336
[200/200] Training loss: 0.01315862
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18924.300568316918 ----------
[1/200] Training loss: 0.17352298
[2/200] Training loss: 0.05951647
[3/200] Training loss: 0.05338649
[4/200] Training loss: 0.05198221
[5/200] Training loss: 0.04562941
[6/200] Training loss: 0.04748396
[7/200] Training loss: 0.04193491
[8/200] Training loss: 0.04050765
[9/200] Training loss: 0.03878977
[10/200] Training loss: 0.03636716
[50/200] Training loss: 0.01755862
[100/200] Training loss: 0.01366090
[150/200] Training loss: 0.01235060
[200/200] Training loss: 0.01178171
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11331.650894728446 ----------
[1/200] Training loss: 0.16361471
[2/200] Training loss: 0.05805618
[3/200] Training loss: 0.05087636
[4/200] Training loss: 0.05058204
[5/200] Training loss: 0.04606237
[6/200] Training loss: 0.04215308
[7/200] Training loss: 0.04087881
[8/200] Training loss: 0.03997652
[9/200] Training loss: 0.03971471
[10/200] Training loss: 0.03543905
[50/200] Training loss: 0.01769523
[100/200] Training loss: 0.01625122
[150/200] Training loss: 0.01491901
[200/200] Training loss: 0.01350274
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10656.641121854484 ----------
[1/200] Training loss: 0.16772276
[2/200] Training loss: 0.06133007
[3/200] Training loss: 0.05668176
[4/200] Training loss: 0.05238228
[5/200] Training loss: 0.04828499
[6/200] Training loss: 0.04775693
[7/200] Training loss: 0.04196825
[8/200] Training loss: 0.04351556
[9/200] Training loss: 0.03780600
[10/200] Training loss: 0.03282931
[50/200] Training loss: 0.01836602
[100/200] Training loss: 0.01542692
[150/200] Training loss: 0.01324507
[200/200] Training loss: 0.01185601
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20767.651769037348 ----------
[1/200] Training loss: 0.19143580
[2/200] Training loss: 0.07078532
[3/200] Training loss: 0.05870232
[4/200] Training loss: 0.05380984
[5/200] Training loss: 0.05118402
[6/200] Training loss: 0.04941103
[7/200] Training loss: 0.04549906
[8/200] Training loss: 0.04643751
[9/200] Training loss: 0.04373795
[10/200] Training loss: 0.04162714
[50/200] Training loss: 0.01908711
[100/200] Training loss: 0.01449582
[150/200] Training loss: 0.01341118
[200/200] Training loss: 0.01295143
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7924.070166271877 ----------
[1/200] Training loss: 0.14893798
[2/200] Training loss: 0.05392462
[3/200] Training loss: 0.05106538
[4/200] Training loss: 0.04351246
[5/200] Training loss: 0.04168773
[6/200] Training loss: 0.04021681
[7/200] Training loss: 0.03629055
[8/200] Training loss: 0.03537024
[9/200] Training loss: 0.03525870
[10/200] Training loss: 0.03105569
[50/200] Training loss: 0.01880734
[100/200] Training loss: 0.01545972
[150/200] Training loss: 0.01389431
[200/200] Training loss: 0.01401119
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15336.972321811108 ----------
[1/200] Training loss: 0.16758077
[2/200] Training loss: 0.06302491
[3/200] Training loss: 0.04949960
[4/200] Training loss: 0.05393848
[5/200] Training loss: 0.04715499
[6/200] Training loss: 0.04618301
[7/200] Training loss: 0.04216686
[8/200] Training loss: 0.03759876
[9/200] Training loss: 0.03561611
[10/200] Training loss: 0.03798553
[50/200] Training loss: 0.01749203
[100/200] Training loss: 0.01473470
[150/200] Training loss: 0.01322187
[200/200] Training loss: 0.01194768
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20693.945008141873 ----------
[1/200] Training loss: 0.18041716
[2/200] Training loss: 0.05199603
[3/200] Training loss: 0.04888808
[4/200] Training loss: 0.04561267
[5/200] Training loss: 0.04412529
[6/200] Training loss: 0.04133786
[7/200] Training loss: 0.03585809
[8/200] Training loss: 0.03740438
[9/200] Training loss: 0.03379560
[10/200] Training loss: 0.03282198
[50/200] Training loss: 0.01783474
[100/200] Training loss: 0.01505478
[150/200] Training loss: 0.01397959
[200/200] Training loss: 0.01253494
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8098.007656207791 ----------
[1/200] Training loss: 0.16063299
[2/200] Training loss: 0.05681583
[3/200] Training loss: 0.05009881
[4/200] Training loss: 0.04724035
[5/200] Training loss: 0.04452175
[6/200] Training loss: 0.04052449
[7/200] Training loss: 0.03659338
[8/200] Training loss: 0.03851976
[9/200] Training loss: 0.03254867
[10/200] Training loss: 0.03158492
[50/200] Training loss: 0.01712902
[100/200] Training loss: 0.01407500
[150/200] Training loss: 0.01331472
[200/200] Training loss: 0.01248971
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13255.863608230133 ----------
[1/200] Training loss: 0.19305633
[2/200] Training loss: 0.06013320
[3/200] Training loss: 0.05317366
[4/200] Training loss: 0.04847225
[5/200] Training loss: 0.04411905
[6/200] Training loss: 0.04420175
[7/200] Training loss: 0.04180806
[8/200] Training loss: 0.03885067
[9/200] Training loss: 0.03784625
[10/200] Training loss: 0.03709236
[50/200] Training loss: 0.01680477
[100/200] Training loss: 0.01479909
[150/200] Training loss: 0.01370204
[200/200] Training loss: 0.01314417
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15597.264375524319 ----------
[1/200] Training loss: 0.17839238
[2/200] Training loss: 0.05637434
[3/200] Training loss: 0.05214075
[4/200] Training loss: 0.04639452
[5/200] Training loss: 0.04260138
[6/200] Training loss: 0.04002471
[7/200] Training loss: 0.03908287
[8/200] Training loss: 0.03656812
[9/200] Training loss: 0.03689739
[10/200] Training loss: 0.03328221
[50/200] Training loss: 0.01774829
[100/200] Training loss: 0.01449710
[150/200] Training loss: 0.01384740
[200/200] Training loss: 0.01265982
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8001.723064440558 ----------
[1/200] Training loss: 0.16160459
[2/200] Training loss: 0.06333526
[3/200] Training loss: 0.05461882
[4/200] Training loss: 0.04743667
[5/200] Training loss: 0.04470630
[6/200] Training loss: 0.04112327
[7/200] Training loss: 0.03585246
[8/200] Training loss: 0.03290866
[9/200] Training loss: 0.03378608
[10/200] Training loss: 0.03118548
[50/200] Training loss: 0.01742339
[100/200] Training loss: 0.01579236
[150/200] Training loss: 0.01491250
[200/200] Training loss: 0.01419949
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18168.402461416357 ----------
[1/200] Training loss: 0.15972814
[2/200] Training loss: 0.05789669
[3/200] Training loss: 0.05048617
[4/200] Training loss: 0.04615407
[5/200] Training loss: 0.04361635
[6/200] Training loss: 0.03982445
[7/200] Training loss: 0.03545839
[8/200] Training loss: 0.03539538
[9/200] Training loss: 0.03273586
[10/200] Training loss: 0.03403968
[50/200] Training loss: 0.01696386
[100/200] Training loss: 0.01393520
[150/200] Training loss: 0.01368618
[200/200] Training loss: 0.01202754
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7923.318496690638 ----------
[1/200] Training loss: 0.16189338
[2/200] Training loss: 0.06360202
[3/200] Training loss: 0.05346014
[4/200] Training loss: 0.05073309
[5/200] Training loss: 0.04747224
[6/200] Training loss: 0.04440251
[7/200] Training loss: 0.04283632
[8/200] Training loss: 0.03963088
[9/200] Training loss: 0.03627636
[10/200] Training loss: 0.03608121
[50/200] Training loss: 0.01793349
[100/200] Training loss: 0.01470299
[150/200] Training loss: 0.01268623
[200/200] Training loss: 0.01253404
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19194.647587283285 ----------
[1/200] Training loss: 0.17324421
[2/200] Training loss: 0.06521969
[3/200] Training loss: 0.05576965
[4/200] Training loss: 0.05186618
[5/200] Training loss: 0.04940941
[6/200] Training loss: 0.04834098
[7/200] Training loss: 0.04686872
[8/200] Training loss: 0.04420442
[9/200] Training loss: 0.04087838
[10/200] Training loss: 0.03955195
[50/200] Training loss: 0.01811395
[100/200] Training loss: 0.01533908
[150/200] Training loss: 0.01468901
[200/200] Training loss: 0.01345748
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16912.62912737106 ----------
[1/200] Training loss: 0.15047622
[2/200] Training loss: 0.06179596
[3/200] Training loss: 0.05389991
[4/200] Training loss: 0.05101130
[5/200] Training loss: 0.04617784
[6/200] Training loss: 0.04449717
[7/200] Training loss: 0.03966143
[8/200] Training loss: 0.04033430
[9/200] Training loss: 0.03970908
[10/200] Training loss: 0.03611873
[50/200] Training loss: 0.01954849
[100/200] Training loss: 0.01549038
[150/200] Training loss: 0.01369916
[200/200] Training loss: 0.01228303
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8542.432440470337 ----------
[1/200] Training loss: 0.17536503
[2/200] Training loss: 0.06227616
[3/200] Training loss: 0.05227996
[4/200] Training loss: 0.04965129
[5/200] Training loss: 0.04766445
[6/200] Training loss: 0.04659904
[7/200] Training loss: 0.04605929
[8/200] Training loss: 0.04231037
[9/200] Training loss: 0.03771045
[10/200] Training loss: 0.03900126
[50/200] Training loss: 0.01849219
[100/200] Training loss: 0.01610574
[150/200] Training loss: 0.01522687
[200/200] Training loss: 0.01398002
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11791.546124236635 ----------
[1/200] Training loss: 0.14510115
[2/200] Training loss: 0.05598885
[3/200] Training loss: 0.04967624
[4/200] Training loss: 0.04769385
[5/200] Training loss: 0.04484136
[6/200] Training loss: 0.03885205
[7/200] Training loss: 0.04067739
[8/200] Training loss: 0.03712220
[9/200] Training loss: 0.03456529
[10/200] Training loss: 0.03307871
[50/200] Training loss: 0.01929845
[100/200] Training loss: 0.01574370
[150/200] Training loss: 0.01403356
[200/200] Training loss: 0.01251882
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8379.195665456202 ----------
[1/200] Training loss: 0.15644664
[2/200] Training loss: 0.05802171
[3/200] Training loss: 0.05196239
[4/200] Training loss: 0.04906734
[5/200] Training loss: 0.04494737
[6/200] Training loss: 0.04272980
[7/200] Training loss: 0.03958616
[8/200] Training loss: 0.03505847
[9/200] Training loss: 0.03511637
[10/200] Training loss: 0.03290827
[50/200] Training loss: 0.01771249
[100/200] Training loss: 0.01443673
[150/200] Training loss: 0.01276120
[200/200] Training loss: 0.01133240
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19847.908101359197 ----------
[1/200] Training loss: 0.15043640
[2/200] Training loss: 0.05937425
[3/200] Training loss: 0.05379217
[4/200] Training loss: 0.05253112
[5/200] Training loss: 0.04651476
[6/200] Training loss: 0.04460636
[7/200] Training loss: 0.04076390
[8/200] Training loss: 0.03563032
[9/200] Training loss: 0.03390926
[10/200] Training loss: 0.03420346
[50/200] Training loss: 0.01785949
[100/200] Training loss: 0.01489279
[150/200] Training loss: 0.01392783
[200/200] Training loss: 0.01186307
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 31815.94870501271 ----------
[1/200] Training loss: 0.16385823
[2/200] Training loss: 0.05709751
[3/200] Training loss: 0.05410179
[4/200] Training loss: 0.05019183
[5/200] Training loss: 0.05082350
[6/200] Training loss: 0.04585700
[7/200] Training loss: 0.04539804
[8/200] Training loss: 0.03934434
[9/200] Training loss: 0.03831829
[10/200] Training loss: 0.03285250
[50/200] Training loss: 0.01818367
[100/200] Training loss: 0.01647030
[150/200] Training loss: 0.01410373
[200/200] Training loss: 0.01340149
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6448.202230079327 ----------
[1/200] Training loss: 0.16545184
[2/200] Training loss: 0.05493919
[3/200] Training loss: 0.04785758
[4/200] Training loss: 0.04552394
[5/200] Training loss: 0.04194223
[6/200] Training loss: 0.04113616
[7/200] Training loss: 0.03708723
[8/200] Training loss: 0.03690188
[9/200] Training loss: 0.03476746
[10/200] Training loss: 0.03033738
[50/200] Training loss: 0.01859384
[100/200] Training loss: 0.01430102
[150/200] Training loss: 0.01336292
[200/200] Training loss: 0.01141750
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5415.316241919764 ----------
[1/200] Training loss: 0.16826995
[2/200] Training loss: 0.06075125
[3/200] Training loss: 0.05389557
[4/200] Training loss: 0.05065796
[5/200] Training loss: 0.04774117
[6/200] Training loss: 0.04251322
[7/200] Training loss: 0.03871874
[8/200] Training loss: 0.04028714
[9/200] Training loss: 0.03571831
[10/200] Training loss: 0.03504377
[50/200] Training loss: 0.01945888
[100/200] Training loss: 0.01670831
[150/200] Training loss: 0.01448932
[200/200] Training loss: 0.01331687
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9354.119947915999 ----------
[1/200] Training loss: 0.16728843
[2/200] Training loss: 0.05748290
[3/200] Training loss: 0.05028063
[4/200] Training loss: 0.04833891
[5/200] Training loss: 0.04468715
[6/200] Training loss: 0.04180315
[7/200] Training loss: 0.03974805
[8/200] Training loss: 0.04033152
[9/200] Training loss: 0.03515180
[10/200] Training loss: 0.03268489
[50/200] Training loss: 0.01692448
[100/200] Training loss: 0.01381877
[150/200] Training loss: 0.01305300
[200/200] Training loss: 0.01198915
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14715.01274209438 ----------
[1/200] Training loss: 0.13985156
[2/200] Training loss: 0.05770713
[3/200] Training loss: 0.04852007
[4/200] Training loss: 0.04601519
[5/200] Training loss: 0.04375993
[6/200] Training loss: 0.03887058
[7/200] Training loss: 0.03700274
[8/200] Training loss: 0.03105416
[9/200] Training loss: 0.02990160
[10/200] Training loss: 0.02777028
[50/200] Training loss: 0.01632814
[100/200] Training loss: 0.01407728
[150/200] Training loss: 0.01226065
[200/200] Training loss: 0.01127905
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19752.30619446752 ----------
[1/200] Training loss: 0.17929309
[2/200] Training loss: 0.05787999
[3/200] Training loss: 0.05231874
[4/200] Training loss: 0.04936683
[5/200] Training loss: 0.04526195
[6/200] Training loss: 0.04429231
[7/200] Training loss: 0.04053816
[8/200] Training loss: 0.03921156
[9/200] Training loss: 0.03640119
[10/200] Training loss: 0.03661110
[50/200] Training loss: 0.01738734
[100/200] Training loss: 0.01431573
[150/200] Training loss: 0.01292901
[200/200] Training loss: 0.01187119
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19160.7724270187 ----------
[1/200] Training loss: 0.16031034
[2/200] Training loss: 0.05994243
[3/200] Training loss: 0.05530513
[4/200] Training loss: 0.05063395
[5/200] Training loss: 0.04681598
[6/200] Training loss: 0.04422656
[7/200] Training loss: 0.04237618
[8/200] Training loss: 0.04380716
[9/200] Training loss: 0.04174575
[10/200] Training loss: 0.03674102
[50/200] Training loss: 0.01827624
[100/200] Training loss: 0.01496494
[150/200] Training loss: 0.01342443
[200/200] Training loss: 0.01256115
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13049.921992104015 ----------
[1/200] Training loss: 0.15885987
[2/200] Training loss: 0.05945127
[3/200] Training loss: 0.04918251
[4/200] Training loss: 0.04743204
[5/200] Training loss: 0.04485989
[6/200] Training loss: 0.04222393
[7/200] Training loss: 0.03730113
[8/200] Training loss: 0.03199687
[9/200] Training loss: 0.03536515
[10/200] Training loss: 0.03054948
[50/200] Training loss: 0.01856474
[100/200] Training loss: 0.01558875
[150/200] Training loss: 0.01381475
[200/200] Training loss: 0.01276701
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17226.994630521018 ----------
[1/200] Training loss: 0.15920003
[2/200] Training loss: 0.05422849
[3/200] Training loss: 0.04697363
[4/200] Training loss: 0.04409345
[5/200] Training loss: 0.03892224
[6/200] Training loss: 0.03718597
[7/200] Training loss: 0.03472790
[8/200] Training loss: 0.03377827
[9/200] Training loss: 0.03323215
[10/200] Training loss: 0.03121029
[50/200] Training loss: 0.01762261
[100/200] Training loss: 0.01559688
[150/200] Training loss: 0.01312549
[200/200] Training loss: 0.01253902
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19498.149245505327 ----------
[1/200] Training loss: 0.15491998
[2/200] Training loss: 0.05504216
[3/200] Training loss: 0.05163633
[4/200] Training loss: 0.04605031
[5/200] Training loss: 0.04451926
[6/200] Training loss: 0.04394131
[7/200] Training loss: 0.04129819
[8/200] Training loss: 0.03769643
[9/200] Training loss: 0.03599076
[10/200] Training loss: 0.03363317
[50/200] Training loss: 0.01792133
[100/200] Training loss: 0.01413370
[150/200] Training loss: 0.01276147
[200/200] Training loss: 0.01174145
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13332.021602142715 ----------
[1/200] Training loss: 0.18926148
[2/200] Training loss: 0.06785466
[3/200] Training loss: 0.05954543
[4/200] Training loss: 0.05682417
[5/200] Training loss: 0.05361476
[6/200] Training loss: 0.05359128
[7/200] Training loss: 0.04991262
[8/200] Training loss: 0.04675981
[9/200] Training loss: 0.04605645
[10/200] Training loss: 0.04474444
[50/200] Training loss: 0.01964277
[100/200] Training loss: 0.01660768
[150/200] Training loss: 0.01529059
[200/200] Training loss: 0.01426923
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17844.06680104062 ----------
[1/200] Training loss: 0.17246091
[2/200] Training loss: 0.06282985
[3/200] Training loss: 0.04975060
[4/200] Training loss: 0.04771382
[5/200] Training loss: 0.04448732
[6/200] Training loss: 0.04294816
[7/200] Training loss: 0.04206458
[8/200] Training loss: 0.03576854
[9/200] Training loss: 0.03686229
[10/200] Training loss: 0.03615454
[50/200] Training loss: 0.01763764
[100/200] Training loss: 0.01521143
[150/200] Training loss: 0.01371549
[200/200] Training loss: 0.01289255
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13310.765567765064 ----------
[1/200] Training loss: 0.18649722
[2/200] Training loss: 0.06544216
[3/200] Training loss: 0.05495681
[4/200] Training loss: 0.05356227
[5/200] Training loss: 0.05095714
[6/200] Training loss: 0.04912955
[7/200] Training loss: 0.04842647
[8/200] Training loss: 0.04416740
[9/200] Training loss: 0.04541008
[10/200] Training loss: 0.03957726
[50/200] Training loss: 0.02009696
[100/200] Training loss: 0.01535289
[150/200] Training loss: 0.01464985
[200/200] Training loss: 0.01321778
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7714.028000986255 ----------
[1/200] Training loss: 0.14911251
[2/200] Training loss: 0.05859485
[3/200] Training loss: 0.04955458
[4/200] Training loss: 0.04456355
[5/200] Training loss: 0.04199219
[6/200] Training loss: 0.03974756
[7/200] Training loss: 0.03962391
[8/200] Training loss: 0.03657667
[9/200] Training loss: 0.03547587
[10/200] Training loss: 0.03367406
[50/200] Training loss: 0.01792741
[100/200] Training loss: 0.01548417
[150/200] Training loss: 0.01449222
[200/200] Training loss: 0.01336883
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14598.975854490616 ----------
[1/200] Training loss: 0.16552164
[2/200] Training loss: 0.06126246
[3/200] Training loss: 0.05427059
[4/200] Training loss: 0.05032476
[5/200] Training loss: 0.04399475
[6/200] Training loss: 0.04575325
[7/200] Training loss: 0.04326139
[8/200] Training loss: 0.03924641
[9/200] Training loss: 0.03778761
[10/200] Training loss: 0.03708949
[50/200] Training loss: 0.01918505
[100/200] Training loss: 0.01533855
[150/200] Training loss: 0.01348671
[200/200] Training loss: 0.01277210
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9111.746265123937 ----------
[1/200] Training loss: 0.15346345
[2/200] Training loss: 0.05942126
[3/200] Training loss: 0.05217266
[4/200] Training loss: 0.04980795
[5/200] Training loss: 0.04434967
[6/200] Training loss: 0.04143291
[7/200] Training loss: 0.03923608
[8/200] Training loss: 0.03568055
[9/200] Training loss: 0.03750188
[10/200] Training loss: 0.03394607
[50/200] Training loss: 0.01759153
[100/200] Training loss: 0.01552023
[150/200] Training loss: 0.01391277
[200/200] Training loss: 0.01207033
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12676.472064419186 ----------
[1/200] Training loss: 0.15938851
[2/200] Training loss: 0.05720015
[3/200] Training loss: 0.04997500
[4/200] Training loss: 0.04802178
[5/200] Training loss: 0.04244452
[6/200] Training loss: 0.04122847
[7/200] Training loss: 0.03997083
[8/200] Training loss: 0.03691110
[9/200] Training loss: 0.03306433
[10/200] Training loss: 0.03001570
[50/200] Training loss: 0.01894927
[100/200] Training loss: 0.01596951
[150/200] Training loss: 0.01422288
[200/200] Training loss: 0.01366249
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16708.643032873733 ----------
[1/200] Training loss: 0.14454427
[2/200] Training loss: 0.05395273
[3/200] Training loss: 0.05020450
[4/200] Training loss: 0.04209010
[5/200] Training loss: 0.04196570
[6/200] Training loss: 0.03758992
[7/200] Training loss: 0.03468275
[8/200] Training loss: 0.03122569
[9/200] Training loss: 0.02936918
[10/200] Training loss: 0.02892412
[50/200] Training loss: 0.01734741
[100/200] Training loss: 0.01502316
[150/200] Training loss: 0.01303941
[200/200] Training loss: 0.01172007
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14798.388020321672 ----------
[1/200] Training loss: 0.14144789
[2/200] Training loss: 0.05911878
[3/200] Training loss: 0.04984419
[4/200] Training loss: 0.04750324
[5/200] Training loss: 0.04175077
[6/200] Training loss: 0.03985029
[7/200] Training loss: 0.03662765
[8/200] Training loss: 0.03424775
[9/200] Training loss: 0.03505172
[10/200] Training loss: 0.03185707
[50/200] Training loss: 0.01770247
[100/200] Training loss: 0.01460098
[150/200] Training loss: 0.01313492
[200/200] Training loss: 0.01255996
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17454.987138351033 ----------
[1/200] Training loss: 0.16111890
[2/200] Training loss: 0.06004893
[3/200] Training loss: 0.05403809
[4/200] Training loss: 0.04564594
[5/200] Training loss: 0.04110386
[6/200] Training loss: 0.03892479
[7/200] Training loss: 0.03223735
[8/200] Training loss: 0.03324850
[9/200] Training loss: 0.03022393
[10/200] Training loss: 0.02992766
[50/200] Training loss: 0.01730696
[100/200] Training loss: 0.01481202
[150/200] Training loss: 0.01402317
[200/200] Training loss: 0.01280828
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7640.428783779088 ----------
[1/200] Training loss: 0.20648935
[2/200] Training loss: 0.06522361
[3/200] Training loss: 0.05317082
[4/200] Training loss: 0.05010213
[5/200] Training loss: 0.05006167
[6/200] Training loss: 0.04741357
[7/200] Training loss: 0.04341696
[8/200] Training loss: 0.04185924
[9/200] Training loss: 0.04044640
[10/200] Training loss: 0.03803526
[50/200] Training loss: 0.01967817
[100/200] Training loss: 0.01610174
[150/200] Training loss: 0.01427714
[200/200] Training loss: 0.01353519
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12971.658953271937 ----------
[1/200] Training loss: 0.15412876
[2/200] Training loss: 0.05674697
[3/200] Training loss: 0.05222053
[4/200] Training loss: 0.05232231
[5/200] Training loss: 0.04609491
[6/200] Training loss: 0.04255685
[7/200] Training loss: 0.04261651
[8/200] Training loss: 0.04055670
[9/200] Training loss: 0.03584376
[10/200] Training loss: 0.03312909
[50/200] Training loss: 0.01861243
[100/200] Training loss: 0.01572654
[150/200] Training loss: 0.01411718
[200/200] Training loss: 0.01323163
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12540.313233727458 ----------
[1/200] Training loss: 0.17023861
[2/200] Training loss: 0.06217850
[3/200] Training loss: 0.05788636
[4/200] Training loss: 0.05421034
[5/200] Training loss: 0.05094173
[6/200] Training loss: 0.04865108
[7/200] Training loss: 0.04711635
[8/200] Training loss: 0.04513946
[9/200] Training loss: 0.03905179
[10/200] Training loss: 0.03915928
[50/200] Training loss: 0.01850797
[100/200] Training loss: 0.01581582
[150/200] Training loss: 0.01388069
[200/200] Training loss: 0.01338089
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11261.389257103228 ----------
[1/200] Training loss: 0.17957931
[2/200] Training loss: 0.06272371
[3/200] Training loss: 0.05528728
[4/200] Training loss: 0.05323977
[5/200] Training loss: 0.05168115
[6/200] Training loss: 0.04866394
[7/200] Training loss: 0.04548167
[8/200] Training loss: 0.04448093
[9/200] Training loss: 0.04315183
[10/200] Training loss: 0.04036406
[50/200] Training loss: 0.01862952
[100/200] Training loss: 0.01613556
[150/200] Training loss: 0.01501670
[200/200] Training loss: 0.01374210
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10464.326829758329 ----------
[1/200] Training loss: 0.17298154
[2/200] Training loss: 0.05447191
[3/200] Training loss: 0.04967182
[4/200] Training loss: 0.05030530
[5/200] Training loss: 0.04587207
[6/200] Training loss: 0.04319979
[7/200] Training loss: 0.03865088
[8/200] Training loss: 0.03946608
[9/200] Training loss: 0.03524459
[10/200] Training loss: 0.03376321
[50/200] Training loss: 0.01877152
[100/200] Training loss: 0.01491314
[150/200] Training loss: 0.01342796
[200/200] Training loss: 0.01213770
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10926.584827840765 ----------
[1/200] Training loss: 0.15804835
[2/200] Training loss: 0.06124084
[3/200] Training loss: 0.05276937
[4/200] Training loss: 0.05011711
[5/200] Training loss: 0.04425585
[6/200] Training loss: 0.04484889
[7/200] Training loss: 0.03876268
[8/200] Training loss: 0.03780012
[9/200] Training loss: 0.03634845
[10/200] Training loss: 0.03331222
[50/200] Training loss: 0.01789799
[100/200] Training loss: 0.01531143
[150/200] Training loss: 0.01365526
[200/200] Training loss: 0.01301784
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11777.400052643197 ----------
[1/200] Training loss: 0.14900049
[2/200] Training loss: 0.05828117
[3/200] Training loss: 0.05336957
[4/200] Training loss: 0.05056521
[5/200] Training loss: 0.04428314
[6/200] Training loss: 0.04339152
[7/200] Training loss: 0.04093844
[8/200] Training loss: 0.04055067
[9/200] Training loss: 0.03614436
[10/200] Training loss: 0.03511563
[50/200] Training loss: 0.01817892
[100/200] Training loss: 0.01517794
[150/200] Training loss: 0.01332130
[200/200] Training loss: 0.01207157
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13986.71769930315 ----------
[1/200] Training loss: 0.18157722
[2/200] Training loss: 0.05960752
[3/200] Training loss: 0.05487653
[4/200] Training loss: 0.05417003
[5/200] Training loss: 0.05342071
[6/200] Training loss: 0.04903926
[7/200] Training loss: 0.04880524
[8/200] Training loss: 0.04821858
[9/200] Training loss: 0.04566058
[10/200] Training loss: 0.04200359
[50/200] Training loss: 0.01854192
[100/200] Training loss: 0.01579753
[150/200] Training loss: 0.01408244
[200/200] Training loss: 0.01315272
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18188.38926348345 ----------
[1/200] Training loss: 0.16740617
[2/200] Training loss: 0.05918218
[3/200] Training loss: 0.05346743
[4/200] Training loss: 0.04962886
[5/200] Training loss: 0.04594954
[6/200] Training loss: 0.04313504
[7/200] Training loss: 0.03936540
[8/200] Training loss: 0.03927316
[9/200] Training loss: 0.03610217
[10/200] Training loss: 0.03444014
[50/200] Training loss: 0.01798159
[100/200] Training loss: 0.01339192
[150/200] Training loss: 0.01259751
[200/200] Training loss: 0.01056181
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 24725.158037917572 ----------
[1/200] Training loss: 0.17229715
[2/200] Training loss: 0.06250851
[3/200] Training loss: 0.05796196
[4/200] Training loss: 0.05372215
[5/200] Training loss: 0.05238108
[6/200] Training loss: 0.04934214
[7/200] Training loss: 0.04664546
[8/200] Training loss: 0.04447043
[9/200] Training loss: 0.04295773
[10/200] Training loss: 0.04283441
[50/200] Training loss: 0.01708274
[100/200] Training loss: 0.01380048
[150/200] Training loss: 0.01274740
[200/200] Training loss: 0.01165317
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5974.205888651646 ----------
[1/200] Training loss: 0.16886873
[2/200] Training loss: 0.06256032
[3/200] Training loss: 0.05202889
[4/200] Training loss: 0.05159349
[5/200] Training loss: 0.05058422
[6/200] Training loss: 0.04942948
[7/200] Training loss: 0.04600717
[8/200] Training loss: 0.04263736
[9/200] Training loss: 0.03908786
[10/200] Training loss: 0.03560568
[50/200] Training loss: 0.01875847
[100/200] Training loss: 0.01701594
[150/200] Training loss: 0.01562725
[200/200] Training loss: 0.01398103
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18033.212913954074 ----------
[1/200] Training loss: 0.15606001
[2/200] Training loss: 0.05177323
[3/200] Training loss: 0.05138955
[4/200] Training loss: 0.04928902
[5/200] Training loss: 0.04142714
[6/200] Training loss: 0.04037804
[7/200] Training loss: 0.03951078
[8/200] Training loss: 0.03624284
[9/200] Training loss: 0.03580104
[10/200] Training loss: 0.03421523
[50/200] Training loss: 0.01787493
[100/200] Training loss: 0.01447847
[150/200] Training loss: 0.01297305
[200/200] Training loss: 0.01242985
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5472.208694850737 ----------
[1/200] Training loss: 0.15482998
[2/200] Training loss: 0.05924564
[3/200] Training loss: 0.04814841
[4/200] Training loss: 0.04578730
[5/200] Training loss: 0.04274537
[6/200] Training loss: 0.03670273
[7/200] Training loss: 0.03664250
[8/200] Training loss: 0.03303253
[9/200] Training loss: 0.03233857
[10/200] Training loss: 0.02995481
[50/200] Training loss: 0.01785400
[100/200] Training loss: 0.01616665
[150/200] Training loss: 0.01353049
[200/200] Training loss: 0.01276438
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15823.01159703803 ----------
[1/200] Training loss: 0.19027014
[2/200] Training loss: 0.06626762
[3/200] Training loss: 0.05982576
[4/200] Training loss: 0.05566412
[5/200] Training loss: 0.05214269
[6/200] Training loss: 0.04864340
[7/200] Training loss: 0.04610496
[8/200] Training loss: 0.04676805
[9/200] Training loss: 0.04156530
[10/200] Training loss: 0.04242653
[50/200] Training loss: 0.02111236
[100/200] Training loss: 0.01687701
[150/200] Training loss: 0.01433313
[200/200] Training loss: 0.01269057
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9619.398733808679 ----------
[1/200] Training loss: 0.14025544
[2/200] Training loss: 0.05552600
[3/200] Training loss: 0.05017693
[4/200] Training loss: 0.04443323
[5/200] Training loss: 0.04144936
[6/200] Training loss: 0.04146500
[7/200] Training loss: 0.04077820
[8/200] Training loss: 0.03467380
[9/200] Training loss: 0.03106306
[10/200] Training loss: 0.02977554
[50/200] Training loss: 0.01787151
[100/200] Training loss: 0.01366020
[150/200] Training loss: 0.01277951
[200/200] Training loss: 0.01183792
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14105.483756326828 ----------
[1/200] Training loss: 0.17096798
[2/200] Training loss: 0.06476552
[3/200] Training loss: 0.05485045
[4/200] Training loss: 0.05093372
[5/200] Training loss: 0.04875163
[6/200] Training loss: 0.04832257
[7/200] Training loss: 0.04604755
[8/200] Training loss: 0.04465478
[9/200] Training loss: 0.04234558
[10/200] Training loss: 0.04011498
[50/200] Training loss: 0.01866648
[100/200] Training loss: 0.01631341
[150/200] Training loss: 0.01429969
[200/200] Training loss: 0.01312386
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17735.59313922148 ----------
[1/200] Training loss: 0.16763849
[2/200] Training loss: 0.05220748
[3/200] Training loss: 0.04451242
[4/200] Training loss: 0.03362583
[5/200] Training loss: 0.03390542
[6/200] Training loss: 0.03609010
[7/200] Training loss: 0.02902948
[8/200] Training loss: 0.03055034
[9/200] Training loss: 0.03925820
[10/200] Training loss: 0.03442370
[50/200] Training loss: 0.01543403
[100/200] Training loss: 0.01498711
[150/200] Training loss: 0.01056793
[200/200] Training loss: 0.01020015
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adamax ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21274.505305646944 ----------
[1/200] Training loss: 0.15880390
[2/200] Training loss: 0.05552057
[3/200] Training loss: 0.04828772
[4/200] Training loss: 0.04493324
[5/200] Training loss: 0.04102482
[6/200] Training loss: 0.03543469
[7/200] Training loss: 0.03385952
[8/200] Training loss: 0.03148300
[9/200] Training loss: 0.03262383
[10/200] Training loss: 0.02700415
[50/200] Training loss: 0.01722503
[100/200] Training loss: 0.01434326
[150/200] Training loss: 0.01220748
[200/200] Training loss: 0.01169917
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7650.6323921621015 ----------
[1/200] Training loss: 0.17114634
[2/200] Training loss: 0.06407881
[3/200] Training loss: 0.05448635
[4/200] Training loss: 0.05565457
[5/200] Training loss: 0.05139303
[6/200] Training loss: 0.04762497
[7/200] Training loss: 0.04638405
[8/200] Training loss: 0.04664072
[9/200] Training loss: 0.04264430
[10/200] Training loss: 0.04213274
[50/200] Training loss: 0.01825141
[100/200] Training loss: 0.01664866
[150/200] Training loss: 0.01422702
[200/200] Training loss: 0.01355329
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8582.550203756457 ----------
[1/200] Training loss: 0.14849011
[2/200] Training loss: 0.05917752
[3/200] Training loss: 0.05146533
[4/200] Training loss: 0.04844829
[5/200] Training loss: 0.04551711
[6/200] Training loss: 0.04543633
[7/200] Training loss: 0.04076028
[8/200] Training loss: 0.03926318
[9/200] Training loss: 0.04004215
[10/200] Training loss: 0.03086929
[50/200] Training loss: 0.01763151
[100/200] Training loss: 0.01426186
[150/200] Training loss: 0.01279926
[200/200] Training loss: 0.01173177
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19879.430977771975 ----------
[1/200] Training loss: 0.18083516
[2/200] Training loss: 0.06229977
[3/200] Training loss: 0.05330051
[4/200] Training loss: 0.05422912
[5/200] Training loss: 0.04722132
[6/200] Training loss: 0.04677686
[7/200] Training loss: 0.04561457
[8/200] Training loss: 0.04021797
[9/200] Training loss: 0.03797846
[10/200] Training loss: 0.03472224
[50/200] Training loss: 0.01658616
[100/200] Training loss: 0.01537238
[150/200] Training loss: 0.01422093
[200/200] Training loss: 0.01335720
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12288.754534125905 ----------
[1/200] Training loss: 0.17388350
[2/200] Training loss: 0.06023625
[3/200] Training loss: 0.05524792
[4/200] Training loss: 0.04887892
[5/200] Training loss: 0.04785724
[6/200] Training loss: 0.04236957
[7/200] Training loss: 0.04013480
[8/200] Training loss: 0.04065096
[9/200] Training loss: 0.03656582
[10/200] Training loss: 0.03134320
[50/200] Training loss: 0.01866673
[100/200] Training loss: 0.01481233
[150/200] Training loss: 0.01381715
[200/200] Training loss: 0.01267272
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13730.050546156048 ----------
[1/200] Training loss: 0.17712191
[2/200] Training loss: 0.06658488
[3/200] Training loss: 0.05612213
[4/200] Training loss: 0.05396730
[5/200] Training loss: 0.05484157
[6/200] Training loss: 0.05377857
[7/200] Training loss: 0.04979270
[8/200] Training loss: 0.04735744
[9/200] Training loss: 0.04749694
[10/200] Training loss: 0.04418753
[50/200] Training loss: 0.01997832
[100/200] Training loss: 0.01636407
[150/200] Training loss: 0.01423212
[200/200] Training loss: 0.01348343
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14886.681833101693 ----------
[1/200] Training loss: 0.16820960
[2/200] Training loss: 0.06138594
[3/200] Training loss: 0.05504841
[4/200] Training loss: 0.05213066
[5/200] Training loss: 0.04971702
[6/200] Training loss: 0.04955084
[7/200] Training loss: 0.04625656
[8/200] Training loss: 0.04221243
[9/200] Training loss: 0.04034947
[10/200] Training loss: 0.03834645
[50/200] Training loss: 0.01801324
[100/200] Training loss: 0.01607975
[150/200] Training loss: 0.01482687
[200/200] Training loss: 0.01462933
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17354.305056671095 ----------
[1/200] Training loss: 0.14449241
[2/200] Training loss: 0.04150624
[3/200] Training loss: 0.04121310
[4/200] Training loss: 0.04107613
[5/200] Training loss: 0.03971534
[6/200] Training loss: 0.03702199
[7/200] Training loss: 0.03044734
[8/200] Training loss: 0.02895810
[9/200] Training loss: 0.02867791
[10/200] Training loss: 0.02807010
[50/200] Training loss: 0.02676632
[100/200] Training loss: 0.02600194
[150/200] Training loss: 0.02561925
[200/200] Training loss: 0.02512579
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Rprop ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15152.567571207197 ----------
[1/200] Training loss: 0.13838066
[2/200] Training loss: 0.05188986
[3/200] Training loss: 0.04999407
[4/200] Training loss: 0.04303498
[5/200] Training loss: 0.04126421
[6/200] Training loss: 0.03821295
[7/200] Training loss: 0.03780840
[8/200] Training loss: 0.03340564
[9/200] Training loss: 0.03498201
[10/200] Training loss: 0.03090277
[50/200] Training loss: 0.01731278
[100/200] Training loss: 0.01451264
[150/200] Training loss: 0.01368994
[200/200] Training loss: 0.01231827
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4978.161307149458 ----------
[1/200] Training loss: 0.16754918
[2/200] Training loss: 0.06074768
[3/200] Training loss: 0.05513661
[4/200] Training loss: 0.04810694
[5/200] Training loss: 0.04723149
[6/200] Training loss: 0.04353944
[7/200] Training loss: 0.04172725
[8/200] Training loss: 0.03845688
[9/200] Training loss: 0.03731018
[10/200] Training loss: 0.03812059
[50/200] Training loss: 0.01882580
[100/200] Training loss: 0.01692957
[150/200] Training loss: 0.01566424
[200/200] Training loss: 0.01360056
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13601.242296202212 ----------
[1/200] Training loss: 0.14911235
[2/200] Training loss: 0.06009609
[3/200] Training loss: 0.05104447
[4/200] Training loss: 0.05045455
[5/200] Training loss: 0.04904541
[6/200] Training loss: 0.04501210
[7/200] Training loss: 0.04250187
[8/200] Training loss: 0.04032409
[9/200] Training loss: 0.03958663
[10/200] Training loss: 0.03669139
[50/200] Training loss: 0.01630681
[100/200] Training loss: 0.01454721
[150/200] Training loss: 0.01265585
[200/200] Training loss: 0.01099877
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15201.417828610593 ----------
[1/200] Training loss: 0.16885262
[2/200] Training loss: 0.06021791
[3/200] Training loss: 0.05148192
[4/200] Training loss: 0.05020679
[5/200] Training loss: 0.04477521
[6/200] Training loss: 0.04047968
[7/200] Training loss: 0.03783381
[8/200] Training loss: 0.03820874
[9/200] Training loss: 0.03448842
[10/200] Training loss: 0.03515071
[50/200] Training loss: 0.01651304
[100/200] Training loss: 0.01501932
[150/200] Training loss: 0.01331969
[200/200] Training loss: 0.01260153
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15145.778553775306 ----------
[1/200] Training loss: 0.14458884
[2/200] Training loss: 0.05549074
[3/200] Training loss: 0.04909271
[4/200] Training loss: 0.04723580
[5/200] Training loss: 0.04235917
[6/200] Training loss: 0.03812162
[7/200] Training loss: 0.03501847
[8/200] Training loss: 0.03392348
[9/200] Training loss: 0.03371024
[10/200] Training loss: 0.03141708
[50/200] Training loss: 0.01778704
[100/200] Training loss: 0.01548808
[150/200] Training loss: 0.01382029
[200/200] Training loss: 0.01261384
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 16 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8716.531879136335 ----------
[1/200] Training loss: 0.18520551
[2/200] Training loss: 0.06767816
[3/200] Training loss: 0.05539123
[4/200] Training loss: 0.05174547
[5/200] Training loss: 0.04932384
[6/200] Training loss: 0.04993959
[7/200] Training loss: 0.04690823
[8/200] Training loss: 0.04485880
[9/200] Training loss: 0.04454164
[10/200] Training loss: 0.04208777
[50/200] Training loss: 0.01936691
[100/200] Training loss: 0.01634172
[150/200] Training loss: 0.01200498
[200/200] Training loss: 0.01227563
---batch_size---: 16 ---n_steps---: 2 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5815.993810175523 ----------
[1/100] Training loss: 0.16325459
[2/100] Training loss: 0.05899330
[3/100] Training loss: 0.05428216
[4/100] Training loss: 0.05033349
[5/100] Training loss: 0.04830045
[6/100] Training loss: 0.04651227
[7/100] Training loss: 0.04372127
[8/100] Training loss: 0.04276478
[9/100] Training loss: 0.03899455
[10/100] Training loss: 0.03828895
[50/100] Training loss: 0.01839766
[100/100] Training loss: 0.01618430
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 100
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22061.00704863674 ----------
[1/200] Training loss: 0.13495476
[2/200] Training loss: 0.05769154
[3/200] Training loss: 0.05064835
[4/200] Training loss: 0.04530084
[5/200] Training loss: 0.04379860
[6/200] Training loss: 0.03748109
[7/200] Training loss: 0.03787267
[8/200] Training loss: 0.03329132
[9/200] Training loss: 0.03273317
[10/200] Training loss: 0.03123303
[50/200] Training loss: 0.01776121
[100/200] Training loss: 0.01572527
[150/200] Training loss: 0.01378439
[200/200] Training loss: 0.01258528
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 3937.8646497816553 ----------
[1/200] Training loss: 0.15392600
[2/200] Training loss: 0.05856505
[3/200] Training loss: 0.04908350
[4/200] Training loss: 0.04679225
[5/200] Training loss: 0.04401087
[6/200] Training loss: 0.03962199
[7/200] Training loss: 0.03470676
[8/200] Training loss: 0.03392765
[9/200] Training loss: 0.03402819
[10/200] Training loss: 0.02910594
[50/200] Training loss: 0.01668684
[100/200] Training loss: 0.01355492
[150/200] Training loss: 0.01250951
[200/200] Training loss: 0.01169825
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9417.249704664308 ----------
[1/200] Training loss: 0.15473506
[2/200] Training loss: 0.05559004
[3/200] Training loss: 0.05328760
[4/200] Training loss: 0.04936429
[5/200] Training loss: 0.04676204
[6/200] Training loss: 0.04337889
[7/200] Training loss: 0.04259539
[8/200] Training loss: 0.03978086
[9/200] Training loss: 0.03895253
[10/200] Training loss: 0.03457138
[50/200] Training loss: 0.01916385
[100/200] Training loss: 0.01575288
[150/200] Training loss: 0.01456630
[200/200] Training loss: 0.01327281
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7865.0146853009755 ----------
[1/200] Training loss: 0.17876179
[2/200] Training loss: 0.06445834
[3/200] Training loss: 0.05462173
[4/200] Training loss: 0.05109622
[5/200] Training loss: 0.05055752
[6/200] Training loss: 0.04645077
[7/200] Training loss: 0.04450887
[8/200] Training loss: 0.04156478
[9/200] Training loss: 0.04127397
[10/200] Training loss: 0.03796275
[50/200] Training loss: 0.01978666
[100/200] Training loss: 0.01544936
[150/200] Training loss: 0.01380224
[200/200] Training loss: 0.01290489
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12633.612943255781 ----------
[1/200] Training loss: 0.17273656
[2/200] Training loss: 0.06266950
[3/200] Training loss: 0.05830614
[4/200] Training loss: 0.05463405
[5/200] Training loss: 0.05188419
[6/200] Training loss: 0.05164549
[7/200] Training loss: 0.04833759
[8/200] Training loss: 0.04603655
[9/200] Training loss: 0.04513613
[10/200] Training loss: 0.04367531
[50/200] Training loss: 0.01966894
[100/200] Training loss: 0.01609992
[150/200] Training loss: 0.01493060
[200/200] Training loss: 0.01387662
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18094.898286533695 ----------
[1/200] Training loss: 0.15382747
[2/200] Training loss: 0.06020807
[3/200] Training loss: 0.05230830
[4/200] Training loss: 0.05037035
[5/200] Training loss: 0.04594944
[6/200] Training loss: 0.04257464
[7/200] Training loss: 0.03988703
[8/200] Training loss: 0.03713278
[9/200] Training loss: 0.03572084
[10/200] Training loss: 0.03388124
[50/200] Training loss: 0.01846139
[100/200] Training loss: 0.01552630
[150/200] Training loss: 0.01465513
[200/200] Training loss: 0.01307267
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13944.662634857827 ----------
[1/200] Training loss: 0.15014118
[2/200] Training loss: 0.06255604
[3/200] Training loss: 0.05333411
[4/200] Training loss: 0.04951855
[5/200] Training loss: 0.04616429
[6/200] Training loss: 0.04156606
[7/200] Training loss: 0.04181345
[8/200] Training loss: 0.03755544
[9/200] Training loss: 0.03352651
[10/200] Training loss: 0.03233376
[50/200] Training loss: 0.01883369
[100/200] Training loss: 0.01536189
[150/200] Training loss: 0.01428015
[200/200] Training loss: 0.01358337
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19088.97566659877 ----------
[1/200] Training loss: 0.16647921
[2/200] Training loss: 0.05713649
[3/200] Training loss: 0.05119561
[4/200] Training loss: 0.04416915
[5/200] Training loss: 0.04295494
[6/200] Training loss: 0.03975874
[7/200] Training loss: 0.03749669
[8/200] Training loss: 0.03470947
[9/200] Training loss: 0.03293325
[10/200] Training loss: 0.03485879
[50/200] Training loss: 0.01906897
[100/200] Training loss: 0.01680259
[150/200] Training loss: 0.01464439
[200/200] Training loss: 0.01335176
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16034.363099293965 ----------
[1/200] Training loss: 0.15315456
[2/200] Training loss: 0.06237045
[3/200] Training loss: 0.05110684
[4/200] Training loss: 0.05019669
[5/200] Training loss: 0.04254545
[6/200] Training loss: 0.04001276
[7/200] Training loss: 0.03934455
[8/200] Training loss: 0.03453031
[9/200] Training loss: 0.03176865
[10/200] Training loss: 0.03025432
[50/200] Training loss: 0.01705253
[100/200] Training loss: 0.01478933
[150/200] Training loss: 0.01304072
[200/200] Training loss: 0.01204485
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22433.937148882273 ----------
[1/200] Training loss: 0.15953618
[2/200] Training loss: 0.05720704
[3/200] Training loss: 0.05462562
[4/200] Training loss: 0.04847753
[5/200] Training loss: 0.04894030
[6/200] Training loss: 0.04249264
[7/200] Training loss: 0.04136618
[8/200] Training loss: 0.03783839
[9/200] Training loss: 0.03649875
[10/200] Training loss: 0.03574269
[50/200] Training loss: 0.01778998
[100/200] Training loss: 0.01447988
[150/200] Training loss: 0.01300578
[200/200] Training loss: 0.01269939
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5679.649637081498 ----------
[1/200] Training loss: 0.16231378
[2/200] Training loss: 0.05764866
[3/200] Training loss: 0.04921578
[4/200] Training loss: 0.04449785
[5/200] Training loss: 0.04137116
[6/200] Training loss: 0.03910778
[7/200] Training loss: 0.03853137
[8/200] Training loss: 0.03457741
[9/200] Training loss: 0.03443829
[10/200] Training loss: 0.03261011
[50/200] Training loss: 0.01701689
[100/200] Training loss: 0.01440487
[150/200] Training loss: 0.01328792
[200/200] Training loss: 0.01242949
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7798.241340199725 ----------
[1/200] Training loss: 0.17007754
[2/200] Training loss: 0.06463785
[3/200] Training loss: 0.05417008
[4/200] Training loss: 0.05482150
[5/200] Training loss: 0.04812198
[6/200] Training loss: 0.04528489
[7/200] Training loss: 0.04179259
[8/200] Training loss: 0.04009612
[9/200] Training loss: 0.03719746
[10/200] Training loss: 0.03681549
[50/200] Training loss: 0.01995889
[100/200] Training loss: 0.01675245
[150/200] Training loss: 0.01582230
[200/200] Training loss: 0.01494947
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6511300082725237 ---num_layers---: 2
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 29601.805350349834 ----------
[1/200] Training loss: 0.15791922
[2/200] Training loss: 0.05939560
[3/200] Training loss: 0.04975654
[4/200] Training loss: 0.04829708
[5/200] Training loss: 0.04861228
[6/200] Training loss: 0.04108833
[7/200] Training loss: 0.03986430
[8/200] Training loss: 0.03828248
[9/200] Training loss: 0.03656904
[10/200] Training loss: 0.03433217
[50/200] Training loss: 0.01786755
[100/200] Training loss: 0.01491663
[150/200] Training loss: 0.01295964
[200/200] Training loss: 0.01178971
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10807.578452178823 ----------
[1/200] Training loss: 0.18107321
[2/200] Training loss: 0.05840068
[3/200] Training loss: 0.05152959
[4/200] Training loss: 0.04724107
[5/200] Training loss: 0.04274169
[6/200] Training loss: 0.04280681
[7/200] Training loss: 0.03917689
[8/200] Training loss: 0.03700772
[9/200] Training loss: 0.03558519
[10/200] Training loss: 0.03325419
[50/200] Training loss: 0.01781152
[100/200] Training loss: 0.01413115
[150/200] Training loss: 0.01365620
[200/200] Training loss: 0.01245373
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7316.523491385782 ----------
[1/200] Training loss: 0.14318129
[2/200] Training loss: 0.06047628
[3/200] Training loss: 0.05459099
[4/200] Training loss: 0.05427841
[5/200] Training loss: 0.04830821
[6/200] Training loss: 0.04787399
[7/200] Training loss: 0.04340053
[8/200] Training loss: 0.04124498
[9/200] Training loss: 0.03919589
[10/200] Training loss: 0.03365515
[50/200] Training loss: 0.01845003
[100/200] Training loss: 0.01500671
[150/200] Training loss: 0.01346224
[200/200] Training loss: 0.01318744
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13973.438231158429 ----------
[1/200] Training loss: 0.14825770
[2/200] Training loss: 0.06031387
[3/200] Training loss: 0.05005856
[4/200] Training loss: 0.04763811
[5/200] Training loss: 0.04203812
[6/200] Training loss: 0.04233855
[7/200] Training loss: 0.03583722
[8/200] Training loss: 0.03436764
[9/200] Training loss: 0.03344548
[10/200] Training loss: 0.02971049
[50/200] Training loss: 0.01848032
[100/200] Training loss: 0.01641513
[150/200] Training loss: 0.01496332
[200/200] Training loss: 0.01372990
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.029246319992685878
----FITNESS-----------RMSE---- 12445.520318572462 ----------
[1/200] Training loss: 0.17056404
[2/200] Training loss: 0.06606959
[3/200] Training loss: 0.05659154
[4/200] Training loss: 0.05208045
[5/200] Training loss: 0.04647126
[6/200] Training loss: 0.04265150
[7/200] Training loss: 0.03819860
[8/200] Training loss: 0.03772410
[9/200] Training loss: 0.03590080
[10/200] Training loss: 0.03493561
[50/200] Training loss: 0.01851762
[100/200] Training loss: 0.01414958
[150/200] Training loss: 0.01328841
[200/200] Training loss: 0.01090021
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 26883.6949841349 ----------
[1/200] Training loss: 0.13699362
[2/200] Training loss: 0.05971394
[3/200] Training loss: 0.05488925
[4/200] Training loss: 0.05224226
[5/200] Training loss: 0.05093517
[6/200] Training loss: 0.04724434
[7/200] Training loss: 0.04352462
[8/200] Training loss: 0.04135976
[9/200] Training loss: 0.03726855
[10/200] Training loss: 0.03354828
[50/200] Training loss: 0.01551255
[100/200] Training loss: 0.01371381
[150/200] Training loss: 0.01163817
[200/200] Training loss: 0.01137068
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 26906.753353015298 ----------
[1/200] Training loss: 0.14582674
[2/200] Training loss: 0.05880910
[3/200] Training loss: 0.05139087
[4/200] Training loss: 0.04823967
[5/200] Training loss: 0.04493465
[6/200] Training loss: 0.04396982
[7/200] Training loss: 0.04046143
[8/200] Training loss: 0.03839657
[9/200] Training loss: 0.03828829
[10/200] Training loss: 0.03424899
[50/200] Training loss: 0.01751581
[100/200] Training loss: 0.01453306
[150/200] Training loss: 0.01404334
[200/200] Training loss: 0.01211370
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4884.416239429232 ----------
[1/200] Training loss: 0.15815038
[2/200] Training loss: 0.05883490
[3/200] Training loss: 0.05340166
[4/200] Training loss: 0.05410356
[5/200] Training loss: 0.04735847
[6/200] Training loss: 0.04478603
[7/200] Training loss: 0.03933175
[8/200] Training loss: 0.04030658
[9/200] Training loss: 0.03697253
[10/200] Training loss: 0.03499121
[50/200] Training loss: 0.01729757
[100/200] Training loss: 0.01450551
[150/200] Training loss: 0.01346886
[200/200] Training loss: 0.01226489
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17511.68569841293 ----------
[1/200] Training loss: 0.16066547
[2/200] Training loss: 0.05916099
[3/200] Training loss: 0.05241385
[4/200] Training loss: 0.05220748
[5/200] Training loss: 0.04561341
[6/200] Training loss: 0.04586233
[7/200] Training loss: 0.04119646
[8/200] Training loss: 0.04041532
[9/200] Training loss: 0.03574551
[10/200] Training loss: 0.03602503
[50/200] Training loss: 0.01816211
[100/200] Training loss: 0.01450600
[150/200] Training loss: 0.01254392
[200/200] Training loss: 0.01192662
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5015.671241219863 ----------
[1/200] Training loss: 0.17836013
[2/200] Training loss: 0.05763091
[3/200] Training loss: 0.05407432
[4/200] Training loss: 0.04852612
[5/200] Training loss: 0.04633484
[6/200] Training loss: 0.04248335
[7/200] Training loss: 0.03748159
[8/200] Training loss: 0.03539833
[9/200] Training loss: 0.03445033
[10/200] Training loss: 0.03109857
[50/200] Training loss: 0.01857638
[100/200] Training loss: 0.01670545
[150/200] Training loss: 0.01553256
[200/200] Training loss: 0.01398846
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14635.960098333146 ----------
[1/200] Training loss: 0.14431934
[2/200] Training loss: 0.05753069
[3/200] Training loss: 0.05312386
[4/200] Training loss: 0.04607995
[5/200] Training loss: 0.04722984
[6/200] Training loss: 0.04331223
[7/200] Training loss: 0.04124359
[8/200] Training loss: 0.03887138
[9/200] Training loss: 0.03857788
[10/200] Training loss: 0.03537201
[50/200] Training loss: 0.01913656
[100/200] Training loss: 0.01518376
[150/200] Training loss: 0.01342869
[200/200] Training loss: 0.01230703
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12998.183873141663 ----------
[1/200] Training loss: 0.17335028
[2/200] Training loss: 0.06193067
[3/200] Training loss: 0.05269684
[4/200] Training loss: 0.04983855
[5/200] Training loss: 0.04945090
[6/200] Training loss: 0.04577850
[7/200] Training loss: 0.04100211
[8/200] Training loss: 0.04112935
[9/200] Training loss: 0.03633315
[10/200] Training loss: 0.03687185
[50/200] Training loss: 0.01839215
[100/200] Training loss: 0.01516031
[150/200] Training loss: 0.01391334
[200/200] Training loss: 0.01296779
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14595.021068843991 ----------
[1/200] Training loss: 0.21118116
[2/200] Training loss: 0.06962392
[3/200] Training loss: 0.06159828
[4/200] Training loss: 0.05764020
[5/200] Training loss: 0.05438087
[6/200] Training loss: 0.05342604
[7/200] Training loss: 0.05364519
[8/200] Training loss: 0.04772586
[9/200] Training loss: 0.04715286
[10/200] Training loss: 0.04737665
[50/200] Training loss: 0.01992109
[100/200] Training loss: 0.01721834
[150/200] Training loss: 0.01596703
[200/200] Training loss: 0.01436518
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15658.434149045683 ----------
[1/200] Training loss: 0.01631912
[2/200] Training loss: 0.00144428
[3/200] Training loss: 0.00139568
[4/200] Training loss: 0.00106494
[5/200] Training loss: 0.00091431
[6/200] Training loss: 0.00090685
[7/200] Training loss: 0.00076473
[8/200] Training loss: 0.00070024
[9/200] Training loss: 0.00060210
[10/200] Training loss: 0.00053878
[50/200] Training loss: 0.00021899
[100/200] Training loss: 0.00015444
[150/200] Training loss: 0.00016438
[200/200] Training loss: 0.00012818
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14254.136241807148 ----------
[1/200] Training loss: 0.16476378
[2/200] Training loss: 0.05477958
[3/200] Training loss: 0.04880285
[4/200] Training loss: 0.04977274
[5/200] Training loss: 0.03881573
[6/200] Training loss: 0.03637034
[7/200] Training loss: 0.03667183
[8/200] Training loss: 0.03181764
[9/200] Training loss: 0.03113452
[10/200] Training loss: 0.03332038
[50/200] Training loss: 0.01793393
[100/200] Training loss: 0.01578912
[150/200] Training loss: 0.01389405
[200/200] Training loss: 0.01258134
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 24634.39156951111 ----------
[1/200] Training loss: 0.17291662
[2/200] Training loss: 0.06547614
[3/200] Training loss: 0.05485715
[4/200] Training loss: 0.05157275
[5/200] Training loss: 0.05021128
[6/200] Training loss: 0.04610814
[7/200] Training loss: 0.04723627
[8/200] Training loss: 0.04343838
[9/200] Training loss: 0.04090982
[10/200] Training loss: 0.03950129
[50/200] Training loss: 0.01863521
[100/200] Training loss: 0.01663250
[150/200] Training loss: 0.01546916
[200/200] Training loss: 0.01431430
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12877.54572890347 ----------
[1/200] Training loss: 0.15978259
[2/200] Training loss: 0.05770581
[3/200] Training loss: 0.05314799
[4/200] Training loss: 0.04988770
[5/200] Training loss: 0.05134572
[6/200] Training loss: 0.04742088
[7/200] Training loss: 0.04642244
[8/200] Training loss: 0.04423151
[9/200] Training loss: 0.04307516
[10/200] Training loss: 0.04236420
[50/200] Training loss: 0.01863241
[100/200] Training loss: 0.01503925
[150/200] Training loss: 0.01283188
[200/200] Training loss: 0.01205168
---batch_size---: 16 ---n_steps---: 2 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16554.052071924867 ----------
[1/200] Training loss: 0.15504456
[2/200] Training loss: 0.06139862
[3/200] Training loss: 0.05348720
[4/200] Training loss: 0.05179180
[5/200] Training loss: 0.04879719
[6/200] Training loss: 0.04678138
[7/200] Training loss: 0.04588349
[8/200] Training loss: 0.04215616
[9/200] Training loss: 0.04096740
[10/200] Training loss: 0.04086809
[50/200] Training loss: 0.01816352
[100/200] Training loss: 0.01615878
[150/200] Training loss: 0.01476310
[200/200] Training loss: 0.01368163
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17340.43505797937 ----------
[1/200] Training loss: 0.16563884
[2/200] Training loss: 0.05859863
[3/200] Training loss: 0.05258787
[4/200] Training loss: 0.04820784
[5/200] Training loss: 0.04391032
[6/200] Training loss: 0.03938221
[7/200] Training loss: 0.04186347
[8/200] Training loss: 0.03627637
[9/200] Training loss: 0.03241749
[10/200] Training loss: 0.03403890
[50/200] Training loss: 0.01930299
[100/200] Training loss: 0.01573249
[150/200] Training loss: 0.01347925
[200/200] Training loss: 0.01269772
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7133.0484366783885 ----------
[1/200] Training loss: 0.16090182
[2/200] Training loss: 0.06381911
[3/200] Training loss: 0.05448674
[4/200] Training loss: 0.05305840
[5/200] Training loss: 0.05034713
[6/200] Training loss: 0.04576623
[7/200] Training loss: 0.04284902
[8/200] Training loss: 0.04352150
[9/200] Training loss: 0.03479580
[10/200] Training loss: 0.03801950
[50/200] Training loss: 0.01725157
[100/200] Training loss: 0.01471978
[150/200] Training loss: 0.01388918
[200/200] Training loss: 0.01258112
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 16 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18136.09880872951 ----------
[1/200] Training loss: 0.14364303
[2/200] Training loss: 0.05128586
[3/200] Training loss: 0.05041190
[4/200] Training loss: 0.04467710
[5/200] Training loss: 0.04012681
[6/200] Training loss: 0.03370426
[7/200] Training loss: 0.03424508
[8/200] Training loss: 0.03003431
[9/200] Training loss: 0.03044674
[10/200] Training loss: 0.02760047
[50/200] Training loss: 0.01781064
[100/200] Training loss: 0.01519435
[150/200] Training loss: 0.01400996
[200/200] Training loss: 0.01250516
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16647.804900346473 ----------
[1/200] Training loss: 0.13955195
[2/200] Training loss: 0.05733943
[3/200] Training loss: 0.05360365
[4/200] Training loss: 0.05024673
[5/200] Training loss: 0.04639459
[6/200] Training loss: 0.04195447
[7/200] Training loss: 0.04089267
[8/200] Training loss: 0.03901947
[9/200] Training loss: 0.03666262
[10/200] Training loss: 0.03427167
[50/200] Training loss: 0.01639257
[100/200] Training loss: 0.01399853
[150/200] Training loss: 0.01260467
[200/200] Training loss: 0.01166495
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14474.847149452045 ----------
[1/200] Training loss: 0.14097366
[2/200] Training loss: 0.05491265
[3/200] Training loss: 0.04936773
[4/200] Training loss: 0.04015775
[5/200] Training loss: 0.04030986
[6/200] Training loss: 0.03661982
[7/200] Training loss: 0.03601722
[8/200] Training loss: 0.03529259
[9/200] Training loss: 0.03464372
[10/200] Training loss: 0.03122570
[50/200] Training loss: 0.01809515
[100/200] Training loss: 0.01452004
[150/200] Training loss: 0.01224972
[200/200] Training loss: 0.01133911
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 31772.204204304115 ----------
[1/200] Training loss: 0.16567659
[2/200] Training loss: 0.06032919
[3/200] Training loss: 0.05259781
[4/200] Training loss: 0.05145949
[5/200] Training loss: 0.04414804
[6/200] Training loss: 0.04257333
[7/200] Training loss: 0.03827896
[8/200] Training loss: 0.03559639
[9/200] Training loss: 0.03509083
[10/200] Training loss: 0.03011905
[50/200] Training loss: 0.01605094
[100/200] Training loss: 0.01365943
[150/200] Training loss: 0.01225256
[200/200] Training loss: 0.01182248
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 16 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15187.085039598613 ----------
[1/200] Training loss: 0.15785507
[2/200] Training loss: 0.05928533
[3/200] Training loss: 0.05047596
[4/200] Training loss: 0.04639494
[5/200] Training loss: 0.04633876
[6/200] Training loss: 0.04354784
[7/200] Training loss: 0.04318015
[8/200] Training loss: 0.03952415
[9/200] Training loss: 0.03692413
[10/200] Training loss: 0.03552412
[50/200] Training loss: 0.01838032
[100/200] Training loss: 0.01574208
[150/200] Training loss: 0.01500621
[200/200] Training loss: 0.01374614
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11321.808689427675 ----------
[1/200] Training loss: 0.14190998
[2/200] Training loss: 0.05895695
[3/200] Training loss: 0.05163483
[4/200] Training loss: 0.04743000
[5/200] Training loss: 0.04282487
[6/200] Training loss: 0.04051599
[7/200] Training loss: 0.03599057
[8/200] Training loss: 0.03777226
[9/200] Training loss: 0.03567315
[10/200] Training loss: 0.03214790
[50/200] Training loss: 0.01948764
[100/200] Training loss: 0.01514968
[150/200] Training loss: 0.01458756
[200/200] Training loss: 0.01363320
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8341.884679135765 ----------
[1/200] Training loss: 0.13668904
[2/200] Training loss: 0.06091387
[3/200] Training loss: 0.05377463
[4/200] Training loss: 0.04800666
[5/200] Training loss: 0.04441077
[6/200] Training loss: 0.03998502
[7/200] Training loss: 0.03832088
[8/200] Training loss: 0.03684727
[9/200] Training loss: 0.03538178
[10/200] Training loss: 0.03455857
[50/200] Training loss: 0.01938794
[100/200] Training loss: 0.01504179
[150/200] Training loss: 0.01409845
[200/200] Training loss: 0.01285724
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 16 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16155.27653740412 ----------
[1/200] Training loss: 0.15147722
[2/200] Training loss: 0.05439726
[3/200] Training loss: 0.04407759
[4/200] Training loss: 0.04247318
[5/200] Training loss: 0.03908664
[6/200] Training loss: 0.03761524
[7/200] Training loss: 0.03455319
[8/200] Training loss: 0.03418123
[9/200] Training loss: 0.03021693
[10/200] Training loss: 0.03095241
[50/200] Training loss: 0.01766226
[100/200] Training loss: 0.01531687
[150/200] Training loss: 0.01346114
[200/200] Training loss: 0.01288675
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18986.250182698004 ----------
[1/200] Training loss: 0.13869612
[2/200] Training loss: 0.06200285
[3/200] Training loss: 0.05453339
[4/200] Training loss: 0.05277004
[5/200] Training loss: 0.05055909
[6/200] Training loss: 0.04886918
[7/200] Training loss: 0.04361851
[8/200] Training loss: 0.04373015
[9/200] Training loss: 0.03905204
[10/200] Training loss: 0.03541991
[50/200] Training loss: 0.01881283
[100/200] Training loss: 0.01751438
[150/200] Training loss: 0.01492216
[200/200] Training loss: 0.01358011
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20024.508183723265 ----------
[1/200] Training loss: 0.15158356
[2/200] Training loss: 0.05845010
[3/200] Training loss: 0.04834905
[4/200] Training loss: 0.04869271
[5/200] Training loss: 0.04228715
[6/200] Training loss: 0.04404215
[7/200] Training loss: 0.03800276
[8/200] Training loss: 0.03876304
[9/200] Training loss: 0.03459761
[10/200] Training loss: 0.03514705
[50/200] Training loss: 0.01737321
[100/200] Training loss: 0.01496598
[150/200] Training loss: 0.01341212
[200/200] Training loss: 0.01148105
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12936.343842059858 ----------
[1/200] Training loss: 0.18341014
[2/200] Training loss: 0.06165132
[3/200] Training loss: 0.05123085
[4/200] Training loss: 0.04736936
[5/200] Training loss: 0.04484417
[6/200] Training loss: 0.03911931
[7/200] Training loss: 0.03988079
[8/200] Training loss: 0.03810895
[9/200] Training loss: 0.03749246
[10/200] Training loss: 0.03387734
[50/200] Training loss: 0.02024679
[100/200] Training loss: 0.01701304
[150/200] Training loss: 0.01556914
[200/200] Training loss: 0.01325318
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6346.281430885334 ----------
[1/200] Training loss: 0.16839846
[2/200] Training loss: 0.06108048
[3/200] Training loss: 0.06052112
[4/200] Training loss: 0.05282813
[5/200] Training loss: 0.05279580
[6/200] Training loss: 0.04959795
[7/200] Training loss: 0.04907792
[8/200] Training loss: 0.04693502
[9/200] Training loss: 0.04658280
[10/200] Training loss: 0.04567961
[50/200] Training loss: 0.01869882
[100/200] Training loss: 0.01505527
[150/200] Training loss: 0.01359350
[200/200] Training loss: 0.01302975
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11759.852380025864 ----------
[1/200] Training loss: 0.17947069
[2/200] Training loss: 0.06404566
[3/200] Training loss: 0.05382570
[4/200] Training loss: 0.05203375
[5/200] Training loss: 0.05086878
[6/200] Training loss: 0.04964429
[7/200] Training loss: 0.04393509
[8/200] Training loss: 0.04212719
[9/200] Training loss: 0.04147218
[10/200] Training loss: 0.04069413
[50/200] Training loss: 0.02070432
[100/200] Training loss: 0.01558423
[150/200] Training loss: 0.01331947
[200/200] Training loss: 0.01319360
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6656.39932696349 ----------
[1/200] Training loss: 0.15967777
[2/200] Training loss: 0.05735418
[3/200] Training loss: 0.05346969
[4/200] Training loss: 0.04964776
[5/200] Training loss: 0.04738770
[6/200] Training loss: 0.04384084
[7/200] Training loss: 0.04304349
[8/200] Training loss: 0.04173347
[9/200] Training loss: 0.03988969
[10/200] Training loss: 0.03732769
[50/200] Training loss: 0.01866209
[100/200] Training loss: 0.01692169
[150/200] Training loss: 0.01585731
[200/200] Training loss: 0.01397826
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11859.452263911686 ----------
[1/200] Training loss: 0.19106200
[2/200] Training loss: 0.06463089
[3/200] Training loss: 0.05967873
[4/200] Training loss: 0.05825286
[5/200] Training loss: 0.05231715
[6/200] Training loss: 0.05215300
[7/200] Training loss: 0.04932396
[8/200] Training loss: 0.04834647
[9/200] Training loss: 0.04751086
[10/200] Training loss: 0.04319643
[50/200] Training loss: 0.01892869
[100/200] Training loss: 0.01540963
[150/200] Training loss: 0.01450563
[200/200] Training loss: 0.01288469
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15509.47684481975 ----------
[1/200] Training loss: 0.17243893
[2/200] Training loss: 0.05558319
[3/200] Training loss: 0.05424325
[4/200] Training loss: 0.05068156
[5/200] Training loss: 0.04875150
[6/200] Training loss: 0.04507284
[7/200] Training loss: 0.04089833
[8/200] Training loss: 0.03991235
[9/200] Training loss: 0.03802440
[10/200] Training loss: 0.03377145
[50/200] Training loss: 0.01800291
[100/200] Training loss: 0.01488242
[150/200] Training loss: 0.01344621
[200/200] Training loss: 0.01161078
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19970.46218794147 ----------
[1/200] Training loss: 0.15494065
[2/200] Training loss: 0.06088401
[3/200] Training loss: 0.05154997
[4/200] Training loss: 0.04755401
[5/200] Training loss: 0.04519065
[6/200] Training loss: 0.04219659
[7/200] Training loss: 0.04041327
[8/200] Training loss: 0.03579243
[9/200] Training loss: 0.03385945
[10/200] Training loss: 0.03323053
[50/200] Training loss: 0.01852180
[100/200] Training loss: 0.01576328
[150/200] Training loss: 0.01458275
[200/200] Training loss: 0.01341100
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18274.864924261412 ----------
[1/200] Training loss: 0.14842584
[2/200] Training loss: 0.05537892
[3/200] Training loss: 0.05062408
[4/200] Training loss: 0.04564660
[5/200] Training loss: 0.04293129
[6/200] Training loss: 0.04218904
[7/200] Training loss: 0.03871859
[8/200] Training loss: 0.03753625
[9/200] Training loss: 0.03426774
[10/200] Training loss: 0.03277000
[50/200] Training loss: 0.01786446
[100/200] Training loss: 0.01458388
[150/200] Training loss: 0.01291033
[200/200] Training loss: 0.01192854
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 16 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 25910.430332204058 ----------
[1/200] Training loss: 0.14657485
[2/200] Training loss: 0.05915969
[3/200] Training loss: 0.05633656
[4/200] Training loss: 0.05181672
[5/200] Training loss: 0.04559745
[6/200] Training loss: 0.04453526
[7/200] Training loss: 0.04120723
[8/200] Training loss: 0.03929211
[9/200] Training loss: 0.03666749
[10/200] Training loss: 0.03372250
[50/200] Training loss: 0.01786983
[100/200] Training loss: 0.01523976
[150/200] Training loss: 0.01363590
[200/200] Training loss: 0.01284776
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10935.36794076907 ----------
[1/200] Training loss: 0.17810819
[2/200] Training loss: 0.06591815
[3/200] Training loss: 0.05321998
[4/200] Training loss: 0.05230028
[5/200] Training loss: 0.05211939
[6/200] Training loss: 0.04914934
[7/200] Training loss: 0.04958316
[8/200] Training loss: 0.04552969
[9/200] Training loss: 0.04372201
[10/200] Training loss: 0.04187995
[50/200] Training loss: 0.01933903
[100/200] Training loss: 0.01609232
[150/200] Training loss: 0.01513045
[200/200] Training loss: 0.01390536
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19440.71850524049 ----------
[1/200] Training loss: 0.15935250
[2/200] Training loss: 0.06153060
[3/200] Training loss: 0.05552774
[4/200] Training loss: 0.05287476
[5/200] Training loss: 0.05169356
[6/200] Training loss: 0.04923112
[7/200] Training loss: 0.04666970
[8/200] Training loss: 0.04537528
[9/200] Training loss: 0.04456882
[10/200] Training loss: 0.04220372
[50/200] Training loss: 0.01871974
[100/200] Training loss: 0.01542880
[150/200] Training loss: 0.01295907
[200/200] Training loss: 0.01216419
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16765.003549060166 ----------
[1/200] Training loss: 0.14183084
[2/200] Training loss: 0.05484019
[3/200] Training loss: 0.04964774
[4/200] Training loss: 0.04450416
[5/200] Training loss: 0.04328243
[6/200] Training loss: 0.03795846
[7/200] Training loss: 0.03286083
[8/200] Training loss: 0.03215727
[9/200] Training loss: 0.02981754
[10/200] Training loss: 0.02818808
[50/200] Training loss: 0.01881775
[100/200] Training loss: 0.01622246
[150/200] Training loss: 0.01437151
[200/200] Training loss: 0.01360611
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15069.572522138775 ----------
[1/200] Training loss: 0.15099243
[2/200] Training loss: 0.05656350
[3/200] Training loss: 0.05302990
[4/200] Training loss: 0.05138336
[5/200] Training loss: 0.04564717
[6/200] Training loss: 0.04291559
[7/200] Training loss: 0.04131489
[8/200] Training loss: 0.03791980
[9/200] Training loss: 0.03853083
[10/200] Training loss: 0.03505519
[50/200] Training loss: 0.01750554
[100/200] Training loss: 0.01420366
[150/200] Training loss: 0.01268368
[200/200] Training loss: 0.01125500
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 25769.510045788607 ----------
[1/200] Training loss: 0.15738463
[2/200] Training loss: 0.06075184
[3/200] Training loss: 0.05266062
[4/200] Training loss: 0.04910683
[5/200] Training loss: 0.04481759
[6/200] Training loss: 0.04288290
[7/200] Training loss: 0.04018508
[8/200] Training loss: 0.03846479
[9/200] Training loss: 0.03546891
[10/200] Training loss: 0.03512278
[50/200] Training loss: 0.01833813
[100/200] Training loss: 0.01571988
[150/200] Training loss: 0.01379333
[200/200] Training loss: 0.01309012
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8478.22198341138 ----------
[1/200] Training loss: 0.13101854
[2/200] Training loss: 0.05228883
[3/200] Training loss: 0.04478491
[4/200] Training loss: 0.04254908
[5/200] Training loss: 0.03970508
[6/200] Training loss: 0.03782167
[7/200] Training loss: 0.03681963
[8/200] Training loss: 0.03214403
[9/200] Training loss: 0.03257146
[10/200] Training loss: 0.03013152
[50/200] Training loss: 0.01583019
[100/200] Training loss: 0.01408287
[150/200] Training loss: 0.01241225
[200/200] Training loss: 0.01142407
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10745.031223779668 ----------
[1/200] Training loss: 0.14056201
[2/200] Training loss: 0.05357401
[3/200] Training loss: 0.04739308
[4/200] Training loss: 0.04357185
[5/200] Training loss: 0.04218676
[6/200] Training loss: 0.03873097
[7/200] Training loss: 0.03508034
[8/200] Training loss: 0.03540394
[9/200] Training loss: 0.03393886
[10/200] Training loss: 0.03161612
[50/200] Training loss: 0.01747878
[100/200] Training loss: 0.01453851
[150/200] Training loss: 0.01254043
[200/200] Training loss: 0.01206475
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19910.322147067334 ----------
[1/200] Training loss: 0.17846363
[2/200] Training loss: 0.06432746
[3/200] Training loss: 0.05324016
[4/200] Training loss: 0.05137730
[5/200] Training loss: 0.04690219
[6/200] Training loss: 0.04390231
[7/200] Training loss: 0.03950924
[8/200] Training loss: 0.03961248
[9/200] Training loss: 0.03806764
[10/200] Training loss: 0.03569637
[50/200] Training loss: 0.01877731
[100/200] Training loss: 0.01520171
[150/200] Training loss: 0.01370826
[200/200] Training loss: 0.01285168
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15301.003365792716 ----------
[1/200] Training loss: 0.17744259
[2/200] Training loss: 0.06115124
[3/200] Training loss: 0.05583275
[4/200] Training loss: 0.05138379
[5/200] Training loss: 0.05118770
[6/200] Training loss: 0.04813770
[7/200] Training loss: 0.04566815
[8/200] Training loss: 0.04439516
[9/200] Training loss: 0.04284074
[10/200] Training loss: 0.04192021
[50/200] Training loss: 0.01829335
[100/200] Training loss: 0.01553725
[150/200] Training loss: 0.01429371
[200/200] Training loss: 0.01336160
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8495.621931324393 ----------
[1/200] Training loss: 0.14812987
[2/200] Training loss: 0.05660460
[3/200] Training loss: 0.04957259
[4/200] Training loss: 0.04232082
[5/200] Training loss: 0.04105775
[6/200] Training loss: 0.03733754
[7/200] Training loss: 0.03269667
[8/200] Training loss: 0.03248004
[9/200] Training loss: 0.03057320
[10/200] Training loss: 0.03202599
[50/200] Training loss: 0.01648740
[100/200] Training loss: 0.01491729
[150/200] Training loss: 0.01343395
[200/200] Training loss: 0.01230989
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 24090.49339469825 ----------
[1/200] Training loss: 0.18411390
[2/200] Training loss: 0.05954297
[3/200] Training loss: 0.05707465
[4/200] Training loss: 0.05517259
[5/200] Training loss: 0.04878820
[6/200] Training loss: 0.04503044
[7/200] Training loss: 0.04483224
[8/200] Training loss: 0.04332394
[9/200] Training loss: 0.04370060
[10/200] Training loss: 0.03974255
[50/200] Training loss: 0.02005751
[100/200] Training loss: 0.01696999
[150/200] Training loss: 0.01518397
[200/200] Training loss: 0.01419363
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 16 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5992.494305379021 ----------
[1/200] Training loss: 0.15056929
[2/200] Training loss: 0.06052367
[3/200] Training loss: 0.05141762
[4/200] Training loss: 0.04766114
[5/200] Training loss: 0.04771012
[6/200] Training loss: 0.04783776
[7/200] Training loss: 0.04302300
[8/200] Training loss: 0.03826137
[9/200] Training loss: 0.03890160
[10/200] Training loss: 0.03484215
[50/200] Training loss: 0.01706000
[100/200] Training loss: 0.01366916
[150/200] Training loss: 0.01226350
[200/200] Training loss: 0.01162505
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23138.205289088433 ----------
[1/200] Training loss: 0.01972146
[2/200] Training loss: 0.00187221
[3/200] Training loss: 0.00160702
[4/200] Training loss: 0.00142388
[5/200] Training loss: 0.00125879
[6/200] Training loss: 0.00104473
[7/200] Training loss: 0.00095375
[8/200] Training loss: 0.00085207
[9/200] Training loss: 0.00071460
[10/200] Training loss: 0.00070335
[50/200] Training loss: 0.00027403
[100/200] Training loss: 0.00017248
[150/200] Training loss: 0.00013309
[200/200] Training loss: 0.00012256
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6894.520142838079 ----------
[1/200] Training loss: 0.15051987
[2/200] Training loss: 0.06205062
[3/200] Training loss: 0.05200693
[4/200] Training loss: 0.04936324
[5/200] Training loss: 0.04517579
[6/200] Training loss: 0.04188593
[7/200] Training loss: 0.04055911
[8/200] Training loss: 0.03415536
[9/200] Training loss: 0.03393424
[10/200] Training loss: 0.03453745
[50/200] Training loss: 0.01778567
[100/200] Training loss: 0.01596975
[150/200] Training loss: 0.01402480
[200/200] Training loss: 0.01367880
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20386.09722335298 ----------
[1/200] Training loss: 0.16533271
[2/200] Training loss: 0.05889956
[3/200] Training loss: 0.04882662
[4/200] Training loss: 0.04944848
[5/200] Training loss: 0.04282642
[6/200] Training loss: 0.04399665
[7/200] Training loss: 0.04174206
[8/200] Training loss: 0.03947609
[9/200] Training loss: 0.03799987
[10/200] Training loss: 0.03542080
[50/200] Training loss: 0.01900183
[100/200] Training loss: 0.01514420
[150/200] Training loss: 0.01394567
[200/200] Training loss: 0.01252153
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10748.826912737966 ----------
[1/200] Training loss: 0.18129949
[2/200] Training loss: 0.06115687
[3/200] Training loss: 0.04948824
[4/200] Training loss: 0.04258030
[5/200] Training loss: 0.03830656
[6/200] Training loss: 0.03658508
[7/200] Training loss: 0.03549181
[8/200] Training loss: 0.03257803
[9/200] Training loss: 0.03164822
[10/200] Training loss: 0.02824479
[50/200] Training loss: 0.01592870
[100/200] Training loss: 0.01394988
[150/200] Training loss: 0.01279936
[200/200] Training loss: 0.01156057
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16662.355175664692 ----------
[1/200] Training loss: 0.16480471
[2/200] Training loss: 0.06233497
[3/200] Training loss: 0.05351915
[4/200] Training loss: 0.04684079
[5/200] Training loss: 0.04353401
[6/200] Training loss: 0.03935569
[7/200] Training loss: 0.03857756
[8/200] Training loss: 0.03356604
[9/200] Training loss: 0.03350234
[10/200] Training loss: 0.03176546
[50/200] Training loss: 0.01777982
[100/200] Training loss: 0.01589755
[150/200] Training loss: 0.01358852
[200/200] Training loss: 0.01333930
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13353.948329988401 ----------
[1/200] Training loss: 0.15590415
[2/200] Training loss: 0.05465843
[3/200] Training loss: 0.04981039
[4/200] Training loss: 0.04316364
[5/200] Training loss: 0.04277777
[6/200] Training loss: 0.03874999
[7/200] Training loss: 0.03664664
[8/200] Training loss: 0.03470523
[9/200] Training loss: 0.03251813
[10/200] Training loss: 0.03392578
[50/200] Training loss: 0.01837464
[100/200] Training loss: 0.01504624
[150/200] Training loss: 0.01387045
[200/200] Training loss: 0.01306931
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11918.260946967053 ----------
[1/200] Training loss: 0.16546918
[2/200] Training loss: 0.05631290
[3/200] Training loss: 0.05120792
[4/200] Training loss: 0.04826763
[5/200] Training loss: 0.04295167
[6/200] Training loss: 0.03969163
[7/200] Training loss: 0.03925322
[8/200] Training loss: 0.03524107
[9/200] Training loss: 0.03321387
[10/200] Training loss: 0.03221083
[50/200] Training loss: 0.01712949
[100/200] Training loss: 0.01549707
[150/200] Training loss: 0.01462687
[200/200] Training loss: 0.01319333
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11528.793171880568 ----------
[1/200] Training loss: 0.17787022
[2/200] Training loss: 0.05664327
[3/200] Training loss: 0.05291267
[4/200] Training loss: 0.04888000
[5/200] Training loss: 0.04824781
[6/200] Training loss: 0.04540948
[7/200] Training loss: 0.04278169
[8/200] Training loss: 0.04095657
[9/200] Training loss: 0.03825677
[10/200] Training loss: 0.03578303
[50/200] Training loss: 0.01814146
[100/200] Training loss: 0.01639164
[150/200] Training loss: 0.01583691
[200/200] Training loss: 0.01347760
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13588.326754976126 ----------
[1/200] Training loss: 0.15612688
[2/200] Training loss: 0.05778787
[3/200] Training loss: 0.05380257
[4/200] Training loss: 0.04939350
[5/200] Training loss: 0.04383222
[6/200] Training loss: 0.04549073
[7/200] Training loss: 0.03922736
[8/200] Training loss: 0.03824078
[9/200] Training loss: 0.03391224
[10/200] Training loss: 0.03274923
[50/200] Training loss: 0.01635482
[100/200] Training loss: 0.01414524
[150/200] Training loss: 0.01327576
[200/200] Training loss: 0.01177111
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16344.017621135876 ----------
[1/200] Training loss: 0.14256797
[2/200] Training loss: 0.05813631
[3/200] Training loss: 0.05220727
[4/200] Training loss: 0.04521928
[5/200] Training loss: 0.04575159
[6/200] Training loss: 0.04297950
[7/200] Training loss: 0.03896454
[8/200] Training loss: 0.03765644
[9/200] Training loss: 0.03503391
[10/200] Training loss: 0.03015250
[50/200] Training loss: 0.01850020
[100/200] Training loss: 0.01418355
[150/200] Training loss: 0.01329362
[200/200] Training loss: 0.01232576
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16979.84687799039 ----------
[1/200] Training loss: 0.15109999
[2/200] Training loss: 0.05578848
[3/200] Training loss: 0.05404252
[4/200] Training loss: 0.04782366
[5/200] Training loss: 0.04631554
[6/200] Training loss: 0.04010354
[7/200] Training loss: 0.04045217
[8/200] Training loss: 0.03794082
[9/200] Training loss: 0.03688826
[10/200] Training loss: 0.03409092
[50/200] Training loss: 0.01866971
[100/200] Training loss: 0.01546761
[150/200] Training loss: 0.01550028
[200/200] Training loss: 0.01377914
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20746.015328250385 ----------
[1/200] Training loss: 0.07718262
[2/200] Training loss: 0.04296968
[3/200] Training loss: 0.03689222
[4/200] Training loss: 0.03167527
[5/200] Training loss: 0.02851970
[6/200] Training loss: 0.02651996
[7/200] Training loss: 0.02510195
[8/200] Training loss: 0.02491628
[9/200] Training loss: 0.02318952
[10/200] Training loss: 0.02163313
[50/200] Training loss: 0.01510548
[100/200] Training loss: 0.01265401
[150/200] Training loss: 0.01115963
[200/200] Training loss: 0.00999760
---batch_size---: 2 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15132.796701204968 ----------
[1/200] Training loss: 0.18769906
[2/200] Training loss: 0.06483486
[3/200] Training loss: 0.05522393
[4/200] Training loss: 0.04935261
[5/200] Training loss: 0.04536737
[6/200] Training loss: 0.04441040
[7/200] Training loss: 0.04346288
[8/200] Training loss: 0.04026633
[9/200] Training loss: 0.04224478
[10/200] Training loss: 0.03758936
[50/200] Training loss: 0.01963104
[100/200] Training loss: 0.01594526
[150/200] Training loss: 0.01472754
[200/200] Training loss: 0.01271477
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7357.198379818231 ----------
[1/200] Training loss: 0.15401672
[2/200] Training loss: 0.05347017
[3/200] Training loss: 0.05094424
[4/200] Training loss: 0.04430288
[5/200] Training loss: 0.04010267
[6/200] Training loss: 0.03862369
[7/200] Training loss: 0.03697581
[8/200] Training loss: 0.03281209
[9/200] Training loss: 0.03319045
[10/200] Training loss: 0.02835800
[50/200] Training loss: 0.01685554
[100/200] Training loss: 0.01366207
[150/200] Training loss: 0.01270907
[200/200] Training loss: 0.01183909
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13431.790945365403 ----------
[1/200] Training loss: 0.15240673
[2/200] Training loss: 0.05520181
[3/200] Training loss: 0.05003929
[4/200] Training loss: 0.04379942
[5/200] Training loss: 0.04206044
[6/200] Training loss: 0.03652233
[7/200] Training loss: 0.03393790
[8/200] Training loss: 0.02861441
[9/200] Training loss: 0.02705463
[10/200] Training loss: 0.02800461
[50/200] Training loss: 0.01630871
[100/200] Training loss: 0.01312468
[150/200] Training loss: 0.01175867
[200/200] Training loss: 0.01072665
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14713.308805295972 ----------
[1/200] Training loss: 0.15747867
[2/200] Training loss: 0.05967944
[3/200] Training loss: 0.05335320
[4/200] Training loss: 0.04759467
[5/200] Training loss: 0.04196280
[6/200] Training loss: 0.04102161
[7/200] Training loss: 0.03737661
[8/200] Training loss: 0.03459544
[9/200] Training loss: 0.02998319
[10/200] Training loss: 0.03042291
[50/200] Training loss: 0.01775550
[100/200] Training loss: 0.01496502
[150/200] Training loss: 0.01374743
[200/200] Training loss: 0.01311780
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 25142.04637653825 ----------
[1/200] Training loss: 0.16869169
[2/200] Training loss: 0.05678678
[3/200] Training loss: 0.05811179
[4/200] Training loss: 0.05291289
[5/200] Training loss: 0.05076790
[6/200] Training loss: 0.04844597
[7/200] Training loss: 0.04578195
[8/200] Training loss: 0.04431445
[9/200] Training loss: 0.04291315
[10/200] Training loss: 0.04013825
[50/200] Training loss: 0.01819682
[100/200] Training loss: 0.01492362
[150/200] Training loss: 0.01425092
[200/200] Training loss: 0.01345080
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14685.760722550262 ----------
[1/200] Training loss: 0.16744307
[2/200] Training loss: 0.05857145
[3/200] Training loss: 0.05623176
[4/200] Training loss: 0.05155591
[5/200] Training loss: 0.04634490
[6/200] Training loss: 0.04579702
[7/200] Training loss: 0.04050426
[8/200] Training loss: 0.03927314
[9/200] Training loss: 0.03631647
[10/200] Training loss: 0.03300864
[50/200] Training loss: 0.01729473
[100/200] Training loss: 0.01399587
[150/200] Training loss: 0.01374858
[200/200] Training loss: 0.01214174
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 16 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4794.453462074692 ----------
[1/200] Training loss: 0.14634666
[2/200] Training loss: 0.05138942
[3/200] Training loss: 0.04507587
[4/200] Training loss: 0.04435209
[5/200] Training loss: 0.04338979
[6/200] Training loss: 0.04194012
[7/200] Training loss: 0.03816762
[8/200] Training loss: 0.03375560
[9/200] Training loss: 0.03479977
[10/200] Training loss: 0.03367370
[50/200] Training loss: 0.01669143
[100/200] Training loss: 0.01380633
[150/200] Training loss: 0.01297696
[200/200] Training loss: 0.01183931
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 16 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7781.874581358916 ----------
[1/200] Training loss: 0.14747862
[2/200] Training loss: 0.06041768
[3/200] Training loss: 0.05242575
[4/200] Training loss: 0.04836167
[5/200] Training loss: 0.04460676
[6/200] Training loss: 0.04300792
[7/200] Training loss: 0.03824341
[8/200] Training loss: 0.03531596
[9/200] Training loss: 0.03254913
[10/200] Training loss: 0.03032760
[50/200] Training loss: 0.01722479
[100/200] Training loss: 0.01351644
[150/200] Training loss: 0.01220741
[200/200] Training loss: 0.01185363
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12322.738007439742 ----------
[1/200] Training loss: 0.16318727
[2/200] Training loss: 0.05871586
[3/200] Training loss: 0.05559736
[4/200] Training loss: 0.05196783
[5/200] Training loss: 0.04981313
[6/200] Training loss: 0.04759066
[7/200] Training loss: 0.04506368
[8/200] Training loss: 0.04222479
[9/200] Training loss: 0.03971803
[10/200] Training loss: 0.03959477
[50/200] Training loss: 0.02134577
[100/200] Training loss: 0.01653058
[150/200] Training loss: 0.01397781
[200/200] Training loss: 0.01259850
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 16 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21268.12977203214 ----------
[1/200] Training loss: 0.18905760
[2/200] Training loss: 0.05866833
[3/200] Training loss: 0.05304609
[4/200] Training loss: 0.05091478
[5/200] Training loss: 0.04582561
[6/200] Training loss: 0.04824004
[7/200] Training loss: 0.04304563
[8/200] Training loss: 0.04209174
[9/200] Training loss: 0.04056112
[10/200] Training loss: 0.03638275
[50/200] Training loss: 0.01965333
[100/200] Training loss: 0.01548467
[150/200] Training loss: 0.01406497
[200/200] Training loss: 0.01333077
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 16 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6740.3732834317125 ----------
[1/200] Training loss: 0.15954361
[2/200] Training loss: 0.05958943
[3/200] Training loss: 0.05325834
[4/200] Training loss: 0.04938889
[5/200] Training loss: 0.04681070
[6/200] Training loss: 0.04114812
[7/200] Training loss: 0.03422462
[8/200] Training loss: 0.03099439
[9/200] Training loss: 0.03081186
[10/200] Training loss: 0.02820351
[50/200] Training loss: 0.01692316
[100/200] Training loss: 0.01468180
[150/200] Training loss: 0.01375430
[200/200] Training loss: 0.01253403
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19265.058940994706 ----------
[1/200] Training loss: 0.15570288
[2/200] Training loss: 0.05867778
[3/200] Training loss: 0.05138139
[4/200] Training loss: 0.04363069
[5/200] Training loss: 0.04333038
[6/200] Training loss: 0.03803928
[7/200] Training loss: 0.03604259
[8/200] Training loss: 0.03204454
[9/200] Training loss: 0.03132163
[10/200] Training loss: 0.03000692
[50/200] Training loss: 0.01634901
[100/200] Training loss: 0.01496671
[150/200] Training loss: 0.01320274
[200/200] Training loss: 0.01216292
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 16 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15473.24348674188 ----------
[1/200] Training loss: 0.16667246
[2/200] Training loss: 0.06177724
[3/200] Training loss: 0.05524183
[4/200] Training loss: 0.05327849
[5/200] Training loss: 0.05097564
[6/200] Training loss: 0.04832836
[7/200] Training loss: 0.04819367
[8/200] Training loss: 0.04636443
[9/200] Training loss: 0.04362991
[10/200] Training loss: 0.04068470
[50/200] Training loss: 0.01912694
[100/200] Training loss: 0.01606788
[150/200] Training loss: 0.01564064
[200/200] Training loss: 0.01408057
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 16 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11542.354006007614 ----------
[1/200] Training loss: 0.15811983
[2/200] Training loss: 0.05909841
[3/200] Training loss: 0.05111679
[4/200] Training loss: 0.04935563
[5/200] Training loss: 0.04573055
[6/200] Training loss: 0.04352966
[7/200] Training loss: 0.03926607
[8/200] Training loss: 0.03510310
[9/200] Training loss: 0.10489713
[10/200] Training loss: 0.06351337
[50/200] Training loss: 0.01751817
[100/200] Training loss: 0.01523458
[150/200] Training loss: 0.01411826
[200/200] Training loss: 0.01330626
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13750.371049538991 ----------
[1/200] Training loss: 0.14897783
[2/200] Training loss: 0.06304036
[3/200] Training loss: 0.05427797
[4/200] Training loss: 0.04851324
[5/200] Training loss: 0.04677961
[6/200] Training loss: 0.04230927
[7/200] Training loss: 0.04219236
[8/200] Training loss: 0.03884420
[9/200] Training loss: 0.03693307
[10/200] Training loss: 0.03593634
[50/200] Training loss: 0.01805691
[100/200] Training loss: 0.01472658
[150/200] Training loss: 0.01377196
[200/200] Training loss: 0.01251068
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 16 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22584.03117249 ----------
[1/200] Training loss: 0.13539436
[2/200] Training loss: 0.05467220
[3/200] Training loss: 0.05230976
[4/200] Training loss: 0.04907924
[5/200] Training loss: 0.04429586
[6/200] Training loss: 0.04170330
[7/200] Training loss: 0.04052759
[8/200] Training loss: 0.03800470
[9/200] Training loss: 0.03323885
[10/200] Training loss: 0.03338053
[50/200] Training loss: 0.01926866
[100/200] Training loss: 0.01586896
[150/200] Training loss: 0.01375329
[200/200] Training loss: 0.01258604
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14921.07341983143 ----------
[1/200] Training loss: 0.13908649
[2/200] Training loss: 0.05667962
[3/200] Training loss: 0.05125360
[4/200] Training loss: 0.05024099
[5/200] Training loss: 0.04704277
[6/200] Training loss: 0.04351014
[7/200] Training loss: 0.04191220
[8/200] Training loss: 0.03970542
[9/200] Training loss: 0.03698420
[10/200] Training loss: 0.03511979
[50/200] Training loss: 0.01981857
[100/200] Training loss: 0.01407262
[150/200] Training loss: 0.01297097
[200/200] Training loss: 0.01185063
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5494.714005296363 ----------
[1/200] Training loss: 0.03542556
[2/200] Training loss: 0.00253318
[3/200] Training loss: 0.00212623
[4/200] Training loss: 0.00193334
[5/200] Training loss: 0.00168932
[6/200] Training loss: 0.00157394
[7/200] Training loss: 0.00135818
[8/200] Training loss: 0.00127678
[9/200] Training loss: 0.00119466
[10/200] Training loss: 0.00111142
[50/200] Training loss: 0.00023486
[100/200] Training loss: 0.00017529
[150/200] Training loss: 0.00016046
[200/200] Training loss: 0.00014519
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 16 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16809.55442598048 ----------
[1/200] Training loss: 0.01983742
[2/200] Training loss: 0.00219173
[3/200] Training loss: 0.00186209
[4/200] Training loss: 0.00169446
[5/200] Training loss: 0.00146451
[6/200] Training loss: 0.00118579
[7/200] Training loss: 0.00102062
[8/200] Training loss: 0.00090700
[9/200] Training loss: 0.00071644
[10/200] Training loss: 0.00071140
[50/200] Training loss: 0.00024625
[100/200] Training loss: 0.00018048
[150/200] Training loss: 0.00014085
[200/200] Training loss: 0.00012800
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 16 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: SmoothL1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12455.236970848848 ----------
[1/200] Training loss: 0.16839449
[2/200] Training loss: 0.05729867
[3/200] Training loss: 0.05216297
[4/200] Training loss: 0.04926826
[5/200] Training loss: 0.04426943
[6/200] Training loss: 0.04171224
[7/200] Training loss: 0.03689223
[8/200] Training loss: 0.03629773
[9/200] Training loss: 0.03244653
[10/200] Training loss: 0.03169512
[50/200] Training loss: 0.01946290
[100/200] Training loss: 0.01599539
[150/200] Training loss: 0.01410414
[200/200] Training loss: 0.01266757
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13391.624845402443 ----------
[1/200] Training loss: 0.16013872
[2/200] Training loss: 0.05498026
[3/200] Training loss: 0.04873315
[4/200] Training loss: 0.04495285
[5/200] Training loss: 0.04051041
[6/200] Training loss: 0.04172934
[7/200] Training loss: 0.03755602
[8/200] Training loss: 0.03417341
[9/200] Training loss: 0.03319697
[10/200] Training loss: 0.03076062
[50/200] Training loss: 0.01709671
[100/200] Training loss: 0.01400862
[150/200] Training loss: 0.01267385
[200/200] Training loss: 0.01197665
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17536.26094696358 ----------
[1/200] Training loss: 0.13112838
[2/200] Training loss: 0.05383707
[3/200] Training loss: 0.05047491
[4/200] Training loss: 0.04470693
[5/200] Training loss: 0.03975903
[6/200] Training loss: 0.03731216
[7/200] Training loss: 0.03873161
[8/200] Training loss: 0.03272068
[9/200] Training loss: 0.03031704
[10/200] Training loss: 0.03015969
[50/200] Training loss: 0.01725402
[100/200] Training loss: 0.01491706
[150/200] Training loss: 0.01331629
[200/200] Training loss: 0.01240284
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22921.525254659646 ----------
[1/200] Training loss: 0.15409855
[2/200] Training loss: 0.06153751
[3/200] Training loss: 0.05363322
[4/200] Training loss: 0.04886807
[5/200] Training loss: 0.04634511
[6/200] Training loss: 0.04379464
[7/200] Training loss: 0.04183467
[8/200] Training loss: 0.04200852
[9/200] Training loss: 0.04055139
[10/200] Training loss: 0.03587680
[50/200] Training loss: 0.01837554
[100/200] Training loss: 0.01551535
[150/200] Training loss: 0.01444074
[200/200] Training loss: 0.01305389
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8316.441546719367 ----------
[1/200] Training loss: 0.17632050
[2/200] Training loss: 0.06331298
[3/200] Training loss: 0.05592269
[4/200] Training loss: 0.04942387
[5/200] Training loss: 0.04785355
[6/200] Training loss: 0.04551463
[7/200] Training loss: 0.04256987
[8/200] Training loss: 0.04204500
[9/200] Training loss: 0.04052149
[10/200] Training loss: 0.03757311
[50/200] Training loss: 0.01775530
[100/200] Training loss: 0.01532975
[150/200] Training loss: 0.01415876
[200/200] Training loss: 0.01310842
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5649.183126789218 ----------
[1/200] Training loss: 0.16319785
[2/200] Training loss: 0.05712997
[3/200] Training loss: 0.05431175
[4/200] Training loss: 0.05389390
[5/200] Training loss: 0.04880105
[6/200] Training loss: 0.04876907
[7/200] Training loss: 0.04705356
[8/200] Training loss: 0.04699290
[9/200] Training loss: 0.04276224
[10/200] Training loss: 0.03889120
[50/200] Training loss: 0.01763455
[100/200] Training loss: 0.01528883
[150/200] Training loss: 0.01393886
[200/200] Training loss: 0.01246570
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22163.08353997701 ----------
[1/200] Training loss: 0.15597057
[2/200] Training loss: 0.05995225
[3/200] Training loss: 0.05257114
[4/200] Training loss: 0.05043004
[5/200] Training loss: 0.04865257
[6/200] Training loss: 0.04398802
[7/200] Training loss: 0.04242832
[8/200] Training loss: 0.04086079
[9/200] Training loss: 0.03563671
[10/200] Training loss: 0.03669984
[50/200] Training loss: 0.01808215
[100/200] Training loss: 0.01534850
[150/200] Training loss: 0.01433865
[200/200] Training loss: 0.01373266
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10597.053175293591 ----------
[1/200] Training loss: 0.16501303
[2/200] Training loss: 0.06058232
[3/200] Training loss: 0.05351180
[4/200] Training loss: 0.04750636
[5/200] Training loss: 0.04715563
[6/200] Training loss: 0.04153202
[7/200] Training loss: 0.03780370
[8/200] Training loss: 0.03842561
[9/200] Training loss: 0.03321561
[10/200] Training loss: 0.03222531
[50/200] Training loss: 0.01644374
[100/200] Training loss: 0.01381256
[150/200] Training loss: 0.01251589
[200/200] Training loss: 0.01236861
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 16 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20977.385919127293 ----------
[1/200] Training loss: 0.17972013
[2/200] Training loss: 0.05517716
[3/200] Training loss: 0.05290334
[4/200] Training loss: 0.04704340
[5/200] Training loss: 0.04595483
[6/200] Training loss: 0.04240484
[7/200] Training loss: 0.04054036
[8/200] Training loss: 0.03891565
[9/200] Training loss: 0.03604288
[10/200] Training loss: 0.03266634
[50/200] Training loss: 0.01636451
[100/200] Training loss: 0.01388397
[150/200] Training loss: 0.01265236
[200/200] Training loss: 0.01152479
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 16 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18754.614152255974 ----------
[1/200] Training loss: 0.16704124
[2/200] Training loss: 0.05690526
[3/200] Training loss: 0.05233113
[4/200] Training loss: 0.04768608
[5/200] Training loss: 0.04738513
[6/200] Training loss: 0.04380702
[7/200] Training loss: 0.04226503
[8/200] Training loss: 0.03947284
[9/200] Training loss: 0.03692105
[10/200] Training loss: 0.03593454
[50/200] Training loss: 0.01821105
[100/200] Training loss: 0.01543794
[150/200] Training loss: 0.01398279
[200/200] Training loss: 0.01287736
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 16 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12420.959705272375 ----------
[1/200] Training loss: 0.16526627
[2/200] Training loss: 0.06115558
[3/200] Training loss: 0.05573146
[4/200] Training loss: 0.04956155
[5/200] Training loss: 0.04738680
[6/200] Training loss: 0.04467811
[7/200] Training loss: 0.04240838
[8/200] Training loss: 0.04159109
[9/200] Training loss: 0.04097302
[10/200] Training loss: 0.03732041
[50/200] Training loss: 0.01885970
[100/200] Training loss: 0.01537862
[150/200] Training loss: 0.01370674
[200/200] Training loss: 0.01382440
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 16 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23661.71726650456 ----------
[1/200] Training loss: 0.16446830
[2/200] Training loss: 0.05955547
[3/200] Training loss: 0.05245200
[4/200] Training loss: 0.04740351
[5/200] Training loss: 0.04417509
[6/200] Training loss: 0.04445337
[7/200] Training loss: 0.04269414
[8/200] Training loss: 0.03957977
[9/200] Training loss: 0.03505882
[10/200] Training loss: 0.03331908
[50/200] Training loss: 0.01878895
[100/200] Training loss: 0.01528522
[150/200] Training loss: 0.01442149
[200/200] Training loss: 0.01296108
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 16 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8866.658446111478 ----------
[1/200] Training loss: 0.17575259
[2/200] Training loss: 0.05850202
[3/200] Training loss: 0.04884409
[4/200] Training loss: 0.04852464
[5/200] Training loss: 0.04436393
[6/200] Training loss: 0.03956077
[7/200] Training loss: 0.04142538
[8/200] Training loss: 0.03568008
[9/200] Training loss: 0.03424574
[10/200] Training loss: 0.03293838
[50/200] Training loss: 0.01795488
[100/200] Training loss: 0.01427529
[150/200] Training loss: 0.01241848
[200/200] Training loss: 0.01100444
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10674.524064331861 ----------
[1/200] Training loss: 0.18868939
[2/200] Training loss: 0.06146451
[3/200] Training loss: 0.05296245
[4/200] Training loss: 0.05324653
[5/200] Training loss: 0.04876256
[6/200] Training loss: 0.04579098
[7/200] Training loss: 0.04449094
[8/200] Training loss: 0.04200281
[9/200] Training loss: 0.03837058
[10/200] Training loss: 0.03580165
[50/200] Training loss: 0.02356809
[100/200] Training loss: 0.01606112
[150/200] Training loss: 0.01531096
[200/200] Training loss: 0.01437774
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18131.12329669621 ----------
[1/200] Training loss: 0.16563735
[2/200] Training loss: 0.05835362
[3/200] Training loss: 0.05236288
[4/200] Training loss: 0.04791205
[5/200] Training loss: 0.04820370
[6/200] Training loss: 0.04058964
[7/200] Training loss: 0.03483327
[8/200] Training loss: 0.03260498
[9/200] Training loss: 0.03188460
[10/200] Training loss: 0.03046359
[50/200] Training loss: 0.01576841
[100/200] Training loss: 0.01209666
[150/200] Training loss: 0.01117913
[200/200] Training loss: 0.01080245
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 16 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15220.219709320887 ----------
[1/200] Training loss: 0.17567226
[2/200] Training loss: 0.06451762
[3/200] Training loss: 0.05721096
[4/200] Training loss: 0.05625636
[5/200] Training loss: 0.05312559
[6/200] Training loss: 0.05297797
[7/200] Training loss: 0.04965608
[8/200] Training loss: 0.05105758
[9/200] Training loss: 0.04708040
[10/200] Training loss: 0.04646690
[50/200] Training loss: 0.02130305
[100/200] Training loss: 0.01623529
[150/200] Training loss: 0.01432489
[200/200] Training loss: 0.01331292
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5903.081568130327 ----------
[1/200] Training loss: 0.17366940
[2/200] Training loss: 0.06154077
[3/200] Training loss: 0.05217881
[4/200] Training loss: 0.04720000
[5/200] Training loss: 0.04065233
[6/200] Training loss: 0.03919653
[7/200] Training loss: 0.03922992
[8/200] Training loss: 0.03470606
[9/200] Training loss: 0.03063301
[10/200] Training loss: 0.03193748
[50/200] Training loss: 0.01639752
[100/200] Training loss: 0.01431827
[150/200] Training loss: 0.01266720
[200/200] Training loss: 0.01163173
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16059.699623591969 ----------
[1/200] Training loss: 0.17342511
[2/200] Training loss: 0.05689485
[3/200] Training loss: 0.04823373
[4/200] Training loss: 0.04587637
[5/200] Training loss: 0.04425921
[6/200] Training loss: 0.04237489
[7/200] Training loss: 0.03889129
[8/200] Training loss: 0.03649673
[9/200] Training loss: 0.03425159
[10/200] Training loss: 0.03274080
[50/200] Training loss: 0.01626161
[100/200] Training loss: 0.01368679
[150/200] Training loss: 0.01269050
[200/200] Training loss: 0.01176816
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11589.469012858182 ----------
[1/200] Training loss: 0.16766669
[2/200] Training loss: 0.06264781
[3/200] Training loss: 0.05408964
[4/200] Training loss: 0.04831495
[5/200] Training loss: 0.04815135
[6/200] Training loss: 0.04795745
[7/200] Training loss: 0.04267886
[8/200] Training loss: 0.04044219
[9/200] Training loss: 0.03676100
[10/200] Training loss: 0.03607465
[50/200] Training loss: 0.01711823
[100/200] Training loss: 0.01448173
[150/200] Training loss: 0.01378471
[200/200] Training loss: 0.01218330
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14378.04882451023 ----------
[1/200] Training loss: 0.18066262
[2/200] Training loss: 0.06406742
[3/200] Training loss: 0.05739770
[4/200] Training loss: 0.05336142
[5/200] Training loss: 0.05106967
[6/200] Training loss: 0.05172243
[7/200] Training loss: 0.04893799
[8/200] Training loss: 0.04890627
[9/200] Training loss: 0.04497534
[10/200] Training loss: 0.04233253
[50/200] Training loss: 0.01805769
[100/200] Training loss: 0.01552151
[150/200] Training loss: 0.01320524
[200/200] Training loss: 0.01239050
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9182.905858169297 ----------
[1/200] Training loss: 0.18841977
[2/200] Training loss: 0.06020361
[3/200] Training loss: 0.05119288
[4/200] Training loss: 0.05030226
[5/200] Training loss: 0.04617357
[6/200] Training loss: 0.04337868
[7/200] Training loss: 0.03768920
[8/200] Training loss: 0.03907897
[9/200] Training loss: 0.03669934
[10/200] Training loss: 0.03252175
[50/200] Training loss: 0.01866772
[100/200] Training loss: 0.01462663
[150/200] Training loss: 0.01315267
[200/200] Training loss: 0.01216412
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6920.146530240527 ----------
[1/200] Training loss: 0.17921778
[2/200] Training loss: 0.05799853
[3/200] Training loss: 0.05626756
[4/200] Training loss: 0.05245862
[5/200] Training loss: 0.04549031
[6/200] Training loss: 0.04600436
[7/200] Training loss: 0.04424640
[8/200] Training loss: 0.04130188
[9/200] Training loss: 0.03669397
[10/200] Training loss: 0.03371780
[50/200] Training loss: 0.01712530
[100/200] Training loss: 0.01502501
[150/200] Training loss: 0.01405819
[200/200] Training loss: 0.01207854
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18752.04266206751 ----------
[1/200] Training loss: 0.18555428
[2/200] Training loss: 0.06205085
[3/200] Training loss: 0.05365694
[4/200] Training loss: 0.05571389
[5/200] Training loss: 0.04882357
[6/200] Training loss: 0.04552145
[7/200] Training loss: 0.04128259
[8/200] Training loss: 0.04135152
[9/200] Training loss: 0.03701529
[10/200] Training loss: 0.03744050
[50/200] Training loss: 0.01920412
[100/200] Training loss: 0.01598219
[150/200] Training loss: 0.01376758
[200/200] Training loss: 0.01244879
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16119.287825459287 ----------
[1/200] Training loss: 0.16408174
[2/200] Training loss: 0.05979596
[3/200] Training loss: 0.05310109
[4/200] Training loss: 0.04805399
[5/200] Training loss: 0.04450689
[6/200] Training loss: 0.03856102
[7/200] Training loss: 0.04074691
[8/200] Training loss: 0.03644541
[9/200] Training loss: 0.03205922
[10/200] Training loss: 0.03167778
[50/200] Training loss: 0.01654526
[100/200] Training loss: 0.01398683
[150/200] Training loss: 0.01307443
[200/200] Training loss: 0.01182469
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15068.22511114033 ----------
[1/200] Training loss: 0.17996658
[2/200] Training loss: 0.06428690
[3/200] Training loss: 0.05483267
[4/200] Training loss: 0.05414048
[5/200] Training loss: 0.04998461
[6/200] Training loss: 0.04709599
[7/200] Training loss: 0.04811816
[8/200] Training loss: 0.04469426
[9/200] Training loss: 0.04290260
[10/200] Training loss: 0.04223597
[50/200] Training loss: 0.01924901
[100/200] Training loss: 0.01648515
[150/200] Training loss: 0.01480041
[200/200] Training loss: 0.01388258
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15108.491916799638 ----------
[1/200] Training loss: 0.15321561
[2/200] Training loss: 0.05797013
[3/200] Training loss: 0.05462186
[4/200] Training loss: 0.05123017
[5/200] Training loss: 0.04730913
[6/200] Training loss: 0.04491849
[7/200] Training loss: 0.04194218
[8/200] Training loss: 0.04133333
[9/200] Training loss: 0.03893649
[10/200] Training loss: 0.03587045
[50/200] Training loss: 0.01793529
[100/200] Training loss: 0.01422571
[150/200] Training loss: 0.01288729
[200/200] Training loss: 0.01237584
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 24002.83449928362 ----------
[1/200] Training loss: 0.18374054
[2/200] Training loss: 0.06405822
[3/200] Training loss: 0.04971079
[4/200] Training loss: 0.04945396
[5/200] Training loss: 0.04752521
[6/200] Training loss: 0.04128146
[7/200] Training loss: 0.04340911
[8/200] Training loss: 0.03938965
[9/200] Training loss: 0.04038075
[10/200] Training loss: 0.03705815
[50/200] Training loss: 0.02254177
[100/200] Training loss: 0.01623279
[150/200] Training loss: 0.01454220
[200/200] Training loss: 0.01365876
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15494.572469093815 ----------
[1/200] Training loss: 0.16293537
[2/200] Training loss: 0.06328939
[3/200] Training loss: 0.05358830
[4/200] Training loss: 0.04680285
[5/200] Training loss: 0.04295311
[6/200] Training loss: 0.04010808
[7/200] Training loss: 0.03622535
[8/200] Training loss: 0.03539525
[9/200] Training loss: 0.03479487
[10/200] Training loss: 0.03388833
[50/200] Training loss: 0.01757271
[100/200] Training loss: 0.01308216
[150/200] Training loss: 0.01206146
[200/200] Training loss: 0.01088088
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6005.256031178022 ----------
[1/200] Training loss: 0.15982797
[2/200] Training loss: 0.05601953
[3/200] Training loss: 0.05359405
[4/200] Training loss: 0.05066080
[5/200] Training loss: 0.04797846
[6/200] Training loss: 0.04466198
[7/200] Training loss: 0.04340427
[8/200] Training loss: 0.03631515
[9/200] Training loss: 0.03410486
[10/200] Training loss: 0.03531139
[50/200] Training loss: 0.01649600
[100/200] Training loss: 0.01441833
[150/200] Training loss: 0.01274218
[200/200] Training loss: 0.01261444
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12570.536981370366 ----------
[1/200] Training loss: 0.15646741
[2/200] Training loss: 0.06217466
[3/200] Training loss: 0.05452086
[4/200] Training loss: 0.05235350
[5/200] Training loss: 0.04871309
[6/200] Training loss: 0.04755925
[7/200] Training loss: 0.04007556
[8/200] Training loss: 0.04055924
[9/200] Training loss: 0.03931908
[10/200] Training loss: 0.03542893
[50/200] Training loss: 0.01680523
[100/200] Training loss: 0.01476166
[150/200] Training loss: 0.01288973
[200/200] Training loss: 0.01189067
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10877.605986613047 ----------
[1/200] Training loss: 0.13720917
[2/200] Training loss: 0.05472116
[3/200] Training loss: 0.04374882
[4/200] Training loss: 0.04624990
[5/200] Training loss: 0.04032790
[6/200] Training loss: 0.03843449
[7/200] Training loss: 0.03499472
[8/200] Training loss: 0.03226862
[9/200] Training loss: 0.02982907
[10/200] Training loss: 0.03034537
[50/200] Training loss: 0.01659871
[100/200] Training loss: 0.01463419
[150/200] Training loss: 0.01284414
[200/200] Training loss: 0.01195570
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 20005.580021583977 ----------
[1/200] Training loss: 0.14693718
[2/200] Training loss: 0.05571066
[3/200] Training loss: 0.05039134
[4/200] Training loss: 0.04424022
[5/200] Training loss: 0.04208537
[6/200] Training loss: 0.04140537
[7/200] Training loss: 0.03719745
[8/200] Training loss: 0.03315782
[9/200] Training loss: 0.03113618
[10/200] Training loss: 0.03154084
[50/200] Training loss: 0.01759179
[100/200] Training loss: 0.01599352
[150/200] Training loss: 0.01491987
[200/200] Training loss: 0.01351846
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15629.90620573265 ----------
[1/200] Training loss: 0.14602980
[2/200] Training loss: 0.05576370
[3/200] Training loss: 0.05110491
[4/200] Training loss: 0.04493782
[5/200] Training loss: 0.04446415
[6/200] Training loss: 0.04113317
[7/200] Training loss: 0.03674089
[8/200] Training loss: 0.03570565
[9/200] Training loss: 0.03426212
[10/200] Training loss: 0.03344975
[50/200] Training loss: 0.01648683
[100/200] Training loss: 0.01380830
[150/200] Training loss: 0.01233542
[200/200] Training loss: 0.01172272
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 3936.703316227932 ----------
[1/200] Training loss: 0.19261477
[2/200] Training loss: 0.06370796
[3/200] Training loss: 0.05691961
[4/200] Training loss: 0.05174844
[5/200] Training loss: 0.04793767
[6/200] Training loss: 0.04714313
[7/200] Training loss: 0.04051188
[8/200] Training loss: 0.03616912
[9/200] Training loss: 0.03065542
[10/200] Training loss: 0.03078317
[50/200] Training loss: 0.01851790
[100/200] Training loss: 0.01574547
[150/200] Training loss: 0.01521696
[200/200] Training loss: 0.01399059
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10587.798638055032 ----------
[1/200] Training loss: 0.13419226
[2/200] Training loss: 0.05532326
[3/200] Training loss: 0.05321324
[4/200] Training loss: 0.04606622
[5/200] Training loss: 0.04549847
[6/200] Training loss: 0.04063796
[7/200] Training loss: 0.03949829
[8/200] Training loss: 0.03578246
[9/200] Training loss: 0.03490673
[10/200] Training loss: 0.03288942
[50/200] Training loss: 0.01731562
[100/200] Training loss: 0.01351441
[150/200] Training loss: 0.01206902
[200/200] Training loss: 0.01094969
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 38469.72336786424 ----------
[1/200] Training loss: 0.18039302
[2/200] Training loss: 0.06072999
[3/200] Training loss: 0.05580760
[4/200] Training loss: 0.05148480
[5/200] Training loss: 0.04906561
[6/200] Training loss: 0.04639956
[7/200] Training loss: 0.04594627
[8/200] Training loss: 0.04105655
[9/200] Training loss: 0.03920195
[10/200] Training loss: 0.03954527
[50/200] Training loss: 0.01813649
[100/200] Training loss: 0.01585154
[150/200] Training loss: 0.01388341
[200/200] Training loss: 0.01346306
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15358.213958660688 ----------
[1/200] Training loss: 0.15021344
[2/200] Training loss: 0.05799476
[3/200] Training loss: 0.05199648
[4/200] Training loss: 0.04663098
[5/200] Training loss: 0.04198487
[6/200] Training loss: 0.03543624
[7/200] Training loss: 0.03480797
[8/200] Training loss: 0.03384301
[9/200] Training loss: 0.03252202
[10/200] Training loss: 0.02957472
[50/200] Training loss: 0.01783679
[100/200] Training loss: 0.01420306
[150/200] Training loss: 0.01239246
[200/200] Training loss: 0.01155878
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13275.367264222863 ----------
[1/200] Training loss: 0.15753737
[2/200] Training loss: 0.05563255
[3/200] Training loss: 0.04820114
[4/200] Training loss: 0.04631843
[5/200] Training loss: 0.04457723
[6/200] Training loss: 0.03922838
[7/200] Training loss: 0.03852394
[8/200] Training loss: 0.03265516
[9/200] Training loss: 0.03058412
[10/200] Training loss: 0.03360149
[50/200] Training loss: 0.01837315
[100/200] Training loss: 0.01569716
[150/200] Training loss: 0.01407822
[200/200] Training loss: 0.01185660
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18414.328334207577 ----------
[1/200] Training loss: 0.17664791
[2/200] Training loss: 0.06238437
[3/200] Training loss: 0.05568442
[4/200] Training loss: 0.05051043
[5/200] Training loss: 0.04112357
[6/200] Training loss: 0.03973851
[7/200] Training loss: 0.03507986
[8/200] Training loss: 0.03466901
[9/200] Training loss: 0.03219224
[10/200] Training loss: 0.03156511
[50/200] Training loss: 0.01698824
[100/200] Training loss: 0.01509095
[150/200] Training loss: 0.01443940
[200/200] Training loss: 0.01280714
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12477.876101324295 ----------
[1/200] Training loss: 0.16397329
[2/200] Training loss: 0.05987958
[3/200] Training loss: 0.05246166
[4/200] Training loss: 0.05058096
[5/200] Training loss: 0.04715368
[6/200] Training loss: 0.04590495
[7/200] Training loss: 0.04075635
[8/200] Training loss: 0.03869269
[9/200] Training loss: 0.03473104
[10/200] Training loss: 0.03218398
[50/200] Training loss: 0.01663573
[100/200] Training loss: 0.01430517
[150/200] Training loss: 0.01374304
[200/200] Training loss: 0.01297086
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15069.460507927946 ----------
[1/200] Training loss: 0.13894264
[2/200] Training loss: 0.05992070
[3/200] Training loss: 0.05232045
[4/200] Training loss: 0.04806260
[5/200] Training loss: 0.04665696
[6/200] Training loss: 0.04222203
[7/200] Training loss: 0.04079911
[8/200] Training loss: 0.04322122
[9/200] Training loss: 0.03600425
[10/200] Training loss: 0.03512848
[50/200] Training loss: 0.01634132
[100/200] Training loss: 0.01446739
[150/200] Training loss: 0.01351670
[200/200] Training loss: 0.01198232
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18103.58992023405 ----------
[1/200] Training loss: 0.17916197
[2/200] Training loss: 0.06327339
[3/200] Training loss: 0.05614417
[4/200] Training loss: 0.04975109
[5/200] Training loss: 0.04686608
[6/200] Training loss: 0.04447725
[7/200] Training loss: 0.04367593
[8/200] Training loss: 0.03938270
[9/200] Training loss: 0.03974058
[10/200] Training loss: 0.03612205
[50/200] Training loss: 0.01786660
[100/200] Training loss: 0.01479911
[150/200] Training loss: 0.01310310
[200/200] Training loss: 0.01284782
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10915.42504898458 ----------
[1/200] Training loss: 0.17641348
[2/200] Training loss: 0.05918838
[3/200] Training loss: 0.05624691
[4/200] Training loss: 0.04923956
[5/200] Training loss: 0.04926161
[6/200] Training loss: 0.04485736
[7/200] Training loss: 0.04421602
[8/200] Training loss: 0.04070692
[9/200] Training loss: 0.03959678
[10/200] Training loss: 0.03596797
[50/200] Training loss: 0.01929532
[100/200] Training loss: 0.01581376
[150/200] Training loss: 0.01369210
[200/200] Training loss: 0.01259052
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6729.648430638855 ----------
[1/200] Training loss: 0.17910422
[2/200] Training loss: 0.06103817
[3/200] Training loss: 0.05638537
[4/200] Training loss: 0.05123892
[5/200] Training loss: 0.04888915
[6/200] Training loss: 0.04482428
[7/200] Training loss: 0.04268315
[8/200] Training loss: 0.04230801
[9/200] Training loss: 0.03869678
[10/200] Training loss: 0.03805477
[50/200] Training loss: 0.01769860
[100/200] Training loss: 0.01579702
[150/200] Training loss: 0.01423491
[200/200] Training loss: 0.01386873
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 19169.57172187214 ----------
[1/200] Training loss: 0.17744447
[2/200] Training loss: 0.06481290
[3/200] Training loss: 0.05308626
[4/200] Training loss: 0.05343010
[5/200] Training loss: 0.04750589
[6/200] Training loss: 0.05019467
[7/200] Training loss: 0.04381688
[8/200] Training loss: 0.04269379
[9/200] Training loss: 0.04138052
[10/200] Training loss: 0.03825471
[50/200] Training loss: 0.01781357
[100/200] Training loss: 0.01592942
[150/200] Training loss: 0.01427276
[200/200] Training loss: 0.01343820
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14932.176800453442 ----------
[1/200] Training loss: 0.16016369
[2/200] Training loss: 0.05783073
[3/200] Training loss: 0.05474243
[4/200] Training loss: 0.04911764
[5/200] Training loss: 0.04516952
[6/200] Training loss: 0.04555682
[7/200] Training loss: 0.04370097
[8/200] Training loss: 0.04024301
[9/200] Training loss: 0.03896910
[10/200] Training loss: 0.03726269
[50/200] Training loss: 0.01726281
[100/200] Training loss: 0.01490800
[150/200] Training loss: 0.01383534
[200/200] Training loss: 0.01232705
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13172.476760275571 ----------
[1/200] Training loss: 0.14691378
[2/200] Training loss: 0.05190273
[3/200] Training loss: 0.04445121
[4/200] Training loss: 0.04849923
[5/200] Training loss: 0.04233075
[6/200] Training loss: 0.03781307
[7/200] Training loss: 0.03779156
[8/200] Training loss: 0.03442829
[9/200] Training loss: 0.03261720
[10/200] Training loss: 0.03201390
[50/200] Training loss: 0.01717397
[100/200] Training loss: 0.01546031
[150/200] Training loss: 0.01476041
[200/200] Training loss: 0.01306781
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5393.2122153684995 ----------
[1/200] Training loss: 0.18626231
[2/200] Training loss: 0.05792923
[3/200] Training loss: 0.05280023
[4/200] Training loss: 0.05114781
[5/200] Training loss: 0.04768117
[6/200] Training loss: 0.04463786
[7/200] Training loss: 0.03739564
[8/200] Training loss: 0.03598943
[9/200] Training loss: 0.03463047
[10/200] Training loss: 0.02992519
[50/200] Training loss: 0.01661953
[100/200] Training loss: 0.01449521
[150/200] Training loss: 0.01282168
[200/200] Training loss: 0.01319904
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.31485732587432397 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12732.542872498016 ----------
[1/200] Training loss: 0.18429158
[2/200] Training loss: 0.05552314
[3/200] Training loss: 0.05202811
[4/200] Training loss: 0.04730692
[5/200] Training loss: 0.04462889
[6/200] Training loss: 0.03926618
[7/200] Training loss: 0.03783774
[8/200] Training loss: 0.03398973
[9/200] Training loss: 0.03297791
[10/200] Training loss: 0.02938209
[50/200] Training loss: 0.01762597
[100/200] Training loss: 0.01359594
[150/200] Training loss: 0.01227211
[200/200] Training loss: 0.01084205
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8168.002693437362 ----------
[1/200] Training loss: 0.16472627
[2/200] Training loss: 0.05957779
[3/200] Training loss: 0.05139166
[4/200] Training loss: 0.04604117
[5/200] Training loss: 0.04448333
[6/200] Training loss: 0.04214443
[7/200] Training loss: 0.03931978
[8/200] Training loss: 0.03396630
[9/200] Training loss: 0.03447520
[10/200] Training loss: 0.03264052
[50/200] Training loss: 0.01740024
[100/200] Training loss: 0.01369253
[150/200] Training loss: 0.01237667
[200/200] Training loss: 0.01180805
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11986.777381765292 ----------
[1/200] Training loss: 0.12699415
[2/200] Training loss: 0.05698498
[3/200] Training loss: 0.04692144
[4/200] Training loss: 0.04231204
[5/200] Training loss: 0.03502615
[6/200] Training loss: 0.03437094
[7/200] Training loss: 0.02918594
[8/200] Training loss: 0.03062563
[9/200] Training loss: 0.02721244
[10/200] Training loss: 0.02505989
[50/200] Training loss: 0.01811326
[100/200] Training loss: 0.01517566
[150/200] Training loss: 0.01392927
[200/200] Training loss: 0.01290527
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12140.492905973793 ----------
[1/200] Training loss: 0.18007221
[2/200] Training loss: 0.05809980
[3/200] Training loss: 0.05446401
[4/200] Training loss: 0.04714959
[5/200] Training loss: 0.04493655
[6/200] Training loss: 0.04432753
[7/200] Training loss: 0.04479136
[8/200] Training loss: 0.03849694
[9/200] Training loss: 0.03878456
[10/200] Training loss: 0.03587620
[50/200] Training loss: 0.01716060
[100/200] Training loss: 0.01317616
[150/200] Training loss: 0.01187982
[200/200] Training loss: 0.01164426
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13066.912718771791 ----------
[1/200] Training loss: 0.15446921
[2/200] Training loss: 0.06056321
[3/200] Training loss: 0.05262336
[4/200] Training loss: 0.05011937
[5/200] Training loss: 0.04815509
[6/200] Training loss: 0.04243841
[7/200] Training loss: 0.03764391
[8/200] Training loss: 0.03508537
[9/200] Training loss: 0.03459557
[10/200] Training loss: 0.03198997
[50/200] Training loss: 0.01854944
[100/200] Training loss: 0.01682276
[150/200] Training loss: 0.01573033
[200/200] Training loss: 0.01489143
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12279.50096705888 ----------
[1/200] Training loss: 0.17589091
[2/200] Training loss: 0.05973773
[3/200] Training loss: 0.05239455
[4/200] Training loss: 0.05063116
[5/200] Training loss: 0.04854571
[6/200] Training loss: 0.04485108
[7/200] Training loss: 0.04370352
[8/200] Training loss: 0.04220279
[9/200] Training loss: 0.04125393
[10/200] Training loss: 0.03760791
[50/200] Training loss: 0.01822115
[100/200] Training loss: 0.01528839
[150/200] Training loss: 0.01323635
[200/200] Training loss: 0.01213869
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12715.691094077427 ----------
[1/200] Training loss: 0.15416496
[2/200] Training loss: 0.05350935
[3/200] Training loss: 0.04869471
[4/200] Training loss: 0.04960570
[5/200] Training loss: 0.04292058
[6/200] Training loss: 0.03971977
[7/200] Training loss: 0.03295740
[8/200] Training loss: 0.03289094
[9/200] Training loss: 0.03239640
[10/200] Training loss: 0.03147571
[50/200] Training loss: 0.01839278
[100/200] Training loss: 0.01373287
[150/200] Training loss: 0.01265184
[200/200] Training loss: 0.01221270
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15058.121529593258 ----------
[1/200] Training loss: 0.17770124
[2/200] Training loss: 0.06003873
[3/200] Training loss: 0.05205102
[4/200] Training loss: 0.04711390
[5/200] Training loss: 0.04722938
[6/200] Training loss: 0.04543236
[7/200] Training loss: 0.04288220
[8/200] Training loss: 0.03937643
[9/200] Training loss: 0.03740291
[10/200] Training loss: 0.03535122
[50/200] Training loss: 0.01778444
[100/200] Training loss: 0.01479949
[150/200] Training loss: 0.01310290
[200/200] Training loss: 0.01190463
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4611.154302341226 ----------
[1/200] Training loss: 0.15145848
[2/200] Training loss: 0.05270999
[3/200] Training loss: 0.04858336
[4/200] Training loss: 0.04461440
[5/200] Training loss: 0.04375493
[6/200] Training loss: 0.03852740
[7/200] Training loss: 0.03741020
[8/200] Training loss: 0.03640015
[9/200] Training loss: 0.03409627
[10/200] Training loss: 0.03332392
[50/200] Training loss: 0.01855666
[100/200] Training loss: 0.01514409
[150/200] Training loss: 0.01345180
[200/200] Training loss: 0.01253579
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.028568654400047657
----FITNESS-----------RMSE---- 5415.750917462878 ----------
[1/200] Training loss: 0.17269106
[2/200] Training loss: 0.06561584
[3/200] Training loss: 0.05768988
[4/200] Training loss: 0.05303898
[5/200] Training loss: 0.04923585
[6/200] Training loss: 0.04733623
[7/200] Training loss: 0.04462488
[8/200] Training loss: 0.04404199
[9/200] Training loss: 0.04052814
[10/200] Training loss: 0.03766220
[50/200] Training loss: 0.02406334
[100/200] Training loss: 0.01612685
[150/200] Training loss: 0.01448066
[200/200] Training loss: 0.01346359
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8224.80540803246 ----------
[1/200] Training loss: 0.17412079
[2/200] Training loss: 0.06384645
[3/200] Training loss: 0.05214803
[4/200] Training loss: 0.04827591
[5/200] Training loss: 0.04730563
[6/200] Training loss: 0.04321609
[7/200] Training loss: 0.04297372
[8/200] Training loss: 0.03811152
[9/200] Training loss: 0.03620338
[10/200] Training loss: 0.03631161
[50/200] Training loss: 0.01795227
[100/200] Training loss: 0.01654163
[150/200] Training loss: 0.01526924
[200/200] Training loss: 0.01320902
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9307.443043070421 ----------
[1/200] Training loss: 0.18081341
[2/200] Training loss: 0.07030640
[3/200] Training loss: 0.05512334
[4/200] Training loss: 0.04499874
[5/200] Training loss: 0.04149559
[6/200] Training loss: 0.03930074
[7/200] Training loss: 0.03421674
[8/200] Training loss: 0.03310601
[9/200] Training loss: 0.03167201
[10/200] Training loss: 0.03015009
[50/200] Training loss: 0.01737643
[100/200] Training loss: 0.01433143
[150/200] Training loss: 0.01300485
[200/200] Training loss: 0.01229667
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14597.432650983528 ----------
[1/200] Training loss: 0.14020856
[2/200] Training loss: 0.05781240
[3/200] Training loss: 0.04863565
[4/200] Training loss: 0.04398493
[5/200] Training loss: 0.04409891
[6/200] Training loss: 0.04279830
[7/200] Training loss: 0.03734358
[8/200] Training loss: 0.03678755
[9/200] Training loss: 0.03450453
[10/200] Training loss: 0.03421360
[50/200] Training loss: 0.01844384
[100/200] Training loss: 0.01499057
[150/200] Training loss: 0.01332833
[200/200] Training loss: 0.01299620
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5659.063703476044 ----------
[1/200] Training loss: 0.16447687
[2/200] Training loss: 0.06099599
[3/200] Training loss: 0.05208469
[4/200] Training loss: 0.05064720
[5/200] Training loss: 0.04908042
[6/200] Training loss: 0.04585221
[7/200] Training loss: 0.04243058
[8/200] Training loss: 0.03852575
[9/200] Training loss: 0.03721309
[10/200] Training loss: 0.03246694
[50/200] Training loss: 0.01879216
[100/200] Training loss: 0.01580535
[150/200] Training loss: 0.01362481
[200/200] Training loss: 0.01282269
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17754.46940913752 ----------
[1/200] Training loss: 0.17195472
[2/200] Training loss: 0.06397853
[3/200] Training loss: 0.06100722
[4/200] Training loss: 0.05878019
[5/200] Training loss: 0.05635451
[6/200] Training loss: 0.05388902
[7/200] Training loss: 0.05170363
[8/200] Training loss: 0.05225078
[9/200] Training loss: 0.04916975
[10/200] Training loss: 0.04722944
[50/200] Training loss: 0.02001271
[100/200] Training loss: 0.01801123
[150/200] Training loss: 0.01545949
[200/200] Training loss: 0.01391169
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6401828105317761 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7230.9838887941105 ----------
[1/200] Training loss: 0.14871875
[2/200] Training loss: 0.05496616
[3/200] Training loss: 0.05272207
[4/200] Training loss: 0.04388244
[5/200] Training loss: 0.04441801
[6/200] Training loss: 0.04236046
[7/200] Training loss: 0.03776640
[8/200] Training loss: 0.03436187
[9/200] Training loss: 0.03380162
[10/200] Training loss: 0.03358734
[50/200] Training loss: 0.01761056
[100/200] Training loss: 0.01529097
[150/200] Training loss: 0.01338336
[200/200] Training loss: 0.01213174
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.028568654400047657
----FITNESS-----------RMSE---- 3624.788131739564 ----------
[1/200] Training loss: 0.15507667
[2/200] Training loss: 0.06058579
[3/200] Training loss: 0.05184853
[4/200] Training loss: 0.05268593
[5/200] Training loss: 0.04546391
[6/200] Training loss: 0.04633705
[7/200] Training loss: 0.04077437
[8/200] Training loss: 0.04013719
[9/200] Training loss: 0.03853356
[10/200] Training loss: 0.03397879
[50/200] Training loss: 0.01609591
[100/200] Training loss: 0.01332466
[150/200] Training loss: 0.01103789
[200/200] Training loss: 0.01041837
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10328.851630263647 ----------
[1/200] Training loss: 0.17035832
[2/200] Training loss: 0.05780622
[3/200] Training loss: 0.05154553
[4/200] Training loss: 0.05285000
[5/200] Training loss: 0.04682900
[6/200] Training loss: 0.04400566
[7/200] Training loss: 0.04176456
[8/200] Training loss: 0.03761757
[9/200] Training loss: 0.03584074
[10/200] Training loss: 0.03388603
[50/200] Training loss: 0.01956622
[100/200] Training loss: 0.01698238
[150/200] Training loss: 0.01581283
[200/200] Training loss: 0.01455676
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.028568654400047657
----FITNESS-----------RMSE---- 15575.098073527499 ----------
[1/200] Training loss: 0.14600464
[2/200] Training loss: 0.05999553
[3/200] Training loss: 0.05225213
[4/200] Training loss: 0.05027994
[5/200] Training loss: 0.04523040
[6/200] Training loss: 0.04006210
[7/200] Training loss: 0.03794574
[8/200] Training loss: 0.03467640
[9/200] Training loss: 0.03406693
[10/200] Training loss: 0.03255757
[50/200] Training loss: 0.01896213
[100/200] Training loss: 0.01547744
[150/200] Training loss: 0.01453706
[200/200] Training loss: 0.01373758
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7545.70129278916 ----------
[1/200] Training loss: 0.17553638
[2/200] Training loss: 0.05955338
[3/200] Training loss: 0.05414835
[4/200] Training loss: 0.05383623
[5/200] Training loss: 0.05187164
[6/200] Training loss: 0.04833270
[7/200] Training loss: 0.04587299
[8/200] Training loss: 0.04311459
[9/200] Training loss: 0.04348531
[10/200] Training loss: 0.03934926
[50/200] Training loss: 0.01755082
[100/200] Training loss: 0.01568961
[150/200] Training loss: 0.01408649
[200/200] Training loss: 0.01312551
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7068.768209525617 ----------
[1/200] Training loss: 0.14265073
[2/200] Training loss: 0.05371740
[3/200] Training loss: 0.05098941
[4/200] Training loss: 0.04542077
[5/200] Training loss: 0.04434375
[6/200] Training loss: 0.04199306
[7/200] Training loss: 0.04026308
[8/200] Training loss: 0.03611366
[9/200] Training loss: 0.03698196
[10/200] Training loss: 0.03467244
[50/200] Training loss: 0.01805559
[100/200] Training loss: 0.01508110
[150/200] Training loss: 0.01290225
[200/200] Training loss: 0.01133473
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11349.496905149585 ----------
[1/200] Training loss: 0.13951022
[2/200] Training loss: 0.06108155
[3/200] Training loss: 0.05026194
[4/200] Training loss: 0.04768811
[5/200] Training loss: 0.04655640
[6/200] Training loss: 0.04120109
[7/200] Training loss: 0.03976607
[8/200] Training loss: 0.03775857
[9/200] Training loss: 0.03607678
[10/200] Training loss: 0.03185288
[50/200] Training loss: 0.01801928
[100/200] Training loss: 0.01562298
[150/200] Training loss: 0.01470332
[200/200] Training loss: 0.01353358
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21238.077502448286 ----------
[1/200] Training loss: 0.15049951
[2/200] Training loss: 0.05884711
[3/200] Training loss: 0.05728223
[4/200] Training loss: 0.04790145
[5/200] Training loss: 0.04684913
[6/200] Training loss: 0.04437096
[7/200] Training loss: 0.03902463
[8/200] Training loss: 0.03881455
[9/200] Training loss: 0.03749723
[10/200] Training loss: 0.03606858
[50/200] Training loss: 0.01842373
[100/200] Training loss: 0.01642589
[150/200] Training loss: 0.01521347
[200/200] Training loss: 0.01365263
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12479.814101179552 ----------
[1/200] Training loss: 0.15162502
[2/200] Training loss: 0.05414659
[3/200] Training loss: 0.04764490
[4/200] Training loss: 0.04366612
[5/200] Training loss: 0.03981497
[6/200] Training loss: 0.03736903
[7/200] Training loss: 0.03574030
[8/200] Training loss: 0.03555702
[9/200] Training loss: 0.03039723
[10/200] Training loss: 0.03003080
[50/200] Training loss: 0.01792368
[100/200] Training loss: 0.01452807
[150/200] Training loss: 0.01337019
[200/200] Training loss: 0.01267582
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16383.780271964099 ----------
[1/200] Training loss: 0.16889416
[2/200] Training loss: 0.06259787
[3/200] Training loss: 0.05194170
[4/200] Training loss: 0.04694968
[5/200] Training loss: 0.04417544
[6/200] Training loss: 0.04645691
[7/200] Training loss: 0.04308432
[8/200] Training loss: 0.04040540
[9/200] Training loss: 0.03588935
[10/200] Training loss: 0.03461403
[50/200] Training loss: 0.01839796
[100/200] Training loss: 0.01596936
[150/200] Training loss: 0.01308010
[200/200] Training loss: 0.01196213
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7679.470033797906 ----------
[1/200] Training loss: 0.15765434
[2/200] Training loss: 0.05856705
[3/200] Training loss: 0.05444744
[4/200] Training loss: 0.04796760
[5/200] Training loss: 0.04409633
[6/200] Training loss: 0.04117541
[7/200] Training loss: 0.03937494
[8/200] Training loss: 0.03765990
[9/200] Training loss: 0.03683936
[10/200] Training loss: 0.03499783
[50/200] Training loss: 0.01658849
[100/200] Training loss: 0.01444260
[150/200] Training loss: 0.01373192
[200/200] Training loss: 0.01264865
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11939.641535657593 ----------
[1/200] Training loss: 0.14653791
[2/200] Training loss: 0.05685932
[3/200] Training loss: 0.05038618
[4/200] Training loss: 0.04797180
[5/200] Training loss: 0.04467611
[6/200] Training loss: 0.04237256
[7/200] Training loss: 0.03986657
[8/200] Training loss: 0.03741573
[9/200] Training loss: 0.03593761
[10/200] Training loss: 0.03244332
[50/200] Training loss: 0.01842197
[100/200] Training loss: 0.01565408
[150/200] Training loss: 0.01444399
[200/200] Training loss: 0.01362201
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15272.014143524095 ----------
[1/200] Training loss: 0.17498598
[2/200] Training loss: 0.05674090
[3/200] Training loss: 0.04772034
[4/200] Training loss: 0.04293898
[5/200] Training loss: 0.03965913
[6/200] Training loss: 0.03495374
[7/200] Training loss: 0.03646825
[8/200] Training loss: 0.03099810
[9/200] Training loss: 0.03017686
[10/200] Training loss: 0.02951316
[50/200] Training loss: 0.01713083
[100/200] Training loss: 0.01520036
[150/200] Training loss: 0.01275430
[200/200] Training loss: 0.01117109
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17234.804785665547 ----------
[1/200] Training loss: 0.16288078
[2/200] Training loss: 0.06058087
[3/200] Training loss: 0.05220179
[4/200] Training loss: 0.04872732
[5/200] Training loss: 0.04727780
[6/200] Training loss: 0.04336704
[7/200] Training loss: 0.03964843
[8/200] Training loss: 0.03807908
[9/200] Training loss: 0.03523355
[10/200] Training loss: 0.03378213
[50/200] Training loss: 0.01782596
[100/200] Training loss: 0.01458771
[150/200] Training loss: 0.01310145
[200/200] Training loss: 0.01298469
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10202.662005574819 ----------
[1/200] Training loss: 0.14452735
[2/200] Training loss: 0.05930541
[3/200] Training loss: 0.05008518
[4/200] Training loss: 0.04873136
[5/200] Training loss: 0.04332010
[6/200] Training loss: 0.03960201
[7/200] Training loss: 0.03802115
[8/200] Training loss: 0.03517383
[9/200] Training loss: 0.03196354
[10/200] Training loss: 0.03198696
[50/200] Training loss: 0.01787355
[100/200] Training loss: 0.01430420
[150/200] Training loss: 0.01308358
[200/200] Training loss: 0.01236491
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.028568654400047657
----FITNESS-----------RMSE---- 19286.117701600808 ----------
[1/200] Training loss: 0.14281922
[2/200] Training loss: 0.05743715
[3/200] Training loss: 0.04963217
[4/200] Training loss: 0.04986454
[5/200] Training loss: 0.04459245
[6/200] Training loss: 0.04175723
[7/200] Training loss: 0.04063792
[8/200] Training loss: 0.03703883
[9/200] Training loss: 0.03503039
[10/200] Training loss: 0.03519227
[50/200] Training loss: 0.01728266
[100/200] Training loss: 0.01437421
[150/200] Training loss: 0.01325231
[200/200] Training loss: 0.01248638
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6118264784515256 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17268.868636943185 ----------
[1/200] Training loss: 0.14384617
[2/200] Training loss: 0.06286217
[3/200] Training loss: 0.05220052
[4/200] Training loss: 0.04781421
[5/200] Training loss: 0.04156159
[6/200] Training loss: 0.04037826
[7/200] Training loss: 0.04033345
[8/200] Training loss: 0.03631870
[9/200] Training loss: 0.03495295
[10/200] Training loss: 0.03117980
[50/200] Training loss: 0.01818120
[100/200] Training loss: 0.01463624
[150/200] Training loss: 0.01305548
[200/200] Training loss: 0.01174387
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9916.677266100778 ----------
[1/200] Training loss: 0.16046836
[2/200] Training loss: 0.06343548
[3/200] Training loss: 0.05146744
[4/200] Training loss: 0.04702757
[5/200] Training loss: 0.04455099
[6/200] Training loss: 0.04111679
[7/200] Training loss: 0.03498964
[8/200] Training loss: 0.03272634
[9/200] Training loss: 0.03216581
[10/200] Training loss: 0.03137854
[50/200] Training loss: 0.01761113
[100/200] Training loss: 0.01428480
[150/200] Training loss: 0.01256849
[200/200] Training loss: 0.01248047
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15653.978919111907 ----------
[1/200] Training loss: 0.14845665
[2/200] Training loss: 0.05173417
[3/200] Training loss: 0.04679710
[4/200] Training loss: 0.04682331
[5/200] Training loss: 0.03964106
[6/200] Training loss: 0.03651475
[7/200] Training loss: 0.03452799
[8/200] Training loss: 0.03271259
[9/200] Training loss: 0.03344589
[10/200] Training loss: 0.03039965
[50/200] Training loss: 0.01788539
[100/200] Training loss: 0.01514002
[150/200] Training loss: 0.01322201
[200/200] Training loss: 0.01204128
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13064.105327193287 ----------
[1/200] Training loss: 0.16988873
[2/200] Training loss: 0.06082453
[3/200] Training loss: 0.05457814
[4/200] Training loss: 0.05228953
[5/200] Training loss: 0.04444494
[6/200] Training loss: 0.04535984
[7/200] Training loss: 0.04212625
[8/200] Training loss: 0.03750042
[9/200] Training loss: 0.03803521
[10/200] Training loss: 0.03588483
[50/200] Training loss: 0.01829868
[100/200] Training loss: 0.01457022
[150/200] Training loss: 0.01391933
[200/200] Training loss: 0.01290959
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.028568654400047657
----FITNESS-----------RMSE---- 11076.42722180758 ----------
[1/200] Training loss: 0.16716493
[2/200] Training loss: 0.05789770
[3/200] Training loss: 0.05559074
[4/200] Training loss: 0.04660425
[5/200] Training loss: 0.04733807
[6/200] Training loss: 0.04445465
[7/200] Training loss: 0.04256172
[8/200] Training loss: 0.03876156
[9/200] Training loss: 0.03706677
[10/200] Training loss: 0.03688470
[50/200] Training loss: 0.01836085
[100/200] Training loss: 0.01590818
[150/200] Training loss: 0.01363569
[200/200] Training loss: 0.01284382
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18545.033620891605 ----------
[1/200] Training loss: 0.17616290
[2/200] Training loss: 0.06706134
[3/200] Training loss: 0.05514509
[4/200] Training loss: 0.05564346
[5/200] Training loss: 0.05416112
[6/200] Training loss: 0.04818347
[7/200] Training loss: 0.05019278
[8/200] Training loss: 0.04889238
[9/200] Training loss: 0.04537173
[10/200] Training loss: 0.04324338
[50/200] Training loss: 0.01808233
[100/200] Training loss: 0.01456827
[150/200] Training loss: 0.01383098
[200/200] Training loss: 0.01202186
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14494.895653298096 ----------
[1/200] Training loss: 0.14151787
[2/200] Training loss: 0.05676701
[3/200] Training loss: 0.05157099
[4/200] Training loss: 0.04838990
[5/200] Training loss: 0.04616593
[6/200] Training loss: 0.04309313
[7/200] Training loss: 0.04093148
[8/200] Training loss: 0.03993335
[9/200] Training loss: 0.03460862
[10/200] Training loss: 0.03501321
[50/200] Training loss: 0.01800807
[100/200] Training loss: 0.01492477
[150/200] Training loss: 0.01319685
[200/200] Training loss: 0.01213647
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21063.873907712226 ----------
[1/200] Training loss: 0.17731946
[2/200] Training loss: 0.06369428
[3/200] Training loss: 0.05632651
[4/200] Training loss: 0.05161976
[5/200] Training loss: 0.04862236
[6/200] Training loss: 0.04338886
[7/200] Training loss: 0.04232394
[8/200] Training loss: 0.04057287
[9/200] Training loss: 0.03643457
[10/200] Training loss: 0.03793858
[50/200] Training loss: 0.01892864
[100/200] Training loss: 0.01619557
[150/200] Training loss: 0.01387156
[200/200] Training loss: 0.01329732
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7688.653198057512 ----------
[1/200] Training loss: 0.19491735
[2/200] Training loss: 0.06244007
[3/200] Training loss: 0.06091928
[4/200] Training loss: 0.05484802
[5/200] Training loss: 0.05346668
[6/200] Training loss: 0.05297673
[7/200] Training loss: 0.04956955
[8/200] Training loss: 0.04571947
[9/200] Training loss: 0.04756853
[10/200] Training loss: 0.04277146
[50/200] Training loss: 0.01908693
[100/200] Training loss: 0.01612958
[150/200] Training loss: 0.01410271
[200/200] Training loss: 0.01290817
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.028568654400047657
----FITNESS-----------RMSE---- 14514.466163107756 ----------
[1/200] Training loss: 0.15264622
[2/200] Training loss: 0.05711512
[3/200] Training loss: 0.05032951
[4/200] Training loss: 0.04323753
[5/200] Training loss: 0.03953883
[6/200] Training loss: 0.03718762
[7/200] Training loss: 0.03361290
[8/200] Training loss: 0.03275719
[9/200] Training loss: 0.02834826
[10/200] Training loss: 0.02897999
[50/200] Training loss: 0.01540451
[100/200] Training loss: 0.01249976
[150/200] Training loss: 0.01145503
[200/200] Training loss: 0.01001957
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.028568654400047657
----FITNESS-----------RMSE---- 22912.564238862484 ----------
[1/200] Training loss: 0.18537266
[2/200] Training loss: 0.06621123
[3/200] Training loss: 0.05511132
[4/200] Training loss: 0.05076071
[5/200] Training loss: 0.05026784
[6/200] Training loss: 0.04555140
[7/200] Training loss: 0.04298409
[8/200] Training loss: 0.04193908
[9/200] Training loss: 0.03962737
[10/200] Training loss: 0.03851898
[50/200] Training loss: 0.01693324
[100/200] Training loss: 0.01635045
[150/200] Training loss: 0.01410010
[200/200] Training loss: 0.01333759
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13516.06925108036 ----------
[1/200] Training loss: 0.17408161
[2/200] Training loss: 0.05547407
[3/200] Training loss: 0.05292465
[4/200] Training loss: 0.04997080
[5/200] Training loss: 0.04654582
[6/200] Training loss: 0.04696332
[7/200] Training loss: 0.04243628
[8/200] Training loss: 0.04051251
[9/200] Training loss: 0.03743077
[10/200] Training loss: 0.03318753
[50/200] Training loss: 0.01714575
[100/200] Training loss: 0.01499900
[150/200] Training loss: 0.01427311
[200/200] Training loss: 0.01326097
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16820.37336089779 ----------
[1/200] Training loss: 0.18976802
[2/200] Training loss: 0.06098971
[3/200] Training loss: 0.05790339
[4/200] Training loss: 0.05187301
[5/200] Training loss: 0.05134304
[6/200] Training loss: 0.04814294
[7/200] Training loss: 0.04624333
[8/200] Training loss: 0.04449896
[9/200] Training loss: 0.04404359
[10/200] Training loss: 0.04203236
[50/200] Training loss: 0.01842981
[100/200] Training loss: 0.01527252
[150/200] Training loss: 0.01391581
[200/200] Training loss: 0.01324213
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12331.554322144471 ----------
[1/50] Training loss: 0.18143019
[2/50] Training loss: 0.06491750
[3/50] Training loss: 0.05587744
[4/50] Training loss: 0.05004807
[5/50] Training loss: 0.04922152
[6/50] Training loss: 0.04518304
[7/50] Training loss: 0.04209859
[8/50] Training loss: 0.04305725
[9/50] Training loss: 0.03554452
[10/50] Training loss: 0.03288665
[50/50] Training loss: 0.01877294
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 50
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15392.40488032978 ----------
[1/200] Training loss: 0.18506524
[2/200] Training loss: 0.06264552
[3/200] Training loss: 0.05736276
[4/200] Training loss: 0.05659754
[5/200] Training loss: 0.05432519
[6/200] Training loss: 0.04922889
[7/200] Training loss: 0.04988769
[8/200] Training loss: 0.04705821
[9/200] Training loss: 0.04707603
[10/200] Training loss: 0.04498201
[50/200] Training loss: 0.01735158
[100/200] Training loss: 0.01488182
[150/200] Training loss: 0.01332745
[200/200] Training loss: 0.01305606
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 5449.631180180912 ----------
[1/200] Training loss: 0.16974188
[2/200] Training loss: 0.06068076
[3/200] Training loss: 0.05764153
[4/200] Training loss: 0.05289262
[5/200] Training loss: 0.05137706
[6/200] Training loss: 0.04933733
[7/200] Training loss: 0.04736725
[8/200] Training loss: 0.04563791
[9/200] Training loss: 0.04365757
[10/200] Training loss: 0.04156317
[50/200] Training loss: 0.01932148
[100/200] Training loss: 0.01547409
[150/200] Training loss: 0.01403374
[200/200] Training loss: 0.01343398
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8562.724799968757 ----------
[1/200] Training loss: 0.15322808
[2/200] Training loss: 0.05960140
[3/200] Training loss: 0.05080188
[4/200] Training loss: 0.04960661
[5/200] Training loss: 0.04698235
[6/200] Training loss: 0.04147770
[7/200] Training loss: 0.04368361
[8/200] Training loss: 0.04027027
[9/200] Training loss: 0.03808706
[10/200] Training loss: 0.03442815
[50/200] Training loss: 0.01910912
[100/200] Training loss: 0.01507557
[150/200] Training loss: 0.01389427
[200/200] Training loss: 0.01248898
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.028568654400047657
----FITNESS-----------RMSE---- 25790.868461531107 ----------
[1/150] Training loss: 0.17274126
[2/150] Training loss: 0.06070118
[3/150] Training loss: 0.05336535
[4/150] Training loss: 0.05048213
[5/150] Training loss: 0.04606569
[6/150] Training loss: 0.04423988
[7/150] Training loss: 0.04058091
[8/150] Training loss: 0.03712909
[9/150] Training loss: 0.03626302
[10/150] Training loss: 0.03460731
[50/150] Training loss: 0.01771748
[100/150] Training loss: 0.01457426
[150/150] Training loss: 0.01305475
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 150
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 6867.075068761081 ----------
[1/200] Training loss: 0.09117373
[2/200] Training loss: 0.05160018
[3/200] Training loss: 0.04765715
[4/200] Training loss: 0.03979949
[5/200] Training loss: 0.03637712
[6/200] Training loss: 0.03044874
[7/200] Training loss: 0.02908446
[8/200] Training loss: 0.02740835
[9/200] Training loss: 0.02594452
[10/200] Training loss: 0.02491808
[50/200] Training loss: 0.01466691
[100/200] Training loss: 0.01262796
[150/200] Training loss: 0.01117874
[200/200] Training loss: 0.01014496
---batch_size---: 4 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 7677.661623176682 ----------
[1/200] Training loss: 0.16152146
[2/200] Training loss: 0.05457931
[3/200] Training loss: 0.04887331
[4/200] Training loss: 0.04364114
[5/200] Training loss: 0.03737025
[6/200] Training loss: 0.03743407
[7/200] Training loss: 0.03197678
[8/200] Training loss: 0.03227154
[9/200] Training loss: 0.03147148
[10/200] Training loss: 0.03097169
[50/200] Training loss: 0.01753285
[100/200] Training loss: 0.01511521
[150/200] Training loss: 0.01347964
[200/200] Training loss: 0.01299481
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10049.635217260377 ----------
[1/200] Training loss: 0.15274706
[2/200] Training loss: 0.05694520
[3/200] Training loss: 0.05140658
[4/200] Training loss: 0.04667237
[5/200] Training loss: 0.04567914
[6/200] Training loss: 0.04044874
[7/200] Training loss: 0.04019760
[8/200] Training loss: 0.03617920
[9/200] Training loss: 0.03523076
[10/200] Training loss: 0.03400364
[50/200] Training loss: 0.01838956
[100/200] Training loss: 0.01487663
[150/200] Training loss: 0.01352071
[200/200] Training loss: 0.01190240
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 48 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16951.758374870733 ----------
[1/200] Training loss: 0.26509110
[2/200] Training loss: 0.07443897
[3/200] Training loss: 0.05923101
[4/200] Training loss: 0.05584308
[5/200] Training loss: 0.05052084
[6/200] Training loss: 0.05317451
[7/200] Training loss: 0.04983730
[8/200] Training loss: 0.05042340
[9/200] Training loss: 0.04350949
[10/200] Training loss: 0.04719512
[50/200] Training loss: 0.02282620
[100/200] Training loss: 0.01653550
[150/200] Training loss: 0.01421884
[200/200] Training loss: 0.01481786
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.028568654400047657
----FITNESS-----------RMSE---- 13424.72346083896 ----------
[1/150] Training loss: 0.13170546
[2/150] Training loss: 0.05121777
[3/150] Training loss: 0.04553355
[4/150] Training loss: 0.04291476
[5/150] Training loss: 0.04034428
[6/150] Training loss: 0.03653894
[7/150] Training loss: 0.03897982
[8/150] Training loss: 0.03340983
[9/150] Training loss: 0.03303622
[10/150] Training loss: 0.03223748
[50/150] Training loss: 0.01699577
[100/150] Training loss: 0.01350028
[150/150] Training loss: 0.01165071
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 150
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9761.967014900225 ----------
[1/200] Training loss: 0.15759010
[2/200] Training loss: 0.05661181
[3/200] Training loss: 0.04676893
[4/200] Training loss: 0.04239006
[5/200] Training loss: 0.04393060
[6/200] Training loss: 0.03689677
[7/200] Training loss: 0.03362122
[8/200] Training loss: 0.03430520
[9/200] Training loss: 0.03186540
[10/200] Training loss: 0.03035201
[50/200] Training loss: 0.01888822
[100/200] Training loss: 0.01677399
[150/200] Training loss: 0.01462678
[200/200] Training loss: 0.01333688
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15580.648510251427 ----------
[1/200] Training loss: 0.15666597
[2/200] Training loss: 0.05614397
[3/200] Training loss: 0.05062819
[4/200] Training loss: 0.04398546
[5/200] Training loss: 0.04267585
[6/200] Training loss: 0.03861569
[7/200] Training loss: 0.04079042
[8/200] Training loss: 0.03311506
[9/200] Training loss: 0.03315444
[10/200] Training loss: 0.02928437
[50/200] Training loss: 0.01859492
[100/200] Training loss: 0.01694317
[150/200] Training loss: 0.01436325
[200/200] Training loss: 0.01347326
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 4983.642643689453 ----------
[1/200] Training loss: 0.17003235
[2/200] Training loss: 0.05926587
[3/200] Training loss: 0.05065631
[4/200] Training loss: 0.05066699
[5/200] Training loss: 0.04658372
[6/200] Training loss: 0.04291754
[7/200] Training loss: 0.04177058
[8/200] Training loss: 0.04158673
[9/200] Training loss: 0.03577437
[10/200] Training loss: 0.03425045
[50/200] Training loss: 0.01696798
[100/200] Training loss: 0.01307715
[150/200] Training loss: 0.01232629
[200/200] Training loss: 0.01102227
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14196.194419632326 ----------
[1/200] Training loss: 0.16386879
[2/200] Training loss: 0.06118634
[3/200] Training loss: 0.05204168
[4/200] Training loss: 0.05200444
[5/200] Training loss: 0.04771062
[6/200] Training loss: 0.04598212
[7/200] Training loss: 0.04424380
[8/200] Training loss: 0.04273888
[9/200] Training loss: 0.03860080
[10/200] Training loss: 0.03579251
[50/200] Training loss: 0.01744919
[100/200] Training loss: 0.01519121
[150/200] Training loss: 0.01434291
[200/200] Training loss: 0.01302563
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12729.575641002335 ----------
[1/200] Training loss: 0.18801395
[2/200] Training loss: 0.06369378
[3/200] Training loss: 0.05338157
[4/200] Training loss: 0.04528115
[5/200] Training loss: 0.04269255
[6/200] Training loss: 0.03748507
[7/200] Training loss: 0.03540334
[8/200] Training loss: 0.03472497
[9/200] Training loss: 0.03321882
[10/200] Training loss: 0.03095150
[50/200] Training loss: 0.01872328
[100/200] Training loss: 0.01594924
[150/200] Training loss: 0.01528600
[200/200] Training loss: 0.01361342
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11188.427592830012 ----------
[1/200] Training loss: 0.18971703
[2/200] Training loss: 0.07333274
[3/200] Training loss: 0.05678389
[4/200] Training loss: 0.04988500
[5/200] Training loss: 0.04669501
[6/200] Training loss: 0.04453479
[7/200] Training loss: 0.03910042
[8/200] Training loss: 0.03868256
[9/200] Training loss: 0.03536383
[10/200] Training loss: 0.03342327
[50/200] Training loss: 0.01808119
[100/200] Training loss: 0.01589662
[150/200] Training loss: 0.01438174
[200/200] Training loss: 0.01351303
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18240.513150676437 ----------
[1/200] Training loss: 0.16301733
[2/200] Training loss: 0.05848985
[3/200] Training loss: 0.05515698
[4/200] Training loss: 0.05007596
[5/200] Training loss: 0.04873550
[6/200] Training loss: 0.04738730
[7/200] Training loss: 0.04726304
[8/200] Training loss: 0.04305102
[9/200] Training loss: 0.04269924
[10/200] Training loss: 0.03895682
[50/200] Training loss: 0.01877764
[100/200] Training loss: 0.01639983
[150/200] Training loss: 0.01474034
[200/200] Training loss: 0.01324579
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15436.76546430631 ----------
[1/200] Training loss: 0.15004180
[2/200] Training loss: 0.06157448
[3/200] Training loss: 0.05097436
[4/200] Training loss: 0.04668518
[5/200] Training loss: 0.04537791
[6/200] Training loss: 0.04036896
[7/200] Training loss: 0.03866821
[8/200] Training loss: 0.03663962
[9/200] Training loss: 0.03344098
[10/200] Training loss: 0.03603789
[50/200] Training loss: 0.01744539
[100/200] Training loss: 0.01466964
[150/200] Training loss: 0.01390176
[200/200] Training loss: 0.01258937
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 17985.81396545622 ----------
[1/200] Training loss: 0.13862836
[2/200] Training loss: 0.05696272
[3/200] Training loss: 0.04728784
[4/200] Training loss: 0.04529938
[5/200] Training loss: 0.04342981
[6/200] Training loss: 0.04083303
[7/200] Training loss: 0.03454246
[8/200] Training loss: 0.03695389
[9/200] Training loss: 0.03157687
[10/200] Training loss: 0.03048425
[50/200] Training loss: 0.01763327
[100/200] Training loss: 0.01488120
[150/200] Training loss: 0.01358815
[200/200] Training loss: 0.01233663
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8685.109556015974 ----------
[1/150] Training loss: 0.15010543
[2/150] Training loss: 0.05993544
[3/150] Training loss: 0.04814339
[4/150] Training loss: 0.04857687
[5/150] Training loss: 0.04413107
[6/150] Training loss: 0.04418527
[7/150] Training loss: 0.03999811
[8/150] Training loss: 0.03875243
[9/150] Training loss: 0.03660679
[10/150] Training loss: 0.03291503
[50/150] Training loss: 0.01711096
[100/150] Training loss: 0.01348373
[150/150] Training loss: 0.01214963
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 150
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16402.88755067229 ----------
[1/200] Training loss: 0.15148270
[2/200] Training loss: 0.05409905
[3/200] Training loss: 0.04851685
[4/200] Training loss: 0.04255245
[5/200] Training loss: 0.04275990
[6/200] Training loss: 0.03865364
[7/200] Training loss: 0.03859869
[8/200] Training loss: 0.03519006
[9/200] Training loss: 0.03403467
[10/200] Training loss: 0.03317846
[50/200] Training loss: 0.01807168
[100/200] Training loss: 0.01593592
[150/200] Training loss: 0.01456238
[200/200] Training loss: 0.01295343
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13368.101735100612 ----------
[1/200] Training loss: 0.15826868
[2/200] Training loss: 0.05751330
[3/200] Training loss: 0.05061698
[4/200] Training loss: 0.04635975
[5/200] Training loss: 0.04346463
[6/200] Training loss: 0.03853801
[7/200] Training loss: 0.03740203
[8/200] Training loss: 0.03508468
[9/200] Training loss: 0.03464266
[10/200] Training loss: 0.03355614
[50/200] Training loss: 0.01917259
[100/200] Training loss: 0.01622892
[150/200] Training loss: 0.01457101
[200/200] Training loss: 0.01347519
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9381.579397947875 ----------
[1/200] Training loss: 0.08373143
[2/200] Training loss: 0.04816092
[3/200] Training loss: 0.04140689
[4/200] Training loss: 0.03473399
[5/200] Training loss: 0.03034320
[6/200] Training loss: 0.02771638
[7/200] Training loss: 0.02590510
[8/200] Training loss: 0.02442929
[9/200] Training loss: 0.02276279
[10/200] Training loss: 0.02202416
[50/200] Training loss: 0.01458189
[100/200] Training loss: 0.01200382
[150/200] Training loss: 0.01085227
[200/200] Training loss: 0.01020738
---batch_size---: 4 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15254.773416868571 ----------
[1/150] Training loss: 0.15123404
[2/150] Training loss: 0.05547745
[3/150] Training loss: 0.05222712
[4/150] Training loss: 0.04784312
[5/150] Training loss: 0.04618927
[6/150] Training loss: 0.04018128
[7/150] Training loss: 0.03718533
[8/150] Training loss: 0.03674568
[9/150] Training loss: 0.03250701
[10/150] Training loss: 0.03001103
[50/150] Training loss: 0.01833931
[100/150] Training loss: 0.01523994
[150/150] Training loss: 0.01444393
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 150
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 34707.67338788355 ----------
[1/200] Training loss: 0.16451234
[2/200] Training loss: 0.05835466
[3/200] Training loss: 0.05283350
[4/200] Training loss: 0.04811869
[5/200] Training loss: 0.04731939
[6/200] Training loss: 0.04299764
[7/200] Training loss: 0.03810712
[8/200] Training loss: 0.03624606
[9/200] Training loss: 0.03527296
[10/200] Training loss: 0.03200205
[50/200] Training loss: 0.01737215
[100/200] Training loss: 0.01461434
[150/200] Training loss: 0.01282827
[200/200] Training loss: 0.01154989
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 22046.40487698618 ----------
[1/200] Training loss: 0.15900479
[2/200] Training loss: 0.05664986
[3/200] Training loss: 0.05426318
[4/200] Training loss: 0.04801107
[5/200] Training loss: 0.04334473
[6/200] Training loss: 0.03928481
[7/200] Training loss: 0.04104012
[8/200] Training loss: 0.03700800
[9/200] Training loss: 0.03390438
[10/200] Training loss: 0.03072918
[50/200] Training loss: 0.01698316
[100/200] Training loss: 0.01455355
[150/200] Training loss: 0.01332130
[200/200] Training loss: 0.01188428
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15715.517427052791 ----------
[1/200] Training loss: 0.17817326
[2/200] Training loss: 0.06307201
[3/200] Training loss: 0.05687458
[4/200] Training loss: 0.05296823
[5/200] Training loss: 0.04992413
[6/200] Training loss: 0.04902442
[7/200] Training loss: 0.04971133
[8/200] Training loss: 0.04409474
[9/200] Training loss: 0.04204382
[10/200] Training loss: 0.04072666
[50/200] Training loss: 0.02110693
[100/200] Training loss: 0.01497284
[150/200] Training loss: 0.01377996
[200/200] Training loss: 0.01289710
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 18595.172706915095 ----------
[1/200] Training loss: 0.19645776
[2/200] Training loss: 0.06117946
[3/200] Training loss: 0.05575025
[4/200] Training loss: 0.05441755
[5/200] Training loss: 0.05089282
[6/200] Training loss: 0.04664764
[7/200] Training loss: 0.04581930
[8/200] Training loss: 0.04322817
[9/200] Training loss: 0.04092468
[10/200] Training loss: 0.04156664
[50/200] Training loss: 0.01848288
[100/200] Training loss: 0.01503528
[150/200] Training loss: 0.01455255
[200/200] Training loss: 0.01390296
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 15021.088642305524 ----------
[1/200] Training loss: 0.14336402
[2/200] Training loss: 0.05785121
[3/200] Training loss: 0.05738727
[4/200] Training loss: 0.04953736
[5/200] Training loss: 0.04751642
[6/200] Training loss: 0.04326674
[7/200] Training loss: 0.04119473
[8/200] Training loss: 0.03872061
[9/200] Training loss: 0.03732518
[10/200] Training loss: 0.03669857
[50/200] Training loss: 0.01697863
[100/200] Training loss: 0.01455097
[150/200] Training loss: 0.01309040
[200/200] Training loss: 0.01249084
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 8902.412257360362 ----------
[1/200] Training loss: 0.14630816
[2/200] Training loss: 0.05736222
[3/200] Training loss: 0.04819227
[4/200] Training loss: 0.04545696
[5/200] Training loss: 0.04124754
[6/200] Training loss: 0.04031217
[7/200] Training loss: 0.03668228
[8/200] Training loss: 0.03560995
[9/200] Training loss: 0.03410923
[10/200] Training loss: 0.03014407
[50/200] Training loss: 0.01806524
[100/200] Training loss: 0.01531191
[150/200] Training loss: 0.01434987
[200/200] Training loss: 0.01317812
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 13820.174239133166 ----------
[1/200] Training loss: 0.17628763
[2/200] Training loss: 0.06235533
[3/200] Training loss: 0.05416183
[4/200] Training loss: 0.05204772
[5/200] Training loss: 0.05261714
[6/200] Training loss: 0.04691047
[7/200] Training loss: 0.04483814
[8/200] Training loss: 0.04398631
[9/200] Training loss: 0.03917371
[10/200] Training loss: 0.03817763
[50/200] Training loss: 0.01961632
[100/200] Training loss: 0.01534037
[150/200] Training loss: 0.01338845
[200/200] Training loss: 0.01270783
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11111.545707056242 ----------
[1/200] Training loss: 0.14962664
[2/200] Training loss: 0.05966101
[3/200] Training loss: 0.05272254
[4/200] Training loss: 0.04897918
[5/200] Training loss: 0.04355250
[6/200] Training loss: 0.03993323
[7/200] Training loss: 0.03807101
[8/200] Training loss: 0.03530577
[9/200] Training loss: 0.03570021
[10/200] Training loss: 0.03360943
[50/200] Training loss: 0.01580361
[100/200] Training loss: 0.01420845
[150/200] Training loss: 0.01253387
[200/200] Training loss: 0.01182087
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9825.908609385699 ----------
[1/200] Training loss: 0.16093075
[2/200] Training loss: 0.06143039
[3/200] Training loss: 0.05303568
[4/200] Training loss: 0.04831776
[5/200] Training loss: 0.04343937
[6/200] Training loss: 0.04057681
[7/200] Training loss: 0.03898095
[8/200] Training loss: 0.03608594
[9/200] Training loss: 0.03469522
[10/200] Training loss: 0.03400174
[50/200] Training loss: 0.01640817
[100/200] Training loss: 0.01347205
[150/200] Training loss: 0.01179501
[200/200] Training loss: 0.01101680
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14949.587285273128 ----------
[1/200] Training loss: 0.16033646
[2/200] Training loss: 0.06009176
[3/200] Training loss: 0.05158095
[4/200] Training loss: 0.04851407
[5/200] Training loss: 0.04748802
[6/200] Training loss: 0.03878854
[7/200] Training loss: 0.03636636
[8/200] Training loss: 0.03512328
[9/200] Training loss: 0.03193850
[10/200] Training loss: 0.03404752
[50/200] Training loss: 0.01857834
[100/200] Training loss: 0.01550154
[150/200] Training loss: 0.01370545
[200/200] Training loss: 0.01271211
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 31106.093550942715 ----------
[1/200] Training loss: 0.16443552
[2/200] Training loss: 0.06198249
[3/200] Training loss: 0.05310732
[4/200] Training loss: 0.05233422
[5/200] Training loss: 0.04943582
[6/200] Training loss: 0.04350183
[7/200] Training loss: 0.04016451
[8/200] Training loss: 0.03827778
[9/200] Training loss: 0.03397996
[10/200] Training loss: 0.03314260
[50/200] Training loss: 0.01899295
[100/200] Training loss: 0.01679761
[150/200] Training loss: 0.01483592
[200/200] Training loss: 0.01422522
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10992.949012890036 ----------
[1/200] Training loss: 0.14691960
[2/200] Training loss: 0.06040297
[3/200] Training loss: 0.05259460
[4/200] Training loss: 0.04643916
[5/200] Training loss: 0.04620856
[6/200] Training loss: 0.04250383
[7/200] Training loss: 0.03982777
[8/200] Training loss: 0.03797587
[9/200] Training loss: 0.03414233
[10/200] Training loss: 0.03168965
[50/200] Training loss: 0.01710605
[100/200] Training loss: 0.01404631
[150/200] Training loss: 0.01292148
[200/200] Training loss: 0.01227071
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 23216.9482921421 ----------
[1/200] Training loss: 0.19940041
[2/200] Training loss: 0.06331534
[3/200] Training loss: 0.05988235
[4/200] Training loss: 0.05462248
[5/200] Training loss: 0.05311254
[6/200] Training loss: 0.05055454
[7/200] Training loss: 0.04846728
[8/200] Training loss: 0.04464524
[9/200] Training loss: 0.04198547
[10/200] Training loss: 0.03986475
[50/200] Training loss: 0.02023801
[100/200] Training loss: 0.01664750
[150/200] Training loss: 0.01490814
[200/200] Training loss: 0.01424634
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11398.43989324855 ----------
[1/200] Training loss: 0.14967914
[2/200] Training loss: 0.05779641
[3/200] Training loss: 0.05288295
[4/200] Training loss: 0.04614636
[5/200] Training loss: 0.04409594
[6/200] Training loss: 0.04234125
[7/200] Training loss: 0.03886851
[8/200] Training loss: 0.03665032
[9/200] Training loss: 0.03526875
[10/200] Training loss: 0.03189197
[50/200] Training loss: 0.01724113
[100/200] Training loss: 0.01543846
[150/200] Training loss: 0.01344896
[200/200] Training loss: 0.01343172
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16246.23870315834 ----------
[1/200] Training loss: 0.17572811
[2/200] Training loss: 0.06323549
[3/200] Training loss: 0.05552718
[4/200] Training loss: 0.05378998
[5/200] Training loss: 0.05058167
[6/200] Training loss: 0.04782807
[7/200] Training loss: 0.04360288
[8/200] Training loss: 0.04259831
[9/200] Training loss: 0.03363127
[10/200] Training loss: 0.03587135
[50/200] Training loss: 0.01607830
[100/200] Training loss: 0.01400992
[150/200] Training loss: 0.01194469
[200/200] Training loss: 0.01072334
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21962.973933418034 ----------
[1/200] Training loss: 0.17429733
[2/200] Training loss: 0.05999714
[3/200] Training loss: 0.05512277
[4/200] Training loss: 0.05181329
[5/200] Training loss: 0.04863891
[6/200] Training loss: 0.04531210
[7/200] Training loss: 0.04300017
[8/200] Training loss: 0.04030422
[9/200] Training loss: 0.04088273
[10/200] Training loss: 0.03762145
[50/200] Training loss: 0.01689560
[100/200] Training loss: 0.01441351
[150/200] Training loss: 0.01397806
[200/200] Training loss: 0.01254557
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14615.220559403133 ----------
[1/200] Training loss: 0.16802816
[2/200] Training loss: 0.05905256
[3/200] Training loss: 0.05310582
[4/200] Training loss: 0.05069582
[5/200] Training loss: 0.04604655
[6/200] Training loss: 0.04122908
[7/200] Training loss: 0.04414691
[8/200] Training loss: 0.04008404
[9/200] Training loss: 0.03646100
[10/200] Training loss: 0.03288816
[50/200] Training loss: 0.01930734
[100/200] Training loss: 0.01512103
[150/200] Training loss: 0.01416739
[200/200] Training loss: 0.01258236
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 21185.502967831562 ----------
[1/200] Training loss: 0.17765195
[2/200] Training loss: 0.06100613
[3/200] Training loss: 0.05104592
[4/200] Training loss: 0.04815822
[5/200] Training loss: 0.04464115
[6/200] Training loss: 0.04321280
[7/200] Training loss: 0.04295270
[8/200] Training loss: 0.04030789
[9/200] Training loss: 0.03845111
[10/200] Training loss: 0.03738509
[50/200] Training loss: 0.01778102
[100/200] Training loss: 0.01489726
[150/200] Training loss: 0.01344161
[200/200] Training loss: 0.01293514
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 14637.844650084247 ----------
[1/200] Training loss: 0.13477890
[2/200] Training loss: 0.05576617
[3/200] Training loss: 0.04746792
[4/200] Training loss: 0.04333121
[5/200] Training loss: 0.04001827
[6/200] Training loss: 0.04050720
[7/200] Training loss: 0.03794250
[8/200] Training loss: 0.03587805
[9/200] Training loss: 0.03517351
[10/200] Training loss: 0.03393964
[50/200] Training loss: 0.01991764
[100/200] Training loss: 0.01549827
[150/200] Training loss: 0.01409919
[200/200] Training loss: 0.01315425
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9450.461999288713 ----------
[1/200] Training loss: 0.15966320
[2/200] Training loss: 0.05935623
[3/200] Training loss: 0.05352664
[4/200] Training loss: 0.05086850
[5/200] Training loss: 0.04479574
[6/200] Training loss: 0.04288503
[7/200] Training loss: 0.04194527
[8/200] Training loss: 0.04079930
[9/200] Training loss: 0.03650244
[10/200] Training loss: 0.03487370
[50/200] Training loss: 0.01865293
[100/200] Training loss: 0.01477452
[150/200] Training loss: 0.01295492
[200/200] Training loss: 0.01224051
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 12548.935253638056 ----------
[1/200] Training loss: 0.14321507
[2/200] Training loss: 0.05274437
[3/200] Training loss: 0.05068173
[4/200] Training loss: 0.04685111
[5/200] Training loss: 0.04336022
[6/200] Training loss: 0.04180815
[7/200] Training loss: 0.03988861
[8/200] Training loss: 0.03640537
[9/200] Training loss: 0.03633459
[10/200] Training loss: 0.03508394
[50/200] Training loss: 0.01810228
[100/200] Training loss: 0.01528993
[150/200] Training loss: 0.01464041
[200/200] Training loss: 0.01271230
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 11015.409207106199 ----------
[1/200] Training loss: 0.22915785
[2/200] Training loss: 0.06189837
[3/200] Training loss: 0.06087253
[4/200] Training loss: 0.05705026
[5/200] Training loss: 0.05802546
[6/200] Training loss: 0.05190098
[7/200] Training loss: 0.04782718
[8/200] Training loss: 0.05130833
[9/200] Training loss: 0.04905138
[10/200] Training loss: 0.04524105
[50/200] Training loss: 0.02322443
[100/200] Training loss: 0.01652585
[150/200] Training loss: 0.01559114
[200/200] Training loss: 0.01470704
---batch_size---: 32 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 9021.084635452657 ----------
[1/200] Training loss: 0.16419042
[2/200] Training loss: 0.05953125
[3/200] Training loss: 0.05088588
[4/200] Training loss: 0.04841348
[5/200] Training loss: 0.04186114
[6/200] Training loss: 0.04147083
[7/200] Training loss: 0.03545062
[8/200] Training loss: 0.03361822
[9/200] Training loss: 0.03056608
[10/200] Training loss: 0.02691989
[50/200] Training loss: 0.01648703
[100/200] Training loss: 0.01418120
[150/200] Training loss: 0.01257708
[200/200] Training loss: 0.01043513
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 16845.674103460507 ----------
[1/200] Training loss: 0.13501423
[2/200] Training loss: 0.05632328
[3/200] Training loss: 0.04879297
[4/200] Training loss: 0.04444861
[5/200] Training loss: 0.04190667
[6/200] Training loss: 0.03669293
[7/200] Training loss: 0.03620684
[8/200] Training loss: 0.03192854
[9/200] Training loss: 0.02897407
[10/200] Training loss: 0.02833560
[50/200] Training loss: 0.01629543
[100/200] Training loss: 0.01475492
[150/200] Training loss: 0.01270008
[200/200] Training loss: 0.01208273
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10831.704574996495 ----------
[1/200] Training loss: 0.17767826
[2/200] Training loss: 0.06549570
[3/200] Training loss: 0.05889762
[4/200] Training loss: 0.05393730
[5/200] Training loss: 0.05247075
[6/200] Training loss: 0.04521059
[7/200] Training loss: 0.04324855
[8/200] Training loss: 0.04115658
[9/200] Training loss: 0.04065628
[10/200] Training loss: 0.03851195
[50/200] Training loss: 0.01851368
[100/200] Training loss: 0.01591847
[150/200] Training loss: 0.01494968
[200/200] Training loss: 0.01350571
---batch_size---: 16 ---n_steps---: 4 ---network_epoch---: 200
---hidden_dim---: 80 ---dropout---: 0.6625704538293802 ---num_layers---: 1
---loss---: L1Loss ---opt---: Adagrad ---learning_rate---: 0.027420900348942248
----FITNESS-----------RMSE---- 10411.073335636436 ----------
-------------------------------------
rmspe_score : 2823.2062446799737
-------------------------------------
high                               0
low                                0
open                               0
volumefrom                         0
volumeto                           0
top_tier_volume_quote              0
top_tier_volume_base               0
top_tier_volume_total              0
cccagg_volume_quote                0
cccagg_volume_base                 0
cccagg_volume_total                0
total_volume_quote                 0
total_volume_base                  0
total_volume_total                 0
zero_balance_addresses_all_time    0
unique_addresses_all_time          0
new_addresses                      0
active_addresses                   0
transaction_count                  0
transaction_count_all_time         0
large_transaction_count            0
average_transaction_value          0
block_height                       0
hashrate                           0
difficulty                         0
block_time                         0
block_size                         0
current_supply                     0
comments                           0
posts                              0
followers                          0
points                             0
dtype: int64
high                               0
low                                0
open                               0
volumefrom                         0
volumeto                           0
top_tier_volume_quote              0
top_tier_volume_base               0
top_tier_volume_total              0
cccagg_volume_quote                0
cccagg_volume_base                 0
cccagg_volume_total                0
total_volume_quote                 0
total_volume_base                  0
total_volume_total                 0
zero_balance_addresses_all_time    0
unique_addresses_all_time          0
new_addresses                      0
active_addresses                   0
transaction_count                  0
transaction_count_all_time         0
large_transaction_count            0
average_transaction_value          0
block_height                       0
hashrate                           0
difficulty                         0
block_time                         0
block_size                         0
current_supply                     0
comments                           0
posts                              0
followers                          0
points                             0
dtype: int64
close    0
dtype: int64
high                               0
low                                0
open                               0
volumefrom                         0
volumeto                           0
top_tier_volume_quote              0
top_tier_volume_base               0
top_tier_volume_total              0
cccagg_volume_quote                0
cccagg_volume_base                 0
cccagg_volume_total                0
total_volume_quote                 0
total_volume_base                  0
total_volume_total                 0
zero_balance_addresses_all_time    0
unique_addresses_all_time          0
new_addresses                      0
active_addresses                   0
transaction_count                  0
transaction_count_all_time         0
large_transaction_count            0
average_transaction_value          0
block_height                       0
hashrate                           0
difficulty                         0
block_time                         0
block_size                         0
current_supply                     0
comments                           0
posts                              0
followers                          0
points                             0
dtype: int64
close    0
dtype: int64
[1/200] Training loss: 1.02570267
[2/200] Training loss: 0.98607106
[3/200] Training loss: 0.98977739
[4/200] Training loss: 0.98011899
[5/200] Training loss: 0.97972084
[6/200] Training loss: 0.97943035
[7/200] Training loss: 0.98224284
[8/200] Training loss: 0.97769696
[9/200] Training loss: 0.98098358
[10/200] Training loss: 0.97700804
[50/200] Training loss: 0.96732560
[100/200] Training loss: 0.95550922
[150/200] Training loss: 0.96033649
[200/200] Training loss: 0.97026873
[1/200] Training loss: 0.07504877
[2/200] Training loss: 0.04903274
[3/200] Training loss: 0.04858599
[4/200] Training loss: 0.04944373
[5/200] Training loss: 0.05037969
[6/200] Training loss: 0.04916338
[7/200] Training loss: 0.04992854
[8/200] Training loss: 0.04677749
[9/200] Training loss: 0.04997046
[10/200] Training loss: 0.04892934
[50/200] Training loss: 0.04117059
[100/200] Training loss: 0.03521901
[150/200] Training loss: 0.03161050
[200/200] Training loss: 0.02754388
[1/200] Training loss: 0.07504877
[2/200] Training loss: 0.04903274
[3/200] Training loss: 0.04858599
[4/200] Training loss: 0.04944373
[5/200] Training loss: 0.05037969
[6/200] Training loss: 0.04916338
[7/200] Training loss: 0.04992854
[8/200] Training loss: 0.04677749
[9/200] Training loss: 0.04997046
[10/200] Training loss: 0.04892934
[50/200] Training loss: 0.04117059
[100/200] Training loss: 0.03521901
[150/200] Training loss: 0.03161050
[200/200] Training loss: 0.02754388
before --- [-0.7568867  -0.76557326 -0.77838093 -0.765971   -0.7879645  -0.7765108
 -0.7703714  -0.7664618  -0.74619037 -0.7268921  -0.72606283 -0.71718585
 -0.7236934  -0.72646475 -0.7077208  -0.62149423 -0.59421605 -0.5688164
 -0.5233739  -0.5170653  -0.52067024 -0.49402246 -0.5042787  -0.575087
 -0.591305   -0.5575404  -0.51426846 -0.5409459  -0.5157875  -0.51358306
 -0.5031829  -0.45691952 -0.45264605 -0.5653046  -0.5132319  -0.5134265
 -0.44642627 -0.4777283  -0.45667833 -0.5102912  -0.47251976 -0.45206216
 -0.45196906 -0.47954345 -0.45399156 -0.395039   -0.33016703 -0.35969198
 -0.3072088  -0.3251235  -0.3524144  -0.29392725 -0.2858119  -0.2981034
 -0.27919438 -0.25501338 -0.2665898  -0.27273342 -0.282702   -0.28125072
 -0.25234777 -0.2149148  -0.20735797 -0.22527687 -0.21680611 -0.28326476
 -0.36783692 -0.347959   -0.40858713 -0.42372617 -0.41774756 -0.43367782
 -0.3706718  -0.40571415 -0.37187344 -0.38468537 -0.3883326  -0.33682686
 -0.3051567  -0.25939685 -0.24347927 -0.27583486 -0.26253635 -0.28762284
 -0.2227128  -0.25338438 -0.27617335 -0.27159947 -0.26424152 -0.2840729
 -0.26985624 -0.27166295 -0.28078106 -0.32389224 -0.32954505 -0.36570022
 -0.36411774 -0.31634387 -0.31039912 -0.32190782 -0.26701292 -0.2564689
 -0.22360134 -0.2030422  -0.13247089 -0.18547447 -0.15694384 -0.14378074
 -0.14185134 -0.20451887 -0.15504406 -0.1612046  -0.13274167 -0.1256672
 -0.13106191 -0.06299544 -0.00663655  0.02942128 -0.00913293 -0.05480817
 -0.08740916 -0.12024287 -0.04869839 -0.03806129 -0.01954579 -0.03295006
 -0.09773316 -0.05306071  0.03696119  0.09324814  0.12835822  0.13155273
  0.13895723  0.21166527  0.21343389  0.4162878   0.3357606   0.45752034
  0.4418143   0.4385225   0.62143075  0.62600464  0.63561356  0.73218954
  0.57702065  0.53796715  0.6907708   0.5062547   0.4817733   0.35253286
  0.39354116 -0.0019696   0.13763711 -0.0049441  -0.06368512 -0.14765647
  0.08514126  0.10998657  0.18676074  0.12479135 -0.01488729 -0.07149158
 -0.02565556  0.10994002  0.07898918  0.10957614  0.17282757  0.10308133
  0.07709786  0.11146747  0.06159493  0.02609983  0.0690079   0.0101611
 -0.03911908 -0.03297122  0.0262987   0.05694067  0.04064229 -0.0337836
 -0.03183304 -0.09053598 -0.11886775 -0.08629214 -0.23679829 -0.23978548
 -0.20266563 -0.19401293 -0.26964045 -0.2608947  -0.19641621 -0.15390588
 -0.11929086 -0.07252399 -0.14312068 -0.12350509 -0.09323969 -0.05258259
 -0.10613199 -0.05285338 -0.05518474 -0.14091203 -0.12772776 -0.14262141
 -0.12993643 -0.17545088 -0.21449168 -0.19180849 -0.22398637 -0.24138485
 -0.2316913  -0.23507622 -0.2661159  -0.27968943 -0.19148692 -0.17889504
 -0.13623238 -0.11067201 -0.10713477 -0.09260924 -0.0615907  -0.06195458
 -0.0272084   0.00679734  0.03562838  0.04606239  0.06807705  0.02566826
  0.11769995  0.16149655  0.18820779  0.3024784   0.2395697   0.3035404
  0.29369453  0.30289727  0.25376096  0.37095106  0.3466008   0.3653913
  0.2962163   0.23881231  0.23947237  0.31191808  0.35497424  0.32962543
  0.33599755  0.37029523  0.30676877  0.3305859   0.27336386  0.35047653
  0.3381681   0.32907963  0.33041665  0.41787872  0.58459014  0.5670394
  0.63139087  0.6090589   0.63669676  0.6265716   0.41716364  0.4451781
  0.41325408  0.3223563   0.34648234  0.40524873  0.35450882  0.41825107
  0.49406052  0.47461417  0.40249848  0.41812834  0.37289315  0.21958175
  0.13228895  0.26712716  0.29917386  0.20449771  0.20216212  0.26092008
  0.20317335  0.15227689  0.17065276  0.23410305  0.365184    0.3985466
  0.4115193   0.3964395   0.45211717  0.4774998   0.4825941   0.47189778
  0.4776352   0.40997493  0.46398553  0.44133618  0.49081522  0.56853294
  0.60108316  0.5847721   0.5919862   0.5494547   0.6050139   0.72561854
  0.6834213   0.64499825  0.7283561   0.6915959   0.75006187  0.7118504
  0.6248411   0.77881676  0.8334747   0.7939304   0.77961224  0.79344386
  0.90783715  0.9125972   0.88407505  0.8599998   0.8774829   0.9179834
  1.          0.9666163   0.9246898   0.9626432   0.9398035   0.93004644
  0.92237115  0.8943948   0.74577993  0.7794641   0.65609235  0.78349215
  0.8327639   0.768827    0.6951416   0.8012207   0.7719369   0.87849414
  0.67487013  0.69930506  0.78292096  0.8466294   0.92389435]
before --- [-0.9025061  -0.90233225 -0.9040367  -0.90247536 -0.90326387 -0.9009365
 -0.90291005 -0.90280104 -0.903063   -0.9007339  -0.8994723  -0.9000526
 -0.90001327 -0.90100384 -0.9017043  -0.89978164 -0.8942001  -0.8894645
 -0.89116144 -0.8887988  -0.8871862  -0.88614136 -0.887536   -0.88563365
 -0.8802744  -0.8858447  -0.88747585 -0.8849074  -0.88420194 -0.8840654
 -0.88607645 -0.8859261  -0.87989426 -0.88124996 -0.88160956 -0.88208956
 -0.88476217 -0.8831921  -0.88039356 -0.88134843 -0.88202107 -0.8818406
 -0.87721366 -0.88034135 -0.8806867  -0.87971926 -0.87604874 -0.87563795
 -0.8718629  -0.873928   -0.87374467 -0.8738282  -0.87121344 -0.8712557
 -0.87043935 -0.8717495  -0.87025    -0.8714366  -0.8719868  -0.8709356
 -0.8714789  -0.8706483  -0.86871654 -0.867625   -0.86578    -0.8693981
 -0.8644929  -0.86227137 -0.86750096 -0.8696203  -0.86895746 -0.87192845
 -0.8697095  -0.870894   -0.8707594  -0.871444   -0.87125087 -0.8729454
 -0.8728283  -0.8718304  -0.8707562  -0.8696369  -0.87020314 -0.86995685
 -0.86901915 -0.8686784  -0.869541   -0.86775255 -0.8687276  -0.8693152
 -0.8690543  -0.8695204  -0.8698853  -0.8706635  -0.86961144 -0.8694444
 -0.86944693 -0.86978644 -0.87083405 -0.8716198  -0.87226284 -0.869019
 -0.8685883  -0.8661559  -0.8655676  -0.8618532  -0.86249286 -0.86539793
 -0.86110246 -0.85964817 -0.8540895  -0.8620792  -0.8622106  -0.8596932
 -0.8611931  -0.85846233 -0.8559856  -0.85270435 -0.8549313  -0.8507276
 -0.8519296  -0.84860176 -0.8508871  -0.8482316  -0.8463598  -0.8405368
 -0.8408292  -0.8462205  -0.84473664 -0.8417518  -0.83991915 -0.83750105
 -0.8370106  -0.8371164  -0.83633953 -0.836049   -0.826463   -0.8212127
 -0.822643   -0.8213463  -0.8217728  -0.8095101  -0.81302744 -0.8078603
 -0.80952835 -0.8053795  -0.8035602  -0.8083106  -0.80950505 -0.81032324
 -0.8115221  -0.8133387  -0.8037766  -0.81216    -0.81630754 -0.8231421
 -0.8192094  -0.8194941  -0.82011944 -0.8210056  -0.8242348  -0.8246909
 -0.8267957  -0.82944727 -0.82845646 -0.82923126 -0.83055776 -0.8302853
 -0.8296889  -0.83190244 -0.834398   -0.8323094  -0.8317863  -0.83347225
 -0.83569294 -0.8376867  -0.83921504 -0.83986914 -0.83914906 -0.8385952
 -0.8398295  -0.8397113  -0.83876973 -0.8453339  -0.8449218  -0.84070027
 -0.84081036 -0.84568137 -0.8461346  -0.8453084  -0.84796494 -0.84840935
 -0.8427319  -0.8453064  -0.845002   -0.8456778  -0.84721327 -0.84838927
 -0.8473601  -0.847315   -0.84633005 -0.8466955  -0.84610766 -0.8477689
 -0.85027814 -0.85222536 -0.8506585  -0.85085183 -0.85084385 -0.85083616
 -0.8525     -0.8542031  -0.8543818  -0.8529656  -0.8524342  -0.8513611
 -0.8522642  -0.85131633 -0.8500424  -0.8511505  -0.84529036 -0.84512484
 -0.84509337 -0.84592223 -0.84430057 -0.8433751  -0.84101135 -0.83981335
 -0.83980465 -0.83787614 -0.8358279  -0.8345908  -0.8326814  -0.83266014
 -0.8303627  -0.8298148  -0.8291713  -0.83001    -0.82902473 -0.82897305
 -0.82988083 -0.8278782  -0.82805234 -0.8302255  -0.82926416 -0.8280438
 -0.8282952  -0.82504475 -0.8208308  -0.82074416 -0.82513475 -0.82535297
 -0.8258048  -0.8260268  -0.825575   -0.82225776 -0.8204173  -0.81733626
 -0.81608564 -0.8162509  -0.8173544  -0.81771654 -0.8157582  -0.81076026
 -0.816407   -0.81686103 -0.8175501  -0.81820285 -0.81760466 -0.8163558
 -0.8170761  -0.8160525  -0.81415087 -0.8159403  -0.81762224 -0.81795895
 -0.81455463 -0.8164617  -0.8181206  -0.81885445 -0.8168074  -0.82032627
 -0.8191252  -0.8189255  -0.82072836 -0.8213697  -0.8212922  -0.81801474
 -0.81823146 -0.8166235  -0.81618226 -0.81360525 -0.8101191  -0.81024307
 -0.81031895 -0.8126997  -0.81345123 -0.8122978  -0.80881536 -0.8115477
 -0.8095266  -0.80703175 -0.80942374 -0.80977654 -0.8085848  -0.8082581
 -0.8050226  -0.801073   -0.80377614 -0.805148   -0.80415726 -0.8023625
 -0.8005529  -0.79821587 -0.79830617 -0.79582655 -0.7976629  -0.79813683
 -0.79652935 -0.79353285 -0.7916715  -0.7935326  -0.7942462  -0.79603076
 -0.7950288  -0.7912691  -0.7901637  -0.7886507  -0.7929888  -0.7925443
 -0.793995   -0.79446614 -0.79232824 -0.79060715 -0.79303735 -0.7916046
 -0.79325795 -0.7949378  -0.7929165  -0.79318076 -0.79257286 -0.7912826
 -0.79184574 -0.7910008  -0.7964983  -0.7969069  -0.79340637]
rmse --- 2340.2505
fitness --- 2340.2504139514645
---batch_size---: 4 ---n_steps---: 1 ---network_epoch---: 200
---hidden_dim---: 16 ---dropout---: 0.5207805082202462 ---num_layers---: 2
---loss---: L1Loss ---opt---: Rprop ---learning_rate---: 0.01836109604171406
----FITNESS-----------RMSE---- 2340.2505 ----------
[1/50] Training loss: 0.13950834
[2/50] Training loss: 0.00248325
[3/50] Training loss: 0.00086441
[4/50] Training loss: 0.00074210
[5/50] Training loss: 0.00066931
[6/50] Training loss: 0.00063074
[7/50] Training loss: 0.00060686
[8/50] Training loss: 0.00057254
[9/50] Training loss: 0.00056802
[10/50] Training loss: 0.00055643
[50/50] Training loss: 0.00019492
before --- 